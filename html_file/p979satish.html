<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c78{color:#595959;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.2pt;font-family:"Arial";font-style:normal}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c120{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.4pt;font-family:"Arial";font-style:normal}.c61{margin-left:-18.2pt;padding-top:1.7pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c58{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c117{color:#595959;font-weight:400;text-decoration:none;vertical-align:super;font-size:9.1pt;font-family:"Arial";font-style:normal}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c97{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.3pt;font-family:"Arial";font-style:normal}.c51{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.9pt;font-family:"Arial";font-style:normal}.c64{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:9.5pt;font-family:"Arial";font-style:normal}.c55{color:#595959;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.1pt;font-family:"Arial";font-style:normal}.c45{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.6pt;font-family:"Arial";font-style:normal}.c37{margin-left:-18.2pt;padding-top:3.8pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c53{color:#595959;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Arial";font-style:normal}.c63{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:9pt;font-family:"Arial";font-style:normal}.c116{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.7pt;font-family:"Arial";font-style:normal}.c44{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.8pt;font-family:"Arial";font-style:normal}.c118{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:15.8pt;font-family:"Arial";font-style:normal}.c83{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c27{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c84{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.5pt;font-family:"Arial";font-style:normal}.c93{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.2pt;font-family:"Arial";font-style:normal}.c115{margin-left:-12.2pt;padding-top:13.9pt;text-indent:20.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-1.3pt}.c76{color:#595959;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.5pt;font-family:"Arial";font-style:normal}.c31{color:#595959;font-weight:400;text-decoration:none;vertical-align:super;font-size:9.7pt;font-family:"Arial";font-style:normal}.c25{color:#595959;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.8pt;font-family:"Arial";font-style:normal}.c70{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:9.5pt;font-family:"Arial";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.5pt;font-family:"Arial";font-style:normal}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.1pt;font-family:"Arial";font-style:normal}.c114{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.5pt;font-family:"Arial";font-style:normal}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c136{margin-left:-272.2pt;padding-top:1.4pt;text-indent:281.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:247pt}.c41{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:7.9pt;font-family:"Arial";font-style:normal}.c35{color:#595959;font-weight:700;text-decoration:none;vertical-align:super;font-size:12.1pt;font-family:"Arial";font-style:normal}.c42{color:#595959;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Arial";font-style:normal}.c49{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:normal}.c126{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Arial";font-style:normal}.c66{margin-left:244.8pt;padding-top:1.7pt;text-indent:-235.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-270.1pt}.c12{margin-left:-9.2pt;padding-top:1.4pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.1pt}.c113{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.5pt;font-family:"Arial";font-style:normal}.c82{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.6pt;font-family:"Arial";font-style:normal}.c122{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.6pt;font-family:"Courier New";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Times New Roman";font-style:normal}.c54{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.8pt;font-family:"Arial";font-style:normal}.c125{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Courier New";font-style:normal}.c135{color:#595959;font-weight:700;text-decoration:none;vertical-align:super;font-size:12.2pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.6pt;font-family:"Arial";font-style:normal}.c75{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Arial";font-style:normal}.c80{margin-left:-8.4pt;padding-top:1.4pt;text-indent:13.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:23pt}.c30{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.4pt;font-family:"Arial";font-style:normal}.c90{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.3pt;font-family:"Arial";font-style:normal}.c129{margin-left:-9.2pt;padding-top:5.8pt;text-indent:10.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-15.8pt}.c21{color:#595959;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.9pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c65{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Courier New";font-style:normal}.c48{margin-left:-9.2pt;padding-top:1.4pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.3pt}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:normal}.c144{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.8pt;font-family:"Arial";font-style:normal}.c105{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Arial";font-style:normal}.c11{color:#595959;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.8pt;font-family:"Arial";font-style:normal}.c20{color:#595959;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.3pt;font-family:"Arial";font-style:normal}.c2{margin-left:-9.2pt;padding-top:3.8pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.1pt}.c52{margin-left:-12.5pt;padding-top:1pt;text-indent:59.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:10pt}.c50{color:#595959;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.4pt;font-family:"Arial";font-style:normal}.c134{margin-left:-18.2pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-6.8pt}.c79{margin-left:227.3pt;padding-top:37.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c148{margin-left:5.3pt;padding-top:10.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:16.5pt}.c0{margin-left:-6.6pt;padding-top:9.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:64.1pt}.c139{margin-left:-9.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.8pt}.c46{margin-left:-272.2pt;padding-top:12.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:247pt}.c119{margin-left:13pt;padding-top:2.2pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:149.7pt}.c91{margin-left:-4.8pt;padding-top:12.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:109.4pt}.c19{margin-left:-18.2pt;padding-top:13.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:50.6pt}.c59{margin-left:-18.2pt;padding-top:22.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c94{margin-left:103.4pt;padding-top:11pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:105.6pt}.c106{margin-left:-255.2pt;padding-top:12.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:462pt}.c88{margin-left:-18.2pt;padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:111pt}.c110{margin-left:2.9pt;padding-top:9.4pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:16pt}.c100{margin-left:-10.1pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:22.2pt}.c132{margin-left:63.6pt;padding-top:14.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:45.1pt}.c62{margin-left:-81.9pt;padding-top:60.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:251.5pt}.c146{margin-left:-18.2pt;padding-top:14.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:73.4pt}.c39{margin-left:-18.2pt;padding-top:10.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c96{margin-left:-15.4pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:107pt}.c32{margin-left:9.6pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:11.8pt}.c28{margin-left:-9.2pt;padding-top:346.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:129.1pt}.c69{margin-left:-9.2pt;padding-top:11.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:103.4pt}.c99{margin-left:-4.6pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:23pt}.c143{margin-left:227.3pt;padding-top:680.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c123{margin-left:-251.6pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:462pt}.c112{margin-left:-18.2pt;padding-top:4.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.9pt}.c128{margin-left:-18.2pt;padding-top:11.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:116.1pt}.c38{margin-left:-184.2pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:356.6pt}.c102{margin-left:35.8pt;padding-top:7.2pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-6.8pt}.c60{margin-left:227.3pt;padding-top:37.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c87{margin-left:227.3pt;padding-top:47.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c137{margin-left:-2.2pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:194.8pt}.c150{margin-left:-272.2pt;padding-top:13pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:247.2pt}.c124{margin-left:-18.2pt;padding-top:21.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:99pt}.c68{margin-left:-18.2pt;padding-top:11.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-0.3pt}.c47{padding-top:1.7pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c81{padding-top:1.7pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c107{padding-top:8.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c127{padding-top:4.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c147{padding-top:13.4pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c43{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c57{padding-top:3.8pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c141{padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c103{padding-top:0.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c140{padding-top:3.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c95{padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c108{padding-top:3.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c142{padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c121{padding-top:12pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c131{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c33{margin-left:-8.6pt;text-indent:216.2pt;margin-right:-5.4pt}.c133{margin-left:-203.6pt;margin-right:388.6pt}.c77{margin-left:-137.4pt;margin-right:300pt}.c111{margin-left:-131.6pt;margin-right:257.8pt}.c36{margin-left:-161.4pt;margin-right:348.2pt}.c101{margin-left:-9.2pt;margin-right:-18.5pt}.c85{margin-left:-121pt;margin-right:310.3pt}.c138{margin-left:-18.2pt;margin-right:-6.8pt}.c72{margin-left:-33.2pt;margin-right:98.4pt}.c29{margin-left:-9.2pt;margin-right:-16.1pt}.c92{margin-left:-18.2pt;margin-right:-8pt}.c74{margin-left:-18.2pt;margin-right:-7pt}.c149{margin-left:-18.2pt;margin-right:-7.3pt}.c130{margin-left:-259pt;margin-right:462pt}.c109{margin-left:-18.2pt;margin-right:-14.7pt}.c86{margin-left:-9.2pt;margin-right:-23.5pt}.c145{margin-left:-9.2pt;margin-right:-15.8pt}.c71{margin-left:-9.2pt;margin-right:-23pt}.c89{margin-left:-10.6pt;margin-right:-8.4pt}.c98{margin-left:-83.1pt;margin-right:277.4pt}.c73{margin-left:-6.6pt;margin-right:70.3pt}.c23{margin-left:18.2pt;margin-right:176.8pt}.c56{text-indent:18.1pt}.c67{margin-left:150.9pt}.c104{text-indent:27.4pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c131"><p class="c32"><span class="c144">Navigating the Maze of Graph Analytics Frameworks using Massive Graph Datasets </span></p><p class="c132"><span class="c27">Nadathur </span><span class="c40">Jiwon </span><span class="c27">Satish</span><span class="c40">Seo</span><span class="c22">&dagger;</span><span class="c27">, Narayanan </span><span class="c9">&#8902;</span><span class="c40">, Jongsoo </span><span class="c27">Sundaram</span><span class="c40">Park</span><span class="c9">&dagger;</span><span class="c40">, M. </span><span class="c22">&dagger;</span><span class="c27">, </span><span class="c40">Amber </span><span class="c27">Md. Mostofa </span><span class="c40">Hassaan</span><span class="c27">Ali </span><span class="c9">&Dagger;</span><span class="c40">, </span><span class="c27">Patwary</span><span class="c22">&dagger;</span><span class="c27">, </span><span class="c40">Shubho Sengupta</span><span class="c9">&dagger;</span><span class="c40">, Zhaoming Yin</span><span class="c9">&sect;</span><span class="c40">, and Pradeep Dubey</span><span class="c9">&dagger; </span></p><p class="c91"><span class="c9">&dagger;</span><span class="c58">Parallel Computing Lab, Intel Labs </span></p><p class="c14 c111"><span class="c9">&#8902;</span><span class="c58">Stanford University </span></p><p class="c14 c72"><span class="c9">&Dagger;</span><span class="c58">Dept. of Electrical and Computer Engineering, UT Austin </span></p><p class="c14 c72"><span class="c9">&Dagger;</span><span class="c58">Dept. of Electrical and Computer Engineering, UT Austin </span></p><p class="c14 c67"><span class="c9">&sect;</span><span class="c58">Georgia Tech </span></p><p class="c14 c67"><span class="c9">&sect;</span><span class="c58">Georgia Tech </span></p><p class="c14 c67"><span class="c9">&sect;</span><span class="c58">Georgia Tech </span></p><p class="c59"><span class="c27">ABSTRACT </span><span class="c1">Graph algorithms are becoming increasingly important for analyz- ing large datasets in many fields. Real-world graph data follows a pattern of sparsity, that is not uniform but highly skewed to- wards a few items. Implementing graph traversal, statistics and machine learning algorithms on such data in a scalable manner is quite challenging. As a result, several graph analytics frameworks (GraphLab, CombBLAS, Giraph, SociaLite and Galois among oth- ers) have been developed, each offering a solution with different programming models and targeted at different users. Unfortunately, the &quot;Ninja performance gap&quot; between optimized code and most of these frameworks is very large (2-30X for most frameworks and up to 560X for Giraph) for common graph algorithms, and moreover varies widely with algorithms. This makes the end-users&rsquo; choice of graph framework dependent not only on ease of use but also on performance. In this work, we offer a quantitative roadmap for im- proving the performance of all these frameworks and bridging the &quot;ninja gap&quot;. We first present hand-optimized baselines that get per- formance close to hardware limits and higher than any published performance figure for these graph algorithms. We characterize the performance of both this native implementation as well as popular graph frameworks on a variety of algorithms. This study helps end- users delineate bottlenecks arising from the algorithms themselves vs. programming model abstractions vs. the framework implemen- tations. Further, by analyzing the system-level behavior of these frameworks, we obtain bottlenecks that are agnostic to specific al- gorithms. We recommend changes to alleviate these bottlenecks (and implement some of them) and reduce the performance gap with respect to native code. These changes will enable end-users to choose frameworks based mostly on ease of use. </span></p><p class="c68"><span class="c27">Categories and Subject Descriptors </span><span class="c1">H.2.4 [Information Systems]: Database Management&mdash;Systems </span></p><p class="c128"><span class="c27">1. INTRODUCTION </span></p><p class="c37"><span class="c1">The rise of big data has been predominantly driven by the need to find relationships among large amounts of data. With the in- crease in large scale datasets from social networks [34, 20], web </span></p><p class="c74 c127"><span class="c120">Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGMOD&rsquo;14, June 22&ndash;27, 2014, Snowbird, UT, USA. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2376-5/14/06 ...$15.00. http://dx.doi.org/10.1145/2588555.2610518. </span></p><p class="c6 c29"><span class="c1">pages [14], bioinformatics [18] and recommendation systems [9, 7], large scale graph processing has gone mainstream. There has been a lot of interest in creating, storing and processing large graph data [35, 10]. While large scale graph data management is an im- portant problem to solve, this paper is concerned with large scale graph processing (specifically, in-memory processing). </span></p><p class="c95 c56 c101"><span class="c1">Graph algorithms are typically irregular and difficult to efficiently implement by end-users, often severely under-utilizing system and processor resources &ndash; unlike regular compute dominated applica- tions such as Linpack [5]. It is often possible for the &quot;Ninja per- formance gap&quot; [29] between naively written graph code and well- tuned hand-optimized code to be multiple orders of magnitude. This difficulty has motivated the rise of numerous in-memory frame- works to help improve productivity and performance on graph com- putations such as GraphLab, Giraph, CombBLAS/KDT, SociaLite and Galois [8, 11, 21, 26, 31]. These frameworks are aimed at com- putations varying from classical graph traversals to graph statistics calculations such as triangle counting to complex machine learning tasks like collaborative filtering. Currently, there is no consensus on what the &ldquo;building blocks&rdquo; of graph processing should be. Dif- ferent programming frameworks use different bases such as sparse matrix operations [11, 22], vertex programs from the point of view of a single vertex [8, 21], declarative programming [30] or generic task based parallelization [26]. Further, all of the approaches dif- fer in performance, and different frameworks often perform better on different algorithms. This makes it extremely difficult for an end-user to identify which framework would be a good fit for the problem at hand from both performance and productivity angles. </span></p><p class="c95 c29 c56"><span class="c1">This work proposes to simplify the end-users&rsquo; choice by creat- ing a roadmap to improve the performance of various graph frame- works in order to bridge the performance gap with respect to native code. Once that gap is bridged, we believe that all frameworks will be feasible to use and the choice of framework then boils down to productivity issues. In order to achieve this, it is essential to first have the right reference point that is only limited by the hardware. This comparison is generally missing from previous work, but it is critical to help set reasonable performance expectations. More- over, this study has to be done at large scale (several 10&rsquo;s of nodes) since many bottlenecks of graph algorithms do not show up until this point. Consequently, we perform this study on both real-world graph data (typically fitting on 1-16 nodes) e.g. [9, 20], but also on synthetic data scaling up to 64 nodes. </span></p><p class="c12"><span class="c1">The performance study in this work is intended to help us tease out performance differences between the different frameworks as those due to the fundamental scaling limits in the algorithms, lim- itations of the programming model or inefficiencies in the imple- mentation of the framework. We follow up this comparison with an analysis of system-level metrics such as CPU utilization, mem- </span></p><p class="c87"><span class="c13">979 </span></p><p class="c6"><span class="c1">ory footprint and network characteristics of each framework. This helps identify key strengths and weaknesses of each framework in an algorithm-agnostic way. We use this knowledge to then propose a roadmap for performance improvements of each framework. </span></p><p class="c6"><span class="c1">We wish to emphasize that this work should not be taken as intro- ducing a new graph benchmark for ranking graph frameworks. Our aim is to analyze the performance of existing graph frameworks and propose changes to improve such performance to an extent where all these frameworks are reasonably close to native code. Our con- tributions can be summarized as follows: </span></p><p class="c14"><span class="c1">&bull; Unlike other framework comparisons, we provide native hand- optimized implementations for all the algorithms under con- sideration (Sec. 5.1). This provides a clear and meaningful reference point that shows the hardware bottlenecks of the algorithms themselves, and exposes the gap in other frame- work implementations. Our native implementation is only about 2 &minus; 2.5&times; off the ideal performance; and is faster than other frameworks by 1.1&minus;568&times; on a single node and 1.6 &minus; 494&times; on multiple nodes (Sec. 5.2 &amp; 5.3). </span></p><p class="c14"><span class="c1">&bull; We compare different graph frameworks that vary not just in implementations but in fundamental programming models &ndash; including vertex programming, sparse matrix operations, declarative programming and task-based programming. We show scaling studies on both real-world and synthetic datasets, ranging in scale from a single node to 64 nodes and graphs of sizes up to 16 billion edges. We do not restrict ourselves only to graph traversals, but also include graph statistics and machine learning algorithms such as collaborative filtering in our study. We believe our workload mix is better representa- tive of data analytics tasks performed on large scale graphs. </span></p><p class="c6"><span class="c1">&bull; We analyze CPU utilization, memory consumption, achieved network bandwidth and total network transfers in order to characterize the frameworks and algorithms better (Sec. 5.4). This analysis explains much of the performance gap observed between the frameworks and native code. </span></p><p class="c6"><span class="c1">&bull; We show quantitatively the performance gains from different optimization techniques (use of better communication lay- ers, improved data structures and increased communication efficiency from compression/latency hiding) used in our na- tive code (Sec. 6.1). We provide specific recommendations to bridge the performance gap for all frameworks. We apply some of these ideas to SociaLite and show that it improves performance by about 2&times; for network bound algorithms. </span></p><p class="c14"><span class="c1">We believe that our roadmap will help framework developers op- timize their frameworks. </span></p><p class="c14"><span class="c27">2. CHOICE OF ALGORITHMS </span></p><p class="c6"><span class="c1">We picked 4 different graph algorithms with varying character- istics in terms of functionality (traversal, statistics, machine learn- ing), data per vertex, amount and type of communication, iterative and non-iterative etc. The list is below: </span></p><p class="c6"><span class="c1">1. PageRank This is an algorithm that is used to rank web pages according to their popularity. This algorithm calculates the prob- ability that a random walk would end in a particular vertex of a graph. This application computes the page rank of every vertex in a directed graph iteratively. At every iteration t, each vertex com- putes the following: </span></p><p class="c14"><span class="c1">PR</span><span class="c22">t+1</span><span class="c1">(i) = r + (1 &minus; r) &lowast; </span><span class="c24">X </span></p><p class="c14"><span class="c9">j|(j,i)&isin;E </span></p><p class="c14"><span class="c1">PR</span><span class="c22">t</span><span class="c1">(j) degree(j) </span><span class="c24">(1) </span></p><p class="c6"><span class="c1">where r is the probability of a random jump (we use 0.3), E is the set of directed edges in the graph and PR</span><span class="c22">t</span><span class="c1">(i) denotes the (unnor- malized) page rank of the vertex at iteration t. </span></p><p class="c6"><span class="c1">2. Breadth First Search (BFS) This is a typical graph traversal algorithm. This algorithm performs the breadth first search of an undirected, unweighted graph from a given start vertex and assigns a &ldquo;distance&rdquo; to each vertex. The distance signifies the minimum number of edges that need to be traversed from the starting vertex to a particular vertex. The algorithm is typically implemented iter- atively. One can think of the algorithm as performing the following computation once per vertex per iteration: </span></p><p class="c6"><span class="c1">Distance(i) = min Distance(j)+1 (2) </span><span class="c8">j|(j,i)&isin;E </span><span class="c1">with all the distances initialized to infinity (the starting vertex is set to 0). In practice, this computation is only performed for all the neighbors of a vertex after its distance has been updated. This algorithm is part of the Graph500 benchmark [23]. </span></p><p class="c6"><span class="c1">3. Triangle Counting The count of the number of triangles in a graph is part of measuring graph statistics. A triangle is formed when two vertices are both neighbors of a common third vertex. This algorithm counts the number of such triangles and reports them. The algorithm works as follows: Each vertex shares its neighborhood list with each of its neighbors. Each vertex then checks if any of their neighbors overlap with the neighborhood list(s) they received. With directed edges and no cycles, the to- tal number of such overlaps gives the number of triangles in the graph. With undirected edges, the total number of overlaps gives 3 times the number of triangles. </span></p><p class="c14"><span class="c1">N</span><span class="c8">triangles </span><span class="c3">= </span><span class="c1">X </span></p><p class="c14"><span class="c9">i,j,k,i&lt;j&lt;k </span></p><p class="c14"><span class="c1">E</span><span class="c9">ij </span><span class="c1">&and; E</span><span class="c8">jk </span><span class="c3">&and; E</span><span class="c8">ik </span><span class="c3">(3) </span></p><p class="c14"><span class="c1">where E</span><span class="c9">ij </span><span class="c1">denotes the presence of an (undirected) edge between vertex i and vertex j. </span></p><p class="c6"><span class="c1">4. Collaborative Filtering This is a machine learning algorithm that estimates how a given user would rate an item given an in- complete set of (user, item) ratings. The matrix-centric and graph- centric views of the problem are shown in Figure 1. Given a ratings matrix R, the goal is to find non-negative factors P and Q that are low-dimensional dense matrices. In a graph centric view, R corresponds to edge weights of a bipartite graph and P and Q cor- respond to vertex properties. </span></p><p class="c14"><span class="c17">N N</span><span class="c16">items </span></p><p class="c14"><span class="c17">K </span></p><p class="c14"><span class="c17">pp</span><span class="c41">0 0</span><span class="c44">0 0 q</span><span class="c116">0 0</span><span class="c113">s </span><span class="c4">r</span><span class="c17">Q </span></p><p class="c14"><span class="c10">p</span><span class="c41">1 </span></p><p class="c14"><span class="c17">1 1 </span></p><p class="c14"><span class="c10">q</span><span class="c41">1 </span></p><p class="c14"><span class="c17">R </span></p><p class="c14"><span class="c97">( ) </span></p><p class="c43"><span class="c17">q</span><span class="c41">v </span><span class="c17">N </span><span class="c10">p</span><span class="c41">2 </span></p><p class="c14"><span class="c10">2 2 q</span><span class="c41">2 </span><span class="c97">(u,v) </span><span class="c17">p</span><span class="c41">u </span></p><p class="c14"><span class="c10">p p</span><span class="c41">u </span><span class="c17">u </span><span class="c10">u </span><span class="c44">R</span><span class="c116">uv </span><span class="c17">v </span><span class="c10">v q </span><span class="c17">q</span><span class="c41">v </span></p><p class="c14"><span class="c17">(a) (a) (b) (b) </span></p><p class="c14"><span class="c1">Figure 1: Collaborative filtering (a) Matrix (b) Graph </span></p><p class="c6"><span class="c1">Collaborative filtering is typically accomplished using incom- plete matrix factorization with regularization to avoid overfitting [19]. The problem can be expressed mathematically as follows: </span></p><p class="c14"><span class="c1">min </span><span class="c8">p,q </span></p><p class="c14"><span class="c118">&asymp; </span><span class="c17">P </span><span class="c1">X </span></p><p class="c14"><span class="c24">(R</span><span class="c9">uv </span><span class="c1">&minus; p</span><span class="c22">T</span><span class="c9">u</span><span class="c24">q</span><span class="c9">v</span><span class="c1">)</span><span class="c22">2 </span><span class="c1">+ &lambda;</span><span class="c8">p</span><span class="c3">||p</span><span class="c8">u</span><span class="c3">||</span><span class="c9">2 </span><span class="c3">+ &lambda;</span><span class="c8">q</span><span class="c3">||q</span><span class="c8">v</span><span class="c3">||</span><span class="c9">2 </span><span class="c3">(4) </span><span class="c9">(u,v)&isin;R</span><span class="c13">980 </span></p><p class="c7 c89"><span class="c82">Algorithm Graph type Vertex Edge Message size Vertex property access pattern (Bytes/edge) active? PageRank Directed, unweighted edges Double (pagerank) Streaming Constant (8) All iterations Breadth First Search Undirected, unweighted edges Int (distance) Random Constant (4) Some iterations Collaborative Filtering Bipartite graph; Undirected, weighted edges Array of Doubles (p</span><span class="c64">u </span><span class="c45">or q</span><span class="c64">v</span><span class="c45">) </span><span class="c82">Triangle Counting Directed, unweighted edges Long (# triangles) </span><span class="c45">Streaming Constant (8</span><span class="c122">K</span><span class="c45">) All iterations </span><span class="c82">Streaming Variable (0-10</span><span class="c70">6</span><span class="c82">) Non-iterative </span></p><p class="c94"><span class="c1">Table 1: Diversity in the characteristics of chosen graph algorithms. </span></p><p class="c39"><span class="c1">where is the rating u &amp; v of are the indices u</span><span class="c22">th </span><span class="c1">user over for users the and v</span><span class="c22">th </span><span class="c1">item, items prespectively, </span><span class="c8">u</span><span class="c3">&amp;q</span><span class="c8">v </span><span class="c3">are dense </span><span class="c1">R</span><span class="c9">uv </span></p><p class="c74 c103"><span class="c1">vectors of length K corresponding to each user and item, respec- tively. This matrix factorization is typically done iteratively using Stochastic Gradient Descent (SGD) or Gradient Descent (GD). For SGD, each iteration consists of performing the following operation for all ratings in a random order: </span></p><p class="c102"><span class="c1">e</span><span class="c8">uv </span><span class="c3">= R</span><span class="c8">uv </span><span class="c3">&minus; p</span><span class="c9">T</span><span class="c8">u</span><span class="c1">q</span><span class="c8">v </span><span class="c3">(5) </span><span class="c1">p</span><span class="c22">&lowast;</span><span class="c9">u </span><span class="c24">= p</span><span class="c9">u </span><span class="c1">+ &gamma;</span><span class="c8">t</span><span class="c3">[e</span><span class="c8">uv</span><span class="c3">q</span><span class="c8">v </span><span class="c3">&minus; &lambda;</span><span class="c8">p</span><span class="c3">p</span><span class="c8">u</span><span class="c3">] (6) </span><span class="c1">q</span><span class="c22">&lowast;</span><span class="c9">v </span><span class="c24">= q</span><span class="c9">v </span><span class="c1">+ &gamma;</span><span class="c8">t</span><span class="c3">[e</span><span class="c8">uv</span><span class="c3">p</span><span class="c8">u </span><span class="c3">&minus; &lambda;</span><span class="c8">q</span><span class="c3">q</span><span class="c8">v</span><span class="c3">] (7) </span><span class="c1">(p</span><span class="c8">u</span><span class="c3">,q</span><span class="c8">v</span><span class="c3">)=(p</span><span class="c9">&lowast;</span><span class="c8">u</span><span class="c1">,q</span><span class="c22">&lowast;</span><span class="c9">v</span><span class="c24">) (8) </span></p><p class="c74 c142"><span class="c1">where &gamma;</span><span class="c8">t </span><span class="c3">is the step size for the t</span><span class="c9">th </span><span class="c3">iteration (typically, &gamma;</span><span class="c8">t </span><span class="c3">= &gamma;</span><span class="c8">0</span><span class="c3">s</span><span class="c9">t </span><span class="c1">and s is the step size reduction factor 0 &lt; s &le; 1). GD performs similar operations but updates all the p</span><span class="c9">u </span><span class="c1">and q</span><span class="c9">v </span><span class="c1">once per iteration instead of once per rating. </span></p><p class="c61"><span class="c1">We implement all these algorithms on all the graph frameworks that we use for comparison, except for those available publicly (de- tails in Section 3). </span><span class="c27">2.1 Challenges </span></p><p class="c37"><span class="c1">The chosen graph algorithms vary widely in their characteristics and correspondingly, their implementations stress different aspects of the hardware system. Table 1 shows the characteristics of the different graph algorithms. The message passing characteristics are based on that of a vertex programming implementation. There are differences from the structure and properties of the graph itself, ver- tex properties, access patterns, message sizes, and whether vertices are active in all iterations. </span></p><p class="c81 c74"><span class="c1">The implications of these characteristics are discussed in Section 3 for those algorithms. For example, Triangle counting and Collab- orative filtering have total message sizes that are much larger than that of the graph itself, necessitating modifications for Giraph. </span></p><p class="c61"><span class="c1">We now discuss the graph frameworks considered and how the algorithms map to them. </span></p><p class="c19"><span class="c27">3. CHOICE OF FRAMEWORKS </span></p><p class="c108 c104 c109"><span class="c1">The wide variety in graph algorithms that need to be imple- mented has necessitated the creation of a variety of graph frame- works. There is clearly no consensus on even what programming model gives the best productivity-performance trade-off. In this pa- per, we consider the following popular graph frameworks - GraphLab, CombBLAS, SociaLite, Galois and Giraph. In addition, we also in- clude hand-optimized code for the algorithms. Each of these frame- works are described below. </span></p><p class="c81 c74"><span class="c1">GraphLab [21] is a graph framework that provides a sequential, shared memory abstraction for running graph algorithms written as &ldquo;vertex programs&rdquo;. GraphLab works by letting vertices in a graph read incoming messages, update the values and send mes- sages asynchronously. GraphLab partitions the graph in a 1-D fash- ion (vertex partitioning). All graph algorithms must be expressed as a program running on a vertex, which can access its own value as well as that of its edges and neighboring vertices. The runtime takes care of scheduling, messaging and synchronization. </span></p><p class="c14 c56 c71"><span class="c1">The Combinatorial BLAS [11] is an extensible distributed-memory parallel graph library offering a small but powerful set of linear al- gebra primitives specifically targeting graph analytics. CombBLAS treats graphs as sparse matrices and partitions the non-zeros of the matrix (edges in the graph) across nodes. As such, this is the only framework that supports an edge-based partitioning of the graph (also referred to as 2-D partitioning in the paper). Graph com- putations are expressed as operations among sparse matrices and vectors using arbitrary user-defined semirings. </span></p><p class="c12"><span class="c1">SociaLite [30, 31] is based on Datalog, a declarative language that can express various graph algorithms succinctly due to its bet- ter support for recursive queries compared to SQL [32]. In So- ciaLite, the graph and its meta data is stored in tables, and declara- tive rules are written to implement graph algorithms. SociaLite ta- bles are horizontally partitioned, or sharded, to support parallelism. Users can specify how they want to shard a table at table declaration time, and the runtime partitions and distributes the tables accord- ingly. SociaLite only supports 1-D partitioning. </span></p><p class="c12"><span class="c1">Giraph[8] is an iterative graph processing system that runs on top of Hadoop framework. Computation proceeds as a sequence of iterations, called supersteps in a bulk synchronous (BSP) fashion. Initially, every vertex is active. In each superstep each active ver- tex invokes a Compute method provided by the user. The Compute method: (1) receives messages sent to the vertex in the previous superstep, (2) computes using the messages, and the vertex and outgoing edge values, which may result in modifications to the val- ues, and (3) may send messages to other vertices. The Compute method does not have direct access to the values of other vertices and their outgoing edges. Inter-vertex communication occurs by sending messages. Computation halts if all vertices have voted to halt and there are no messages in flight. Giraph partitions the ver- tices in a 1-D fashion (vertex partitioning). </span></p><p class="c95 c29 c56"><span class="c1">Since graph computations can be very irregular (little locality, varying amount of work per iteration etc.), Galois [26], a frame- work developed for handling irregular computations can also be used for graph processing. Galois is a work-item based paralleliza- tion framework that can handle graph computations (and other ir- regular problems as well). It provides a rich programming model with coordinated and autonomous scheduling, and with and with- out application-defined priorities. Galois provides its own sched- ulers and scalable data structures, but does not impose a particular partitioning scheme which may be edge or vertex based depending on how the computation is expressed in the framework. </span></p><p class="c12"><span class="c1">Other than the explained differences, a major differentiator of the frameworks is the communication layer between different hard- ware nodes. Our native implementation and CombBLAS use MPI, whereas GraphLab and SociaLite use sockets. Giraph uses a net- work I/O library (netty), while Galois does not have a multi node implementation as yet. </span></p><p class="c47 c145"><span class="c1">Table 2 shows a high-level comparison between the different frameworks under consideration in this paper. </span><span class="c27">3.1 Example - PageRank </span></p><p class="c2"><span class="c1">We explain the differences in programming model between the frameworks with a small example and see how Pagerank can be implemented in all the frameworks. </span></p><p class="c79"><span class="c13">981 </span></p><p class="c14"><span class="c114">Framework Programming Multi node Lang- Graph Communication </span></p><p class="c7"><span class="c114">model usage Partitioning layer Native N/A Yes C/C++ N/A MPI GraphLab Vertex Yes C++ 1-D Sockets CombBLAS Sparse matrix Yes C++ 2-D MPI </span></p><p class="c14"><span class="c114">SociaLite Datalog Yes Java 1-D Sockets Galois Task-based No C/C++ N/A N/A Giraph Vertex Yes Java 1-D Netty </span></p><p class="c14"><span class="c1">Table 2: High level comparison of the graph frameworks </span></p><p class="c14"><span class="c65">1 </span></p><p class="c14"><span class="c65">0 3 </span></p><p class="c14"><span class="c65">2 </span></p><p class="c14"><span class="c1">Figure 2: An example directed graph with 4 vertices </span></p><p class="c6"><span class="c1">Let us begin by finding the most optimal way to execute pager- ank. We refer to this hand-optimized version of the algorithm as native implementation. We observe that pagerank computation as given by equation (1) performs one multiply-add operation per edge. Representing the graph in a Compressed-Sparse Row (CSR) for- mat [15], an efficient way of storing sparse matrix (graph) as adja- cency list, allows for the edges to be stored as a single, contiguous array. This allows all the accesses to the edge array to be regu- lar and improves the memory bandwidth utilization through hard- ware prefetching. Since each vertex has to access the pagerank val- ues of all the vertices with incoming edges, we store the incoming edges in CSR format (not outgoing as would generally be the case). For the multi node setup, the graph is partitioned in a 1-D fashion i.e. partitioning the vertices (along with corresponding in-edges) among the nodes so that each node has roughly the same number of edges. Each node calculates the local updates and packages the pagerank values to be sent to the other nodes. These messages are then used to calculate the remote updates. More details on the op- timizations in pagerank are given in Section 6. </span></p><p class="c6"><span class="c1">Let us see how the page rank algorithm maps to vertex program- ming. In this model, we write a program that executes on a single vertex. This program can only access &ldquo;local&rdquo; data i.e. informa- tion about the vertices and edges that are directly connected to a given vertex. An example of vertex program (in pseudocode) is provided in Algorithm 1. The exact semantics of how the mes- sages are packed and received, how much local data can be ac- cessed, global reductions etc. vary across different implementa- tions. GraphLab [21] and Giraph [8] are both examples of this approach to graph processing. </span></p><p class="c14"><span class="c1">Algorithm 1: Vertex program for one iteration of page rank </span></p><p class="c14"><span class="c1">begin</span><span class="c3">PR &larr;&minus; r </span></p><p class="c7"><span class="c1">for msg &isin; incoming messages do PR &larr;&minus; PR + (1 &minus; r) &lowast; msg Send </span><span class="c22">PR </span></p><p class="c14"><span class="c9">degree </span><span class="c24">to all outgoing edges </span></p><p class="c6"><span class="c1">Another distinct approach to processing large graphs is to treat them as sparse matrices - an approach embodied in frameworks like CombBLAS[11]. The computations in a single iteration of PageRank can be expressed in matrix form as follows: </span></p><p class="c14"><span class="c1">p</span><span class="c8">t+1 </span><span class="c3">= r1 + (1 &minus; r)A</span><span class="c9">T </span><span class="c3">&nbsp;&#771;p</span><span class="c8">t </span><span class="c3">(9) </span></p><p class="c6"><span class="c1">0</span><span class="c3">BB@</span><span class="c1">0 0 0 1 0 0 1 1 0 0 1 1 </span></p><p class="c14"><span class="c24">1</span><span class="c1">C</span><span class="c3">CA </span><span class="c1">(for the graph in Figure 2), p</span><span class="c9">t </span><span class="c1">is the 0 0 0 0vector of the vector all the page rank of vertex out-degrees values at and iteration t, 1 is a vector &nbsp;&#771;p</span><span class="c8">t</span><span class="c3">(i) </span><span class="c1">of all </span><span class="c3">= </span><span class="c1">1&rsquo;s. </span></p><p class="c14"><span class="c9">pd(i) </span><span class="c125">t</span><span class="c9">(i) </span></p><p class="c14"><span class="c24">, d is </span></p><p class="c14"><span class="c1">In SociaLite, pagerank computation is expressed as following: </span></p><p class="c14"><span class="c5">R</span><span class="c30">ANK</span><span class="c1">[n](t + 1, </span><span class="c5">$S</span><span class="c30">UM</span><span class="c1">(v)) :- v = r :- </span><span class="c5">I</span><span class="c30">N</span><span class="c5">E</span><span class="c30">DGE</span><span class="c1">[n](s), </span><span class="c5">R</span><span class="c30">ANK</span><span class="c1">[s](t, v</span><span class="c8">0</span><span class="c3">), </span><span class="c49">O</span><span class="c15">UT</span><span class="c49">D</span><span class="c15">EG</span><span class="c3">[s](d),v = </span><span class="c1">(1 &minus; d r)v</span><span class="c8">0 </span></p><p class="c14"><span class="c24">. </span></p><p class="c6"><span class="c1">, where the PageRank value of a vertex n at iteration t + 1 in the rule head (</span><span class="c5">R</span><span class="c30">ANK</span><span class="c1">[n](t + 1, </span><span class="c5">$S</span><span class="c30">UM</span><span class="c1">(v)) is declared as the summation of the constant term in the first rule, and the normalized PageRank values from neighbor vertices at iteration t in the second rule. The incoming edge table, </span><span class="c5">I</span><span class="c30">N</span><span class="c5">E</span><span class="c30">DGE</span><span class="c1">, stores vertices in its first column and neighboring vertices in its second column. </span><span class="c5">I</span><span class="c30">N</span><span class="c5">E</span><span class="c30">DGE </span><span class="c1">is declared as a tail-nested table [30], effectively implementing a CSR format used in the native implementation and CombBLAS. Another version of PageRank is implemented from the perspective of a vertex that dis- tributes its PageRank value to its neighboring vertices, which is expressed as following: </span></p><p class="c14"><span class="c5">R</span><span class="c30">ANK</span><span class="c1">[n](t + 1, </span><span class="c5">$S</span><span class="c30">UM</span><span class="c1">(v)) :- v = r; :- </span><span class="c5">R</span><span class="c30">ANK</span><span class="c1">[s](t, v</span><span class="c8">0</span><span class="c3">), </span><span class="c49">O</span><span class="c15">UT</span><span class="c49">E</span><span class="c15">DGE</span><span class="c3">[s](n), </span><span class="c49">O</span><span class="c15">UT</span><span class="c49">D</span><span class="c15">EG</span><span class="c3">[s](d),v = </span><span class="c1">(1 &minus; d r)v</span><span class="c8">0 </span></p><p class="c14"><span class="c24">. </span></p><p class="c6"><span class="c1">In this implementation, all join operations in the rule body are locally computed, and there is only a single data transfer for the </span><span class="c5">R</span><span class="c30">ANK </span><span class="c1">table update in the rule head. Compared to the previous ver- sion, there is one less data transfer, reducing communication over- heads. In terms of lock overhead, since we cannot determine which shard of </span><span class="c5">R</span><span class="c30">ANK </span><span class="c1">will be updated, locks must be held for every up- date. Hence, the first version is optimized for a single multi-core machine, while the second is optimized for distributed machines. </span></p><p class="c6"><span class="c1">The pagerank implementation in Galois is very similar to that of GraphLab or Giraph i.e. the parallelization is over vertices. Each work item in Galois is a vertex program for updating its pagerank. Since Galois runs on only a single node with a shared memory abstraction, each task has access to all of the program&rsquo;s data. </span></p><p class="c14"><span class="c27">3.2 Mapping Algorithms to Frameworks </span></p><p class="c6"><span class="c1">As mentioned earlier, the pagerank algorithm can be expressed in a variety of frameworks. In a similar fashion, we also describe the implementations of the other algorithms here: </span></p><p class="c6"><span class="c1">Breadth First Search: Vertex programs are straight-forward to write for this algorithm. Algorithm 2 shows the pseudocode of BFS implementation. All distances are initialized to infinity (except the starting vertex, which is initialized to 0). The iterations continue until there are no updates to distances. </span></p><p class="c14"><span class="c1">Algorithm 2: Vertex program for one iteration of BFS. </span></p><p class="c14"><span class="c1">begin</span><span class="c3">for msg &isin; incoming messages do </span></p><p class="c14"><span class="c1">Distance &larr;&minus; min(Distance, msg + 1) Send Distance to all outgoing edges </span></p><p class="c6"><span class="c1">CombBLAS implementation performs matrix-vector multiplica- tion in every iteration. For example, in order to do traverse the graph from both vertices 0 and 1 in Figure 2, we only have to do </span></p><p class="c14"><span class="c13">982 </span></p><p class="c14"><span class="c1">where A = </span></p><p class="c14"><span class="c1">Triangle counting in SociaLite is a three-way join operation: </span></p><p class="c14"><span class="c5">T</span><span class="c30">RIANGLE</span><span class="c1">(0, </span><span class="c5">$I</span><span class="c30">NC</span><span class="c1">(1)) : &minus;</span><span class="c5">E</span><span class="c30">DGE</span><span class="c1">(x, y), </span><span class="c5">E</span><span class="c30">DGE</span><span class="c1">(y, z), </span><span class="c5">E</span><span class="c30">DGE</span><span class="c1">(x, z). </span></p><p class="c6"><span class="c1">Triangle counting is implemented in Galois (Algorithm 4) by computing set-intersection of neighbors of a node with neighbors of neighbors. We sort the adjacency list of each node by node-id, which allows computing set-intersections in linear time. </span></p><p class="c14"><span class="c1">Algorithm 4: Galois program for Triangle counting. </span></p><p class="c14"><span class="c1">begin</span><span class="c3">Graph G </span></p><p class="c14"><span class="c1">numTriangles = 0 foreach (Node n: G) in parallel do </span></p><p class="c14"><span class="c1">S</span><span class="c9">1 </span><span class="c1">= { m in G.neighbors(n) | m &gt; n } for (m in S1) do </span></p><p class="c14"><span class="c1">S</span><span class="c9">2 </span><span class="c1">= { p in G.neighbors(m) | p &gt; m } numTriangles &larr;&minus; numTriangles + |S</span><span class="c8">1 </span><span class="c3">&cap; S</span><span class="c8">2</span><span class="c3">| </span></p><p class="c6"><span class="c1">Collaborative Filtering: Native implementation follows the Sto- chastic Gradient Descent (SGD) parallelization described in [16]. We process edges in a random order and update data corresponding to both vertices (p and q in equations (5), (6), and (7)). Partition- ing is done so that all updates are local within a single iteration and data sharing happens between iterations. For n processors, the ratings matrix is divided into n</span><span class="c22">2 </span><span class="c1">2-D chunks. Each iteration in- volves n sub-steps where a subset of the updates (on n chunks) are applied [16]. SGD is however, hard if not impossible to imple- ment in most of the other frameworks because of the need for data constancy within iterations. Any data written may not be globally visible right away (especially if the writes are to a remote vertex). Hence, we fall back to Gradient Descent, which does away with processing edges in a random order and requires only an aggregate update from all of the edges. In short, instead of updating edges one at a time, we perform the following computation: </span></p><p class="c14"><span class="c1">p</span><span class="c22">&lowast;</span><span class="c9">u </span><span class="c24">= p</span><span class="c9">u </span><span class="c1">+ &gamma;</span><span class="c8">t </span><span class="c1">the following operation </span></p><p class="c14"><span class="c1">v = A</span><span class="c22">T </span><span class="c1">s = </span></p><p class="c6"><span class="c1">0</span><span class="c3">BB@</span><span class="c1">0</span><span class="c3">12</span><span class="c1">1</span><span class="c3">CCA </span><span class="c1">(10) </span><span class="c3">1</span><span class="c1">where s is the vector of starting vertices. The positions of non-zeros in v indicates the next set of vertices to be explored. </span></p><p class="c14"><span class="c1">Native implementation follows the approach explained in [28]. Optimizations performed are described in Section 6. </span></p><p class="c14"><span class="c1">SociaLite&rsquo;s BFS is implemented as a recursive rule as following: </span></p><p class="c14"><span class="c5">BFS</span><span class="c1">(t, </span><span class="c5">$M</span><span class="c30">IN</span><span class="c1">(d)) :- t = SRC,d = 0 </span></p><p class="c14"><span class="c1">:- </span><span class="c5">BFS</span><span class="c1">(s, d</span><span class="c9">0</span><span class="c1">), </span><span class="c5">E</span><span class="c30">DGE</span><span class="c1">(s, t),d = d</span><span class="c9">0 </span><span class="c1">+ 1. </span></p><p class="c6"><span class="c1">The first rule handles the source node, and the second rule recur- sively follows the neighboring vertices. The details of how this rule is evaluated is explained in [31]. </span></p><p class="c6"><span class="c1">A pseudocode of our BFS implementation in Galois is given in Algorithm 3. We used the bulk-synchronous parallel executor pro- vided by Galois, which maintains the work lists for each level be- hind the scenes, and processes each level in parallel. </span></p><p class="c14"><span class="c1">Algorithm 3: Galois program for Breadth First Search. </span></p><p class="c14"><span class="c1">begin</span><span class="c3">Graph G </span></p><p class="c14"><span class="c1">src.level = 0 worklist[0] = src i &larr;&minus; 0 while NOT worklist[i].empty() do </span></p><p class="c14"><span class="c1">foreach ( n : worklist[i] ) in parallel do </span></p><p class="c14"><span class="c1">for dst: G.neighbors( n ) do </span></p><p class="c14"><span class="c1">if dst.level == &infin; then </span></p><p class="c14"><span class="c1">dst.level &larr;&minus; n.level + 1 worklist[i+1].add(dst) </span></p><p class="c14"><span class="c1">i &larr;&minus; i+1 </span></p><p class="c14"><span class="c1">X </span></p><p class="c14"><span class="c24">[R</span><span class="c9">uv</span><span class="c1">q</span><span class="c8">v </span><span class="c3">&minus; (p</span><span class="c9">T</span><span class="c8">u</span><span class="c1">q</span><span class="c8">v</span><span class="c3">)q</span><span class="c8">v </span><span class="c3">&minus; &lambda;</span><span class="c8">p</span><span class="c3">p</span><span class="c8">u</span><span class="c3">] (11) </span><span class="c9">v|(u,v)&isin;E</span><span class="c1">Triangle counting: In the native implementation, we calculate </span></p><p class="c14"><span class="c1">q</span><span class="c22">&lowast;</span><span class="c9">v </span><span class="c24">= q</span><span class="c9">v </span><span class="c1">+ &gamma;</span><span class="c8">t </span><span class="c1">the neighborhood set of every vertex and send the set to all its neighbors. Then, every vertex computes the intersection of the re- ceived sets with their set of neighbors. The sum of all such inter- sections gives the number of triangles in the graph. Optimizations are possible depending on the data structure used to hold this neigh- borhood set (details in Section 6). </span></p><p class="c7"><span class="c1">The implementation is logically straight-forward with vertex pro- gramming. GraphLab follows a similar template to that of native code. ning where on Giraph, d(i) large is graphs on the the degree as other the of total hand, vertex size has of i. a all memory To messages reduce problem the is O(total when </span><span class="c24">P</span><span class="c9">V</span><span class="c8">i=1 </span><span class="c1">size run- d(i)of </span></p><p class="c14"><span class="c22">2</span><span class="c1">) </span></p><p class="c14"><span class="c1">messages, the neighborhood set distribution is done in phases (Sec- tion 6.1.3). </span></p><p class="c6"><span class="c1">CombBLAS implements triangle counting as the count of the non-zeros that are present in common positions in both A and A</span><span class="c22">2</span><span class="c1">. A</span><span class="c22">2 </span><span class="c1">gives the number of distinct paths of length 2 from vertex i to vertex j. A triangle exists if there is both an edge and a path of length 2 between two vertices. For the example in Figure 2, </span></p><p class="c14"><span class="c1">A</span><span class="c22">2 </span><span class="c1">= </span></p><p class="c7"><span class="c1">0</span><span class="c3">BB@</span><span class="c1">0 1 1 0 0 0 0 1 0 0 0 0 </span><span class="c24">1</span><span class="c1">C</span><span class="c3">CA</span><span class="c1">0 1 1 00</span><span class="c3">BB@</span><span class="c1">1</span><span class="c3">10</span><span class="c1">1</span><span class="c3">CCA </span><span class="c1">= </span></p><p class="c14"><span class="c3">0</span><span class="c1">X </span></p><p class="c14"><span class="c9">u|(u,v)&isin;E</span><span class="c24">[R</span><span class="c9">uv</span><span class="c1">p</span><span class="c8">u </span><span class="c3">&minus; (p</span><span class="c9">T</span><span class="c8">u</span><span class="c1">q</span><span class="c8">v</span><span class="c3">)p</span><span class="c8">u </span><span class="c3">&minus; &lambda;</span><span class="c8">q</span><span class="c3">q</span><span class="c8">v</span><span class="c3">] (12) </span></p><p class="c6"><span class="c1">In CombBLAS, a single GD iteration consists of K matrix-vector multiplications where K is the size of the hidden dimension (length of p or q). The matrix-vector multiplications arise from the terms R</span><span class="c9">uv</span><span class="c1">p</span><span class="c9">u </span><span class="c1">and R</span><span class="c9">uv</span><span class="c1">q</span><span class="c9">v</span><span class="c1">. Other operations can be written as data paral- lel operations on dense vectors. Since CombBLAS does not allow matrices with dimension &lt; number of processors, multiplication with the p matrix has to be performed in K steps instead of as a single sparse-matrix-dense-matrix multiplication. </span></p><p class="c6"><span class="c1">With vertex programming, GD involves aggregating information from all neighbors and sending the updated vector at the end of the iteration. The total size of all the messages sent in a single iteration is O(KE) which is quite large for Giraph. Hence, message passing happens in phases so that only 1/s vertices have to send messages in a given superstep. Thus, s supersteps correspond to a single gradient descent iteration. </span></p><p class="c14"><span class="c1">SociaLite stores the length-K vectors for users and items in sep- 0</span><span class="c3">BB@</span><span class="c1">0 0 0 0 0 0 1 0 0 2 1 0 </span></p><p class="c14"><span class="c24">1</span><span class="c1">C</span><span class="c3">CA</span><span class="c1">, nnz(A &cap; A</span><span class="c22">2</span><span class="c1">) = 2. </span></p><p class="c7"><span class="c1">arate tables. These tables are joined together with the rating table to compute errors and to update user and item vectors. Since this join operation incurs large communication, it is helpful to transfer the tables to target machines in the beginning of each iteration, so that 0 0 0 0the rest of the computations do not involve any communication. </span></p><p class="c14"><span class="c13">983 </span></p><p class="c6 c74 c104"><span class="c1">Galois is the only framework that implements SGD (not just GD) in a fashion similar to that of the native implementation. This is possible because of 2 reasons (1) since partitioning is flexible with Galois, we can apply the n</span><span class="c22">2 </span><span class="c1">uniform 2D chunk partitioning (2) since Galois is running only on a single node, it can maintain glob- ally consistent state after any update without much performance degradation. Each work-item in Galois performs the SGD update on a single edge (u, v) i.e. it updates both p</span><span class="c8">u </span><span class="c3">and q</span><span class="c8">v</span><span class="c3">. </span></p><p class="c103 c74 c104"><span class="c1">We note that SGD has much better convergence characteristics than Gradient Descent. For the Netflix dataset, given a fixed con- vergence criterion, SGD converges in about 40x fewer iterations than GD. Of course, like all machine learning algorithms, both SGD and GD have parameters to choose that affect convergence (learning rate, step size reduction and so forth). We did do a coarse sweep over these parameters to obtain best convergence. </span></p><p class="c146"><span class="c27">4. EXPERIMENTAL SETUP </span></p><p class="c108 c104 c138"><span class="c1">We now describe our experimental setup with details about our frameworks, data sets used and our experimental platform. </span></p><p class="c88"><span class="c27">4.1 Choice of Datasets </span></p><p class="c37"><span class="c1">We use a mix of both real-world datasets and data generators for synthetic data in our experiments. Both classes of datasets ide- ally follow a power law (Zipf law) distribution. For the pagerank and triangle counting problems we use directed graphs, whereas for BFS and Collaborating Filtering we use undirected graphs. Collab- orative Filtering requires edge-weighted, bipartite graphs that rep- resents the ratings matrix. </span></p><p class="c96"><span class="c54">4.1.1 Real world Datasets </span></p><p class="c115"><span class="c4">Dataset # Vertices # Edges Brief Description Facebook [34] 2,937,612 41,919,708 Facebook user interaction graph Wikipedia [14] 3,566,908 84,751,827 Wikipedia Link graph LiveJournal [14] 4,847,571 85,702,475 LiveJournal follower graph </span></p><p class="c99"><span class="c4">Netflix [9] 480,189 users 99,072,112 Netflix Prize </span></p><p class="c52"><span class="c4">17,770 movies ratings Twitter [20] 61,578,415 1,468,365,182 Twitter follower graph Yahoo Music [7] 1,000,990 users 252,800,275 Yahoo! KDDCup 2011 </span></p><p class="c100"><span class="c4">624,961 items ratings music ratings Synthetic 536,870,912 8,589,926,431 Described in Graph500 [23] Section 4 </span></p><p class="c80"><span class="c4">Synthetic 63,367,472 users 16,742,847,256 Described in Collaborative 1,342,176 items ratings Section 4 </span></p><p class="c137"><span class="c4">Filtering </span></p><p class="c148"><span class="c1">Table 3: Real World and largest synthetic datasets </span></p><p class="c74 c104 c107"><span class="c1">Table 3 provides details on the real-world datasets we used. The Facebook, Wikipedia, Livejournal and Twitter graphs were used to run Pagerank, BFS and Triangle Counting, while the Netflix and Yahoo Music datasets were used to run Collaborative Filtering. </span></p><p class="c74 c104 c141"><span class="c1">The major issue with the real world datasets is primarily their small size. While these datasets are useful for algorithmic ex- ploration, they are not really scalable to multiple nodes. In par- ticular, the Facebook, Wikipedia, Livejournal and Netflix datasets were small enough to fit on a single machine. We do have two large real-world datasets - Twitter and Yahoo music. These were run on 4 nodes except for Triangle Counting on Twitter, which re- quired 16 nodes to complete in a reasonable amount of time. Even though the twitter graph itself can fit in fewer nodes, as mentioned in Section 3, the total message size is much larger than the graph and needs more resources. To provide a sense of the size of these datasets, the largest dataset, Twitter, is just over 30 GB in size. Although we do have a few large examples, developing data gen- erators that can produce synthetic data that is representative of real world workloads is essential to study scalability. </span></p><p class="c14 c73"><span class="c54">4.1.2 Synthetic Data Generation </span></p><p class="c2"><span class="c1">We derive our synthetic data generators from Graph500 RMAT data generators [23]. We generate data to scale the graphs up to 64 nodes. To give a sense on size of the synthetic graphs, a large scale 64 node run for Pagerank and BFS processes over 8 Billion edges. We now describe how we use the Graph500 generator to generate both directed graphs and ratings matrices for our applications. </span></p><p class="c29 c56 c121"><span class="c1">We use the default RMAT parameters (A = 0.57, B = C = 0.19) used for Graph500 to generate undirected graphs. The RMAT gen- erator only generates a list of edges (with possible duplicates). For Pagerank, we assign a direction to all the edges generated. For BFS, depending on the framework used we either provide it undirected edges or provide 2 edges in both directions for each generated edge. For triangle counting, we use slightly different RMAT parameters (A = 0.45, B = C = 0.15) to reduce the number of triangles in the graph. We then assign a direction to edges going from the vertex with smaller id to one with larger id to avoid cycles. This is done to make the implementations efficient on all frameworks. </span></p><p class="c95 c29 c56"><span class="c1">In order to generate large rating matrices, we wrote a rating ma- trix generator that follows the power-law distribution of the Netflix data set. In order to achieve this, we start with graphs generated by the Graph500 graph generator. Through experimentation, we found that RMAT parameters of A = 0.40 and B = C = 0.22 generates degree distributions whose tail is reasonably close to that of the Netflix dataset. Finally, we post-processed the graphs to remove all vertices with degree &lt; 5 from the graph. This yielded graphs that closely follow the degree distributions of the Netflix data set. In order to convert these Graph500 graphs into bipartite graphs of N</span><span class="c9">users</span><span class="c1">&times;N</span><span class="c9">movies </span><span class="c1">(items are named movies in case of Netflix data), we first chunk the columns of the Graph500 matrix into chunks of size N</span><span class="c8">movies</span><span class="c3">. We then &ldquo;fold&rdquo; the matrix by performing a logical </span><span class="c1">&ldquo;or&rdquo; of these chunks. </span></p><p class="c12"><span class="c1">Given that the power law distribution is what differentiates a real-world dataset, we believe our data generator is much more rep- resentative of real collaborative filtering workloads as opposed to other data generators such as that by [16]. [16] generates data by sampling uniformly matching the expected number of non-zeros overall but not as a power law distribution. </span></p><p class="c29 c47"><span class="c1">For our experiments, we generate synthetic data of up to 16 Bil- lion ratings for 64 million users &times; 800K movies. For scaling, we generate about 256 million ratings per node, or 16 Billion ratings for 64 nodes. </span><span class="c27">4.2 Framework versions </span></p><p class="c2"><span class="c1">We used GraphLab v2.2 [3] for this comparison. We used the CombBLAS v1.3 code from [1]. We obtained the SociaLite code with additional optimizations over the results in [30] from the au- thors. For Giraph, we used release 1.1.0 from [8]. Finally, we obtained Galois v 2.2.0 from [2]. </span></p><p class="c56 c139"><span class="c1">We implemented all the algorithms on these frameworks wher- ever a public version did not exist. </span><span class="c27">4.3 Experimental Platform </span></p><p class="c29 c56 c140"><span class="c1">We run the benchmarks</span><span class="c22">1 </span><span class="c1">on an Intel</span><span class="c22">&reg; </span><span class="c1">Xeon</span><span class="c22">&reg;2 </span><span class="c1">CPU E5-2697 based system. Each node has 64 GB of DRAM, and has 24 cores supporting 2-way Simultaneous Multi-Threading each running at 2.7 GHz. The nodes are connected with an Mellanox Infiniband </span></p><p class="c129"><span class="c9">1</span><span class="c83">Software and workloads used in performance tests may have been optimized for performance only on Intel micro- </span><span class="c34">processors. Performance tests, such as SYSmark and MobileMark, are measured using specific computer systems, components, software, operations and functions. Any change to any of those factors may cause the results to vary. You should consult other information and performance tests to assist you in fully evaluating your contemplated pur- chases, including the performance of that product when combined with other products. For more information go to http://www.intel.com/performance </span><span class="c8">2</span><span class="c83">Intel, Xeon, and Intel Xeon Phi are trademarks of Intel Corporation in the U.S. and/or other countries. </span></p><p class="c79"><span class="c13">984 </span></p><p class="c14"><span class="c76">0.1 </span></p><p class="c14"><span class="c1">(a) PageRank </span><span class="c117">1 </span><span class="c76">0.01 </span></p><p class="c14"><span class="c76">0.1</span><span class="c1">(d) Triangle counting </span></p><p class="c14"><span class="c1">Figure 3: </span><span class="c5">Performance results for different algorithms on real-world and synthetic graphs that are small enough to run on a single node. The y-axis represents runtime (in log-scale), therefore lower numbers are better. </span></p><p class="c6"><span class="c1">FDR interconnect. The cluster runs on the Red Hat Enterprise Linux Server OS release 6.4. We use a mix of OpenMP directives to parallelize within the node and MPI code to parallelize across nodes in native code. We use the Intel</span><span class="c90">&reg; </span><span class="c1">C++ Composer XE 2013 SP1 Compiler</span><span class="c22">3 </span><span class="c1">and the Intel</span><span class="c90">&reg; </span><span class="c1">MPI library to compile the code. GraphLab uses a similar combination of OpenMP and MPI for best performance. CombBLAS runs best as a pure MPI program. We use multiple MPI processes per node to take advantage of the mul- tiple cores within the node. Moreover, CombBLAS requires the total number of processes to be a square (due to their 2D partition- ing approach). Hence we use 36 MPI processes per node to run on the 48 hardware threads; and we further run on a square number of nodes (1, 2, 4, 9, 16, 36 and 64 nodes). SociaLite uses Java threads and processes for parallelism. Giraph uses the Hadoop framework for parallelism (we run 4 workers per node). Finally, Galois is a single node framework and uses OpenMP for parallelism. </span></p><p class="c14"><span class="c27">5. RESULTS </span></p><p class="c14"><span class="c27">5.1 Native implementation bottlenecks </span></p><p class="c6"><span class="c1">Since different graph frameworks have their programming mod- els with different implementation trade-offs, it is hard to directly compare these frameworks with respect to each other without a clear reference point. As explained in Section 1, we provide a well- optimized native implementation of these algorithms for both sin- gle and multi node systems. Since the native code is optimized, it is easy to see which aspects of the system are stressed by a partic- ular algorithm. Table 4 provides data on the achieved limits of the native implementations on a single node and 4 nodes system. </span></p><p class="c14"><span class="c93">Algorithm Single Node 4 Nodes </span></p><p class="c14"><span class="c93">H/W limitation Efficiency H/W limitation Efficiency PageRank Memory BW 78 GBps (92%) Network BW 2.3 GBps ( 42%) </span></p><p class="c7"><span class="c93">BFS Memory BW 64 GBps (74%) Memory BW 54 GBps (63%) Coll. Filtering Memory BW 47 GBps (54%) Memory BW 35 GBps (41%) Triangle Count. Memory BW 45 GBps (52%) Network BW 2.2 GBps ( 40%) </span></p><p class="c14"><span class="c1">Table 4: </span><span class="c5">Efficiency achieved by native implementations of different algorithms on single and 4 nodes. </span></p><p class="c6"><span class="c1">We find that on both single and multi node implementations, the algorithm performance is dependent on either memory or network bandwidth. The efficiencies are generally within 2-2.5X off the </span></p><p class="c6"><span class="c9">3</span><span class="c83">Intel&rsquo;s compilers may or may not optimize to the same degree for non-Intel microprocessors for optimizations that are </span><span class="c34">not unique to Intel microprocessors. These optimizations include SSE2, SSE3, and SSE3 instruction sets and other opti- mizations. Intel does not guarantee the availability, functionality, or effectiveness of any optimization on microprocessors not manufactured by Intel. Microprocessor-dependent optimizations in this product are intended for use with Intel micro- processors. Certain optimizations not specific to Intel micro-architecture are reserved for Intel microprocessors. Please refer to the applicable product User and Reference Guides for more information regarding the specific instruction sets covered by this notice. Notice revision #20110804 </span></p><p class="c6"><span class="c1">ideal results. Given the diversity of bottlenecks within the 4 al- gorithms, and also between single and multiple node implementa- tions, we expect that it would be difficult for any one framework to excel at all scales in terms performance and productivity. </span></p><p class="c14"><span class="c27">5.2 Single node results </span></p><p class="c6"><span class="c1">We now show the results of running the Pagerank, BFS, Collab- orative Filtering and Triangle Counting algorithms on the Comb- BLAS, GraphLab, SociaLite, Giraph and Galois frameworks. </span></p><p class="c6"><span class="c1">In the following, we only compare time taken per iteration on various frameworks for Collaborative Filtering and Pagerank. As described in Section 3, the native code for Collaborative Filtering implements Stochastic Gradient Descent which converges much faster than the Gradient Descent implementation in other frame- works. However, we do not see much difference in performance per iteration between Stochastic Gradient Descent and Gradient Descent in native code. Hence we compare time/iteration here to separate out the fact that the frameworks are not expressive enough to express SGD from other potential performance differences. Sim- ilarly, some Pagerank implementations differ in whether early con- vergence is detected for the algorithm, and hence we report time per iteration to normalize for this. </span></p><p class="c7"><span class="c105">Algorithm CombBLAS GraphLab SociaLite Giraph Galois PageRank 1.9 3.6 2.0 39.0 1.2 BFS 2.5 9.3 7.3 567.8 1.1 Coll. Filtering 3.5 5.1 5.8 54.4 1.1 Triangle Count. 33.9 3.2 4.7 484.3 2.5 </span></p><p class="c6"><span class="c1">Table 5: </span><span class="c5">Summary of performance differences for single node bench- marks on different frameworks for our applications. Each entry is a slowdown factor from native code, hence lower numbers indicate bet- ter performance. </span></p><p class="c6"><span class="c1">Figures 3(a), 3(b) and 3(d) show the results of running Pager- ank, BFS, and Triangle Counting respectively on the real-world Livejournal, Facebook and Wikipedia datasets described in Sec- tion 4.1 on our frameworks. We also show the results of running a synthetic scale-free RMAT graph (obtained using the Graph500 data generator). Figure 3(c) shows the performance of our frame- works on the Netflix [9] dataset, as well as synthetically generated collaborative filtering dataset. For convenience, we also present the geometric mean of this data across datasets in Table 5. This table shows the slowdowns of each framework w.r.t. native code. </span></p><p class="c6"><span class="c1">We see the following key inferences: (1) Native code, as ex- pected, delivers best performance as it is optimized for the underly- ing architecture. (2) Galois performs better than other frameworks, </span></p><p class="c14"><span class="c13">985 </span></p><p class="c14"><span class="c21">Native </span></p><p class="c14"><span class="c21">Socialite </span></p><p class="c14"><span class="c76">0.1</span><span class="c117">110 </span></p><p class="c14"><span class="c21">Combblas </span></p><p class="c14"><span class="c21">Giraph </span></p><p class="c14"><span class="c21">Graphlab </span></p><p class="c14"><span class="c21">Galois </span></p><p class="c14"><span class="c76">100 </span></p><p class="c14"><span class="c76">10 </span></p><p class="c14"><span class="c51">Native </span></p><p class="c14"><span class="c51">Socialite </span></p><p class="c14"><span class="c76">0</span><span class="c117">1 </span><span class="c1">(b) Breadth-First Search </span></p><p class="c14"><span class="c51">Combblas </span></p><p class="c14"><span class="c51">Giraph </span></p><p class="c14"><span class="c51">Graphlab </span></p><p class="c14"><span class="c51">Galois </span></p><p class="c14"><span class="c51">Native </span></p><p class="c14"><span class="c51">Socialite </span></p><p class="c14"><span class="c11">1</span><span class="c31">10 </span></p><p class="c14"><span class="c1">(c) Collaborative Filtering </span></p><p class="c14"><span class="c11">1000 </span></p><p class="c14"><span class="c11">100 </span></p><p class="c14"><span class="c11">10 </span></p><p class="c14"><span class="c51">Combblas </span></p><p class="c14"><span class="c51">Giraph </span></p><p class="c14"><span class="c51">Graphlab </span></p><p class="c14"><span class="c51">Galois </span></p><p class="c14"><span class="c76">10000 </span></p><p class="c14"><span class="c76">1000 </span></p><p class="c14"><span class="c76">100 </span></p><p class="c14"><span class="c25">Native </span></p><p class="c14"><span class="c25">Socialite </span></p><p class="c14"><span class="c76">10 </span></p><p class="c14"><span class="c25">Combblas </span></p><p class="c14"><span class="c25">Giraph </span></p><p class="c14"><span class="c25">Graphlab </span></p><p class="c14"><span class="c25">Galois </span></p><p class="c6"><span class="c1">and is close to native performance (geometric mean of 1.1-1.2X for pagerank, BFS and collaborative filtering, and 2.5X for triangle counting). (3) Giraph, on the other hand, is 2-3 orders of magnitude slower than native code (4) CombBLAS and GraphLab perform well on average. CombBLAS is very good for all algorithms except for Triangle Counting, where it ran out of memory for real-world inputs while computing the A</span><span class="c22">2 </span><span class="c1">matrix product. This is an express- ibility problem in CombBLAS. GraphLab is 3-9X off from native code, but performs reasonably consistently across algorithms. (5) SociaLite performance is typically comparable to GraphLab (some- times slightly better and sometimes slightly worse). </span></p><p class="c14"><span class="c1">Finally, note that the trends on the synthetic dataset are in line with real-world data, showing that our synthetic generators are ef- fective in modeling real-world data. </span><span class="c27">5.3 Multi node results </span></p><p class="c6"><span class="c1">We first show our scaling results of our frameworks on multiple nodes. A major reason for using multiple nodes to process graph data is to store the data in memory across the nodes. Hence a com- mon use case is weak-scaling, where the data per node is kept con- stant (and hence total data set size increases with number of nodes). If we obtain perfect performance scaling, then the runtime should be constant as we increase node count and data set size. In this study, we include CombBLAS, GraphLab, SociaLite and Giraph frameworks. Galois is currently only a single node framework and we hence do not include results here. </span></p><p class="c6"><span class="c1">Figures 4(a), 4(b), 4(c) and 4(d) show the results of multi node runs on synthetically generated data sets for our benchmarks. The data sizes are chosen so that all frameworks could complete without running out of memory. Figure 5 shows the corresponding perfor- mance results for larger real-world graphs. We run each algorithm using one large dataset &ndash; we use the Twitter dataset [20] for Pager- ank, BFS and Triangle Counting and the Yahoo Music KDDCup dataset 2011 dataset for Collaborative Filtering [7]. </span></p><p class="c14"><span class="c84">Algorithm CombBLAS GraphLab SociaLite Giraph PageRank 2.5 12.1 7.9 74.4 </span></p><p class="c7"><span class="c84">BFS 7.1 29.5 18.9 494.3 Coll. Filtering 3.5 7.1 7.0 87.9 Triangle Count. 13.1 3.6 1.5 54.4 </span></p><p class="c6"><span class="c1">Table 6: </span><span class="c5">Summary of performance differences for multi node bench- marks on different frameworks. Each entry is a slowdown factor from native code, hence lower numbers indicate better performance. </span></p><p class="c14"><span class="c1">As a convenient summary of performance, Table 6 shows the geometric mean of the performance differences between our frame- works combining real-world and synthetic datasets at different scales. The table shows performance slowdowns of different frameworks for specific algorithms compared to the native code for that algo- rithm &ndash; hence lower numbers are better. </span></p><p class="c6"><span class="c1">We note the following trends in our multi-node results. (1) There is wide variability in our multi node results; as an example, na- tive code performs anywhere between 2X to more than 560X better than other frameworks on multi node runs (still up to 30X discount- ing Giraph runtimes). (2) Giraph performs worse by far than other frameworks and is frequently 2-3 orders magnitude off from na- tive performance. (3) CombBLAS is competitive for Pagerank (ge- omean of 2.5X native performance), BFS (7.1X off native) and Col- laborative Filtering (3.5X off native). However, it performs poorly on Triangle Counting due to extra computations performed as a re- sult of framework expressibility issues. CombBLAS also runs out of memory for the Twitter data set and hence this data point is not plotted. (4) GraphLab performs well for Triangle Counting, due to data structure optimizations performed for this case, namely the </span></p><p class="c14"><span class="c20">Pagerank (Weak scaling, 128M edges/node) </span></p><p class="c14"><span class="c20">Native Combblas Graphlab Socialite Giraph </span></p><p class="c14"><span class="c20">100 </span></p><p class="c14"><span class="c78">10 </span></p><p class="c14"><span class="c135">a ret</span><span class="c42">i</span><span class="c78">1 </span></p><p class="c14"><span class="c20">10.11 2 4 8 16 32 64 </span></p><p class="c14"><span class="c42">Number of nodes </span><span class="c1">(a) PageRank </span></p><p class="c14"><span class="c53">10 </span></p><p class="c14"><span class="c53">BFS (Weak scaling, 128M undirected edges/node) </span></p><p class="c14"><span class="c126">Native Combblas Graphlab Socialite Giraph </span></p><p class="c14"><span class="c53">1000 </span></p><p class="c14"><span class="c53">100 </span></p><p class="c14"><span class="c55">1 </span></p><p class="c14"><span class="c53">01 2 4 8 16 32 64 </span></p><p class="c14"><span class="c75">Number of nodes </span><span class="c1">(b) Breadth-First Search </span></p><p class="c14"><span class="c20">100 </span></p><p class="c14"><span class="c20">Collaborative Filtering (Weak scaling, 250 M edges/node) </span></p><p class="c14"><span class="c20">Native Combblas Graphlab Socialite Giraph </span></p><p class="c14"><span class="c20">10000 </span></p><p class="c14"><span class="c20">1000 </span></p><p class="c14"><span class="c55">10 </span></p><p class="c14"><span class="c20">11 2 4 8 16 32 64 </span></p><p class="c14"><span class="c20">Number of nodes </span><span class="c1">(c) Collaborative Filtering </span></p><p class="c14"><span class="c20">10 </span></p><p class="c14"><span class="c20">Triangle Counting (Weak scaling, 32M edges/node) </span></p><p class="c14"><span class="c20">Native Combblas Graphlab Socialite Giraph </span></p><p class="c14"><span class="c20">1000 </span></p><p class="c14"><span class="c20">100 </span></p><p class="c14"><span class="c35">m i</span><span class="c42">T</span><span class="c55">1 </span></p><p class="c43"><span class="c20">1 2 4 8 16 32 64 0</span><span class="c42">Number of nodes </span><span class="c1">(d) Triangle Counting </span></p><p class="c6"><span class="c1">Figure 4: </span><span class="c5">Performance results for different algorithms on large scale synthetic graphs. The y-axis represents runtime in log-scale. We per- form weak-scaling, where the amount of graph data per node is kept constant, (a) 128 M edges/node for pagerank, (b) 128 M edges/node for BFS, (c) 256M ratings/node for SGD, and (d) 32M edges/node for tri- angle counting. Horizontal lines represent perfect scaling. </span></p><p class="c14"><span class="c13">986 </span></p><p class="c62"><span class="c20">Triangle Count. (Twitter, 16 nodes) </span></p><p class="c150"><span class="c1">Figure 5: </span><span class="c5">Performance results for large real world graphs run on mul- tiple nodes. We show the results of running (in log-scale) Pagerank, BFS and Triangle Counting on a subset of Twitter [20], and Collabora- tive Filtering on the Yahoo Music KDDCup dataset [7] </span></p><p class="c46"><span class="c1">cuckoo hash data structure that allows for a fast union of neighbor lists, which is the primary operation in the algorithm. (5) GraphLab performance drops off significantly for multi node runs (especially for Pagerank) due to network bottlenecks. (6) SociaLite perfor- mance is typically between GraphLab and CombBLAS, except for Triangle Counting, where it performs best among our frameworks. It also shows more stable performance for different scales than GraphLab. (7) Native performance is relatively stable on various benchmarks. The performance noticeably drops for Collaborative Filtering where network traffic increases with number of nodes, and the algorithm gradually becomes more network limited. (8) Finally, the trends we see in real-world data are broadly similar to those for synthetic data. In particular, CombBLAS performs best among non-native frameworks for three of the four algorithms - Pagerank, BFS and Collaborative Filtering, while SociaLite performs best for Triangle Counting. </span></p><p class="c136"><span class="c1">Given the various performance issues we see in the frameworks, we next delve deeper into other metrics beside runtime to gain more insights into the frameworks. </span></p><p class="c14 c130"><span class="c20">100 </span></p><p class="c106"><span class="c20">10 </span></p><p class="c123"><span class="c20">1 </span></p><p class="c119"><span class="c20">Pagerank (Twitter, 4 nodes) </span></p><p class="c38"><span class="c20">BFS (Twitter, 4 nodes) </span></p><p class="c38"><span class="c20">BFS (Twitter, 4 nodes) </span></p><p class="c43 c77"><span class="c20">Collaborative Filt. (Yahoo Music, 4 nodes) </span></p><p class="c43 c77"><span class="c20">Collaborative Filt. (Yahoo Music, 4 nodes) </span></p><p class="c43 c77"><span class="c20">Collaborative Filt. (Yahoo Music, 4 nodes) </span></p><p class="c6 c29 c56"><span class="c1">Peak Network Transfer Rate: The peak network transfer rate when running different algorithms is relatively stable for a given framework, and is dependent mostly on the communication layer used. Note that Giraph has the lowest peak traffic rate of less than 0.5 GigaBytes per second (GBps), and CombBLAS and Na- tive code the highest of over 5 GBps. Although SociaLite and GraphLab use similar communication layers, SociaLite achieves about twice the peak rate of GraphLab (discussion in Section 6). In fact, the currently available SociaLite code (from the authors) does indeed have low transfer rates, and we were able to improve this behavior substantially using network traffic analysis, which is an important contribution of this paper. </span></p><p class="c6 c29 c56"><span class="c1">Peak Network Transfer Rate: The peak network transfer rate when running different algorithms is relatively stable for a given framework, and is dependent mostly on the communication layer used. Note that Giraph has the lowest peak traffic rate of less than 0.5 GigaBytes per second (GBps), and CombBLAS and Na- tive code the highest of over 5 GBps. Although SociaLite and GraphLab use similar communication layers, SociaLite achieves about twice the peak rate of GraphLab (discussion in Section 6). In fact, the currently available SociaLite code (from the authors) does indeed have low transfer rates, and we were able to improve this behavior substantially using network traffic analysis, which is an important contribution of this paper. </span></p><p class="c6 c29 c56"><span class="c1">Peak Network Transfer Rate: The peak network transfer rate when running different algorithms is relatively stable for a given framework, and is dependent mostly on the communication layer used. Note that Giraph has the lowest peak traffic rate of less than 0.5 GigaBytes per second (GBps), and CombBLAS and Na- tive code the highest of over 5 GBps. Although SociaLite and GraphLab use similar communication layers, SociaLite achieves about twice the peak rate of GraphLab (discussion in Section 6). In fact, the currently available SociaLite code (from the authors) does indeed have low transfer rates, and we were able to improve this behavior substantially using network traffic analysis, which is an important contribution of this paper. </span></p><p class="c6 c29 c56"><span class="c1">Peak Network Transfer Rate: The peak network transfer rate when running different algorithms is relatively stable for a given framework, and is dependent mostly on the communication layer used. Note that Giraph has the lowest peak traffic rate of less than 0.5 GigaBytes per second (GBps), and CombBLAS and Na- tive code the highest of over 5 GBps. Although SociaLite and GraphLab use similar communication layers, SociaLite achieves about twice the peak rate of GraphLab (discussion in Section 6). In fact, the currently available SociaLite code (from the authors) does indeed have low transfer rates, and we were able to improve this behavior substantially using network traffic analysis, which is an important contribution of this paper. </span></p><p class="c66"><span class="c1">Data volumes: The amount of data transferred varies consider- ably based on algorithm (as shown in Figure 6), as well as platform (depending on compression schemes and data structures used). We show the latter impact in Section 6. </span></p><p class="c66"><span class="c1">These measurements are very useful for predicting the slow- downs we see with graph frameworks compared to native imple- mentation. For example, we look at only the measured network parameters for pagerank to estimate performance differences (net- work bytes sent/peak network bandwidth) from Figure 6. We can estimate that the frameworks would be 1.75, 9.8, 5.6, 32.7&times; slower than native code (assuming all network transfers happen at the peak measured rate). Even this rough estimation is within a factor of 2.5 of observed performance differences (Table 6). Similar analysis can be done for other algorithms as well. Bandwidth bound code will need to estimate the number of reads/writes and scale it with the memory footprint. </span></p><p class="c66"><span class="c1">In the next section, we will discuss optimizations that we per- form in native code, show examples of how such optimizations are also useful in other frameworks and describe our key recommenda- tions for other frameworks as well. </span></p><p class="c124"><span class="c27">5.4 Framework Analysis </span></p><p class="c104 c112"><span class="c1">We did further investigation of CPU utilization, memory foot- print and network traffic to narrow down the reasons for the ob- served performance trends. We use the sar/sysstat monitoring tools [4] available in Linux to measure these. These results are summarized in Figure 6. We briefly describe our key findings in this section, and defer a detailed reasoning on what can be done to address these problems to Section 6. </span></p><p class="c74 c81"><span class="c1">Memory footprint: First, note that our dataset sizes chosen are such that the memory footprint of one or more frameworks (usually Giraph) is more than 50% of total memory &ndash; hence we cannot in- crease dataset sizes per node. The memory footprint is usually not due to just the input graph size, but due to the large intermediate data structures, e.g. message buffers that need to be kept. It is possi- ble to avoid buffering entire messages at once (e.g. GraphLab) and hence improve this behavior of Giraph as discussed in Section 6. Large memory footprints are also seen in CombBLAS (Triangle Counting), where the problem is framework expressibility. </span></p><p class="c81 c74"><span class="c1">CPU utilization: The CPU utilization of various frameworks is high when it is not I/O or network limited. Giraph has especially low CPU utilization across the board. This is because memory lim- itations restrict the number of workers that can be assigned to a single node to 4 (even though the number of cores per node is 24). Each worker requires more memory and we cannot add more work- ers within our memory limit. This limits the utilization to 4/24 &sim; 16%. Our native code is network limited for Triangle Counting (unlike CombBLAS and GraphLab which are memory bandwidth </span></p><p class="c14 c33"><span class="c93">96746.8 </span><span class="c20">1000 </span></p><p class="c14 c29"><span class="c1">bound) and has relatively low CPU utilization. Pagerank is limited by network traffic for all the frameworks as well. </span></p><p class="c28"><span class="c27">6. DISCUSSION </span></p><p class="c29 c56 c108"><span class="c1">In this section, we discuss the various optimization techniques used to improve the performance of our native code. We then present recommendations on how the frameworks can be optimized to achieve native code level performance. </span><span class="c27">6.1 Native implementations </span></p><p class="c0"><span class="c54">6.1.1 Key optimization techniques </span></p><p class="c29 c57"><span class="c1">The key features for performance and scalability that our native code uses are listed below. Note that not all techniques have been applied or are applicable to every algorithm. However, we maxi- mize the use of these techniques to obtain best possible native code. Data structures: Careful selection of data structure can ben- efit performance significantly. For example, algorithms like BFS and Triangle Counting can take advantage of bit-vectors instead of other data structures for constant time lookups while minimizing cache misses. In our native BFS and Triangle Counting code, this results in a benefit of slightly over 2X. GraphLab keeps a cuckoo- hash data structure in Triangle Counting for the same reason, result- ing an efficient code that runs at only 2-3X off native performance. </span></p><p class="c47 c86"><span class="c1">Data Compression: In many cases, the data communicated among nodes is the id&rsquo;s of destination vertices of the edges traversed. Such data has been observed to be compressible using techniques like bit-vectors and delta coding [28]. For BFS and Pagerank, this re- sults in a net benefit of about 3.2 and 2.2X respectively (and a cor- responding reduction of bytes transferred in Figure 6). GraphLab and CombBLAS perform a limited form of compression that takes </span></p><p class="c14 c23"><span class="c20">Native </span></p><p class="c14 c133"><span class="c20">Combblas </span></p><p class="c14 c36"><span class="c20">Graphlab </span></p><p class="c14 c36"><span class="c20">Graphlab </span></p><p class="c14 c85"><span class="c20">Socialite </span></p><p class="c14 c85"><span class="c20">Socialite </span></p><p class="c14 c85"><span class="c20">Socialite </span></p><p class="c14 c98"><span class="c20">Giraph </span></p><p class="c14 c98"><span class="c20">Giraph </span></p><p class="c14 c98"><span class="c20">Giraph </span></p><p class="c14 c98"><span class="c20">Giraph </span></p><p class="c14 c29"><span class="c1">bound) and has relatively low CPU utilization. Pagerank is limited by network traffic for all the frameworks as well. </span></p><p class="c14 c29"><span class="c1">bound) and has relatively low CPU utilization. Pagerank is limited by network traffic for all the frameworks as well. </span></p><p class="c14 c29"><span class="c1">bound) and has relatively low CPU utilization. Pagerank is limited by network traffic for all the frameworks as well. </span></p><p class="c14 c29"><span class="c1">bound) and has relatively low CPU utilization. Pagerank is limited by network traffic for all the frameworks as well. </span></p><p class="c14 c29"><span class="c1">bound) and has relatively low CPU utilization. Pagerank is limited by network traffic for all the frameworks as well. </span></p><p class="c143"><span class="c13">987 </span></p><p class="c14"><span class="c26">0</span><span class="c63">20 </span></p><p class="c14"><span class="c1">(d) Triangle Counting </span></p><p class="c6"><span class="c1">Figure 6: </span><span class="c5">Performance metrics including CPU utilization, peak achieved network bandwidth, memory footprint and bytes sent over the network for a 4-node run of our benchmarks. Metrics are normalized so that a value of 100 on the y-axis corresponds to (1) 100% CPU utilization, (2) 5.5 GB/s/node peak network BW (network limit) (3) 64 GB of memory/node (memory capacity) and (4) bytes sent by Giraph/node for each algorithm (1.4 GB/node for pagerank, 0.73 GB/node for BFS, 37.4 GB/node for Collaborative Filtering and 13.0 GB/node for Triangle Counting). Higher numbers are better for CPU utilization and peak network traffic while lower numbers are better for memory footprint and network bytes sent. </span></p><p class="c14"><span class="c1">advantage of local reductions to avoid repeated communication of the same vertex data to different target vertices in the same node. </span></p><p class="c6"><span class="c1">Overlap of Computation and Communication: Overlap of computation and communication is possible for many applications where an entire message need not be received before computation can start on the portion of the message that has been received. This allows for hiding the latency of communication with useful com- putation, and has been shown to improve performance of various optimized implementations [28]. Native code for BFS, pagerank and Triangle Counting all benefit between 1.2-2X. Apart from la- tency hiding, overlapping communication and computation allows for blocking of a very large message into multiple smaller ones, leading to lower memory footprint for buffer storage. This opti- mization specifically helps Triangle Counting, and both GraphLab and native codes perform this, leading to low memory footprints. </span></p><p class="c6"><span class="c1">Message passing mechanisms: Using the right underlying mes- sage passing mechanisms can boost performance significantly. The native code uses MPI message passing [6] to drive the underlying fabric (FDR InfiniBand network in our case) for high bandwidth and low latency communication among the nodes. Although it is possible to implement message passing using other techniques such as TCP sockets using IP over IB [33], this suffers from between 2.5-3X lower bandwidth than MPI code, as observed in case of GraphLab. However, if it not possible to use MPI, we observe that multiple sockets between a pair of nodes regains up to 2X of this performance, as in SociaLite. </span></p><p class="c6"><span class="c1">Partitioning schemes: Most frameworks running on multiple nodes support partitioning the graph among the nodes in such a way that nodes either own a subset of vertices (Giraph, SociaLite) or in a 2D fashion where they store a subset of edges (CombBLAS). GraphLab performs a more advanced partitioning scheme where some nodes with large degree are duplicated in multiple nodes to avoid problems of load imbalance during computation. In line with observations in recent research [25], 2D partitioning as in Comb- BLAS or advanced 1D partitioning such as GraphLab gives better load balancing, leading to better performance and scalability. </span></p><p class="c14"><span class="c54">6.1.2 Impact of the optimization techniques </span></p><p class="c6"><span class="c1">We now look at each algorithm and discuss the optimizations having high impact on each (we do not describe all optimizations due to space limits). Figure 7 shows the impact of various per- formance optimizations on native code for Pagerank and BFS. The main optimizations come from software prefetch instructions that help hide the long latency of irregular memory accesses and mes- </span></p><p class="c14"><span class="c50">20 161284</span><span class="c25">0</span><span class="c50">W </span></p><p class="c14"><span class="c50">/</span><span class="c25">S</span><span class="c50">m </span></p><p class="c14"><span class="c50">o</span><span class="c25">c</span><span class="c50">. mmo</span><span class="c25">cPagerank BFS </span></p><p class="c14"><span class="c1">Figure 7: </span><span class="c5">Effect of optimizations performed on native implementa- tions of Pagerank and BFS. </span></p><p class="c6"><span class="c1">sage optimizations (compressing message containing edges to be sent to other nodes and overlapping computation with communi- cation). For BFS, additional optimization was to use bit-vectors (a data structure optimization) to compactly maintain the list of al- ready visited vertices [12, 28]. For Triangle counting, the same data structure optimization (bit vectors), for quick constant time lookups to identify common neighbors of different vertices, gave a speedup of around 2.2X. </span></p><p class="c6"><span class="c1">For Collaborative filtering, the key optimizations revolve around efficient parallelization of Stochastic Gradient Descent. We adopt the diagonal parallelization technique used in Gemulla et al. [16] to parallelize SGD both within and across nodes without using locks. </span></p><p class="c14"><span class="c54">6.1.3 Testcase: Improving SociaLite and Giraph </span></p><p class="c6"><span class="c1">We select SociaLite and Giraph to demonstrate the possible per- formance improvement of the frameworks using the analysis in Section 5.4 and depth of understanding of the optimization tech- niques. </span></p><p class="c6"><span class="c1">SociaLite: We first observed that the code corresponding to pub- lished SociaLite results (obtained from the authors) exhibited poor peak network performance of about 0.5 GBps (like Giraph does to- day) since it uses sockets for communication. Although changing SociaLite to use MPI would have been ideal, it is much difficult due to language and other issues. However, using multiple sock- ets to communicate between two workers allows for much higher peak bandwidths (of close to 2 GBps). This, along with minor optimizations such as merging communication data for batch pro- cessing, and using custom memory allocators to minimize garbage </span></p><p class="c14"><span class="c13">988 </span></p><p class="c14"><span class="c26">100 </span></p><p class="c7"><span class="c26">20</span><span class="c63">80 604020 </span><span class="c26">0</span><span class="c1">(a) PageRank </span></p><p class="c14"><span class="c18">Native </span></p><p class="c14"><span class="c18">Socalite </span></p><p class="c14"><span class="c18">Combblas </span></p><p class="c14"><span class="c18">Giraph </span></p><p class="c14"><span class="c18">Graphlab </span></p><p class="c14"><span class="c26">100 </span></p><p class="c7"><span class="c26">20</span><span class="c63">80 604020 </span><span class="c26">0</span><span class="c1">(b) Breadth-First Search </span></p><p class="c14"><span class="c18">Native </span></p><p class="c14"><span class="c18">Socalite </span></p><p class="c14"><span class="c18">Combblas </span></p><p class="c14"><span class="c18">Giraph </span></p><p class="c14"><span class="c18">Graphlab </span></p><p class="c14"><span class="c26">100 </span></p><p class="c7"><span class="c26">20</span><span class="c63">80 604020 </span><span class="c26">0</span><span class="c1">(c) Collaborative Filtering </span></p><p class="c14"><span class="c18">Native </span></p><p class="c14"><span class="c18">Socalite </span></p><p class="c14"><span class="c18">Combblas </span></p><p class="c14"><span class="c18">Giraph </span></p><p class="c14"><span class="c18">Graphlab </span></p><p class="c14"><span class="c26">100 </span></p><p class="c14"><span class="c26">20</span><span class="c63">406080 </span></p><p class="c14"><span class="c18">Native </span></p><p class="c14"><span class="c18">Socalite </span></p><p class="c14"><span class="c18">Combblas </span></p><p class="c14"><span class="c18">Giraph </span></p><p class="c14"><span class="c18">Graphlab </span></p><p class="c6 c74"><span class="c1">collection time (these had minor impacts on performance), allowed SociaLite results to improve by 1.6-2.4X for network limited al- gorithms (Pagerank and Triangle Counting). Table 7 presents the old and new SociaLite performance for a 4-node setup for these benchmarks. The results in this paper correspond to the optimized version of SociaLite. </span></p><p class="c110"><span class="c5">Algorithm Time Before Time After Speedup PageRank 4.6 1.9 2.4 Triangle Counting 7.6 4.9 1.6 </span></p><p class="c134"><span class="c1">Table 7: </span><span class="c5">Summary of performance speedups for SociaLite on per- forming network optimizations. Results are for 4 nodes in seconds. </span></p><p class="c74 c104 c147"><span class="c1">Giraph: One major problem in Giraph is that it tries to buffer all outgoing messages in memory before sending any messages (due to its bulk synchronous model). This leads to high memory consump- tion especially for Triangle Counting which can run out of memory for the data set sizes we use in this work. The native code deals with this problem by breaking up this large message into multiple smaller messages and using computation-communication overlap as described previously. We perform a conceptually similar opti- mization at the Giraph code level by breaking up each superstep (iteration) into 100 smaller supersteps, and processing 1% of all vertices at each smaller superstep. This results in much smaller memory footprint (since only 1% messages are created at any time), at the cost of finer grained synchronization. It was only using this optimization that we were able to run Triangle Counting to run on Giraph. A similar optimization was also used in Collaborative Fil- tering to reduce memory footprint. </span><span class="c27">6.2 Roadmap for framework improvements </span></p><p class="c57 c92"><span class="c1">Based on our experience in optimizing native code as well as some frameworks as described in the previous section, we present recommendations for bridging the performance gap of all the frame- works w.r.t. native code. It is however, to be noted that some per- formance differences between frameworks come from the program- ming abstractions themselves, and we note these wherever possible. CombBLAS: CombBLAS incorporates many of our optimiza- tions, and is hence one of the best performing frameworks. For Pagerank and Collaborative Filtering, it is already within 4&times; of na- tive performance. CombBLAS needs to use data structures such as bitvectors for compression in order to improve BFS performance. Triangle counting performance is limited by CombBLAS&rsquo; program- ming abstraction and techniques to perform inter-operation op- timization (combine A</span><span class="c22">2 </span><span class="c1">computation with intersection with A, thereby also achieving overlap of computation and communication) can make it more efficient. </span></p><p class="c81 c74"><span class="c1">GraphLab: GraphLab is mainly limited by network bandwidth. It achieves only 20-25% of what the network hardware provides. We believe this 4 &minus; 5&times; gap can be minimized by incorporating MPI, or at least by using multiple sockets between pairs of nodes as in SociaLite. Additionally, data compression, prefetching and overlapping computation and communication can also help perfor- mance. Incorporating these changes should allow GraphLab to be within 5&times; of native performance. </span></p><p class="c141 c104 c149"><span class="c1">Giraph: Giraph does not have most of our optimizations, and hence performs worst among all frameworks. The most serious impediment to performance is poor network utilization (&lt; 10%). Boosting network bandwidth by 10x should make Giraph very com- petitive with other frameworks. Techniques like data compres- sion (bitvectors) and overlapping computation and communication should also help. Performance will also improve if we can run more workers per node, thereby improving CPU utilization. This would </span></p><p class="c6 c29"><span class="c1">require addressing the high memory consumption of each worker. Techniques to reduce message buffer sizes (e.g. avoiding dupli- cated communication across nodes if multiple targets are present in the same node) are necessary to achieve this. </span></p><p class="c29 c56 c95"><span class="c1">SociaLite: SociaLite performs best among all frameworks for multi-node triangle counting (within 2&times; of native) but its perfor- mance for other algorithms is mostly limited by network bandwidth (though it achieves better network BW than Giraph and GraphLab). Even after improvements, the network BW is still about 3 &minus; 4&times; lower than hardware peak. Fixing this along with the use of data compression (for BFS) will help SociaLite to achieve performance within 5&times; of native performance. </span></p><p class="c12"><span class="c1">Galois: The Galois framework, although limited to single-node, does implement optimizations such as prefetching, and as such is one of the best performing single-node frameworks. </span></p><p class="c48"><span class="c1">Our observations above and in Section 5 should help the end users of the frameworks make informed choices about which frame- works to use. Our analysis should also help framework developers with guidance on what factors limit their performance and are im- portant to optimize for. </span></p><p class="c69"><span class="c27">7. RELATED WORK </span></p><p class="c57 c29"><span class="c1">The list of graph frameworks developed in recent years to tackle large scale problems is too long to be listed. Some distributed graph frameworks that we have not studied in this paper include GPS [27], GraphX [35], etc. In addition to these, others have imple- mented graph workloads on generic distributed frameworks such as Hadoop, YARN, Stratosphere etc. [17]. Our choice was moti- vated mainly by a need to explore a variety of programming models and not just different implementations of the same programming model. Most common graph programming models are based on vertex-programming and there have been other efforts comparing the differences between various runtime implementations of that model [17, 24]. We briefly distinguish our contributions from these. Graph Partitioning System (GPS) [27] uses a vertex program- ming model with Long Adjacency List Partitioning (LALP) i.e. vertex partitioning except for the large degree vertices which are split among multiple nodes. [27] showed that GPS with LALP achieves a 12X performance improvement compared to Giraph, putting it at a performance level comparable to that of the frame- works studied (but much slower than native code). </span></p><p class="c12"><span class="c1">GraphX [35] is a graph framework built on top of Spark [36] and uses vertex programming. [35] showed that GraphX is about 7X slower than GraphLab for pagerank (including file read). This would put GraphX at the slower end of the spectrum of frameworks considered in this paper. </span></p><p class="c12"><span class="c1">In addition to above mentioned graph frameworks, there have also been other graph framework comparison efforts in literature. [17] looks at a number of graph algorithms and frameworks. It is quite limited in the types of graph frameworks that are considered (all frameworks are either generic, non-graph specific ones or ver- tex programming based). While our goal is not to produce a graph benchmark, we do share algorithms with [17] including Pagerank and BFS. Also, we include a hand-optimized native implementation (close to hardware limits) which puts the runtime differences be- tween other frameworks in better perspective, something no other graph comparison effort has done to the best of our knowledge. </span></p><p class="c95 c29 c56"><span class="c1">[13] shows Pagerank running on a large cluster at about 20 mil- lion edges/sec/node with Giraph. In contrast, our Giraph imple- mentation achieves about 9 million edges/sec/node and the differ- ence can be attributed to precision (we use double precision), mem- ory size (they are able to fit much larger graphs in a single node) and improved CPU utilization achieved with more memory and more </span></p><p class="c60"><span class="c13">989 </span></p><p class="c14"><span class="c1">[15] workers per node. In contrast, our native implementation processes </span></p><p class="c14"><span class="c1">J. Dongarra. Compressed Row Storage. around 640 million edges/sec/node. </span></p><p class="c14"><span class="c1">http://web.eecs.utk.edu/~dongarra/ [24] looks at a variety of vertex programming frameworks but at </span></p><p class="c14"><span class="c1">etemplates/node373.html. a lower level (Bulk synchronous vs autonomous, scheduling poli- </span></p><p class="c14"><span class="c1">[16] R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis. cies etc.). Their work is useful in order to understand and improve </span></p><p class="c14"><span class="c1">Large-scale matrix factorization with distributed stochastic the implementations of vertex programming models, whereas our </span></p><p class="c14"><span class="c1">gradient descent. In ACM SIGKDD, pages 69&ndash;77, 2011. work has different goals. We consider a wider variety of frame- works and our work is more interested in answering the more gen- eral productivity-vs-performance question for graph analytics. </span></p><p class="c6"><span class="c1">In short, our goal is not to come up with a new graph processing benchmark or propose a new graph framework, but to analyze ex- isting approaches better to find out where they fall short especially when used for more general machine learning problems on graphs (such as collaborative filtering) with a native, hand-optimized im- plementation as a reference point. </span></p><p class="c14"><span class="c1">[17] Y. Guo, M. Biczak, A. L. Varbanescu, A. Iosup, C. Martella, </span></p><p class="c14"><span class="c1">and T. L. Willke. Towards benchmarking graph-processing platforms. In Poster at Supercomputing, 2013. [18] T. Ideker, O. Ozier, B. Schwikowski, and A. F. Siegel. </span></p><p class="c7"><span class="c1">Discovering regulatory and signalling circuits in molecular interaction networks. Bioinformatics, 18(1):233&ndash;240, 2002. [19] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization </span></p><p class="c7"><span class="c1">techniques for recommender systems. Computer, 42(8):30&ndash;37, 2009. </span><span class="c27">8. CONCLUSION </span></p><p class="c14"><span class="c1">[20] H. Kwak, C. Lee, H. Park, and S. B. Moon. What is twitter, a </span></p><p class="c14"><span class="c1">social network or a news media? In WWW, pages 591&ndash;600, In this paper, we aim at reducing the performance gap between </span></p><p class="c14"><span class="c1">2010. optimized hand-coded implementations and popular graph frame- works for a set of graph algorithms. To this end, we first developed native implementations that are only limited by the hardware and showed that there is a 2-30X performance gap between native code and most frameworks (up to 560X on Giraph). Using a set of per- formance metrics, we analyzed the behavior of various frameworks and the causes of the performance gap between native and frame- work code. We use our understanding of optimization techniques from native code and our analysis to propose a set of recommended changes for each framework to bridge the Ninja gap. These rec- ommendations will enable different frameworks to be made com- petitive with respect to performance, thus simplifying the choice of framework for the end user. </span></p><p class="c14"><span class="c1">[21] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, C. Guestrin, and </span></p><p class="c14"><span class="c1">J. M. Hellerstein. GraphLab: A new parallel framework for machine learning. In UAI, July 2010. [22] T. Mattson, D. Bader, et al. Standards for graph algorithm </span></p><p class="c14"><span class="c1">primitives. In HPEC, pages 1&ndash;2, 2014. [23] R. C. Murphy, K. B. Wheeler, B. W. Barrett, and J. A. Ang. </span></p><p class="c14"><span class="c1">Introducing the graph 500. Cray User&rsquo;s Group (CUG), 2010. [24] D. Nguyen, A. Lenharth, and K. Pingali. A lightweight </span></p><p class="c43"><span class="c1">infrastructure for graph analytics. In Proc. SOSP, 2013. [25] M. M. A. Patwary, R. H. Bisseling, and F. Manne. Parallel </span></p><p class="c7"><span class="c1">greedy graph matching using an edge partitioning approach. In ICFP Workshops at HLPP&rsquo;10, pages 45&ndash;54. ACM, 2010. [26] K. Pingali, D. Nguyen, M. Kulkarni, et al. The tao of </span><span class="c27">9. </span><span class="c3">[1] Combinatorial </span><span class="c27">REFERENCES </span><span class="c3">Blas v 1.3. http://gauss.cs.ucsb. </span></p><p class="c7"><span class="c1">parallelism in algorithms. In PLDI, pages 12&ndash;25, New York, NY, USA, 2011. ACM. edu/~aydin/CombBLAS/html/. </span></p><p class="c14"><span class="c1">[27] S. Salihoglu and J. Widom. Gps: A graph processing system. [2] Galois v 2.2.0. http://iss.ices.utexas.edu/?p= </span></p><p class="c14"><span class="c1">projects/galois/download. </span></p><p class="c43"><span class="c1">In Scientific and Statistical Database Management. Stanford InfoLab, July 2013. [3] Graphlab v 2.2. http://graphlab.org. [4] sar Manual Page. http://pagesperso-orange.fr/ </span></p><p class="c14"><span class="c1">sebastien.godard/man_sar.html. [5] The Linpack Benchmark. </span></p><p class="c14"><span class="c1">http://www.top500.org/project/linpack/. [6] The Message Passing Interface (MPI) standard. http: </span></p><p class="c14"><span class="c1">//www.mcs.anl.gov/research/projects/mpi/. [7] Yahoo! - Movie, Music, and Images Ratings Data Sets. </span></p><p class="c14"><span class="c1">http://webscope.sandbox.yahoo.com/catalog.php?datatype=r. [8] Apache giraph. http://giraph.apache.org/, 2013. [9] J. Bennett and S. Lanning. The Netflix Prize. In KDD Cup </span></p><p class="c14"><span class="c1">and Workshop at ACM SIGKDD, 2007. [10] N. Bronson, Z. Amsden, et al. Tao: Facebooks distributed </span></p><p class="c43"><span class="c1">data store for the social graph. In USENIX ATC, 2013. [11] A. Buluc and J. R. Gilbert. The combinatorial blas: design, </span></p><p class="c43"><span class="c1">implementation, and applications. HPCA&rsquo;11, 25(4):496&ndash;509. [12] J. Chhugani, N. Satish, C. Kim, J. Sewall, and P. Dubey. Fast and efficient graph traversal algorithm for cpus: Maximizing single-node efficiency. In IPDPS, pages 378&ndash;389, 2012. [13] A. Ching. Scaling apache giraph to a trillion edges. www. </span></p><p class="c14"><span class="c1">facebook.com/notes/facebook-engineering/ scaling-apache-giraph-to-a-trillion-edges/ 10151617006153920, 2013. [14] T. Davis. The University of Florida Sparse Matrix </span></p><p class="c14"><span class="c1">[28] N. Satish, C. Kim, J. Chhugani, and P. Dubey. Large-scale </span></p><p class="c43"><span class="c1">energy-efficient graph traversal: a path to efficient data-intensive supercomputing. In SC, pages 1&ndash;11, 2012. [29] N. Satish, C. Kim, J. Chhugani, P. Dubey, et al. Can </span></p><p class="c7"><span class="c1">Traditional Programming Bridge the Ninja Performance Gap for Parallel Applications? In ISCA&rsquo;12, pages 440&ndash;451, 2012. [30] J. Seo, S. Guo, , and M. S. Lam. SociaLite: Datalog </span></p><p class="c14"><span class="c1">extensions for efficient social network analysis. ICDE&rsquo;13, pages 278&ndash;289, 2013. [31] J. Seo, J. Park, J. Shin, and M. S. Lam. Distributed </span></p><p class="c7"><span class="c1">SociaLite: A datalog-based language for large-scale graph analysis. Proceedings of the VLDB Endowment, 6(14), 2013. [32] J. D. Ullman. Principles of database and knowledge-base </span></p><p class="c14"><span class="c1">systems, volume ii. 1989. [33] K. V. IP over InfiniBand (IPoIB) architecture. RFC 4392. [34] C. Wilson, B. Boe, A. Sala, K. P. N. Puttaswamy, and B. Y. </span></p><p class="c43"><span class="c1">Zhao. User interactions in social networks and their implications. In EuroSys, pages 205&ndash;218, 2009. [35] R. S. Xin, J. E. Gonzalez, M. J. Franklin, and I. Stoica. </span></p><p class="c14"><span class="c1">Graphx: a resilient distributed graph system on spark. In GRADES, page 2, 2013. [36] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and </span></p><p class="c14"><span class="c1">I. Stoica. Spark: cluster computing with working sets. In USENIX Hot Cloud, pages 10&ndash;10, 2010. </span></p><p class="c14"><span class="c1">Collection. http://www.cise.ufl.edu/research/sparse/matrices. </span></p><p class="c14"><span class="c13">990 </span></p></body></html>