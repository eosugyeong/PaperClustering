<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c72{margin-left:-9.2pt;padding-top:1pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-4.1pt}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Times New Roman";font-style:normal}.c127{margin-left:-18.5pt;padding-top:1.4pt;text-indent:27.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-8.5pt}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9.3pt;font-family:"Arial";font-style:normal}.c56{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Times New Roman";font-style:normal}.c106{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:italic}.c85{margin-left:-18.5pt;padding-top:1.4pt;text-indent:27.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7.3pt}.c62{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.8pt;font-family:"Arial";font-style:italic}.c54{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.4pt;font-family:"Arial";font-style:normal}.c40{margin-left:-9.7pt;padding-top:1.7pt;text-indent:18.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.5pt}.c32{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.6pt;font-family:"Times New Roman";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.4pt;font-family:"Arial";font-style:normal}.c47{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.6pt;font-family:"Arial";font-style:normal}.c48{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.1pt;font-family:"Arial";font-style:italic}.c55{margin-left:-9.2pt;padding-top:1.7pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.5pt}.c117{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:normal}.c151{margin-left:-18.5pt;padding-top:10.6pt;text-indent:18.7pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-8.2pt}.c136{margin-left:-5.1pt;padding-top:1pt;text-indent:18.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-10.8pt}.c94{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:italic}.c50{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:15.6pt;font-family:"Arial";font-style:italic}.c96{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.4pt;font-family:"Courier New";font-style:normal}.c138{margin-left:-18.2pt;padding-top:1.4pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-8.7pt}.c36{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c130{margin-left:-14.2pt;padding-top:1.2pt;text-indent:27.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-6.6pt}.c141{margin-left:-18.5pt;padding-top:25.9pt;text-indent:18.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-8.5pt}.c110{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:15.6pt;font-family:"Arial";font-style:normal}.c25{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Arial";font-style:italic}.c116{margin-left:-14.2pt;padding-top:1.2pt;text-indent:27.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:10pt}.c98{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Times New Roman";font-style:italic}.c27{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.6pt;font-family:"Arial";font-style:italic}.c39{margin-left:-9.2pt;padding-top:1pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.1pt}.c99{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.1pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Times New Roman";font-style:italic}.c147{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c63{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:15.6pt;font-family:"Arial";font-style:italic}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.6pt;font-family:"Arial";font-style:normal}.c113{margin-left:-18.2pt;padding-top:1.4pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-8.5pt}.c143{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.2pt;font-family:"Arial";font-style:italic}.c70{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9.3pt;font-family:"Courier New";font-style:italic}.c95{margin-left:-18.2pt;padding-top:3.8pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.5pt;font-family:"Times New Roman";font-style:normal}.c33{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.8pt;font-family:"Arial";font-style:normal}.c118{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.3pt;font-family:"Arial";font-style:italic}.c144{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.6pt;font-family:"Times New Roman";font-style:normal}.c79{margin-left:-9.2pt;padding-top:1.2pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:4.3pt}.c45{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c21{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c134{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c149{margin-left:-9.2pt;padding-top:1pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-4.3pt}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c58{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Times New Roman";font-style:normal}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.1pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Times New Roman";font-style:normal}.c122{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.3pt;font-family:"Arial";font-style:italic}.c60{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:italic}.c69{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9.3pt;font-family:"Courier New";font-style:normal}.c84{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:9pt;font-family:"Times New Roman";font-style:normal}.c77{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.6pt;font-family:"Arial";font-style:italic}.c8{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.5pt;font-family:"Arial";font-style:normal}.c49{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Arial";font-style:normal}.c65{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:15.6pt;font-family:"Courier New";font-style:italic}.c129{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.3pt;font-family:"Times New Roman";font-style:normal}.c80{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c74{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Times New Roman";font-style:normal}.c67{margin-left:-18.2pt;padding-top:4.1pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-8.5pt}.c92{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.3pt;font-family:"Arial";font-style:normal}.c35{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:italic}.c43{margin-left:-9.2pt;padding-top:1.2pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.8pt}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.8pt;font-family:"Courier New";font-style:normal}.c53{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:italic}.c93{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Courier New";font-style:normal}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:italic}.c59{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Times New Roman";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9.3pt;font-family:"Arial";font-style:italic}.c82{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.2pt;font-family:"Arial";font-style:normal}.c109{margin-left:-9.2pt;padding-top:1.2pt;text-indent:26.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9.4pt}.c66{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.1pt;font-family:"Arial";font-style:normal}.c41{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:italic}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.8pt;font-family:"Times New Roman";font-style:normal}.c75{margin-left:-12.7pt;padding-top:1.2pt;text-indent:83.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:14.8pt}.c128{margin-left:-18.5pt;padding-top:10.8pt;text-indent:18.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:47.2pt}.c68{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:15.6pt;font-family:"Arial";font-style:normal}.c107{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.1pt;font-family:"Courier New";font-style:italic}.c103{margin-left:-18.5pt;padding-top:3.8pt;text-indent:27.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-8.5pt}.c111{margin-left:-9.2pt;padding-top:1pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-8.2pt}.c71{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.8pt;font-family:"Times New Roman";font-style:normal}.c100{margin-left:-5.1pt;padding-top:1pt;text-indent:18.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.4pt}.c83{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:9pt;font-family:"Times New Roman";font-style:normal}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Times New Roman";font-style:normal}.c142{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.8pt;font-family:"Courier New";font-style:italic}.c78{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:italic}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c125{margin-left:-9.4pt;padding-top:1.7pt;text-indent:18.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.5pt}.c90{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:20.9pt;font-family:"Arial";font-style:normal}.c44{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.5pt;font-family:"Arial";font-style:normal}.c38{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c11{color:#ffffff;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.1pt;font-family:"Arial";font-style:normal}.c139{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.1pt;font-family:"Courier New";font-style:italic}.c89{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Times New Roman";font-style:italic}.c76{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.5pt;font-family:"Courier New";font-style:normal}.c42{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.3pt;font-family:"Courier New";font-style:normal}.c30{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.4pt;font-family:"Times New Roman";font-style:normal}.c87{margin-left:-9.2pt;padding-top:1pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.1pt}.c37{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:italic}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:normal}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Times New Roman";font-style:italic}.c104{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:11pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:italic}.c124{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.1pt;font-family:"Arial";font-style:italic}.c131{margin-left:-18.2pt;padding-top:14.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:123.5pt}.c91{margin-left:-18.2pt;padding-top:10.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:52pt}.c145{margin-left:-18.2pt;padding-top:13.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-8.5pt}.c108{margin-left:-9.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:6.7pt}.c132{margin-left:-18.2pt;padding-top:11pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:116.1pt}.c57{margin-left:-0.5pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:65.2pt}.c102{margin-left:227.3pt;padding-top:37.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c64{margin-left:-18.2pt;padding-top:14.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:130.7pt}.c133{margin-left:227.3pt;padding-top:47.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c148{margin-left:-18.5pt;padding-top:11pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-8.2pt}.c88{margin-left:-14.2pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:13.1pt}.c105{margin-left:-9.2pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:1.2pt}.c120{margin-left:128.6pt;padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:131pt}.c52{margin-left:-18.2pt;padding-top:9.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:90.9pt}.c115{margin-left:-9.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-13.4pt}.c137{margin-left:227.3pt;padding-top:44.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c140{margin-left:-18.2pt;padding-top:11pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:0.4pt}.c119{margin-left:-9.2pt;padding-top:21.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:103.4pt}.c46{margin-left:34.6pt;padding-top:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:36.7pt}.c114{margin-left:-5.1pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:5.3pt}.c146{margin-left:-9.4pt;padding-top:7.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.5pt}.c150{margin-left:68.8pt;padding-top:221pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-6pt}.c73{margin-left:-18.2pt;padding-top:30pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c29{margin-left:8.6pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:57.8pt}.c135{padding-top:1.2pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c97{padding-top:1.4pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c28{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c51{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c123{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c101{margin-left:49pt;margin-right:50.9pt}.c81{margin-left:-9.2pt;margin-right:-17.5pt}.c126{margin-left:-9.2pt;margin-right:-17pt}.c61{margin-left:-18.5pt;margin-right:-7.8pt}.c121{margin-left:12.2pt;margin-right:-3.1pt}.c86{margin-left:9.8pt;margin-right:-75.7pt}.c112{margin-left:-5.1pt;margin-right:-15.8pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c123"><p class="c7 c101"><span class="c134">A Platform for Scalable One-Pass Analytics using MapReduce </span></p><p class="c46"><span class="c36">Boduo Li, Edward Mazur, Yanlei Diao, Andrew McGregor, Prashant Shenoy </span><span class="c124">Department of Computer Science University of Massachusetts, Amherst, Massachusetts, USA </span><span class="c147">{boduo, mazur, yanlei, mcgregor, shenoy}@cs.umass.edu </span></p><p class="c141"><span class="c10">ABSTRACT </span><span class="c1">Today&rsquo;s one-pass analytics applications tend to be data-intensive in nature and require the ability to process high volumes of data effi- ciently. MapReduce is a popular programming model for processing large datasets using a cluster of machines. However, the traditional MapReduce model is not well-suited for one-pass analytics, since it is geared towards batch processing and requires the data set to be fully loaded into the cluster before running analytical queries. This paper examines, from a systems standpoint, what architectural design changes are necessary to bring the benefits of the MapRe- duce model to incremental one-pass analytics. Our empirical and theoretical analyses of Hadoop-based MapReduce systems show that the widely-used sort-merge implementation for partitioning and parallel processing poses a fundamental barrier to incremental one-pass analytics, despite various optimizations. To address these limitations, we propose a new data analysis platform that employs hash techniques to enable fast in-memory processing, and a new fre- quent key based technique to extend such processing to workloads that require a large key-state space. Evaluation of our Hadoop-based prototype using real-world workloads shows that our new platform significantly improves the progress of map tasks, allows the reduce progress to keep up with the map progress, with up to 3 orders of magnitude reduction of internal data spills, and enables results to be returned continuously during the job. </span></p><p class="c91"><span class="c10">Categories and Subject Descriptors </span><span class="c1">H.2.4 [Database Management]: Systems </span></p><p class="c128"><span class="c10">General Terms </span><span class="c1">Algorithms, Design, Experimentation, Performance </span></p><p class="c140"><span class="c10">Keywords </span><span class="c1">Parallel processing, one-pass analytics, incremental computation </span></p><p class="c132"><span class="c10">1. INTRODUCTION </span></p><p class="c95"><span class="c1">Today, real-time analytics on large, continuously-updated datasets has become essential to meet many enterprise business needs. Like </span></p><p class="c73"><span class="c9">Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. </span><span class="c3">SIGMOD&rsquo;11, </span><span class="c9">June12&ndash;16, 2011, Athens, Greece. Copyright 2011 ACM 978-1-4503-0661-4/11/06 ...$10.00. </span></p><p class="c28 c81"><span class="c1">traditional warehouse applications, real-time analytics using incre- mental one-pass processing tends to be data-intensive in nature and requires the ability to collect and analyze enormous datasets effi- ciently. At the same time, MapReduce has emerged as a popular model for parallel processing of large datasets using a commodity cluster of machines. The key benefits of this model are that it har- nesses compute and I/O parallelism on commodity hardware and can easily scale as the datasets grow in size. However, the MapReduce model is not well-suited for incremental one-pass analytics since it is primarily designed for batch processing of queries on large datasets. Furthermore, MapReduce implementations require the entire data set to be loaded into the cluster before running analytical queries, thereby incurring long latencies and making them unsuitable for producing incremental results. </span></p><p class="c125"><span class="c1">In this paper, we take a step towards bringing the many benefits of the MapReduce model to incremental one-pass analytics. In the new model, the MapReduce system </span><span class="c13">reads input data only once, performs incremental processing as more data is read, and utilizes system resources efficiently to achieve high performance and scalability</span><span class="c1">. Our goal is to design a platform to support such scalable, incremental one-pass analytics. This platform can be used to support interactive data analysis, which may involve online aggregation with early approximate answers, and, in the future, stream query processing, which provides near real-time insights as new data arrives. </span></p><p class="c40"><span class="c1">We argue that, in order to support incremental one-pass analytics, a MapReduce system should avoid any blocking operations and also computational and I/O bottlenecks that prevent data from &ldquo;smoothly&rdquo; flowing through map and reduce phases on the processing pipeline. We further argue that, from a performance standpoint, the system needs to perform </span><span class="c13">fast in-memory processing </span><span class="c1">of a MapReduce query program for all, or most, of the data. In the event that some subset of data has to be staged to disks, the I/O cost of such disk operations must be minimized. </span></p><p class="c55"><span class="c1">Our recent benchmarking study evaluated existing MapReduce platforms including Hadoop and MapReduce Online (which per- forms pipelining of intermediate data [6]). Our results revealed that the main mechanism for parallel processing used in these systems, based on a sort-merge technique, is subject to significant CPU and I/O bottlenecks as well as blocking: In particular, we found that the sort step is CPU-intensive, whereas the merge step is potentially blocking and can incur significant I/O costs due to intermediate data. Furthermore, MapReduce Online&rsquo;s pipelining functionality only redistributes workloads between the map and reduce tasks, and is not effective for reducing blocking or I/O overhead. </span></p><p class="c81 c97"><span class="c1">Building on these benchmarking results, in this paper we perform an in-depth analysis of Hadoop, using a theoretically sound analyt- ical model to explain the empirical results. Given the complexity of the Hadoop software and its myriad of configuration parameters, </span></p><p class="c133"><span class="c1">985 </span></p><p class="c28"><span class="c1">we seek to understand whether the above performance limitations are inherent to Hadoop or whether tuning of key system parameters can overcome these drawbacks, from the standpoint of incremental one-pass analytics. Our key results are two-fold: We show that our analytical model can be used to choose appropriate values of Hadoop parameters, thereby reducing I/O and startup costs. However, both theoretical and empirical analyses show that the sort-merge im- plementation, used to support partitioning and parallel processing, poses a fundamental barrier to incremental one-pass analytics. De- spite a range of optimizations, I/O and CPU bottlenecks as well as blocking persists, and the reduce progress falls significantly behind the map progress. </span></p><p class="c28"><span class="c1">We next propose a new data analysis platform, based on MapRe- duce, that is geared for incremental one-pass analytics. Based on the insights from our experimental and analytical evaluation of current platforms, we design two key mechanisms into MapReduce: </span></p><p class="c28"><span class="c1">Our first mechanism replaces the sort-merge implementation in MapReduce with a purely hash-based framework, which is designed to address the computational and I/O bottlenecks as well as blocking behavior of sort-merge. We devise two hash techniques to suit different user reduce functions, depending on whether the reduce function permits incremental processing. Besides eliminating the sorting cost from the map tasks, these hash techniques enable fast in- memory processing of the reduce function when the memory reaches a sufficient size as determined by the workload and algorithm. </span></p><p class="c28"><span class="c1">Our second mechanism further brings the benefits of fast in- memory processing to workloads that require a large key-state space that far exceeds available memory. We propose an efficient tech- nique to identify frequent keys and then update their states using a full in-memory processing path, both saving I/Os and enabling early answers for these keys. Less frequent keys trigger I/Os to stage data to disk but have limited impact on the overall efficiency. </span></p><p class="c28"><span class="c1">We have built a prototype of our incremental one-pass analyt- ics platform on Hadoop 0.20.1. Using a range of workloads in click stream analysis and web document analysis, our results show that our hash techniques significantly improve the progress of the map tasks, due to the elimination of sorting, and given sufficient memory, enable fast in-memory processing of the reduce function. For challenging workloads that require a large key-state space, our frequent-key mechanism significantly reduces I/Os and enables the reduce progress to keep up with the map progress, thereby realizing incremental processing. For instance, for sessionization over a click stream, the reducers output user sessions as data is read and finish as soon as all mappers finish reading the data in 34.5 minutes, trig- gering only 0.1GB internal data spill to disk in the job. In contrast, the original Hadoop system returns all the results towards the end of the 81 minute job, writing 370GB internal data spill to disk. </span></p><p class="c6"><span class="c10">2. BACKGROUND </span></p><p class="c28"><span class="c1">To provide a technical context for the discussion in this paper, we begin with background on MapReduce systems, and summarize the key results of our recent benchmarking study.</span><span class="c32">1 </span></p><p class="c6"><span class="c10">2.1 The MapReduce Model </span></p><p class="c28"><span class="c1">At the API level, the MapReduce </span><span class="c13">programming model </span><span class="c1">simply includes two functions: The </span><span class="c21">map </span><span class="c1">function transforms input data into </span><span class="c16">&#12296;</span><span class="c1">key, value</span><span class="c16">&#12297; </span><span class="c1">pairs, and the </span><span class="c21">reduce </span><span class="c1">function is applied to each list of values that correspond to the same key. This programming model abstracts away complex distributed systems issues, thereby providing users with rapid utilization of computing resources. </span></p><p class="c6"><span class="c56">1</span><span class="c20">Details of our benchmarking study can be found in our technical report [4]. </span></p><p class="c6"><span class="c44">Data Data Load </span></p><p class="c6"><span class="c44">Load </span></p><p class="c6"><span class="c44">Map( ) </span></p><p class="c6"><span class="c44">Map( ) </span></p><p class="c6"><span class="c44">Local Sort </span></p><p class="c6"><span class="c44">... </span></p><p class="c6"><span class="c44">Local Sort </span></p><p class="c6"><span class="c44">* Combine </span></p><p class="c6"><span class="c44">* Combine </span></p><p class="c6"><span class="c44">Map Write </span></p><p class="c6"><span class="c44">Map Write </span></p><p class="c7"><span class="c44">Merge &amp; * Combine </span></p><p class="c6"><span class="c44">Reduce( ) </span></p><p class="c6"><span class="c44">Final Write </span></p><p class="c6"><span class="c8">Shuf</span><span class="c76">fl</span><span class="c8">e </span></p><p class="c7"><span class="c44">* optional </span><span class="c1">Figure 1: </span><span class="c9">Architecture of the Hadoop implementation of MapReduce. </span></p><p class="c28"><span class="c1">To achieve parallelism, the MapReduce system essentially imple- ments &ldquo;</span><span class="c13">group data by key, then apply the reduce function to each group</span><span class="c1">&quot;. This </span><span class="c13">computation model</span><span class="c1">, referred to as MapReduce group- by, permits parallelism because both the extraction of </span><span class="c16">&#12296;</span><span class="c1">key, value</span><span class="c16">&#12297; </span><span class="c1">pairs and the application of the reduce function to each group can be performed in parallel on many nodes. The system code of MapRe- duce implements this computation model (and other functionality such as load balancing and fault tolerance). </span></p><p class="c28"><span class="c1">The MapReduce program of an analytical query includes both the map and reduce functions compiled from the query (e.g., us- ing a MapReduce-based query compiler [16]) and the MapReduce system&rsquo;s code for parallelism. </span></p><p class="c6"><span class="c10">2.2 Common MapReduce Implementations </span></p><p class="c28"><span class="c1">Hadoop. We first consider Hadoop, the most popular open-source implementation of MapReduce. Hadoop uses block-level schedul- ing and a sort-merge technique [21] to implement the group-by functionality for parallel processing (Google&rsquo;s MapReduce system is reported to use a similar implementation [7], but further details are lacking due to the use of proprietary code). </span></p><p class="c28"><span class="c1">The Hadoop Distributed File System (HDFS) handles the reading of job input data and writing of job output data. The unit of data storage in HDFS is a 64MB block by default and can be set to other values during configuration. These blocks serve as the task granularity for MapReduce jobs. </span></p><p class="c28"><span class="c1">Given a query job, several map tasks (mappers) and reduce tasks (reducers) are started to run concurrently on each node. As Fig. 1 shows, each mapper reads a chunk of input data, applies the map function to extract </span><span class="c16">&#12296;</span><span class="c1">key, value</span><span class="c16">&#12297; </span><span class="c1">pairs, then assigns these data items to partitions that correspond to different reducers, and finally sorts the data items in each partition by the key. Hadoop currently performs a </span><span class="c13">sort </span><span class="c1">on the compound </span><span class="c16">&#12296;</span><span class="c1">partition, key</span><span class="c16">&#12297; </span><span class="c1">to achieve both partitioning and sorting in each partition. Given the relatively small block size, a properly-tuned buffer will allow such sorting to complete in memory. Then the sorted map output is written to disk for fault tolerance. A mapper completes after the write finishes. </span></p><p class="c28"><span class="c1">Map output is then shuffled to the reducers. To do so, reducers pe- riodically poll a centralized service asking about completed mappers and once notified, requests data directly from the completed map- pers. In most cases, this data transfer happens soon after a mapper completes and so this data is available in the mapper&rsquo;s memory. </span></p><p class="c28"><span class="c1">Over time, a reducer collects pieces of sorted output from many completed mappers. Unlike before, this data cannot be assumed to fit in memory for large workloads. As the reducer&rsquo;s buffer fills up, these sorted pieces of data are merged and written to a file on </span></p><p class="c6"><span class="c1">986 </span></p><p class="c51"><span class="c44">Merge &amp; * Combine ... </span></p><p class="c6"><span class="c44">Reduce( ) </span></p><p class="c6"><span class="c44">Final Write </span></p><p class="c6"><span class="c1">(f) </span><span class="c9">MR Online: CPU iowait. </span><span class="c1">Figure 2: Experimental results using the sessionization workload. </span></p><p class="c28"><span class="c1">disk. A background thread merges these on-disk files progressively whenever the number of such files exceeds a threshold (in a so-called </span><span class="c13">multi-pass merge </span><span class="c1">phase). When a reducer has collected all of the map output, it will proceed to complete the multi-pass merge so that the number of on-disk files becomes less than the threshold. Then it will perform a final merge to produce all </span><span class="c16">&#12296;</span><span class="c1">key, value</span><span class="c16">&#12297; </span><span class="c1">pairs in sorted order of the key. As the final merge proceeds, the reducer applies the reduce function to each group of values that share the same key, and writes the reduce output back to HDFS. </span></p><p class="c28"><span class="c1">Additionally, if the reduce function is commutative and asso- ciative, as shown in Fig. 1, a </span><span class="c21">combine </span><span class="c1">function is applied after the map function to perform partial aggregation. It can be further applied in each reducer when its input data buffer fills up. </span></p><p class="c28"><span class="c1">MapReduce Online. We next consider an advanced system, MapReduce Online, that implements a Hadoop Online Prototype (HOP) with pipelining of data [6]. This prototype has two unique features: First, as each mapper produces output, it can push data ea- gerly to the reducers, with the granularity of transmission controlled by a parameter. Second, an adaptive mechanism is used to balance the work between the mappers and reducers. A potential benefit of HOP is that with pipelining, reducers receive map output earlier and can begin multi-pass merge earlier, thereby reducing the time required for the multi-pass merge after all mappers finish. </span></p><p class="c6"><span class="c10">2.3 Summary of Benchmarking Results </span></p><p class="c6"><span class="c1">The requirements for scalable streaming analytics&mdash;</span><span class="c13">incremental processing </span><span class="c1">and </span><span class="c13">fast in-memory processing </span><span class="c1">whenever possible&mdash;require the MapReduce program of a query to be non-blocking and have low CPU and I/O overheads. In our recent benchmarking study, we examined whether current MapReduce systems meet these require- ments. We considered applications such as click stream analysis and web document analysis in our benchmark. Due to space constraints, we mainly report results on click stream analysis in this section. </span></p><p class="c28"><span class="c1">Given a click stream, an important task is sessionization that reorders page clicks into individual user sessions. In its MapReduce program, the map function extracts the user id from each click and groups the clicks by user id. The reduce function arranges the clicks </span></p><p class="c6"><span class="c1">Table 1: Workloads in click analysis and Hadoop running time. </span></p><p class="c6"><span class="c9">Metric </span><span class="c3">Sessionization Page frequency Clicks per user </span><span class="c9">Input 256GB 508GB 256GB Map output 269GB 1.8GB 2.6 GB Reduce spill 370GB 0.2GB 1.4 GB Reduce output 256GB 0.02GB 0.6GB Running time 4860 sec 2400 sec 1440 sec </span></p><p class="c28"><span class="c1">of each user by timestamp, streams out the clicks of the current session, and closes the session if the user has had no activity in the past 5 minutes. A large amount of intermediate data occurs in this task due to the reorganization of all the clicks by user id. Other click analysis tasks include counting the number of visits to each url and counting the number of clicks that each user has made. For these problems, using a combine function can significantly reduce intermediate data sizes. Our study used the click log from the World Cup 1998 website </span><span class="c32">2 </span><span class="c1">and replicated it to larger sizes as needed. </span></p><p class="c28"><span class="c1">Our test cluster contains ten compute nodes and one head node. It runs CentOS 5.4, Sun Java 1.6u16, and Hadoop 0.20.1. Each compute node has 4 2.83GHz Intel Xeon cores, 8GB RAM, a 250GB Western Digital RE3 HDD, and a 64GB Intel X25-E SSD. The Hadoop configuration used the default settings and 4 reducers per node unless stated otherwise. The JVM heap size was 1GB, and map and reduce buffers were about 140MB and 500MB, respectively. All I/O operations used the disk as the default storage device. </span></p><p class="c28"><span class="c1">Table 1 shows the running time of the workloads as well as the sizes of input, output, and intermediate data in click stream analysis. Fig. 2(a) shows the task timeline for the sessionization workload, i.e., the number of tasks for the four main operations: </span><span class="c13">map </span><span class="c1">(including sorting), </span><span class="c13">shuffle</span><span class="c1">, </span><span class="c13">merge </span><span class="c1">(the multi-pass part), and </span><span class="c13">reduce </span><span class="c1">(including the final merge to produce a single sorted run). In this task, time is roughly evenly split between the map and reduce phases. A key observation is that the CPU utilization, as shown in Fig. 2(b), is low in an extended period (from time 1,800 to 2,400) after all mappers have finished. The CPU iowait in Fig. 2(c) shows that this is largely due to disk I/O requests in multi-pass merge, as further verified by </span></p><p class="c6"><span class="c56">2</span><span class="c20">http://ita.ee.lbl.gov/html/contrib/WorldCup.html </span></p><p class="c6"><span class="c1">987 </span></p><p class="c6"><span class="c1">(a) </span><span class="c9">Hadoop: Task timeline. </span></p><p class="c6"><span class="c1">(c) </span><span class="c9">Hadoop: CPU iowait. </span></p><p class="c6"><span class="c1">(d) </span><span class="c9">Hadoop: CPU utilization (HDD+SSD). </span><span class="c1">(e) </span><span class="c9">MR Online: CPU utilization. </span></p><p class="c6"><span class="c1">(b) </span><span class="c9">Hadoop: CPU utilization. </span></p><p class="c28"><span class="c1">the number of bytes read from disk in that period. Multi-pass merge is not only I/O intensive but also blocking&mdash;the reduce function cannot be applied until all the data has been merged into a sorted run. To reduce disk contention, we used additional hardware so that the disk handled only the input and output with HDFS, and all the intermediate data was passed to a fast SSD. As Fig. 2(d) shows, such change can reduce overall running time but does not eliminate the I/O bottleneck or blocking incurred in the multi-pass merge. </span></p><p class="c28"><span class="c1">Another main observation regards the CPU cost. We observe from Fig. 2(b) that CPUs are busy in the map phase. However, the map function in the sessionization workload is CPU light: it simply extracts the user id from each click record and emits a key-value pair where the value contains the rest of the record. The rest of the cost in the map phase is attributed to sorting of the map output buffer. </span></p><p class="c28"><span class="c1">In simpler workloads, such as counting the number of clicks per user, there is an effective combine function to reduce the size of intermediate data. As intermediate data is reduced, the merge phase shrinks as there is less data to merge, and then the reduce phase also shrinks as most data is processed in memory only. However, the overhead of sorting becomes more dominant in the overall cost. </span></p><p class="c28"><span class="c1">We finally considered MapReduce Online. Fig. 2(e) shows that CPU utilization still has low values in the middle of the job. While CPU can be idle due to I/O wait or network wait (given the different communication model used here), the CPU iowait in Fig. 2(f) again shows a spike in the middle of the job. Hence, the problems of blocking and I/O activity due to multi-pass merge persist. A final note is that pipelining does not reduce the total sort-merge cost but only rebalances the work between the mappers and reducers. </span></p><p class="c6"><span class="c1">In summary, our benchmarking study made several key observa- tions of the sort-merge implementation of MapReduce group-by: </span></p><p class="c6"><span class="c69">&#9654; </span><span class="c1">The sorting step of sort-merge incurs high CPU cost, hence </span></p><p class="c6"><span class="c1">not suitable for fast in-memory processing. </span><span class="c69">&#9654; </span><span class="c1">Multi-pass merge in sort-merge is blocking and can incur high I/O cost given substantial intermediate data, hence a poor fit for incremental processing or fast in-memory processing. </span><span class="c69">&#9654; </span><span class="c1">Using extra storage devices and alternative storage architec- </span></p><p class="c7"><span class="c1">tures does not eliminate blocking or the I/O bottleneck. </span><span class="c69">&#9654; </span><span class="c1">The Hadoop Online Prototype with pipelining does not elimi- nate blocking, the I/O bottleneck, or the CPU bottleneck. </span></p><p class="c6"><span class="c10">3. OPTIMIZING HADOOP </span></p><p class="c6"><span class="c1">Building on our previous benchmarking results, we perform an in- depth analysis of Hadoop in this section. Our goal is to understand whether the performance issues identified by our benchmarking study are inherent to Hadoop or whether they can be overcome by appropriate tuning of key system parameters. </span><span class="c10">3.1 An Analytical Model for Hadoop </span></p><p class="c28"><span class="c1">The Hadoop system has a large number of parameters. While our previous experiments used the default settings, we examine these parameters more carefully in this study. After a nearly year-long effort to experiment with Hadoop, we identified several parameters that impact performance from the standpoint of incremental one- pass analytics, which are listed in Part (1) of Table 2. Our analysis below focuses on the effects of these parameters on I/O and startup costs. We do not aim to model the actual running time because it depends on numerous factors such as the actual server configuration, how map and reduces tasks are interleaved, how CPU and I/O operations are interleaved, and even how simultaneous I/O requests are served. Once we optimize these parameters based on our model, we will evaluate performance empirically using the actual running time and the progress with respect to incremental processing. </span></p><p class="c6"><span class="c1">Table 2: Symbols used in Hadoop analysis. </span><span class="c9">Symbol Description (1) System Settings </span></p><p class="c6"><span class="c53">R </span><span class="c9">Number of reduce tasks per node </span><span class="c53">C </span><span class="c9">Map input chunk size </span><span class="c53">F </span><span class="c9">Merge factor that controls how often on-disk files are merged (2) Workload Description </span></p><p class="c6"><span class="c53">D </span><span class="c9">Input data size </span><span class="c53">KK</span><span class="c60">m r </span><span class="c20">Ratio of output size to input size for the map function </span></p><p class="c6"><span class="c20">Ratio of output size to input size for the reduce function </span><span class="c9">(3) Hardware Resources </span></p><p class="c7"><span class="c53">N </span><span class="c9">Number of nodes in the cluster </span><span class="c53">BB</span><span class="c60">m r </span><span class="c9">Output buffer size per map task </span></p><p class="c6"><span class="c20">Shuffle buffer size per reduce task </span><span class="c9">(4) Symbols Used in the Analysis </span></p><p class="c6"><span class="c53">U </span><span class="c9">Bytes read and written per node, </span><span class="c53">U U</span><span class="c27">1</span><span class="c9">: </span><span class="c60">i </span><span class="c9">map is the number input; </span><span class="c27">2</span><span class="c9">: of bytes of the following </span><span class="c92">= </span><span class="c53">U</span><span class="c80">1 </span><span class="c33">+ </span><span class="c9">types </span></p><p class="c6"><span class="c89">... </span><span class="c33">+ </span><span class="c94">U</span><span class="c80">5 </span><span class="c9">map internal spills; </span><span class="c27">3</span><span class="c9">: map output; </span></p><p class="c6"><span class="c20">where </span></p><p class="c7"><span class="c27">4</span><span class="c9">: reduce internal spills; </span><span class="c27">5</span><span class="c9">: reduce output </span><span class="c53">ST </span><span class="c60">i </span><span class="c9">Number of sequential I/O requests per node for IO type </span><span class="c53">i </span></p><p class="c7"><span class="c9">Time measurement for startup and I/O cost </span><span class="c53">h </span><span class="c9">Height of the tree structure for multi-pass merge </span></p><p class="c28"><span class="c1">Our analysis makes several assumptions for simplicity: The MapReduce job under consideration does not use a combine func- tion. Each reducer processes an equal number of </span><span class="c16">&#12296;</span><span class="c1">key, value</span><span class="c16">&#12297; </span><span class="c1">pairs. Finally, when a reducer pulls a mapper for data, the mapper has just finished so its output can be read directly from its local memory. The last assumption frees us from the onerous task of modeling the caching behavior at each node in a highly complex system. </span></p><p class="c28"><span class="c1">1. Modeling I/O Cost in Bytes. We first analyze the I/O cost of the </span><span class="c13">existing </span><span class="c1">sort-merge implementation of Hadoop. We summarize our main result in the following proposition. </span></p><p class="c28"><span class="c1">Proposition 3.1 </span><span class="c13">Given the workload description (</span><span class="c0">D</span><span class="c13">, </span><span class="c0">K</span><span class="c17">m</span><span class="c23">, </span><span class="c37">K</span><span class="c17">r</span><span class="c23">) and </span><span class="c13">the hardware description (</span><span class="c0">N</span><span class="c13">, </span><span class="c0">B</span><span class="c17">m</span><span class="c23">, </span><span class="c37">B</span><span class="c17">r</span><span class="c23">), as defined in Table 2, the I/O </span><span class="c13">cost in terms of bytes read and written in a Hadoop job is: </span></p><p class="c6"><span class="c0">U </span><span class="c14">= </span><span class="c41">D</span><span class="c0">N </span><span class="c63">&middot; </span><span class="c110">(</span><span class="c19">1 </span><span class="c110">+ </span><span class="c41">K</span><span class="c35">m </span><span class="c14">+ </span><span class="c0">K</span><span class="c17">m</span><span class="c37">K</span><span class="c17">r</span><span class="c68">) + </span><span class="c5">2</span><span class="c0">D</span><span class="c37">CN </span><span class="c16">&middot; </span><span class="c0">&lambda;</span><span class="c17">F</span><span class="c14">(</span><span class="c41">CK</span><span class="c0">B</span><span class="c17">m </span><span class="c35">m </span></p><p class="c6"><span class="c5">, </span><span class="c0">B</span><span class="c17">m</span><span class="c68">) </span><span class="c50">&middot; </span></p><p class="c6"><span class="c49">[</span><span class="c35">C</span><span class="c25">&middot;</span><span class="c35">K</span><span class="c24">m</span><span class="c107">&gt;</span><span class="c17">B</span><span class="c24">m</span><span class="c66">] </span><span class="c14">+ </span><span class="c5">2</span><span class="c0">R </span><span class="c16">&middot; </span><span class="c0">&lambda;</span><span class="c17">F</span><span class="c14">( </span><span class="c0">NRB</span><span class="c41">DK</span><span class="c35">m </span></p><p class="c6"><span class="c17">r </span><span class="c5">, </span><span class="c0">B</span><span class="c17">r</span><span class="c68">)</span><span class="c45">, </span><span class="c58">(1) </span><span class="c13">where </span></p><p class="c6"><span class="c49">[</span><span class="c25">&middot;</span><span class="c49">] </span><span class="c98">is an indicator function, and </span><span class="c41">&lambda;</span><span class="c35">F</span><span class="c110">(</span><span class="c63">&middot;</span><span class="c110">) </span><span class="c98">is defined to be: </span></p><p class="c6"><span class="c0">&lambda;</span><span class="c17">F</span><span class="c14">(</span><span class="c0">n</span><span class="c5">, </span><span class="c0">b</span><span class="c14">) = </span></p><p class="c6"><span class="c5">( 2</span><span class="c0">F</span><span class="c14">(</span><span class="c0">F </span><span class="c45">1 </span></p><p class="c6"><span class="c16">&minus; </span><span class="c5">1</span><span class="c14">)</span><span class="c41">n</span><span class="c47">2 </span><span class="c14">+ </span><span class="c19">3</span><span class="c5">2</span><span class="c41">n </span><span class="c63">&minus; </span><span class="c5">2</span><span class="c14">(</span><span class="c0">F </span><span class="c41">F</span><span class="c16">&minus; </span><span class="c47">2 </span></p><p class="c6"><span class="c5">1</span><span class="c14">)</span><span class="c19">) </span></p><p class="c6"><span class="c16">&middot; </span><span class="c0">b</span><span class="c5">. </span><span class="c1">(2) </span></p><p class="c51"><span class="c13">Analysis</span><span class="c1">. Our analysis includes five I/O-types listed in Table 2. Each map task reads a data chunk of size </span><span class="c0">C </span><span class="c1">as input, and writes </span><span class="c0">C </span><span class="c16">&middot; </span><span class="c0">K</span><span class="c17">m </span><span class="c1">tasks in </span><span class="c58">bytes as output. </span><span class="c1">total and </span><span class="c0">D</span><span class="c5">/</span><span class="c14">(</span><span class="c0">C </span><span class="c58">Given the workload </span><span class="c37">D</span><span class="c58">, we have </span><span class="c16">&middot; </span><span class="c0">N</span><span class="c14">) </span><span class="c1">map tasks per node. So, the </span><span class="c37">D</span><span class="c45">/</span><span class="c37">C </span><span class="c1">input </span><span class="c58">map </span><span class="c1">cost, </span><span class="c0">U</span><span class="c4">1</span><span class="c1">, and output cost, </span><span class="c0">U</span><span class="c4">3</span><span class="c1">, of all map tasks on a node are: </span></p><p class="c6"><span class="c0">U</span><span class="c4">1 </span><span class="c14">= </span><span class="c41">D</span><span class="c0">N </span><span class="c74">and </span><span class="c41">U</span><span class="c84">3 </span><span class="c110">= </span><span class="c41">D </span><span class="c63">&middot; </span><span class="c0">N </span><span class="c41">K</span><span class="c35">m </span></p><p class="c6"><span class="c19">. </span></p><p class="c6"><span class="c1">The size of the reduce output on each node is </span><span class="c0">U</span><span class="c4">5 </span><span class="c68">= </span><span class="c1">Map and reduce internal spills result from the multi-pass </span><span class="c35">D</span><span class="c25">&middot;</span><span class="c35">K</span><span class="c17">N </span><span class="c24">m</span><span class="c22">&middot;</span><span class="c17">K</span><span class="c1">merge </span></p><p class="c6"><span class="c24">r </span><span class="c5">. </span></p><p class="c28"><span class="c1">operation, which can take place in a map task if the map output exceeds the memory size and hence needs to use external sorting, or in a reduce task if the reduce input data does not fit in memory. </span></p><p class="c51"><span class="c1">We make a general analysis of multi-pass merge first. Suppose that our task is to merge </span><span class="c0">n </span><span class="c1">sorted runs, each of size </span><span class="c0">b</span><span class="c1">. As these initial sorted runs are generated, they are written to spill files on disk as background </span><span class="c0">f</span><span class="c4">1</span><span class="c5">, </span><span class="c0">f</span><span class="c4">2</span><span class="c5">,... </span><span class="c1">Whenever thread the number merges the files on disk reaches </span><span class="c5">2</span><span class="c0">F </span><span class="c16">&minus; </span><span class="c5">1</span><span class="c1">, a </span><span class="c13">smallest </span><span class="c0">F </span><span class="c1">files into a new file on </span></p><p class="c6"><span class="c1">988 </span></p><p class="c6"><span class="c106">Final Merged File </span></p><p class="c6"><span class="c143">hh-1</span><span class="c118">21 </span></p><p class="c6"><span class="c42">... </span></p><p class="c6"><span class="c1">Figure 3: </span><span class="c9">Analysis of the tree of files created in multi-pass merge. </span></p><p class="c51"><span class="c1">disk. this We label the new merged files as process, where an unshaded box </span><span class="c0">m</span><span class="c1">denotes </span><span class="c4">1</span><span class="c5">, </span><span class="c0">m</span><span class="c4">2</span><span class="c5">,... </span><span class="c1">an Fig. 3 illustrates initial spill file and a shaded box denotes a merged file. For example, after the first and decreasing the </span><span class="c5">2</span><span class="c0">F </span><span class="c16">&minus; </span><span class="c1">resulting </span><span class="c5">1 </span><span class="c1">size. initial Similarly, files runs on generated, disk after are the </span><span class="c0">m</span><span class="c1">first </span><span class="c0">f</span><span class="c4">11</span><span class="c5">, ,..., </span><span class="c0">f</span><span class="c17">F</span><span class="c66">+</span><span class="c4">1</span><span class="c0">F</span><span class="c47">2 </span><span class="c14">+ </span><span class="c0">f</span><span class="c5">,..., </span><span class="c17">F </span><span class="c0">F </span><span class="c1">are </span><span class="c16">&minus; </span><span class="c0">f</span><span class="c1">merged </span><span class="c4">2</span><span class="c17">F</span><span class="c22">&minus;</span><span class="c4">1 </span><span class="c5">1 </span><span class="c1">initial in together order runs of are generated, the files Among the resulting them, files </span><span class="c0">m</span><span class="c4">1</span><span class="c5">, </span><span class="c1">decreasing size. After </span><span class="c0">f</span><span class="c1">on </span><span class="c17">F</span><span class="c2">2</span><span class="c1">on </span><span class="c66">+</span><span class="c4">1</span><span class="c1">the disk </span><span class="c5">,..., </span><span class="c1">disk initial will are </span><span class="c0">f</span><span class="c17">F</span><span class="c1">runs, </span><span class="c2">2</span><span class="c1">be </span><span class="c66">+</span><span class="c17">F</span><span class="c22">&minus;</span><span class="c4">1 </span><span class="c0">m</span><span class="c4">1</span><span class="c0">m</span><span class="c5">,..., </span><span class="c1">a </span><span class="c17">F</span><span class="c66">+</span><span class="c4">1</span><span class="c1">will final </span><span class="c5">, </span><span class="c0">mm</span><span class="c1">be merge </span><span class="c17">F</span><span class="c4">2</span><span class="c5">, ,..., </span><span class="c1">merged </span><span class="c0">f</span><span class="c17">F</span><span class="c2">2</span><span class="c66">+</span><span class="c4">1</span><span class="c1">combines </span><span class="c0">m</span><span class="c5">,..., </span><span class="c17">F </span><span class="c1">together in </span><span class="c0">f</span><span class="c17">F</span><span class="c2">2</span><span class="c66">+</span><span class="c17">F</span><span class="c22">&minus;</span><span class="c4">1</span><span class="c1">and . </span></p><p class="c51"><span class="c1">order of all the remaining files (there are at most </span><span class="c5">2</span><span class="c0">F </span><span class="c16">&minus; </span><span class="c5">1 </span><span class="c1">of them). For the (</span><span class="c5">2 </span><span class="c16">&le; </span><span class="c0">i </span><span class="c16">&le; </span><span class="c1">this recursively size of all analysis, let </span><span class="c0">&alpha;h</span><span class="c1">) the and files gives let in </span><span class="c0">&alpha;</span><span class="c1">the </span><span class="c4">1 </span><span class="c0">&alpha;</span><span class="c14">= </span><span class="c17">i i </span><span class="c14">= </span><span class="c1">denote </span><span class="c0">b</span><span class="c1">. </span><span class="c14">(</span><span class="c0">i </span><span class="c1">Then </span><span class="c16">&minus; </span><span class="c5">1</span><span class="c14">)</span><span class="c0">Fb </span><span class="c1">the </span><span class="c0">&alpha;</span><span class="c17">i </span><span class="c1">size </span><span class="c14">= </span><span class="c16">&minus; </span><span class="c1">first </span><span class="c0">h </span><span class="c1">levels is: </span></p><p class="c51"><span class="c1">of a merged file on level </span><span class="c0">i &alpha;</span><span class="c14">(</span><span class="c0">i </span><span class="c17">i</span><span class="c22">&minus;</span><span class="c4">1 </span><span class="c16">&minus; </span><span class="c14">+ (</span><span class="c0">F </span><span class="c16">&minus; </span><span class="c5">1</span><span class="c14">)</span><span class="c0">b</span><span class="c1">. Solving </span><span class="c5">2</span><span class="c14">)</span><span class="c0">b</span><span class="c1">. Hence, the total </span></p><p class="c6"><span class="c0">F</span><span class="c14">(</span><span class="c0">&alpha;</span><span class="c17">h </span><span class="c14">+ </span></p><p class="c6"><span class="c42">... ... ... </span></p><p class="c6"><span class="c42">... </span></p><p class="c6"><span class="c42">... ... ... ... ... ... ... ... ... </span><span class="c38">1 F</span><span class="c104">2 </span><span class="c38">+1 </span><span class="c34">... </span><span class="c38">F</span><span class="c104">2</span><span class="c38">+F-1 </span></p><p class="c6"><span class="c38">2 </span><span class="c34">... </span></p><p class="c6"><span class="c42">... </span></p><p class="c6"><span class="c82">F </span><span class="c34">... </span></p><p class="c6"><span class="c38">1 2 </span><span class="c34">... </span><span class="c38">F </span></p><p class="c6"><span class="c38">F+1 F+2 </span><span class="c34">... </span><span class="c38">2F </span></p><p class="c6"><span class="c42">... </span></p><p class="c6"><span class="c38">F</span><span class="c104">2</span><span class="c38">-F+1 F</span><span class="c104">2</span><span class="c38">-F+2 </span><span class="c34">... </span><span class="c38">F</span><span class="c104">2 </span></p><p class="c6"><span class="c35">h</span><span class="c25">&minus;</span><span class="c84">1</span><span class="c90">C</span><span class="c5">) </span></p><p class="c6"><span class="c5">. </span><span class="c17">i</span><span class="c66">=</span><span class="c4">1</span><span class="c1">If we count all the spill files (unshaded boxes) in the tree, we have </span><span class="c0">n </span><span class="c14">= (</span><span class="c0">F </span><span class="c14">+ (</span><span class="c0">F </span><span class="c16">&minus; </span><span class="c5">1</span><span class="c14">)(</span><span class="c0">h </span><span class="c16">&minus; </span><span class="c5">2</span><span class="c14">))</span><span class="c0">F</span><span class="c1">. Then we substitute </span><span class="c0">h </span><span class="c1">with </span><span class="c0">n </span><span class="c1">and </span><span class="c0">F </span><span class="c1">using the above formula and get </span></p><p class="c6"><span class="c0">&lambda;</span><span class="c17">F</span><span class="c14">(</span><span class="c0">n</span><span class="c5">, </span><span class="c0">b</span><span class="c14">) = </span></p><p class="c6"><span class="c5">(</span><span class="c37">hF </span><span class="c68">+ </span><span class="c14">(</span><span class="c0">F </span><span class="c16">&minus; </span><span class="c5">1</span><span class="c14">)(</span><span class="c0">h </span><span class="c16">&minus; </span><span class="c5">2 </span><span class="c14">(</span><span class="c0">&alpha;</span><span class="c17">i </span><span class="c14">+(</span><span class="c0">F</span><span class="c16">&minus;</span><span class="c5">1</span><span class="c14">)</span><span class="c0">b</span><span class="c14">)) = </span><span class="c0">bF </span></p><p class="c6"><span class="c5">2</span><span class="c14">)(</span><span class="c0">h </span><span class="c14">+ </span><span class="c5">1</span><span class="c14">) </span></p><p class="c6"><span class="c5">( 2</span><span class="c0">F</span><span class="c14">(</span><span class="c0">F </span><span class="c45">1 </span></p><p class="c6"><span class="c16">&minus; </span><span class="c5">1</span><span class="c14">)</span><span class="c41">n</span><span class="c47">2 </span><span class="c14">+ </span><span class="c19">3</span><span class="c5">2</span><span class="c41">n </span><span class="c63">&minus; </span><span class="c5">2</span><span class="c14">(</span><span class="c0">F </span><span class="c41">F</span><span class="c16">&minus; </span><span class="c47">2 </span></p><p class="c6"><span class="c5">1</span><span class="c14">)</span><span class="c19">) </span></p><p class="c6"><span class="c16">&middot; </span><span class="c0">b </span></p><p class="c51"><span class="c1">Then, the total read once. The I/O remaining cost is </span><span class="c5">2</span><span class="c0">&lambda;</span><span class="c1">issue </span><span class="c17">F</span><span class="c14">(</span><span class="c0">n</span><span class="c5">, </span><span class="c1">is </span><span class="c0">b</span><span class="c14">) </span><span class="c1">to as each file is written once and derive the exact numbers for </span><span class="c0">n </span><span class="c1">and </span><span class="c0">b </span><span class="c1">in the multi-pass merge in a map or reduce task. </span></p><p class="c51"><span class="c1">In a map task, if its output fits in the map buffer, then the merge operation is not needed. Otherwise, we use the available memory to produce sorted runs So, </span><span class="c0">b </span><span class="c14">= </span><span class="c0">B</span><span class="c17">m </span><span class="c58">and </span><span class="c37">n </span><span class="c68">= </span><span class="c1">tasks, we have the I/O </span><span class="c35">C</span><span class="c25">&middot;</span><span class="c35">K</span><span class="c1">cost </span><span class="c17">B</span><span class="c1">of </span><span class="c24">m m </span><span class="c1">size . for As </span><span class="c0">B</span><span class="c1">map each </span><span class="c17">m </span><span class="c58">each and later merge them back. </span><span class="c1">node handles </span><span class="c0">D</span><span class="c5">/</span><span class="c14">(</span><span class="c0">C </span><span class="c16">&middot; </span><span class="c0">N</span><span class="c14">) </span><span class="c1">map internal spills on this node as: </span></p><p class="c6"><span class="c0">U</span><span class="c4">2 </span><span class="c14">= </span></p><p class="c6"><span class="c5">{ </span><span class="c17">C</span><span class="c22">&middot;</span><span class="c17">N </span><span class="c4">2</span><span class="c17">D</span><span class="c5">0 </span><span class="c16">&middot; </span><span class="c0">&lambda;</span><span class="c17">F</span><span class="c14">( </span><span class="c77">C</span><span class="c48">&middot;</span><span class="c77">K</span><span class="c17">B</span><span class="c24">m </span><span class="c78">m </span><span class="c5">, </span><span class="c0">B</span><span class="c17">m</span><span class="c68">) </span><span class="c58">if </span><span class="c37">C </span><span class="c50">&middot; </span><span class="c37">K</span><span class="c17">m </span><span class="c65">&gt; </span><span class="c37">B</span><span class="c17">m</span><span class="c45">; </span></p><p class="c6"><span class="c1">otherwise</span><span class="c5">. </span></p><p class="c28"><span class="c1">In a reduce task, as we do not have a combine function, the input for reduce usually cannot fit in memory. The size of input to each reduce task is handles </span><span class="c0">R </span><span class="c1">reduce </span><span class="c77">D</span><span class="c48">&middot;</span><span class="c77">K</span><span class="c17">N</span><span class="c22">&middot;</span><span class="c17">R </span><span class="c1">tasks, </span><span class="c78">m </span><span class="c1">. So, </span><span class="c0">b </span><span class="c14">= </span><span class="c1">we have </span><span class="c0">B</span><span class="c17">r </span><span class="c58">and </span><span class="c37">n </span><span class="c1">the reduce </span><span class="c68">= </span><span class="c1">internal </span><span class="c17">N</span><span class="c22">&middot;</span><span class="c17">R</span><span class="c22">&middot;</span><span class="c17">B</span><span class="c35">D</span><span class="c25">&middot;</span><span class="c35">K</span><span class="c24">m r </span><span class="c1">. spill As each node </span></p><p class="c6"><span class="c1">cost: </span></p><p class="c6"><span class="c0">U</span><span class="c4">4 </span><span class="c14">= </span><span class="c5">2</span><span class="c0">R </span><span class="c16">&middot; </span><span class="c0">&lambda;</span><span class="c17">F</span><span class="c14">( </span><span class="c0">N </span><span class="c41">D </span><span class="c16">&middot; </span><span class="c63">&middot; </span><span class="c41">K</span><span class="c35">m </span></p><p class="c7"><span class="c0">R </span><span class="c16">&middot; </span><span class="c0">B</span><span class="c17">r </span><span class="c5">, </span><span class="c0">B</span><span class="c17">r</span><span class="c68">) </span><span class="c1">Summing up </span><span class="c0">U</span><span class="c4">1</span><span class="c5">,...,</span><span class="c0">U</span><span class="c4">5</span><span class="c1">, we then have Eq. 1 in the proposition. </span></p><p class="c6"><span class="c1">2. Modeling the Number of I/O requests. The following proposi- tion models the number of I/O requests in a Hadoop job. </span></p><p class="c28"><span class="c1">Proposition 3.2 </span><span class="c13">Given the workload description (</span><span class="c0">D</span><span class="c13">, </span><span class="c0">K</span><span class="c17">m</span><span class="c23">, </span><span class="c37">K</span><span class="c17">r</span><span class="c23">) and </span><span class="c13">the hardware description (</span><span class="c0">N</span><span class="c13">, </span><span class="c0">B</span><span class="c17">m</span><span class="c23">, </span><span class="c37">B</span><span class="c17">r</span><span class="c23">), as defined in Table 2, the </span><span class="c13">number of I/O requests in a Hadoop job is: </span></p><p class="c6"><span class="c0">S </span><span class="c14">= </span><span class="c0">CN </span></p><p class="c6"><span class="c41">D</span><span class="c5">(</span><span class="c37">&lambda;</span><span class="c17">F</span><span class="c14">(</span><span class="c0">&alpha;</span><span class="c5">,1</span><span class="c14">)(</span><span class="c63">&radic;</span><span class="c0">F </span><span class="c14">+ </span><span class="c5">1</span><span class="c14">)</span><span class="c47">2 </span><span class="c14">+ </span><span class="c0">&alpha; </span><span class="c16">&minus; </span><span class="c5">1</span><span class="c19">)) </span></p><p class="c6"><span class="c14">+ </span><span class="c0">R </span></p><p class="c6"><span class="c5">(</span><span class="c37">&alpha; </span><span class="c68">+ </span><span class="c45">1 </span><span class="c68">+ </span><span class="c5">(</span><span class="c37">&beta;K</span><span class="c17">r</span><span class="c68">(</span><span class="c16">&radic;</span><span class="c37">F </span><span class="c68">+ </span><span class="c45">1</span><span class="c68">) </span><span class="c50">&minus; </span><span class="c37">&beta;</span><span class="c16">&radic;</span><span class="c37">F </span><span class="c68">+ </span><span class="c37">&lambda;</span><span class="c17">F</span><span class="c14">(</span><span class="c0">&beta;</span><span class="c5">,1</span><span class="c14">)(</span><span class="c63">&radic;</span><span class="c0">F </span><span class="c14">+ </span><span class="c5">1</span><span class="c14">)</span><span class="c47">2</span><span class="c19">) </span></p><p class="c6"><span class="c5">, </span><span class="c1">(3) </span></p><p class="c6"><span class="c13">where </span><span class="c0">&alpha; </span><span class="c14">= </span><span class="c13">an indicator </span><span class="c77">CK</span><span class="c13">function. </span></p><p class="c6"><span class="c17">B</span><span class="c24">m </span><span class="c78">m </span><span class="c13">, </span><span class="c0">&beta; </span><span class="c14">= </span><span class="c17">NRB</span><span class="c77">DK</span><span class="c78">m </span><span class="c24">r </span><span class="c13">, </span><span class="c0">&lambda;</span><span class="c17">F</span><span class="c14">(</span><span class="c16">&middot;</span><span class="c14">) </span><span class="c13">is defined in Eq. 2, and </span></p><p class="c6"><span class="c49">[</span><span class="c25">&middot;</span><span class="c49">] </span><span class="c98">is </span></p><p class="c28"><span class="c1">Our analysis again considers the five types of I/O in Table 2, but it is more involved than the analysis above due to the need to consider memory allocation. Details are left to [4] due to space constraints. We note that for common workloads, the I/O cost is dominated by the cost of reading and writing all the bytes, not the seek time. </span></p><p class="c28"><span class="c1">3. Modeling the Startup Cost. Since the number of map tasks is usually much larger than reduce tasks, we consider the startup cost for map tasks. If </span><span class="c0">c</span><span class="c17">m </span><span class="c1">total map startup cost </span><span class="c58">is the cost in second of creating </span><span class="c1">per node is </span><span class="c0">c</span><span class="c17">start </span><span class="c50">&middot; </span><span class="c17">CN</span><span class="c35">D</span><span class="c5">. </span></p><p class="c6"><span class="c58">a map task, the </span></p><p class="c51"><span class="c1">4. Combining All in Time Measurement. Let </span><span class="c0">U </span><span class="c1">be the number of bytes read and written in a Hadoop job and let </span><span class="c0">S </span><span class="c1">be the number of byte define I/O and requests the </span><span class="c0">c</span><span class="c17">seek </span><span class="c1">time denote made. measurement the Let disk </span><span class="c0">c</span><span class="c17">byte </span><span class="c0">T </span><span class="c1">seek denote that time combines the for sequential each the I/O cost I/O request. of time reading We per </span></p><p class="c6"><span class="c1">and writing all the bytes, the seek cost of all I/O requests, and the map startup cost as follows: </span></p><p class="c6"><span class="c0">T </span><span class="c14">= </span><span class="c0">c</span><span class="c17">byte </span><span class="c16">&middot; </span><span class="c0">U </span><span class="c14">+ </span><span class="c0">c</span><span class="c17">seek </span><span class="c16">&middot; </span><span class="c0">S </span><span class="c14">+ </span><span class="c0">c</span><span class="c17">start </span><span class="c50">&middot; </span><span class="c0">D</span><span class="c37">CN</span><span class="c5">. </span><span class="c1">(4) The above formula is our complete analytical model that captures the effects of the involved parameters. </span><span class="c10">3.2 Optimizing Hadoop based on the Model </span></p><p class="c28"><span class="c1">Our analytical model enables us to predict system behaviors as Hadoop parameters vary. Then, given a workload and system config- uration, we can choose values of these parameters that minimize the time cost in our model, thereby optimizing Hadoop performance. </span></p><p class="c28"><span class="c1">Optimizations. To show the effectiveness of our model, we com- pare the predicted system behavior based on our model and the actual running time measured in our Hadoop cluster. We used the sessionization task and configured the workload, our cluster, and Hadoop as follows: (1) Workload: </span><span class="c0">D</span><span class="c1">=97GB, </span><span class="c0">K</span><span class="c17">m</span><span class="c58">=</span><span class="c37">K</span><span class="c17">r</span><span class="c58">=1;</span><span class="c56">3 </span><span class="c58">(2) Hard- </span><span class="c1">ware: </span><span class="c0">N</span><span class="c1">=10, </span><span class="c0">B</span><span class="c17">m</span><span class="c58">=140MB, </span><span class="c37">B</span><span class="c17">r</span><span class="c58">=260MB; (3) Hadoop: </span><span class="c37">R</span><span class="c58">=4 or 8, and </span><span class="c1">varied values of </span><span class="c0">C </span><span class="c1">and </span><span class="c0">F</span><span class="c1">. We also fed these parameter values to our analytical model. In addition, we set the constants in our model by assuming sequential disk access speed to be 80MB/s, disk seek time to be 4 ms, and the map task startup cost to be 100 ms. </span></p><p class="c28"><span class="c13">Our first goal is to validate our model. </span><span class="c1">In our experiment, we var- ied the map input chunk size, </span><span class="c0">C</span><span class="c1">, and the the merge factor, </span><span class="c0">F</span><span class="c1">. Under 100 different combinations of </span><span class="c14">(</span><span class="c0">C</span><span class="c5">, </span><span class="c0">F</span><span class="c14">)</span><span class="c1">, we measured the running time in a real Hadoop system, and calculated the time cost predicted by our model. The result is shown as a 3-D plot in Fig. 4(a).</span><span class="c32">4 </span><span class="c1">Note that our goal is not to compare the absolute values of these two time measurements: In fact, they are not directly comparable, as the former is simply a linear combination of the startup cost and I/O </span></p><p class="c28"><span class="c56">3</span><span class="c20">We used a smaller dataset in this set of experiments compared to the bench- </span><span class="c9">mark because changing Hadoop configurations often required reloading data into HDFS, which was very time-consuming. </span><span class="c144">4</span><span class="c20">For either real running time or modeled time cost, the 100 data points were </span><span class="c9">interpolated into a finer-grained mesh. </span></p><p class="c6"><span class="c1">989 </span></p><p class="c6"><span class="c49">[</span><span class="c35">CK</span><span class="c24">m</span><span class="c107">&gt;</span><span class="c17">B</span><span class="c24">m</span><span class="c66">] </span><span class="c16">&middot; </span></p><p class="c6"><span class="c1">(f) </span><span class="c9">Progress report using MR online </span></p><p class="c6"><span class="c1">Figure 4: Results of optimizing Hadoop parameters. </span></p><p class="c28"><span class="c1">costs based on our model, whereas the latter is the actual running time affected by many system factors as stated above. Instead, we expect our model to predict the changes of the time measurement when parameters are tuned, so as to identify the optimal parameter setting. Fig. 4(a) shows that indeed the performance predicted by our model and the actual running time exhibit very similar trends as the parameters </span><span class="c0">C </span><span class="c1">and </span><span class="c0">F </span><span class="c1">are varied. </span></p><p class="c28"><span class="c13">Our next goal is to show how to optimize the parameters based on our model. </span><span class="c1">To reveal more details from the 3-D plot, we show the results of a smaller range of </span><span class="c14">(</span><span class="c0">C</span><span class="c5">, </span><span class="c0">F</span><span class="c14">) </span><span class="c1">in Fig. 4(b), where the solid lines are for the actual running time and the dashed lines are for predication using our model. </span></p><p class="c51"><span class="c13">(1) Optimizing the Chunk Size</span><span class="c1">. When the chunk size </span><span class="c0">C </span><span class="c1">is very small, the MapReduce job uses many map tasks and the map startup cost dominates. As </span><span class="c0">C </span><span class="c1">increases, the map startup cost reduces, but once the map output exceeds its buffer size, multi-pass merge is incurred with increased I/O cost. The time cost jumps up at this point, and then remains nearly constant since the reduction of startup cost is not significant. When </span><span class="c0">C </span><span class="c1">exceeds a large size (whose exact value depends on the merge factor), the number of passes of on-disk merge goes up, thus incurring more I/Os. Overall, good performance is observed at the maximum value of </span><span class="c0">C </span><span class="c1">that allows the map output to fit in the buffer. Given a particular workload, we can easily estimate estimate map </span><span class="c0">K</span><span class="c17">m</span><span class="c58">, the </span><span class="c1">memory </span><span class="c58">ratio </span><span class="c1">the map </span><span class="c58">of </span><span class="c1">size </span><span class="c58">output </span><span class="c1">output (given </span><span class="c58">size </span><span class="c1">buffer the </span><span class="c58">to </span><span class="c1">use </span><span class="c58">input </span><span class="c1">size of other </span><span class="c58">size, </span><span class="c0">B</span><span class="c17">m </span><span class="c58">to for </span><span class="c1">metadata). </span><span class="c58">be the about map </span><span class="c84">2</span><span class="c4">3 </span><span class="c1">Then </span><span class="c58">function </span><span class="c1">of the we total </span><span class="c58">and </span></p><p class="c6"><span class="c1">can choose the maximum </span><span class="c0">C </span><span class="c1">such that </span><span class="c0">C </span><span class="c16">&middot; </span><span class="c0">K</span><span class="c17">m </span><span class="c50">&le; </span><span class="c37">B</span><span class="c17">m</span><span class="c58">. </span></p><p class="c28"><span class="c13">(2) Optimizing the Merge Factor</span><span class="c1">. We then investigate the merge factor, </span><span class="c0">F</span><span class="c1">, that controls how frequently on-disk files are merged in the multi-pass merge phase. Fig. 4(b) shows three curves for three </span><span class="c0">F </span><span class="c1">values. The time cost decreases with larger values of </span><span class="c0">F </span><span class="c1">(from 4 to 16), mainly due to fewer I/O bytes incurred in the multi-pass merge. When </span><span class="c0">F </span><span class="c1">goes up to the number of initial sorted runs (around 16), the time cost does not decrease further because all the runs are merged </span></p><p class="c6"><span class="c1">in a single pass. For several other workloads tested, one-pass merge was also observed to provide the best performance. </span></p><p class="c28"><span class="c1">Our model can also reveal potential benefits of small </span><span class="c0">F </span><span class="c1">values. When </span><span class="c0">F </span><span class="c1">is small, the number of files to merge in each step is small, so the reads of the input files and the writes of the output file are mostly sequential I/O. As such, a smaller </span><span class="c0">F </span><span class="c1">value incurs more I/O bytes, but fewer disk seeks. According to our model, the benefits of small </span><span class="c0">F </span><span class="c1">values can be shown only when the system is given limited memory but a very large data set, e.g., several terabytes per node, which is beyond the current storage capacity of our cluster. </span></p><p class="c28"><span class="c13">(3) Effect of the Number of Reducers</span><span class="c1">. The third relevant parameter is the number of reducers per node, </span><span class="c0">R</span><span class="c1">. The original MapReduce proposal [7] has recommended </span><span class="c0">R </span><span class="c1">to be the number of cores per node times a small constant (e.g., 1 or 2). As this parameter does not change the workload but only distributes it over a variable number of reduce workers, our model shows little difference as </span><span class="c0">R </span><span class="c1">varies. Empirically, we varied </span><span class="c0">R </span><span class="c1">from 4 to 8 (given 4 cores on each node) while configuring </span><span class="c0">C </span><span class="c1">and </span><span class="c0">F </span><span class="c1">using the most appropriate values as reported above. Interestingly, the run with </span><span class="c0">R</span><span class="c1">=4 took 4,187 seconds, whereas the run with </span><span class="c0">R</span><span class="c1">=8 took 4,723 seconds. The reasons are two- fold. First, by tuning the merge factor, </span><span class="c0">F</span><span class="c1">, we have minimized the work in multi-pass merge. Second, given 4 cores on each node, we have only 4 reduce task slots per node. Then for </span><span class="c0">R</span><span class="c1">=8, the reducers are started in two waves. In the first wave, 4 reducers are started. As some of these reducers finish, a reducer in the second wave can be started. As a consequence, the reducers in the first wave can read map output soon after their map tasks finish, hence directly from the local memory. In contrast, the reducers in the second wave are started long after the mappers have finished. So they have to fetch map output from disks, hence incurring high I/O costs in shuffling. Our conclusion is that optimizing the merge factor, </span><span class="c0">F</span><span class="c1">, can reduce the actual I/O cost in multi-pass merge, and is a more effective method than enlarging the number of reducers beyond the number of reduce task slots available at each node. </span></p><p class="c6"><span class="c1">990 </span></p><p class="c6"><span class="c30">2400 </span></p><p class="c6"><span class="c71">) s(e mitg ninnu</span><span class="c31">r</span><span class="c30">2000 1600 </span></p><p class="c6"><span class="c30">1200 </span></p><p class="c6"><span class="c31">Hadoop </span></p><p class="c6"><span class="c30">800 </span></p><p class="c6"><span class="c30">400 </span></p><p class="c6"><span class="c31">model </span></p><p class="c6"><span class="c1">(a) </span><span class="c31">m</span><span class="c18">ap </span><span class="c9">Comparing </span><span class="c18">inp</span><span class="c30">0 </span></p><p class="c6"><span class="c18">u</span><span class="c30">16 </span><span class="c18">t c</span><span class="c83">32 </span><span class="c18">hu</span><span class="c83">64</span><span class="c18">nk </span><span class="c83">128 </span><span class="c18">size </span><span class="c9">running </span><span class="c83">512 </span><span class="c18">(M</span><span class="c30">4 </span><span class="c18">B) </span></p><p class="c6"><span class="c9">time in the real system and time measurement in our model </span></p><p class="c6"><span class="c12">8 12 16 20 24 </span><span class="c18">sort-merge factor </span></p><p class="c6"><span class="c1">(d) </span><span class="c9">CPU utilization </span></p><p class="c6"><span class="c30">F=4, Hadoop </span><span class="c71">) s(p oodaHn ie mitg ninnu</span><span class="c31">r</span><span class="c30">2200F=4, model </span></p><p class="c6"><span class="c12">1600 </span><span class="c30">F=10, Hadoop </span></p><p class="c6"><span class="c30">2000 </span></p><p class="c6"><span class="c30">F=10, model F=16, Hadoop </span></p><p class="c6"><span class="c12">1400 </span><span class="c30">F=16, model </span></p><p class="c6"><span class="c30">1800 </span><span class="c12">1200 </span><span class="c30">1600 </span><span class="c12">1000 </span><span class="c30">800 </span><span class="c83">16 32 64 128 256 512 </span><span class="c1">(b) </span><span class="c9">Effects of the map input chunk size </span><span class="c53">C </span><span class="c9">and the </span></p><p class="c6"><span class="c74">(c) </span><span class="c129">Progress of incremental processing </span><span class="c9">merge factor </span><span class="c53">F </span><span class="c9">on time measurements </span><span class="c30">1400 </span></p><p class="c6"><span class="c71">) s(l edomy bt nemerusaeme mi</span><span class="c31">tmap input chunk size (MB) </span></p><p class="c6"><span class="c1">(e) </span><span class="c9">CPU IOwait </span></p><p class="c28"><span class="c1">We also compared the I/O costs predicated by our model and those actually observed. Not only do we see matching trends, the predicted numbers are also close to the actual numbers, with less than 10% difference. (For details, see [4].) All of the above results show that given a particular workload and hardware configuration, one can run our model to find the optimal values of the chunk size </span><span class="c0">C </span><span class="c1">and merge factor </span><span class="c0">F</span><span class="c1">, and choose an appropriate value of </span><span class="c0">R </span><span class="c1">based on the recommendation above. </span></p><p class="c28"><span class="c1">Analysis of Optimized Hadoop. We finally reran the 240GB ses- sionization workload described in our benchmark (see &sect;2). We optimized Hadoop using 64MB data chunks, one-pass merge, and 4 reducers per node as suggested by the above results. The total running time was reduced from 4,860 seconds to 4,187 seconds, a 14% reduction of the total running time. </span></p><p class="c28"><span class="c1">Given our goal of one-pass analytics, a key requirement is to perform </span><span class="c13">incremental processing </span><span class="c1">and deliver a query answer as soon as all relevant data has arrived. In this regard, we propose metrics for the map and reduce progress, as defined below. </span></p><p class="c28"><span class="c1">Definition 1 (Incremental Map and Reduce Progress) </span><span class="c13">The map progress is defined to be the percentage of map tasks that have completed. The reduce progress is defined to be: </span><span class="c47">1</span><span class="c84">3</span><span class="c63">&middot; </span><span class="c98">% of shuffle </span><span class="c13">tasks completed + </span><span class="c47">1</span><span class="c84">3</span><span class="c63">&middot; </span><span class="c98">% of combine function or reduce function </span><span class="c13">completed + </span><span class="c47">1</span><span class="c84">3</span><span class="c63">&middot; </span><span class="c98">% of reduce output produced. </span></p><p class="c28"><span class="c1">Note that our definition differs from the default Hadoop progress metric where the reduce progress includes the work on multi-pass merge. In contrast, we discount multi-pass merge because it is irrelevant to a user query, and emphasize the actual work on the reduce function or combine function and the output of answers. </span></p><p class="c28"><span class="c1">Fig. 4(c) shows the progress of optimized Hadoop in bold lines (and the progress of stock Hadoop in thin lines as a reference). The map progress increases steadily and reaches 100% around 2,000 seconds. The reduce progress increases to around 33% in these 200 seconds, mainly because the shuffle progress would keep up with the map progress. Then the reduce progress slows down, due to the overhead of merging, and lags far behind the map progress. The optimal reduce progress, as marked by a dashed line in this plot, keeps up with the map progress, thereby realizing fast incremental processing. As can be seen, there is a big gap between the opti- mal reduce progress and what the optimized Hadoop can currently achieve. </span></p><p class="c6"><span class="c1">Fig. 4(d) and 4(e) further show the CPU utilization and CPU iowait using optimized Hadoop. We make two main observations: (1) The CPU utilization exhibits a smaller dip in the middle of a job compared to stock Hadoop in Fig. 2(b). However, the CPU cycles consumed by the mappers, shown as the area under the curves before 2,000 seconds, are about the same as those using stock Hadoop. Hence, the CPU overhead due to sorting, as mentioned in our benchmark, still exists. (2) The CPU iowait plot still shows a spike in the middle of job, due to the blocking of CPU by the I/O operations in the remaining single-pass merge. </span><span class="c10">3.3 Pipelining in Hadoop </span></p><p class="c28"><span class="c1">Another attempt to optimize Hadoop for one-pass analytics would be to pipeline data from mappers to reducers so that reducers can start the work earlier. This idea has been implemented in MapRe- duce Online [6], as described in &sect; 2.2. In our benchmark, we observe that pipelining data from mappers to reducers can result in small performance benefits. For instance, for sessionization, Fig. 4(f) shows 5% improvement in total running time over the version of stock Hadoop, 0.19.2, on which MapReduce Online&rsquo;s code is based. </span></p><p class="c28"><span class="c1">However, many concerns remain: (1) The overall performance gain of MapReduce Online over Hadoop is small (e.g., 5%), less that the gain of our model-based optimization (e.g., 14%). (2) The reduce progress lags far behind the map progress, as shown in Fig. 4(f). (3) The CPU utilization and iowait in MapReduce Online, shown in Fig. 2(e) and 2(f), still show the blocking and high I/O overhead due to multi-pass merge. (4) MapReduce Online has an extension to pe- riodically output snapshots (e.g., when reducers have received 25%, 50%, 75%, ..., of the data). However, this is done by repeating the merge operation for each snapshot, not by incremental processing. It can incur high I/O overhead and significantly increased running time. (The interested reader is referred to [4] for the details.) </span></p><p class="c6"><span class="c1">We close the discussion in this section with the summary below: </span></p><p class="c6"><span class="c69">&#9654; </span><span class="c1">Our analytical model can be used to choose appropriate values </span></p><p class="c6"><span class="c1">of Hadoop parameters, thereby improving performance. </span><span class="c69">&#9654; </span><span class="c1">Optimized Hadoop, however, still has a significant barrier to fast incremental processing: (1) The remaining one-pass merge can still incur blocking and a substantial I/O cost. (2) The reduce progress falls far behind the map progress. (3) The map tasks still have the high CPU cost of sorting. </span><span class="c69">&#9654; </span><span class="c1">Pipelining from mappers to reducers does not resolve the blocking and I/O overhead in Hadoop, and achieves only a small performance gain over stock Hadoop. </span></p><p class="c6"><span class="c10">4. A NEW HASH-BASED PLATFORM </span></p><p class="c28"><span class="c1">Based on the insights from our experimental and analytical evalu- ation of current MapReduce systems, we next propose a new data analysis platform that transforms MapReduce computation into in- cremental one-pass processing. Our first mechanism replaces the widely used sort-merge implementation for partitioning and par- allel processing with a purely hash-based framework to minimize computational and I/O bottlenecks as well as blocking. Two hash techniques, designed for different types of reduce functions, are described in &sect;4.1 and &sect;4.2, respectively. These techniques enable fast in-memory processing when there is sufficient memory for the current workload. Our second mechanism further brings the bene- fits of fast in-memory processing to workloads that require a large key-state space that far exceeds available memory. Our technique efficiently identifies popular keys and updates their states using a full in-memory processing path. This mechanism is detailed in &sect;4.3. </span></p><p class="c6"><span class="c10">4.1 A Basic Hash Technique (MR-hash) </span></p><p class="c28"><span class="c1">Recall from Section 2 that to support parallel processing, the MapReduce computation model essentially implements &ldquo;</span><span class="c13">group data by key, then apply the reduce function to each group</span><span class="c1">&quot;. The main idea underlying our hash framework is to implement the MapReduce group-by functionality using a series of independent hash functions </span><span class="c0">h</span><span class="c4">1</span><span class="c1">, </span><span class="c0">h</span><span class="c4">2</span><span class="c1">, </span><span class="c0">h</span><span class="c4">3</span><span class="c1">, </span><span class="c5">...</span><span class="c1">, across the mappers and reducers. </span></p><p class="c28"><span class="c1">As depicted in Fig. 5(a), the hash function </span><span class="c0">h</span><span class="c4">1 </span><span class="c1">partitions the map output into subsets corresponding to the scheduled reducers. Hash functions </span><span class="c0">h</span><span class="c4">2</span><span class="c1">, </span><span class="c0">h</span><span class="c4">3</span><span class="c1">, </span><span class="c5">...</span><span class="c1">, are used to implement (recursive) partitioning at each reducer. More specifically, </span><span class="c0">h</span><span class="c4">2 </span><span class="c1">partitions the input data to a reducer to </span><span class="c0">n </span><span class="c1">buckets, where the first bucket, say, </span><span class="c0">D</span><span class="c4">1</span><span class="c1">, is held completely in memory and other buckets are streamed out to disks as their write buffers fill up (which is similar to hybrid hash join [18]). This way, we can perform group-by on </span><span class="c0">D</span><span class="c4">1 </span><span class="c1">using the hash function </span><span class="c0">h</span><span class="c4">3 </span><span class="c1">and apply the reduce function to each group completely in memory. Other buckets are processed subsequently, one at a time, by reading the data from the disk. If a bucket </span><span class="c0">D</span><span class="c17">i </span><span class="c1">fits in memory, we use in-memory processing for the group-by and the reduce function; otherwise, we further partition it using hash function </span><span class="c0">h</span><span class="c4">4</span><span class="c1">, and so on. </span></p><p class="c6"><span class="c1">991 </span></p><p class="c7"><span class="c26">reduce() hash h2 </span></p><p class="c6"><span class="c1">(b) </span><span class="c9">Dynamic Inc-hash: monitoring keys and updating states. </span></p><p class="c6"><span class="c1">Figure 5: Hash techniques in our new data analysis platform. </span></p><p class="c6"><span class="c1">In our implementation, we use standard universal hashing to endure that the hash functions are independent of each other. </span></p><p class="c51"><span class="c1">Following the analysis of the hybrid hash join [18], simple calcu- lation shows that if </span><span class="c0">h</span><span class="c4">2 </span><span class="c1">can evenly distribute the data into buckets, recursive than and number the </span><span class="c5">2</span><span class="c19">&radic;</span><span class="c1">of I/Os </span><span class="c16">|</span><span class="c0">D</span><span class="c1">partitioning buckets, </span><span class="c17">r</span><span class="c50">|</span><span class="c58">, </span><span class="c1">involve </span><span class="c58">where </span><span class="c0">h</span><span class="c1">, </span><span class="c50">|</span><span class="c37">D</span><span class="c5">2</span><span class="c14">(</span><span class="c16">|</span><span class="c0">D</span><span class="c1">can is </span><span class="c17">r</span><span class="c50">| </span><span class="c1">not </span><span class="c58">is </span><span class="c1">be </span><span class="c17">r</span><span class="c50">|&minus;|</span><span class="c37">D</span><span class="c58">the </span><span class="c1">needed derived </span><span class="c58">size </span><span class="c4">1</span><span class="c16">|</span><span class="c14">) </span><span class="c58">of </span><span class="c1">if from bytes the </span><span class="c58">the </span><span class="c1">the memory </span><span class="c58">data </span><span class="c1">read standard </span><span class="c58">sent </span><span class="c1">and size </span><span class="c58">to </span><span class="c1">written. </span><span class="c58">the </span><span class="c1">analysis is </span><span class="c58">reducer, </span><span class="c1">greater </span></p><p class="c51"><span class="c1">The by solving a quadratic equation. </span></p><p class="c51"><span class="c1">The above technique, called MR-hash, exactly matches the cur- rent MapReduce model that collects all the values of the same key into a list and feeds the entire list to the reduce function. This base- line technique in our work is similar to the hash technique used in parallel databases [9], but implemented in the MapReduce con- text. Compared to stock Hadoop, MR-hash offers several benefits: First, on the mapper side, it avoids the CPU cost of sorting as in the sort-merge implementation. Second, it allows early answers to be returned for the buffered bucket, If the application specifies a range </span><span class="c0">D</span><span class="c1">of </span><span class="c4">1</span><span class="c1">keys , as data to be comes more important to the reducer. than others, we can design </span><span class="c0">h</span><span class="c4">2 </span><span class="c1">so that </span><span class="c0">D</span><span class="c4">1 </span><span class="c1">contains those important keys. </span></p><p class="c6"><span class="c10">4.2 An Incremental Hash Technique (INC-hash) </span><span class="c1">Our second hash technique is designed for reduce functions that permit incremental processing, including simple aggregates like sum and count, and more complex problems that have been studied in the area of sublinear-space stream algorithms [14]. In our work, we define three functions to implement incremental processing: The initialize function, </span><span class="c0">init</span><span class="c14">()</span><span class="c1">, reduces a sequence of data items to a state; The combine function, </span><span class="c0">cb</span><span class="c14">()</span><span class="c1">, reduces a sequence of states to a state; and the finalize function, </span><span class="c0">fn</span><span class="c14">()</span><span class="c1">, produces a final answer from a state. The initialize function is applied immediately when the map function finishes processing. This changes the data in subsequent processing from the original key-value pairs to key-state pairs. The combine function can be applied to any intermediate step that collects a set of states for the same key, e.g., in the write buffers of a reducer that pack data to write to disks. Finally, the original reduce function is implemented by </span><span class="c0">cb</span><span class="c14">() </span><span class="c1">followed by </span><span class="c0">fn</span><span class="c14">()</span><span class="c1">. </span></p><p class="c28"><span class="c1">Such incremental processing can offer several benefits: The ini- tialize function reduces the amount of data output from the mappers. In addition, existing data items can be collapsed to a compact </span><span class="c13">state </span><span class="c1">so that the reducer no longer needs to hold all the data in memory. Furthermore, as a result of incremental processing, query answers can be derived as soon as the relevant data is available, e.g., when the count in a group exceeds a query-specified threshold or when a window closes in window-based stream processing. </span></p><p class="c6"><span class="c1">To realize the above benefits, we propose an alternative incre- </span></p><p class="c6"><span class="c26">Bucket 1 Bucket 2 ... </span></p><p class="c6"><span class="c26">Bucket h ... </span></p><p class="c51"><span class="c1">mental hash implementation, called INC-hash. The algorithm is illustrated in Fig. 5(b) (the reader can ignore the darkened boxes for now, as they are used only in the third technique). As a re- ducer receives map output, which includes key-state pairs created by the initialize function, called tuples for simplicity, we build an in-memory key to the state hashtable of computation. </span><span class="c0">H </span><span class="c1">(using hash When function a new tuple </span><span class="c0">h</span><span class="c4">2</span><span class="c1">) that arrives, maps if from its key a </span></p><p class="c51"><span class="c1">already exists in </span><span class="c0">H</span><span class="c1">, we update the key&rsquo;s state with the new tuple using the combine function. If its key does not exist in </span><span class="c0">H</span><span class="c1">, we add a new key-state pair to </span><span class="c0">H </span><span class="c1">if there is still memory. Otherwise, we hash the this tuple bucket, (using and </span><span class="c0">h</span><span class="c1">flush </span><span class="c4">3</span><span class="c1">) to the a bucket, write buffer place the when tuple it becomes in the write full. buffer When of </span></p><p class="c28"><span class="c1">the reducer has seen all the tuples and output the results for all the keys in </span><span class="c0">H</span><span class="c1">, it then reads disk-resident buckets back one at a time, repeating the procedure above to process each bucket. </span></p><p class="c51"><span class="c1">The analysis of INC-hash turns out to be similar to that in Hy- brid Cache for handling expensive predicates [10]. We summarize our main results below. The key improvement of INC-hash over MR-hash is that for those keys in </span><span class="c0">H</span><span class="c1">, their tuples are continuously collapsed into states in memory, avoiding I/Os for those tuples al- together. I/Os will be completely eliminated in INC-hash if the memory is large enough to hold all </span><span class="c13">distinct </span><span class="c1">key-state pairs, whose When size is memory denoted size by </span><span class="c5">A</span><span class="c1">, is less in contrast than </span><span class="c5">A </span><span class="c1">but to all greater the data than items </span><span class="c63">&radic;</span><span class="c5">A</span><span class="c1">, we in MR-hash. can show that tuples that belong to </span><span class="c0">H </span><span class="c1">are simply collapsed into the states in memory, and other tuples are written out and read back exactly once&mdash;no recursive partitioning is needed in INC-hash. The number of buckets, </span><span class="c0">h</span><span class="c1">, can be derived directly from this analysis. </span></p><p class="c6"><span class="c10">4.3 A Dynamic Incremental Hash Technique </span></p><p class="c28"><span class="c1">Our last technique is an extension of the incremental hash ap- proach where we dynamically determine which keys should be processed in memory and which keys will be written to disk for subsequent processing. The basic idea behind the new technique is to recognize hot keys that appear frequently in the data set and hold their states in memory, hence providing incremental in-memory processing for these keys. The benefits of doing so are two-fold. First, prioritizing these keys leads to greater I/O efficiency since in-memory processing of data items of hot keys can greatly decrease the volume of data that needs to be first written to disks and then read back to complete the processing. Second, it is often the case that the answers for the hot keys are more important to the user than the colder keys. Then this technique offers the user the ability to terminate the processing before data is read back from disk if the coverage of data is sufficiently large for those keys in memory. </span></p><p class="c6"><span class="c1">Below we assume that we do not have enough memory to hold </span></p><p class="c6"><span class="c1">992 </span></p><p class="c6"><span class="c26">Bucket 1 </span></p><p class="c7"><span class="c26">reduce() hash h3 </span></p><p class="c6"><span class="c26">map() hash h1 </span></p><p class="c6"><span class="c26">Bucket </span><span class="c139">i </span></p><p class="c7"><span class="c26">reduce() hash h3 </span></p><p class="c6"><span class="c99">(key, value) </span></p><p class="c6"><span class="c26">Bucket 2 ... </span></p><p class="c6"><span class="c26">Bucket h File of Bucket 2 </span></p><p class="c6"><span class="c26">... </span></p><p class="c6"><span class="c1">(a) </span><span class="c9">MR-hash: hashing in mappers and two phase hash processing in reducers. </span></p><p class="c6"><span class="c26">hash h2 </span></p><p class="c7"><span class="c26">File of Bucket h </span></p><p class="c6"><span class="c99">(key, state) </span></p><p class="c6"><span class="c11">t_1 c_1 c_2 </span></p><p class="c6"><span class="c11">t_2 </span></p><p class="c6"><span class="c11">c_s </span></p><p class="c6"><span class="c11">t_s </span></p><p class="c6"><span class="c99">state s_1 state s_2 </span></p><p class="c6"><span class="c99">state s_s </span></p><p class="c7"><span class="c26">memory or disk? </span></p><p class="c6"><span class="c99">key k_1 key k_2 </span></p><p class="c6"><span class="c99">key k_s </span></p><p class="c7"><span class="c26">File of Bucket 1 </span></p><p class="c6"><span class="c26">hash h3 </span></p><p class="c6"><span class="c26">File of </span></p><p class="c6"><span class="c26">File of Bucket 2 </span></p><p class="c6"><span class="c26">Bucket h </span></p><p class="c6"><span class="c1">all states of </span><span class="c13">distinct </span><span class="c1">keys. Our mechanism for recognizing and processing hot keys builds upon ideas in an existing data stream algorithm called the </span><span class="c59">FREQUENT </span><span class="c1">algorithm [12, 3] that can be used to estimate the frequency of different values in a data stream. While we are not interested in the frequencies of the keys per se, we will use estimates of the frequency of each key to date to determine which keys should be processed in memory. However, note that other &ldquo;sketch-based&quot; algorithms for estimating frequencies will be unsuitable for our purposes because they do not explicitly encode a set of hot keys; Rather, additional processing is required to determine frequency estimates and then use them to determine approximate hot keys, which is too costly for us to consider. Dynamic Incremental (DINC) Hash. We use the following nota- tion in our discussion of the algorithm: Let </span><span class="c0">K </span><span class="c1">be the total number of </span><span class="c13">distinct </span><span class="c1">keys. Let </span><span class="c0">M </span><span class="c1">be the total number of key-state pairs in input, called tuples for simplicity. Suppose that the memory con- tains </span><span class="c0">B </span><span class="c1">pages, and each page can hold </span><span class="c0">n</span><span class="c17">p </span><span class="c58">key-state pairs with their </span><span class="c1">associated auxiliary information. Let </span><span class="c21">cb </span><span class="c1">be a combine function that combines a state </span><span class="c0">u </span><span class="c1">and a state </span><span class="c0">v </span><span class="c1">to make a new state </span><span class="c21">cb</span><span class="c14">(</span><span class="c0">u</span><span class="c5">, </span><span class="c0">v</span><span class="c14">)</span><span class="c1">. </span></p><p class="c28"><span class="c1">While receiving tuples, each reducer divides the </span><span class="c0">B </span><span class="c1">pages in mem- ory into two parts: </span><span class="c0">h </span><span class="c1">pages are used as write buffers, one for each of </span><span class="c0">h </span><span class="c1">files that will reside on disk, and </span><span class="c0">B </span><span class="c16">&minus; </span><span class="c0">h </span><span class="c1">pages for &ldquo;hot&quot; key-state pairs. Hence, </span><span class="c0">s </span><span class="c14">= (</span><span class="c0">B </span><span class="c16">&minus; </span><span class="c0">h</span><span class="c14">)</span><span class="c0">n</span><span class="c17">p </span><span class="c58">keys can be processed in-memory at </span><span class="c1">any given time.</span><span class="c32">5 </span><span class="c1">Fig. 5(b) illustrates our algorithm. </span></p><p class="c28"><span class="c1">Our algorithm maintains </span><span class="c0">s </span><span class="c1">counters </span><span class="c0">c</span><span class="c14">[</span><span class="c5">1</span><span class="c14">]</span><span class="c5">,..., </span><span class="c0">c</span><span class="c14">[</span><span class="c0">s</span><span class="c14">] </span><span class="c1">and </span><span class="c0">s </span><span class="c1">associated keys </span><span class="c0">k</span><span class="c14">[</span><span class="c5">1</span><span class="c14">]</span><span class="c5">,..., </span><span class="c0">k</span><span class="c14">[</span><span class="c0">s</span><span class="c14">] </span><span class="c1">referred to as &ldquo;the keys currently being moni- tored&quot; together with the state </span><span class="c0">s</span><span class="c14">[</span><span class="c0">i</span><span class="c14">] </span><span class="c1">of a partial computation for each key </span><span class="c0">k</span><span class="c14">[</span><span class="c0">i</span><span class="c14">]</span><span class="c1">. Initially </span><span class="c0">c</span><span class="c14">[</span><span class="c0">i</span><span class="c14">] = </span><span class="c5">0, </span><span class="c0">k</span><span class="c14">[</span><span class="c0">i</span><span class="c14">] = </span><span class="c16">&perp; </span><span class="c1">for all </span><span class="c0">i </span><span class="c16">&isin; </span><span class="c14">[</span><span class="c0">s</span><span class="c14">]</span><span class="c1">. When a new tuple </span><span class="c14">(</span><span class="c0">k</span><span class="c5">, </span><span class="c0">v</span><span class="c14">) </span><span class="c1">arrives, if this key is currently being monitored, </span><span class="c0">c</span><span class="c14">[</span><span class="c0">i</span><span class="c14">] </span><span class="c1">is incremented and </span><span class="c0">s</span><span class="c14">[</span><span class="c0">i</span><span class="c14">] </span><span class="c1">is updated using the combine function. If </span><span class="c0">k </span><span class="c1">is not being monitored and </span><span class="c0">c</span><span class="c14">[</span><span class="c0">j</span><span class="c14">] = </span><span class="c5">0 </span><span class="c1">for some </span><span class="c0">j</span><span class="c1">, then the key-state pair </span><span class="c14">(</span><span class="c0">k</span><span class="c14">[</span><span class="c0">j</span><span class="c14">]</span><span class="c5">, </span><span class="c0">s</span><span class="c14">[</span><span class="c0">j</span><span class="c14">]) </span><span class="c1">is evicted and </span><span class="c14">(</span><span class="c0">c</span><span class="c14">[</span><span class="c0">j</span><span class="c14">]</span><span class="c5">, </span><span class="c0">k</span><span class="c14">[</span><span class="c0">j</span><span class="c14">]</span><span class="c5">, </span><span class="c0">s</span><span class="c14">[</span><span class="c0">j</span><span class="c14">]) </span><span class="c16">&larr; </span><span class="c14">(</span><span class="c5">1, </span><span class="c0">k</span><span class="c5">, </span><span class="c0">v</span><span class="c14">)</span><span class="c1">. If </span><span class="c0">k </span><span class="c1">is not monitored and all </span><span class="c0">c </span><span class="c70">&gt; </span><span class="c5">0</span><span class="c1">, then the tuple needs to be written to disk and all </span><span class="c0">c</span><span class="c14">[</span><span class="c0">i</span><span class="c14">] </span><span class="c1">are decremented by one. Whenever the algorithm decides to evict a key-state pair in-memory or write out a tuple, it always first assigns the item to a hash bucket and then writes it out through the write buffer of the bucket, as in INC-hash. </span></p><p class="c6"><span class="c1">Once the tuples have all arrived, most of the computation for the hot keys may have already been performed. At this point we have the option to terminate if the partial computation for hot keys is &ldquo;good enough&quot; in a sense we will make explicit shortly. If not, we proceed with performing all the remaining computation: we first write out each key-state pair currently in memory to disk to the appropriate bucket file. We then read each bucket file into memory and complete the processing for each key in the bucket file. I/O Analysis. Suppose there are </span><span class="c0">f</span><span class="c17">i </span><span class="c1">tuples with key </span><span class="c0">k</span><span class="c17">i </span><span class="c1">and note that </span><span class="c0">M </span><span class="c14">= </span><span class="c5">C</span><span class="c17">i </span><span class="c0">f</span><span class="c17">i</span><span class="c1">. Without loss of generality assume </span><span class="c0">f</span><span class="c4">1 </span><span class="c16">&ge; </span><span class="c0">f</span><span class="c4">2 </span><span class="c16">&ge; </span><span class="c5">... </span><span class="c16">&ge; </span><span class="c0">f</span><span class="c17">K</span><span class="c1">. Then the best we can hope for is performing </span><span class="c5">C</span><span class="c4">1</span><span class="c22">&le;</span><span class="c17">i</span><span class="c22">&le;</span><span class="c17">s </span><span class="c0">f</span><span class="c17">i </span><span class="c1">steps of in-memory computation as the tuples are being sent to the reducer. This is achieved if we know the &ldquo;hot&quot; keys, i.e., the top-</span><span class="c0">s</span><span class="c1">, in advance. Existing analysis for the </span><span class="c59">FREQUENT </span><span class="c1">algorithm can be applied to our new setting to show that the above strategy guarantees that </span><span class="c0">M </span><span class="c5">:</span><span class="c14">= </span><span class="c5">C</span><span class="c4">1</span><span class="c22">&le;</span><span class="c17">i</span><span class="c22">&le;</span><span class="c17">s </span><span class="c5">max</span><span class="c14">(</span><span class="c5">0, </span><span class="c0">f</span><span class="c17">i </span><span class="c16">&minus; </span><span class="c77">M</span><span class="c35">s</span><span class="c49">+</span><span class="c84">1</span><span class="c110">) </span><span class="c74">combine operations have been </span><span class="c1">performed. Since every tuple that is not combined with an existing state in memory triggers a write-out, the number of tuples written to disk is </span><span class="c0">M </span><span class="c16">&minus; </span><span class="c0">M </span><span class="c14">+ </span><span class="c0">s </span><span class="c1">where the additional </span><span class="c0">s </span><span class="c1">comes from the write out of the hot key-state pairs in main memory. This result compares favorably with the offline optimal if there are some very popular keys, but does not give any guarantee if there are no keys whose relative frequency is more than </span><span class="c5">1/</span><span class="c14">(</span><span class="c0">s </span><span class="c14">+ </span><span class="c5">1</span><span class="c14">)</span><span class="c1">. If the data is skewed, the </span></p><p class="c6"><span class="c56">5</span><span class="c20">If we use </span><span class="c94">p </span><span class="c142">&gt; </span><span class="c89">1 </span><span class="c20">pages for each of the </span><span class="c94">h </span><span class="c20">write buffers (to reduce random- </span><span class="c9">writes), then </span><span class="c53">s </span><span class="c92">= </span><span class="c53">n</span><span class="c60">p </span><span class="c62">&middot;</span><span class="c33">(</span><span class="c94">B </span><span class="c62">&minus; </span><span class="c94">hp</span><span class="c33">)</span><span class="c20">. We omit </span><span class="c94">p </span><span class="c20">below to simplify the discussion. </span></p><p class="c28"><span class="c1">theoretical analysis can be improved [3]. Note that for INC-hash there is no guarantee on the steps of computation performed before the hash files are read back from disk. This is because the keys chosen for in-memory processing are just the first keys observed. </span></p><p class="c28"><span class="c1">After the input is consumed, we write out all key-state pairs from main memory to the appropriate bucket file. Then the number of unique keys corresponding to each bucket file to be </span><span class="c0">K</span><span class="c5">/</span><span class="c0">h</span><span class="c1">. Conse- quently, if </span><span class="c0">K</span><span class="c5">/</span><span class="c0">h </span><span class="c16">&le; </span><span class="c0">B </span><span class="c16">&middot; </span><span class="c0">n</span><span class="c17">p</span><span class="c45">, </span><span class="c58">then the key-state pairs in each bucket can </span><span class="c1">be processed sequentially in memory. Setting </span><span class="c0">h </span><span class="c1">as small as possible increases </span><span class="c0">s </span><span class="c1">and hence decreases </span><span class="c0">M </span><span class="c1">. Hence we set </span><span class="c0">h </span><span class="c14">= </span><span class="c0">Kn</span><span class="c17">p</span><span class="c45">/</span><span class="c37">B</span><span class="c58">. </span></p><p class="c6"><span class="c1">To compare the different hash techniques, first note that the im- provement of INC-hash over MR-hash is only significant when </span><span class="c0">K </span><span class="c1">is small. This is because the keys processed incrementally in main memory will only account for a small fraction of the tuples. DINC- hash mitigates this in the case when, although </span><span class="c0">K </span><span class="c1">may be large, some keys are considerably more frequent then other keys. By ensuring that it is these keys that are usually monitored in memory, we ensure that a large fraction of the tuples are processed before the remaining data is read back from disk. Approximate Answers and Coverage Estimation. One of the fea- tures of DINC-hash is that a large fraction of the combine operations for a very frequent key will already have been performed once all the tuples have arrived. To estimate the number of combine opera- tions performed for a given key we use the </span><span class="c0">t </span><span class="c1">values: these count the number of key-state tuples that have been combined for key </span><span class="c0">k </span><span class="c1">since most recent time </span><span class="c0">k </span><span class="c1">started being monitored. Define the </span><span class="c13">coverage </span><span class="c1">of key </span><span class="c0">k</span><span class="c17">i </span><span class="c1">to be </span></p><p class="c6"><span class="c1">coverage</span><span class="c14">(</span><span class="c0">k</span><span class="c17">i</span><span class="c14">) = </span></p><p class="c6"><span class="c5">{</span><span class="c37">t</span><span class="c68">[</span><span class="c37">j</span><span class="c68">]</span><span class="c45">/</span><span class="c37">f</span><span class="c17">i </span><span class="c5">0 </span><span class="c1">if </span><span class="c0">k</span><span class="c14">[</span><span class="c0">j</span><span class="c14">] = </span><span class="c0">k</span><span class="c17">i </span><span class="c1">for some </span><span class="c0">j </span></p><p class="c6"><span class="c1">otherwise </span><span class="c19">. </span></p><p class="c51"><span class="c1">Hence, once the tuples have arrived, the state corresponding to coverage</span><span class="c14">(</span><span class="c0">kk</span><span class="c17">i </span><span class="c1">we in main-memory represents the computation performed on a do not know </span><span class="c17">i</span><span class="c14">) </span><span class="c1">fraction of all the tuples with this key. Unfortunately the coverage of a monitored key exactly, but it can be shown that we have a reasonably accurate under-estimate: </span></p><p class="c51"><span class="c0">&gamma;</span><span class="c17">i </span><span class="c5">:</span><span class="c14">= </span><span class="c0">t</span><span class="c14">[</span><span class="c0">j</span><span class="c14">]</span><span class="c5">/</span><span class="c14">(</span><span class="c0">t</span><span class="c14">[</span><span class="c0">j</span><span class="c14">] + </span><span class="c0">M</span><span class="c5">/</span><span class="c14">(</span><span class="c0">s </span><span class="c14">+ </span><span class="c5">1</span><span class="c14">)) </span><span class="c16">&le; </span><span class="c0">t</span><span class="c14">[</span><span class="c0">j</span><span class="c14">]</span><span class="c5">/</span><span class="c0">f</span><span class="c17">i </span><span class="c14">= </span><span class="c1">coverage</span><span class="c14">(</span><span class="c0">k</span><span class="c17">i</span><span class="c14">) </span><span class="c5">. </span><span class="c1">Hence, return for a user-determined the state of the threshold partial computation </span><span class="c0">&phi;</span><span class="c1">, if rather </span><span class="c0">&gamma;</span><span class="c17">i </span><span class="c16">&ge; </span><span class="c0">&phi; </span><span class="c1">we can opt to than to complete the computation. </span></p><p class="c6"><span class="c10">5. PROTOTYPE IMPLEMENTATION </span></p><p class="c28"><span class="c1">We have built a prototype of our incremental one-pass analytics platform on Hadoop. Our prototype is based on Hadoop version 0.20.1 and modifies the internals of Hadoop by replacing key com- ponents with our Hash-based and fast in-memory processing im- plementations. Figure 6 depicts the architecture of our prototype; the shaded components and the enlarged sub-components show the various portions of Hadoop internals that we have built. Broadly these modifications can be grouped into two main components. </span></p><p class="c28"><span class="c1">Hash-based Map Output: Vanilla Hadoop consists of a Map Output Buffer component that manages the map output buffer, col- lects map output data, partitions the data for reducers, sorts the data by partition id and key (external sort if the data exceeds mem- ory), and feeds the sorted data to the combine function if there is one or writes sorted runs to local disks otherwise. Since our de- sign eliminates the sort phase, we replace this component with a new Hash-based Map Output component. Whenever a combine function is used, our Hash-based Map Output component builds an in-memory hash table for key-value pairs output by hashing on the corresponding keys. After the input has been processed, the values of the same key are fed to the combine function, one key at a time. </span></p><p class="c6"><span class="c1">993 </span></p><p class="c6"><span class="c117">System utilities </span></p><p class="c6"><span class="c2">System log </span></p><p class="c6"><span class="c2">manager </span><span class="c54">Progress </span></p><p class="c7"><span class="c54">CPU </span><span class="c2">reporter pro</span><span class="c93">fi</span><span class="c2">ler </span><span class="c54">I/O </span><span class="c2">pro</span><span class="c93">fi</span><span class="c2">ler </span></p><p class="c7"><span class="c117">Hash function library </span></p><p class="c7"><span class="c2">hash Independent generator </span></p><p class="c6"><span class="c2">functions </span></p><p class="c7"><span class="c2">based Frequency- function </span></p><p class="c6"><span class="c2">hash </span></p><p class="c7"><span class="c117">Byte array based memory management library </span></p><p class="c6"><span class="c2">Hash table </span></p><p class="c7"><span class="c2">Key-value/ state buffer </span></p><p class="c6"><span class="c2">BitmapActivity indicator table </span></p><p class="c6"><span class="c2">Sum </span></p><p class="c6"><span class="c15">... </span></p><p class="c7"><span class="c117">File management library </span></p><p class="c7"><span class="c2">Log-based bucket </span><span class="c93">fi</span><span class="c2">le </span></p><p class="c6"><span class="c2">for SSD </span><span class="c15">Bucket </span><span class="c96">fi</span><span class="c15">le </span></p><p class="c6"><span class="c2">for HDD </span></p><p class="c7"><span class="c117">User function Library </span></p><p class="c6"><span class="c2">Sessionization </span></p><p class="c6"><span class="c1">Figure 6: </span><span class="c9">Architecture of our new one-pass analytics platform. </span></p><p class="c28"><span class="c1">In the scenario where no combine function is used, the map output must be grouped by partition id and there is no need to group by keys. In this case, our Hash-based Map Output component records the number of key-value pairs for each partition while processing the input data chunk, and moves records with the same key to a particular segment in the buffer, while scanning the buffer once. </span></p><p class="c28"><span class="c1">HashThread Component: Vanilla Hadoop comprises an </span><span class="c13">In- MemFSMerge </span><span class="c1">thread that performs in-memory and on-disk merges and writes data to disk whenever the shuffle buffer is full. Our pro- totype replaces this component with a HashThread implementation, and provides a user-configurable option to choose between MR-hash, INC-hash, and DINC-hash implementations within HashThread. </span></p><p class="c28"><span class="c1">In order to avoid the performance overhead of creating a large number of Java objects, our prototype implements its own memory management by placing key data structures into byte arrays. Our current prototype includes several byte array-based memory man- agers to provide core functionality such as hash table, key-value or key-state buffer, bitmap, or counter-based activity indicator table, etc., to support our three hash-based approaches. </span></p><p class="c28"><span class="c1">We also implement a bucket file manager that is optimized for hard disks and SSDs and provide a library of common combine and reduce functions as a convenience to the programmer. Our prototype also provides a set of independent hash functions, such as in recursive hybrid hash, in case such multiple hash functions are needed for analytics tasks. Also, if the frequency of hash keys is available a priori, our prototype can customize the hash function to balance the amount of data across buckets. </span></p><p class="c28"><span class="c1">Finally, we implement several &ldquo;utility&rdquo; components such as a sys- tem log manager, a progress reporter for incremental computation, and CPU and I/O profilers to monitor system status. </span></p><p class="c6"><span class="c10">6. PERFORMANCE EVALUATION </span></p><p class="c28"><span class="c1">We present an experimental evaluation of our analytics platform and compare it to optimized Hadoop (1-pass SM) version 0.20.1. We evaluate all three hash techniques (MR-hash, INC-hash and </span></p><p class="c7"><span class="c2">Input split </span></p><p class="c6"><span class="c2">map () </span></p><p class="c7"><span class="c2">Map function </span></p><p class="c7"><span class="c78">MapOutputBuffer --&gt; HashBasedMap OutputBuffer </span></p><p class="c7"><span class="c2">Partition &amp; sort map output </span></p><p class="c6"><span class="c2">combine() </span><span class="c15">* Combine </span></p><p class="c6"><span class="c2">function </span></p><p class="c7"><span class="c2">Map output writer </span></p><p class="c7"><span class="c2">Copy map output </span></p><p class="c7"><span class="c78">InMemFSMerge Thread --&gt; HashThread </span></p><p class="c6"><span class="c2">combine() </span></p><p class="c6"><span class="c2">reduce() </span></p><p class="c7"><span class="c2">Spill &amp; multi-pass on-disk merge </span></p><p class="c7"><span class="c2">* Combine function </span></p><p class="c6"><span class="c2">Reduce function </span></p><p class="c7"><span class="c2">Reduce output writer </span></p><p class="c6"><span class="c1">DINC-hash) in terms of running time, the size of reduce spill data, and the progress made in map and reduce (by Definition 2). </span></p><p class="c28"><span class="c1">In our evaluation, we use two real-world datasets: 236GB of the WorldCup click stream, and 156GB of the GOV2 dataset</span><span class="c32">6</span><span class="c1">. We use workloads over the WorldCup dataset: (1) sessionization where we split the click stream of each user into sessions; (2) user click counting, where we count the number of clicks made by each user; (3) frequent user identification, where we find users who click at least 50 times. We also use a fourth workload over the GOV2 dataset, trigram counting, where we report word trigrams that appear more than 1000 times. Our evaluation environment is a 10-node cluster as described in &sect;2.3. Each compute node is set to hold a task tracker, a data node, four map slots, and four reduce slots. In each experiment, 4 reduce tasks run on each compute node. </span></p><p class="c6"><span class="c10">6.1 Small Key-state Space </span></p><p class="c28"><span class="c1">We first evaluate MR-hash and INC-hash under the workloads with small key-state space, where the distinct key-state pairs fit in memory or slightly exceed the memory size. We consider session- ization, user click counting, and frequent user identification. </span></p><p class="c28"><span class="c1">Sessionization. To support incremental computation of session- ization in reduce, we configure INC-hash to use a fixed-size buffer that holds a user&rsquo;s clicks. A fixed size buffer is used since the order of the map output collected by a reducer is not guaranteed, and yet online sessionization relies on the temporal order of the input se- quence. When the disorder of reduce input in the system is bounded, a sufficiently large buffer can guarantee the input order to the online sessionization algorithm. In the first experiment, we set the buffer size, i.e. the state size, to 0.5KB. </span></p><p class="c28"><span class="c1">Fig. 7(a) shows the comparison of 1-pass SM, MR-hash, and INC- hash in terms of map and reduce progress. Before the map tasks finish, the reduce progress of 1-pass SM and MR-hash is blocked by 33%. MR-hash blocks since incremental computation is not supported. In 1-pass SM, the sort-merge mechanism blocks the reduce function until map tasks finish; a combine function can&rsquo;t be used here since all the records must be kept for output. In contrast, INC-hash&rsquo;s reduce progress keeps up with the map progress up to around 1,300s, because it performs incremental in-memory processing and generates pipelined output until the reduce memory is filled with states. After 1,300s, some data is spilled to disk, so the reduce progress slows down. After map tasks finish, it takes 1-pass SM and MR-hash longer to complete due to the large size of reduce spills (around 250GB as shown in Table 3). In contrast, INC-hash finishes earlier due to smaller reduce spills (51GB). </span></p><p class="c28"><span class="c13">Thus by supporting incremental processing, INC-hash can pro- vide earlier output, and generates less spill data, which further reduces the running time after the map tasks finish. </span></p><p class="c28"><span class="c1">User click counting &amp; Frequent user identification. In con- trast to sessionization, user-click counting can employ a combine function and the states completely fit in memory at the reducers. </span></p><p class="c28"><span class="c1">Fig. 7(b) shows the results for user click counting. 1-pass SM applies the combine function in each reducer whenever its buffer fills up, so its progress is more of a step function. Since MR-hash does not support the combine function, its overall progress only reaches 33% when the map tasks finish. In contrast, INC-hash makes steady progress through 66% due to its full incremental computation. Note that since this query does not allow any early output, no technique can progress beyond 66% until all map tasks finish. </span></p><p class="c28"><span class="c1">This workload generates less shuffled data, reduce spill data, and output data when compared to sessionization (see Table 3). Hence the workload is not as disk- and network-I/O- intensive. Conse- </span></p><p class="c6"><span class="c56">6</span><span class="c20">http://ir.dcs.gla.ac.uk/test_collections/gov2-summary.htm </span></p><p class="c6"><span class="c1">994 </span></p><p class="c6 c61"><span class="c1">Table 3: </span><span class="c9">Comparing optimized Hadoop (using sort-merge), MR-hash, and INC-hash . </span><span class="c20">Sessionization 1-Pass SM MR-hash INC-hash </span><span class="c9">Running time (s) 4424 3577 2258 Map CPU time per node (s) 936 566 571 Reduce CPU time per node (s) 1104 1033 565 Map output / Shuffle (GB) 245 245 245 Reduce spill (GB) 250 256 51 User click counting 1-Pass SM MR-hash INC-hash Running time (s) 1430 1100 1113 Map CPU time per node (s) 853 444 443 Reduce CPU time per node (s) 39 41 35 Map output / Shuffle (GB) 2.5 2.5 2.5 Reduce spill (GB) 1.1 0 0 Frequent user identification 1-Pass SM MR-hash INC-hash Running time (s) 1435 1153 1135 Map CPU time per node (s) 855 442 441 Reduce CPU time per node (s) 38 38 34 Map output / Shuffle (GB) 2.5 2.5 2.5 Reduce spill (GB) 1.1 0 0 </span></p><p class="c148"><span class="c1">Table 4: </span><span class="c9">Comparing sessionization to INC-hash with 0.5KB state, INC- hash with 2KB state, and DINC-hash with 2KB state. </span></p><p class="c75"><span class="c9">INC (0.5KB) INC (2KB) DINC (2KB) Running time (s) 2258 3271 2067 Reduce spill (GB) 51 203 0.1 </span></p><p class="c151"><span class="c1">quently both hash-based techniques have shorter running times, when compared to 1-pass SM, due to the reduction in CPU overhead gained by eliminating the sort phase. </span></p><p class="c138"><span class="c1">We further evaluate MR-hash and INC-hash with frequent user identification. This query is based on user click counting, but allows a user to be output whenever the counter of the user reaches 50. Fig. 7(c) shows 1-pass SM and MR-hash perform similarly as in user click counting, as the reduce function cannot be applied until map tasks finish. The reduce progress of INC-hash completely keeps up with the map progress due to the ability to output early. </span></p><p class="c113"><span class="c13">In summary, given sufficient memory, INC-hash performs fully in- memory incremental processing, due to which, its reducer progress can potentially keep up with the map progress for queries that allow early output. Hash techniques can run faster if I/O and network are not bottlenecks due to the elimination of sorting. </span></p><p class="c52"><span class="c10">6.2 Large Key-state Space </span></p><p class="c67"><span class="c1">We next evaluate INC-hash and DINC-hash for incremental pro- cessing for workloads with a large key-state space, which can trigger substantial I/O. Our evaluation uses two workloads below: </span></p><p class="c127"><span class="c1">Sessionization with varying state size. Fig. 7(d) shows the map and reduce progress under three state sizes: 0.5KB, 1KB, and 2KB. A larger state size implies the reduce memory can hold few states and that the reduce progress diverges earlier from the map progress. Also, larger states cause more data to be spilled to disk, as shown in Table 4. Consequently, after map tasks finish, the time for processing data from disk is longer. To enable DINC-hash for sessionization, we evict a state from memory if: (1) all the clicks in the state belong to an expired session; (2) the counter of the state is zero. Rather than spilling the evicted state to disk, the clicks in it can be directly output. As shown in Table 4, DINC-hash only spills 0.1 GB data in reduce with 2KB state size, in contrast to 203 GB for the same workload in INC-hash. As shown in Fig. 7(e), the reduce progress of DINC-hash closely follows the map progress, and spends little time processing the on-disk data after mappers finish. </span></p><p class="c85"><span class="c1">We further quote numbers about stock Hadoop for this workload (see Table 1). Using DINC-hash, the reducers output continuously and finish as soon as all mappers finish reading the data in 34.5 </span></p><p class="c28 c126"><span class="c1">minutes, with 0.1GB internal spill. In contrast, the original Hadoop system returns all the results towards the end of the 81 minute job, causing 370GB internal data spill to disk, 3 orders of magnitude more than DINC-hash. </span></p><p class="c55"><span class="c1">Trigram Counting. Fig. 7(f) shows the map and reduce progress plot for INC-hash and DINC-hash. The reduce progress in both keeps growing below, but close to the map progress, with DINC- hash finishing a bit faster. In this workload, the reduce memory can only hold 1/30 of the states, but less than half of the input data is spilled to disk in both approaches. This implies that both hash techniques hold a large portion of hot keys in memory. DINC-hash does not outperform INC-hash like with sessionization because the trigrams are distributed more evenly than the user ids, so most hot trigrams appear before the reduce memory fills up. INC-hash naturally holds them in memory. The reduce progress in DINC-hash falls slightly behind that of INC-hash because if the state of a key is evicted, and the key later gets into memory again, the counter in its state starts from zero again, making it harder for a key to reach the threshold of 1,000. Both hash techniques finish the job in the range of 4,100-4,400 seconds. In contrast, 1-pass SM takes 9,023 seconds. So both hash techniques outperform Hadoop. </span></p><p class="c55"><span class="c13">In summary, results in this section show that our hash techniques significantly improve the progress of the map tasks, due to the elimination of sorting, and given sufficient memory, enable fast in-memory processing of the reduce function. For workloads that require a large key-state space, our frequent-key mechanism signifi- cantly reduces I/Os and enables the reduce progress to keep up with the map progress, thereby realizing incremental processing. </span></p><p class="c119"><span class="c10">7. RELATED WORK </span></p><p class="c146"><span class="c1">Query Processing using MapReduce [5, 11, 16, 17, 19, 23] has been a research topic of significant interest lately. To the best of our knowledge, none of these systems support incremental one-pass ana- lytics as defined in our work. The closest work to ours is MapReduce Online [6] which we discussed in detail in Sections 2 and 3. Dryad [23] uses in-memory hashing to implement MapReduce group-by but falls back on the sort-merge implementation when the data size exceeds memory. Merge Reduce Merge [22] implements hash join using a technique similar to our baseline MR-hash, but lacks further implementation details. Several other projects are in parallel to our work: The work in [2] focuses on optimizing Hadoop parameters and ParaTimer [13] aims to provide an indicator of remaining time of MapReduce jobs. Neither of them improves MapReduce for in- cremental computation. Finally, many of the above systems support concurrent MapReduce jobs to increase system resource utilization. However, the resources consumed by each task will not reduce, and concurrency does not help achieve one-pass incremental processing. Parallel Databases: Parallel databases [9, 8] require special hard- ware and lacked sufficient solutions to fault tolerance, hence hav- ing limited scalability. Their implementations use hashing inten- sively. In contrast, our work leverages the massive parallelism of MapReduce and extends it to incremental one-pass analytics. We use MR-hash, a technique similar to hybrid hash used in parallel databases [9], as a baseline. Our more advanced hash techniques emphasize incremental processing and in-memory processing for hot keys in order to support parallel stream processing. Distributed Stream Processing has considered a distributed fed- eration of participating nodes in different administrative domains [1] and the routing of tuples between nodes [20], without using MapReduce. Our work differs from these techniques as it considers the new MapReduce model for massive partitioned parallelism and </span></p><p class="c102"><span class="c1">995 </span></p><p class="c150"><span class="c1">(f) </span><span class="c9">Trigram counting with INC- &amp; DINC-hash. </span></p><p class="c6 c86"><span class="c1">(d) </span><span class="c9">Sessionization with INC-hash. </span><span class="c1">(e) </span><span class="c9">Sessionization with DINC-hash. </span></p><p class="c120"><span class="c1">Figure 7: Progress report using hash implementations. </span></p><p class="c145"><span class="c1">extends it to incremental one-pass processing, which can be later used to support stream processing. Parallel Stream Processing: The systems community has devel- oped parallel stream systems like System S [24] and S4 [15]. These systems adopt a workflow-based programming model and leave many systems issues such as memory management and I/O opera- tions to user code. In contrast, MapReduce systems abstract away these issues in a simple user programming model and automatically handle the memory and I/O related issues in the system. </span></p><p class="c131"><span class="c10">8. CONCLUSIONS </span></p><p class="c103"><span class="c1">In this paper, we examined the architectural design changes that are necessary to bring the benefits of the MapReduce model to incremental one-pass analytics. Our empirical and theoretical anal- yses showed that the widely-used sort-merge implementation for MapReduce partitioned parallelism poses a fundamental barrier to incremental one-pass analytics, despite optimizations. We proposed a new data analysis platform that employs a purely hash-based frame- work, with various techniques to enable incremental processing and fast in-memory processing for frequent keys. Evaluation of our Hadoop-based prototype showed that it can significantly improve the progress of map tasks, allows the reduce progress to keep up with the map progress with up to 3 orders of magnitude reduction of internal data spills, and enables results to be returned early. In future work, we will extend our one-pass analytics platform to support a wider range of incremental computation tasks with minimized I/O, online aggregation with early approximate answers, and stream query processing with window operations. </span></p><p class="c64"><span class="c10">9. REFERENCES </span></p><p class="c88"><span class="c9">[1] D. J. Abadi, Y. Ahmad, et al. The design of the Borealis stream </span></p><p class="c130"><span class="c9">processing engine. In </span><span class="c3">CIDR</span><span class="c9">, 277&ndash;289, 2005. [2] S. Babu. Towards automatic optimization of MapReduce programs. In </span></p><p class="c116"><span class="c3">SoCC</span><span class="c9">, 137&ndash;142, 2010. [3] R. Berinde, G. Cormode, et al. Space-optimal heavy hitters with </span></p><p class="c57"><span class="c9">strong error bounds. In </span><span class="c3">PODS</span><span class="c9">, 157&ndash;166, 2009. </span></p><p class="c6 c112"><span class="c9">[4] B. Li, E. Mazur, et al. A platform for scalable one-pass analytics using </span></p><p class="c114"><span class="c9">MapReduce. Technical report, UMass Amherst, 2010. [5] R. Chaiken, B. Jenkins, et al. Scope: easy and efficient parallel </span></p><p class="c100"><span class="c9">processing of massive data sets. In </span><span class="c3">PVLDB</span><span class="c9">, 1(2):1265&ndash;1276, 2008. [6] T. Condie, N. Conway, et al. MapReduce online. In </span><span class="c3">NSDI</span><span class="c9">, 2010. [7] J. Dean and S. Ghemawat. MapReduce: simplified data processing on </span></p><p class="c136"><span class="c9">large clusters. In </span><span class="c3">OSDI</span><span class="c9">, 10&ndash;10, 2004. [8] D. DeWitt and J. Gray. Parallel database systems: the future of high performance database systems. </span><span class="c3">Commun. ACM</span><span class="c9">, 35(6):85&ndash;98, 1992. [9] D. J. DeWitt, R. H. Gerber, et al. Gamma: a high performance </span></p><p class="c115"><span class="c9">dataflow database machine. In </span><span class="c3">VLDB</span><span class="c9">, 228&ndash;237, 1986. [10] J. M. Hellerstein and J. F. Naughton. Query execution techniques for </span></p><p class="c108"><span class="c9">caching expensive methods. In </span><span class="c3">SIGMOD</span><span class="c9">, 423&ndash;434, 1996. [11] D. Jiang, B. C. Ooi, et al. The performance of MapReduce: an </span></p><p class="c79"><span class="c9">in-depth study. In </span><span class="c3">VLDB</span><span class="c9">, 2010. [12] J. Misra and D. Gries. Finding repeated elements. </span><span class="c3">Sci. Comput. </span></p><p class="c109"><span class="c3">Program.</span><span class="c9">, 2(2):143&ndash;152, 1982. [13] K. Morton, M. Balazinska, et al. Paratimer: a progress indicator for </span></p><p class="c39"><span class="c9">MapReduce dags. In </span><span class="c3">SIGMOD</span><span class="c9">, 507&ndash;518, 2010. [14] S. Muthukrishnan. </span><span class="c3">Data Streams: Algorithms and Applications</span><span class="c9">. Now </span></p><p class="c72"><span class="c9">Publishers, 2006. [15] L. Neumeyer, B. Robbins, et al. S4: distributed stream computing </span></p><p class="c87"><span class="c9">platform. In </span><span class="c3">KDCloud</span><span class="c9">, 2010. [16] C. Olston, B. Reed, et al. Pig latin: a not-so-foreign language for data </span></p><p class="c87"><span class="c9">processing. In </span><span class="c3">SIGMOD</span><span class="c9">, 1099&ndash;1110, 2008. [17] A. Pavlo, E. Paulson, et al. A comparison of approaches to large-scale </span></p><p class="c111"><span class="c9">data analysis. In </span><span class="c3">SIGMOD</span><span class="c9">, 165&ndash;178, 2009. [18] L. D. Shapiro. Join processing in database systems with large main memories. </span><span class="c3">ACM Trans. Database Syst.</span><span class="c9">, 11(3):239&ndash;264, 1986. [19] A. Thusoo, J. S. Sarma, et al. Hive - a warehousing solution over a </span></p><p class="c105"><span class="c9">map-reduce framework. </span><span class="c3">PVLDB</span><span class="c9">, 2(2):1626&ndash;1629, 2009. [20] F. Tian and D. J. DeWitt. Tuple routing strategies for distributed </span></p><p class="c43"><span class="c9">eddies. In </span><span class="c3">VLDB</span><span class="c9">, 333&ndash;344, 2003. [21] T. White. </span><span class="c3">Hadoop: The Definitive Guide</span><span class="c9">. O&rsquo;Reilly Media, Inc., 2009. [22] H.-c. Yang, A. Dasdan, et al. Map-reduce-merge: simplified relational </span></p><p class="c149"><span class="c9">data processing on large clusters. In </span><span class="c3">SIGMOD</span><span class="c9">, 1029&ndash;1040, 2007. [23] Y. Yu, P. K. Gunda, et al. Distributed aggregation for data-parallel </span></p><p class="c126 c135"><span class="c9">computing: interfaces and implementations. In </span><span class="c3">SOSP</span><span class="c9">, 247&ndash;260, 2009. [24] Q. Zou, H. Wang, et al. From a stream of relational queries to </span></p><p class="c29"><span class="c9">distributed stream processing. In </span><span class="c3">VLDB</span><span class="c9">, 2010. </span></p><p class="c137"><span class="c1">996 </span></p><p class="c6 c121"><span class="c1">(a) </span><span class="c9">Sessionization (0.5KB state). </span><span class="c1">(b) </span><span class="c9">User click counting. </span><span class="c1">(c) </span><span class="c9">Frequent user identification (clicks</span><span class="c122">&ge; </span><span class="c27">50</span><span class="c9">). </span></p></body></html>