<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c77{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:9.5pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.5pt;font-family:"Arial";font-style:normal}.c70{margin-left:-18.1pt;padding-top:1.4pt;text-indent:19pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.4pt}.c27{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c35{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.7pt;font-family:"Arial";font-style:normal}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.6pt;font-family:"Arial";font-style:normal}.c80{margin-left:-27.4pt;padding-top:1.7pt;text-indent:36.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.6pt}.c46{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Courier New";font-style:normal}.c61{margin-left:-18.3pt;padding-top:3.8pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c38{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:normal}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.7pt;font-family:"Arial";font-style:normal}.c39{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c76{margin-left:-18.1pt;padding-top:9.4pt;text-indent:24.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-32.2pt}.c36{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c62{margin-left:-18.1pt;padding-top:1.4pt;text-indent:36.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-13.9pt}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.1pt;font-family:"Arial";font-style:normal}.c33{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c53{margin-left:-27.4pt;padding-top:4.1pt;text-indent:36.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c82{margin-left:-18.3pt;padding-top:1.4pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.7pt}.c6{margin-left:-18.1pt;padding-top:1.7pt;text-indent:18.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.1pt}.c56{margin-left:-18.1pt;padding-top:1.4pt;text-indent:35.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-4.6pt}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.7pt;font-family:"Arial";font-style:normal}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c89{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.3pt;font-family:"Courier New";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c79{margin-left:-18.1pt;padding-top:1.7pt;text-indent:18.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-23.5pt}.c63{margin-left:-27.1pt;padding-top:1.4pt;text-indent:36.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c90{margin-left:-18.1pt;padding-top:1.4pt;text-indent:18.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.4pt}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.2pt;font-family:"Arial";font-style:normal}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c47{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Courier New";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c67{margin-left:-27.4pt;padding-top:4.1pt;text-indent:36.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.6pt}.c92{margin-left:-13.5pt;padding-top:1.7pt;text-indent:27.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18.5pt}.c57{margin-left:-22.6pt;padding-top:1.4pt;text-indent:37pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:35.7pt}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.5pt;font-family:"Arial";font-style:normal}.c88{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Courier New";font-style:normal}.c52{margin-left:-27.1pt;padding-top:0.7pt;text-indent:36.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.2pt}.c25{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.2pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.3pt;font-family:"Arial";font-style:normal}.c54{margin-left:-22.6pt;padding-top:1.4pt;text-indent:36.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-2.5pt}.c41{margin-left:-18.1pt;padding-top:1.7pt;text-indent:27.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.7pt}.c74{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.7pt;font-family:"Arial";font-style:normal}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c60{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Arial";font-style:normal}.c45{margin-left:-18.1pt;padding-top:1.7pt;text-indent:27.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c48{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.4pt;font-family:"Arial";font-style:normal}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Courier New";font-style:normal}.c86{margin-left:-18.1pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-23.8pt}.c44{margin-left:-18.1pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-22.3pt}.c37{margin-left:-18.1pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.2pt}.c51{margin-left:218.2pt;padding-top:27.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c29{margin-left:-22.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-11.1pt}.c50{margin-left:-27.1pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:122.8pt}.c81{margin-left:-27.1pt;padding-top:11.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:122.3pt}.c83{margin-left:-27.1pt;padding-top:29pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-15.9pt}.c84{margin-left:-22.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-13.3pt}.c32{margin-left:-27.1pt;padding-top:10.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:82pt}.c55{margin-left:-22.6pt;padding-top:3.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:88.2pt}.c30{margin-left:25.1pt;padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:19.2pt}.c71{margin-left:-18.1pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24pt}.c85{margin-left:-18.1pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.5pt}.c42{margin-left:218.2pt;padding-top:33.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c68{margin-left:25.1pt;padding-top:14.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:35.3pt}.c64{margin-left:-27.1pt;padding-top:5.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-10.2pt}.c73{margin-left:0.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:39.6pt}.c91{margin-left:-18.1pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-13pt}.c65{margin-left:-18.1pt;padding-top:8.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:46.1pt}.c66{margin-left:-22.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-8.2pt}.c78{margin-left:-18.1pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-23pt}.c58{margin-left:235.9pt;padding-top:24.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-128.5pt}.c69{margin-left:-27.4pt;padding-top:19.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:121.6pt}.c87{margin-left:-8.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-10.2pt}.c49{margin-left:-27.4pt;padding-top:20.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c21{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c10{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c11{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c75{padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c59{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c24{margin-left:-18.3pt;margin-right:-25pt}.c43{margin-left:-13.5pt;margin-right:-21.4pt}.c72{text-indent:18.3pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c59"><p class="c3"><span class="c77">SpMachO - Optimizing Sparse Linear Algebra Expressions with Probabilistic Density Estimation </span></p><p class="c11"><span class="c20">David Kernert </span><span class="c40">Technische Universit&auml;t Dresden Database Technology Group </span><span class="c27">david.kernert@sap.com </span></p><p class="c11"><span class="c40">Dresden, Germany </span><span class="c20">Frank K&ouml;hler </span><span class="c40">SAP SE Dietmer-Hopp-Allee 16 </span><span class="c27">frank.koehler@sap.com </span></p><p class="c11"><span class="c40">Walldorf, Germany </span><span class="c20">Wolfgang Lehner </span><span class="c40">Technische Universit&auml;t Dresden Database Technology Group </span><span class="c27">wolfgang.lehner@tu- </span><span class="c40">Dresden, Germany </span><span class="c27">dresden.de </span></p><p class="c11"><span class="c20">ABSTRACT </span><span class="c1">In the age of statistical and scientific databases, there is an emerging trend of integrating analytical algorithms into data- base systems. Many of these algorithms are based on linear algebra with large, sparse matrices. However, linear algebra expressions often contain multiplications of more then two matrices. The execution of sparse matrix chains is nontrivial, since the runtime depends on the parenthesization and on physical properties of intermediate results. Our approach targets to overcome the burden for data scientists of select- ing appropriate algorithms, matrix storage representations, and execution paths. In this paper, we present a sparse matrix chain optimizer (SpMachO) that creates an execu- tion plan, which is composed of multiplication operators and transformations between sparse and dense matrix storage representations. We introduce a comprehensive cost model for sparse-, dense- and hybrid multiplication kernels. More- over, we propose a sparse matrix product density estimator (SpProdest) for intermediate result matrices. We evaluated SpMachO and SpProdest using real-world matrices and random matrix chains. </span></p><p class="c11"><span class="c20">Categories and Subject Descriptors </span><span class="c1">G.1.3 [Numerical Linear Algebra]: Sparse, structured, and very large systems </span></p><p class="c11"><span class="c20">General Terms </span><span class="c1">sparse linear algebra, optimization </span></p><p class="c11"><span class="c20">1. INTRODUCTION </span></p><p class="c10"><span class="c1">In the era of big data and data deluge, scientists and data analysists are confronted with a time-consuming im- plementation overhead, when they want to scale and speed up their existing, handcrafted code that has been working </span></p><p class="c10"><span class="c23">(c) 2015, Copyright is with the authors. Published in Proc. 18th Inter- national Conference on Extending Database Technology (EDBT), March 23-27, 2015, Brussels, Belgium: ISBN 978-3-89318-067-7, on OpenPro- ceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0 </span></p><p class="c10"><span class="c1">for years on small data sets. This set the way for new data- base applications in the fields of scientific computations and advanced analytics on large data. However, since most of the algorithms in science and data mining are composed of linear algebra expressions, conventional SQL-based relational database management systems (RDBMS&rsquo;s) did not match the requirements. On the other hand, numerical algebra sys- tems like R are known for efficient linear algebra algorithms, but they lack scalability and data manipulation capabili- ties. As a consequence, the demand of data scientists for a scalable system that provides a basic set of efficient linear algebra primitives attracted the attention of the database community [11]. Recently emerged systems like SciDB [8] or SystemML [6] reacted by providing a R or R-like interface and deep integrations of linear algebra primitives, such as sparse matrix-matrix and matrix-vector multiplications. </span></p><p class="c10"><span class="c1">In business environments, data analysts often load data from a relational database into a numerical algebra system to perform their analysis by means of linear algebra. For example, financial analysts that use a RDBMS for storing stock price data need the functionality of SQL, e.g., in order to get the average stock prizes, or to find all stocks that belong to a certain group. On the other side, they might want to use matrix multiplications to find correlations [24] of stocks and derivatives. </span></p><p class="c10"><span class="c1">Matrices as first-class citizens have been integrated in many systems, for example in array DBMS&rsquo;s [8], data ware- houses [19], or in-memory column stores [20], but only little has been done in the direction of optimizing the execution of linear algebra expressions. In this paper, we focus on optimizing the execution of sparse linear algebra expressions based on physical properties. The idea is the following: next to the RDBMS optimizer that creates optimized execution plans from SQL expressions, we propose a component (Fig. 1) that generates an optimal execution plan for linear algebra, which is using the native storage and execution engine of the system, and additional operators for linear algebra, such as a matrix multiplication operator. </span></p><p class="c10"><span class="c1">As most of the big matrices occurring in the real world are sparse, linear algebra expressions often contain multipli- cations of three or more sparse, or mixed dense and sparse matrices, e.g. in transitive closure computations, Markov chains [27], linear transformation [13], or linear discrete dy- namical systems [4]. An efficient execution of sparse matrix chain multiplications is nontrivial, in particular, if inter- mediate result matrices become dense. In many situations </span></p><p class="c11"><span class="c26">289 10.5441/002/edbt.2015.26 </span></p><p class="c11"><span class="c60">SQL Linear Algebra </span></p><p class="c11"><span class="c36">DBMS </span></p><p class="c11"><span class="c60">Optimizer SpMachO </span></p><p class="c11"><span class="c60">Relational Operators </span></p><p class="c11"><span class="c60">Linear Algebra Operators Storage Engine </span></p><p class="c11"><span class="c1">Figure 1: DBMS example architecture. </span></p><p class="c10"><span class="c1">the runtime performance can be significantly improved by changing the execution order, or by switching from a sparse to a dense multiplication algorithm in later stages. Since data scientists are usually not familiar with algorithmic de- tails of multiplication kernels and system parameters, and do not have profound knowledge of the characteristics of their matrices, it makes sense to leave these decisions to the system. </span></p><p class="c11"><span class="c1">In particular, the contributions of this work are: </span></p><p class="c10"><span class="c1">&bull; SpMachO, a general matrix chain multiplication op- timizer based on a dynamic programming approach, which leverages density estimations of intermediate re- sults and different multiplication kernels to minimize the total execution runtime. The optimization problem and the SpMachO algorithm are presented in sections 2 and 3. </span></p><p class="c10"><span class="c1">&bull; SpProdest, a sparse matrix density estimator, which predicts the density structure of intermediate and final result matrices, by using a novel skew-aware stochastic density propagation method. It is described in detail in sections 4 and 5. </span></p><p class="c10"><span class="c1">&bull; An extensive evaluation and comparison of the execu- tion runtime of the SpMachO-generated plan against alternative execution strategies and other numerical algebra systems, which is presented in section 6. </span></p><p class="c11"><span class="c1">Finally, we will discuss the related work in section 7, followed by the conclusion in section 8. </span></p><p class="c11"><span class="c20">2. EXPRESSION OPTIMIZATION </span></p><p class="c10"><span class="c1">A lot of data scientists work with numeric algebra systems to run their linear algebra algorithms. However, the user is often let alone with the execution order, although the way of executing large sparse matrix expressions contains a significant optimization potential. </span></p><p class="c10"><span class="c1">Consider a set of linear algebra expressions that consist of matrix multiplication and addition on general R</span><span class="c9">m&#8677;n </span><span class="c1">ma- trices. Further operations, like subtraction or division by a matrix A can be represented by using the corresponding inverse of addition ( A), or inverse of multiplication A </span><span class="c9">1</span><span class="c1">, respectively. Thus, the expression can be reduced to a form: </span></p><p class="c11"><span class="c1">C = A</span><span class="c5">0 </span><span class="c1">+ A</span><span class="c5">1 </span><span class="c1">&middot; A</span><span class="c5">2 </span><span class="c1">&middot; ... &middot; A</span><span class="c5">p </span><span class="c1">+ A</span><span class="c5">p+1 </span><span class="c1">&middot; ...A</span><span class="c4">l </span><span class="c0">+ ..., </span></p><p class="c10"><span class="c1">From a mathematical perspective, the number of operations needed for the element-wise addition of intermediate results, or any element-wise operation in general, is independent from the execution order, thus, it is the same for (A + B) + C as </span></p><p class="c11"><span class="c1">AA</span><span class="c9">S</span><span class="c5">1 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">2 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">3 </span></p><p class="c11"><span class="c1">A</span><span class="c9">S</span><span class="c5">1 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">2 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">3 </span></p><p class="c11"><span class="c9">S</span><span class="c5">1 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">2 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">3 </span></p><p class="c11"><span class="c1">&#8677;</span><span class="c4">S</span><span class="c1">&#8677;</span><span class="c4">S</span><span class="c1">&#8677;</span><span class="c4">S </span><span class="c0">T </span><span class="c4">D </span><span class="c5">S</span><span class="c0">&#8677;</span><span class="c4">S </span></p><p class="c11"><span class="c0">&#8677;</span><span class="c4">D </span></p><p class="c11"><span class="c1">&#8677;</span><span class="c5">S </span></p><p class="c11"><span class="c1">A</span><span class="c9">S</span><span class="c5">1 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">2 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">3 </span></p><p class="c11"><span class="c1">&#8677;</span><span class="c4">S </span><span class="c0">T </span><span class="c4">D </span><span class="c5">S</span><span class="c1">&#8677;</span><span class="c5">D </span></p><p class="c11"><span class="c1">A</span><span class="c9">S</span><span class="c5">1 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">2 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">3 </span></p><p class="c11"><span class="c0">T </span><span class="c4">D </span></p><p class="c11"><span class="c5">S</span><span class="c1">&#8677;</span><span class="c4">S </span><span class="c1">&#8677;</span><span class="c4">S </span></p><p class="c11"><span class="c1">A</span><span class="c9">S</span><span class="c5">1 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">2 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">3 </span></p><p class="c11"><span class="c0">T </span><span class="c4">D </span></p><p class="c11"><span class="c5">S</span><span class="c1">&#8677;</span><span class="c4">S </span><span class="c1">&#8677;</span><span class="c4">D </span></p><p class="c11"><span class="c1">A</span><span class="c9">S</span><span class="c5">1 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">2 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">3 </span></p><p class="c11"><span class="c1">&#8677;</span><span class="c4">S </span></p><p class="c11"><span class="c1">&#8677;</span><span class="c4">S </span></p><p class="c11"><span class="c1">A</span><span class="c9">S</span><span class="c5">1 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">2 </span><span class="c14">A</span><span class="c9">S</span><span class="c5">3 </span></p><p class="c11"><span class="c1">&#8677;</span><span class="c4">S </span></p><p class="c11"><span class="c1">&#8677;</span><span class="c4">D </span></p><p class="c10"><span class="c1">Figure 2: Eight of the possible 128 execution plans for the multiplication of three sparse matrices A</span><span class="c4">1 </span><span class="c0">&middot; A</span><span class="c4">2 </span><span class="c0">&middot; A</span><span class="c4">3</span><span class="c0">. &#8677; are </span><span class="c1">binary multiplication operators, S/D denotes the internal sparse/dense representation type of matrices, and T are unary storage type transformations of matrices. </span></p><p class="c10"><span class="c1">for A + (B + C). Although this indifference might not hold in practise, when different physical representations are used, the addition part in the computation of C is rather cheap, since the complexity is at most O(mn) for dense matrices. Most of the execution time is spent in the computation of multiplications </span></p><p class="c11"><span class="c1">A</span><span class="c4">1 </span><span class="c0">&middot; A</span><span class="c4">2 </span><span class="c0">&middot; ... &middot; A</span><span class="c4">p 1 </span><span class="c0">&middot; A</span><span class="c4">p</span><span class="c0">, A</span><span class="c4">i </span><span class="c0">2 R</span><span class="c5">m</span><span class="c18">i</span><span class="c5">&#8677;m</span><span class="c18">i+1</span><span class="c0">, (1) </span></p><p class="c10"><span class="c1">so the work of this paper focus on the optimization of matrix chain multiplications. The algebraic degree of freedom to execute expression (1) consists in the setting of parenthesis, since matrix multiplications are associative. Altering the parenthesization does not change the result, but the number of operations required in the computation of the complete chain can vary significantly. Finding the optimal parenthe- sization for a dense, non-square matrix chain multiplication is well understood and serves as a text book example for the use of dynamic programming [16, 12]. The idea is to iteratively construct the optimal parenthesization as a com- bination of optimal sub-parenthesizations, by minimizing a cost recurrence expression with respect to the split point k </span></p><p class="c11"><span class="c1">C</span><span class="c4">&pi;</span><span class="c7">B</span><span class="c4">(ij) </span><span class="c1">= </span><span class="c4">i&#63743;k&lt;j </span><span class="c1">min C</span><span class="c4">&pi;</span><span class="c7">B</span><span class="c4">(ik) </span><span class="c1">+ C</span><span class="c4">&pi;</span><span class="c7">B</span><span class="c4">((k+1)j) </span></p><p class="c11"><span class="c1">+ TM A</span><span class="c4">[i...k]</span><span class="c0">,A</span><span class="c4">[k+1...j] </span><span class="c0">. (2) </span><span class="c1">For the mathematical formulation, we introduce </span></p><p class="c11"><span class="c1">&bull; &pi;(ij): a parenthesization for the matrix (sub)chain A</span><span class="c4">[i...j] </span><span class="c1">&pi;</span><span class="c9">B</span><span class="c1">(ij) denotes the optimal (&ldquo;best&rdquo;) one. </span></p><p class="c11"><span class="c1">&bull; C</span><span class="c5">&pi;</span><span class="c1">: the cost for executing the matrix chain multiplica- tion, given a certain parenthesization &pi;. </span></p><p class="c11"><span class="c1">&bull; TM A</span><span class="c4">[i...k]</span><span class="c0">,A</span><span class="c4">[k+1...j] </span><span class="c0">: the cost function for multiplying </span><span class="c1">the two matrices that result from the subchains A</span><span class="c4">[i...k] </span><span class="c0">and A</span><span class="c4">[k+1...j] </span></p><p class="c11"><span class="c1">In the textbook case, the combination cost TM A</span><span class="c4">[i...k]</span><span class="c1">,A</span><span class="c4">[k+1...j] </span><span class="c0">is set equal with the number of flops for multiplying the two </span><span class="c1">dense intermediate result matrices. By using the classical inner product algorithm the costs can be exactly determined a priori as m</span><span class="c5">i</span><span class="c1">m</span><span class="c4">k+1</span><span class="c0">m</span><span class="c4">j+1</span><span class="c0">. </span></p><p class="c11"><span class="c1">However, the dense matrix case has certain limitations that restricts its relevance in practice. Most important, </span></p><p class="c11"><span class="c26">290 </span></p><p class="c21"><span class="c1">many of the real-world matrices in big data environments are sparse. A sparse matrix is not only defined by its row and column dimensions m and n, but also by the number and &rho; = have the pattern of </span><span class="c9">N</span><span class="c4">mn </span><span class="c1">a </span><span class="c7">nz </span><span class="c1">different . As a matter complexity: non-zero elements N</span><span class="c5">nz</span><span class="c1">, or the density</span><span class="c9">1 </span><span class="c1">of fact, algorithms on sparse matrices unlike the naive inner product algorithm for dense matrices, the cost of a multiplication of two sparse matrices rather depends on the number of non- zero elements than on their dimensions. Moreover, sparse matrices are stored in a different data structure than dense matrices, which leads to different actual costs depending on the characteristics of the physical representation. Most of the related work on matrix chain multiplications consider either dense-only [16, 12] or sparse-only [9] multiplications, being agnostic to the fact that the densities of the intermediate result matrices can vary significantly from the initial matrices. For example, the density of the result matrix C = A &middot; B can be much higher, or even less than that of both A and B (see Fig. 5). Despite the mathematical complexity, it is in many cases more efficient to continue using algorithms on dense matrix representations, if the density exceeds a certain threshold. This can be reasoned with the efficient and well- tuned implementations of dense matrix multiplication kernels in BLAS</span><span class="c9">2</span><span class="c1">. </span></p><p class="c10"><span class="c1">Our idea is to take the individual differences of the different matrix representations and multiplication kernels into ac- count, and to exploit the potential performance benefits from changing the physical implementation of the initial matrices or intermediate results. We construct an execution plan for the chain expression that can contain dense, sparse and mixed dense/sparse matrix multiplications. Furthermore, the execution plan can contain conversions from a sparse into a dense representation. Therefore, we adopted the idea of dynamic programming and modified it in such a way that it incorporates the physical properties of the matrices. We extended the recurrence (2) by adding the input and output storage types as independent dimensions, and added cost functions for the storage type conversions: </span></p><p class="c11"><span class="c1">C</span><span class="c46">&Pi;</span><span class="c7">B</span><span class="c4">(ij)S</span><span class="c7">o </span><span class="c1">= </span></p><p class="c3"><span class="c4">i&#63743;k&lt;j </span><span class="c1">min </span><span class="c5">S</span><span class="c16">l</span><span class="c5">,S</span><span class="c16">r</span><span class="c5">,S</span><span class="c16">1</span><span class="c5">,S</span><span class="c16">2</span><span class="c5">2S </span></p><p class="c11"><span class="c1">n</span><span class="c0">C</span><span class="c46">&Pi;</span><span class="c7">B</span><span class="c4">(ik)S</span><span class="c7">l </span><span class="c1">+C</span><span class="c46">&Pi;</span><span class="c7">B</span><span class="c4">((k+1)j)S</span><span class="c7">r </span><span class="c1">+TT</span><span class="c4">S</span><span class="c7">1 </span><span class="c1">A</span><span class="c4">[i..k],S</span><span class="c7">l</span><span class="c4">,&#8674; </span></p><p class="c11"><span class="c1">+TT</span><span class="c4">S</span><span class="c7">2 </span><span class="c1">A</span><span class="c4">[k+1..j],S</span><span class="c7">r</span><span class="c4">,&#8674; </span><span class="c0">+TM</span><span class="c4">S</span><span class="c7">o </span><span class="c1">A</span><span class="c4">[i..k],S</span><span class="c7">1</span><span class="c4">,&#8674;</span><span class="c1">,A</span><span class="c4">[k+1..j],S</span><span class="c7">2</span><span class="c4">,&#8674; </span></p><p class="c11"><span class="c1">o</span><span class="c0">(3) </span></p><p class="c10"><span class="c1">&bull; &#8679;(ij): execution plan for a matrix (sub)chain multipli- cation. It contains the execution order as well as all storage transformations. &#8679;</span><span class="c9">B </span><span class="c1">denotes the optimal plan. </span></p><p class="c10"><span class="c1">&bull; S</span><span class="c9">X</span><span class="c1">: storage type, which is either dense or sparse. The superscript X labels each of the five matrices that are considered per execution node. X = l: left subplan output-, r: right subplan output-, 1: left input-, 2: right input-, o: current product output matrix . </span></p><p class="c11"><span class="c1">&bull; TT</span><span class="c4">S</span><span class="c7">Y </span><span class="c1">A</span><span class="c4">[i..k],S</span><span class="c7">X </span><span class="c1">: cost function for the conversion of a matrix from type S</span><span class="c9">X </span><span class="c1">into type S</span><span class="c9">Y </span><span class="c1">. </span></p><p class="c11"><span class="c5">1</span><span class="c0">In the remainder of this paper, we refer to the non-zero structure </span><span class="c4">2</span><span class="c0">Basic using &rho; rather than N</span><span class="c4">nz </span><span class="c0">Linear Algebra Subprograms, http://www.netlib.org/blas/ </span></p><p class="c10"><span class="c1">Since the cost functions TM in equation (3) depend on the storage types of the input and output matrices, as well as their densities, it could be beneficial to convert a matrix from one into the other representation prior to the multiplication because conversions are usually less costly than multiplica- tions. For example, if the initial matrices are in a sparse representation, and the dense multiplication kernel plus the conversion has a far lower cost than the sparse multiplication, then they are first converted into the dense representation. As a matter of fact, the value of the conversion cost TT</span><span class="c4">S</span><span class="c0">(A, S) </span><span class="c1">equals zero for identity transformations, i.e., when a matrix is already in the optimal representation. Hence, besides the parenthesization split point k, we vary the input and output storage types for each step in recurrence (3). </span></p><p class="c10"><span class="c1">Some of the parameters that contribute to the cost func- tions TM/TT(&middot;), for example the density &rho; of intermediate results, are not known prior to the execution and have to be estimated. Therefore, we developed the sparse matrix product density estimator SpProdest, which is described in section 4 and 5 of this paper. The resulting costs derived from recurrence (3) are minimal, given that the estimated costs encoded in TM and TT are determined precisely. In par- ticular, the optimality, or goodness, of SpMachO depends on two parts, which potentially contain uncertainties: first, the accuracy of the quantitative cost model of the multiplication kernels, and second, the precision of the density estimates provided by SpProdest. </span></p><p class="c11"><span class="c1">The total number of the possible execution plans using our model (3) for a matrix chain multiplication of length p is </span></p><p class="c11"><span class="c1">C</span><span class="c5">p 1 </span><span class="c1">&middot; 2</span><span class="c9">3(p 1)</span><span class="c1">, (4) </span></p><p class="c11"><span class="c1">where C</span><span class="c5">p 1 </span><span class="c1">denotes the Catalan number C</span><span class="c5">n </span><span class="c1">= </span><span class="c9">(2n)! </span></p><p class="c11"><span class="c5">(n+1)!n!</span><span class="c14">. </span><span class="c1">C</span><span class="c5">p 1 </span><span class="c1">reflects the number of possible parenthesizations, which is the same as for the textbook case [12]. The second factor is related to the 2</span><span class="c9">3 </span><span class="c1">{left input-, right input-, output-} storage type combinations that are connected with each of the p 1 type multiplication nodes. The number in (4) resembles the size of the search space, which grows exponentially. To give an example, it yields 2560 for a matrix chain of length p = 4 and already 1376256 for p = 6. As in [12], SpMachO solves the recurrence (3) in O(p</span><span class="c9">3</span><span class="c1">) time using a bottom-up dynamic programming approach. </span></p><p class="c11"><span class="c20">3. SpMachO </span></p><p class="c10"><span class="c1">The pseudocode of SpMachO is sketched in Algorithm 1. The cost of the optimal sub-chain multiplications and the relevant plan information (split points and storage types per sub-chain) are cached in three-dimensional array structures. For each combination in the inner loop, the method chk- MemLimit checks if the total memory consumption of the matrices and intermediate results in the current plan configu- ration would exceed the system limit. The memory required for dense m &#8677; n matrices is O(mn), and O(N</span><span class="c5">nz </span><span class="c1">= mn&rho;) for sparse matrices. Since SpMachO optimizes the runtime performance, the plan may contain conversions from sparse into dense matrix representations whenever the dense kernel leads to a lower overall runtime, due to the efficient dense kernel implementation. However, the conversions into dense representations potentially increase the memory consumption compared to a sparse-only plan. Our strategy is that every conversion is allowed, as long as the total memory consump- tion at every point in time does not exceed a hard memory </span></p><p class="c11"><span class="c26">291 </span></p><p class="c69"><span class="c1">Algorithm 1 SpMachO </span></p><p class="c64"><span class="c1">1: function spMacho(MatrixChain A</span><span class="c4">[1..p]</span><span class="c0">,TM,TT) </span><span class="c1">2: &circ;</span><span class="c22">&#8674;</span><span class="c1">[][] spProdest(A</span><span class="c4">[1..p]</span><span class="c0">) </span><span class="c1">3: for 1 &#63743; j&lt;p, j&gt;i 0 do 4: for i &#63743; k&lt;j do 5: for types 2 {sparse,dense} do 6: if !chkMemLimit(A, k,&#8674;,types) then 7: continue 8: q TT(A</span><span class="c4">[i..k..j]</span><span class="c0">, &circ;</span><span class="c39">&#8674;</span><span class="c0">[][],types) </span><span class="c1">9: q q + TM(A</span><span class="c4">[i..k..j]</span><span class="c1">, &circ;</span><span class="c22">&#8674;</span><span class="c1">[][],types) 10: if q &lt; cost[i][j][S</span><span class="c9">o</span><span class="c1">] then 11: cost[i][j][S</span><span class="c9">o</span><span class="c1">] q 12: plan[i][j][S</span><span class="c9">o</span><span class="c1">] k, types 13: if cost[1][p][&middot;] MAXVAL] then 14: /* memory exceed exception */ 15: else 16: return min(plan[1][p][sparse],plan[1][p][dense]) </span></p><p class="c83"><span class="c1">limit. Execution paths that would exceed the memory limit are automatically skipped (line 6). We assume that there is at least one plan that does not exceed the memory limit. If not, SpMachO returns with an exception (line 13). Finally, the resulting plan, which can be converted into a directed acyclic graph representation as of Fig. 2, is returned to the system for execution. </span></p><p class="c52"><span class="c1">The complexity of Algorithm 1 is O(p</span><span class="c9">3</span><span class="c1">), which can be derived from the dynamic programming loop and is the same as in the dense-only problem. The additional complexity by the introduction of storage type transformations yields a constant factor, since the inner loops over the storage types do not depend on the chain length p. A few pruning methods can be applied to reduce the execution plan space, for example, by excluding that the product of two dense can be sparse. However, they do not lower the asymptotic complexity of the algorithm. </span></p><p class="c80"><span class="c1">The system executes the plan using the corresponding transformation and matrix multiplication operators. While the unary transformation operator either performs a dense- to-sparse or a sparse-to-dense storage transformation, the eight-fold multiplication operator delegates the execution to one of the multiplication kernels. We implemented all algorithms in our prototype using row-major 2D-arrays for dense- and the columnar compressed sparse row layout (CSR) for sparse matrices, since they are to our notion the most common physical representations, and are used in many numerical libraries, e.g. the Intel Math Kernel Library [1]. As mentioned in the beginning, we already showed in [20] that these representations can be mapped onto a columnar storage layout of an in-memory columnar DBMS. </span></p><p class="c32"><span class="c20">3.1 Multiplication Kernels </span></p><p class="c53"><span class="c1">There are eight different general matrix multiply (gemm) kernels that are used in our system. We will use the notation xyz gemm to denote a multiplication kernel, where x is the left-hand input-, y is the right-hand input type and z the output matrix storage type, which can be either sparse (sp) or dense (d). Some of the kernels, for example the stan- dard BLAS ddd gemm, or spdd gemm, are implemented by vendor-tuned C++ libraries, so we call the library instead of providing an own implementation. As of the current status, this is done only for ddd gemm, for which we use the Intel </span></p><p class="c11 c24"><span class="c1">Table 1: The matrix multiplication kernels for the product C</span><span class="c9">m&#8677;n </span><span class="c1">= A</span><span class="c9">m&#8677;k </span><span class="c1">&middot; B</span><span class="c9">k&#8677;n </span><span class="c1">and their cost functions used in SpMachO. N</span><span class="c4">&#8677; </span><span class="c0">denotes the number of actual multiplications, </span><span class="c1">the hat indicates the corresponding estimated value. &crarr;, , are constant parameters. </span></p><p class="c68"><span class="c1">Kernel Cost Function </span></p><p class="c30"><span class="c1">ddd gemm &crarr;(mkn) spdd gemm &crarr; </span><span class="c14">&circ;</span><span class="c1">N</span><span class="c5">&#8677; </span><span class="c1">dspd gemm &crarr;(mk) + </span><span class="c14">&circ;</span><span class="c1">N</span><span class="c4">&#8677; </span><span class="c1">spspd gemm &crarr;N</span><span class="c9">A</span><span class="c5">nz </span><span class="c14">+ &circ;</span><span class="c1">N</span><span class="c5">&#8677; </span><span class="c1">ddsp gemm &crarr; </span><span class="c14">&circ;</span><span class="c1">N</span><span class="c4">&#8677; </span><span class="c0">+ </span><span class="c1">&circ;</span><span class="c0">N</span><span class="c5">C</span><span class="c4">nz </span><span class="c1">+ (mn) spdsp gemm &crarr;N</span><span class="c9">A</span><span class="c5">nz </span><span class="c14">+ &circ;</span><span class="c1">N</span><span class="c5">&#8677; </span><span class="c1">+ </span><span class="c14">&circ;</span><span class="c1">N</span><span class="c9">C</span><span class="c5">nz </span><span class="c1">dspsp gemm &crarr; </span><span class="c14">&circ;</span><span class="c1">N</span><span class="c4">&#8677; </span><span class="c0">+ </span><span class="c1">&circ;</span><span class="c0">N</span><span class="c5">C</span><span class="c4">nz </span><span class="c1">+ (mk) spspsp gemm &crarr;N</span><span class="c9">A</span><span class="c5">nz </span><span class="c14">+ &circ;</span><span class="c1">N</span><span class="c4">&#8677; </span><span class="c0">+ </span><span class="c1">&circ;</span><span class="c0">N</span><span class="c5">C</span><span class="c4">nz </span></p><p class="c58"><span class="c1">MKL implementation. </span></p><p class="c45"><span class="c1">Table 1 lists the kernels that are used in our system. In order to obtain the optimal execution plan via solving recur- rence (3), the cost model and its corresponding parameters have to be determined accurately for each multiplication ker- nel. Since the actual runtime depends on many parameters and external influences, an exact determination cannot be guaranteed. However, even for small variations, the plan gen- erated by SpMachO is still near-optimal, which we verified in the evaluation in section 6. </span></p><p class="c65"><span class="c20">3.2 Execution Time Cost Model </span></p><p class="c61"><span class="c1">The cost for multiplying two dense or sparse matrices generally depends on the matrix dimensions m, k, n, the number and pattern of non-zero elements of both the input and the result matrices, and the implementation details of the corresponding kernel algorithm. The idea is to reduce the dimensions to a set of only a few, significant dimensions, and create the cost model based on the reduced dimension set. On average, it is a fair approximation to assume that the runtime of a single multiplication only depends on the number of non-zero elements, and not on the individual non- zero pattern variations. Hence, we are able to reduce the parameter space to the dimensions m, k, n,&#8674;</span><span class="c5">A</span><span class="c1">, &#8674;</span><span class="c5">B</span><span class="c1">, and &circ;&#8674;</span><span class="c5">C</span><span class="c1">, which corresponds to the product density estimation, which is determined by SpProdest. </span></p><p class="c82"><span class="c1">As an example, we will examine the spspsp gemm kernel that uses the popular Gustavson algorithm [18]. The algo- rithm is based on the sparse accumulator method, which is still commonly used for sparse matrix implementations [15]. In order to not repeat the algorithm description in detail, we only sketch the derivation of our cost model for spspsp gemm, which we consider as the most interesting kernel. </span></p><p class="c41"><span class="c1">For reasonably large sparse matrices, the runtime of sparse kernels is often dominated by main memory bandwidth. Hence, mainly the memory accesses contribute to the run- time of an algorithm, which is in conformance to the external memory (I/O) model. For spspsp gemm, the access pattern can be formulated as </span></p><p class="c76"><span class="c1">TM(m, k, n,&#8674;</span><span class="c4">A</span><span class="c0">,&#8674;</span><span class="c4">B</span><span class="c0">) = m &#8677; (read</span><span class="c4">rowA </span><span class="c0">+ k&#8674;</span><span class="c4">A </span><span class="c0">&#8677; (read</span><span class="c4">colA,rowB</span><span class="c0">+ </span><span class="c1">n&#8674;</span><span class="c4">B </span><span class="c0">&#8677; (read</span><span class="c4">colB,valA,valB </span><span class="c0">+ write</span><span class="c4">colC,valC</span><span class="c0">)) + n&circ;&#8674;</span><span class="c4">C</span><span class="c0">write</span><span class="c4">colC,valC</span><span class="c0">), </span></p><p class="c24 c75"><span class="c1">where the read/write denote the accesses to the respective data structures (rowA, rowB, etc.) in memory. For example, </span></p><p class="c51"><span class="c26">292 </span></p><p class="c11"><span class="c2">10</span><span class="c25">] s[emitnu</span><span class="c2">R</span><span class="c25">40 200</span><span class="c19">3 </span><span class="c2">10</span><span class="c19">4 </span><span class="c2">Matrix dimension m </span></p><p class="c11"><span class="c25">40 </span></p><p class="c11"><span class="c25">200</span><span class="c2">10 </span><span class="c19">3 </span><span class="c2">10 </span><span class="c19">2 </span><span class="c2">10 </span><span class="c19">1 </span><span class="c2">10</span><span class="c19">0 </span><span class="c2">Matrix Density &#8674;</span><span class="c8">A </span></p><p class="c11"><span class="c7">spspd meas spspd est spspd meas </span></p><p class="c11"><span class="c7">spspd est ddd est ddd meas </span></p><p class="c21"><span class="c23">(a) Scaling behavior: varying the dimension (left) and density (right). Other dimensions are fixed: k = 8192,n = 8192, </span><span class="c34">&rho;</span><span class="c4">B </span><span class="c23">= 0.1 </span><span class="c25">10 </span></p><p class="c11"><span class="c1">ddd gemm </span></p><p class="c11"><span class="c25">10 </span></p><p class="c11"><span class="c1">spspd gemm </span><span class="c25">886644220</span><span class="c2">10 </span><span class="c19">2</span><span class="c2">10 </span><span class="c19">1 </span><span class="c2">10</span><span class="c19">0 </span><span class="c2">10</span><span class="c19">1 </span><span class="c2">10</span><span class="c19">2 </span><span class="c2">10</span><span class="c19">3 </span><span class="c25">0</span><span class="c2">10 </span><span class="c19">2</span><span class="c2">10 </span><span class="c19">1 </span><span class="c2">10</span><span class="c19">0 </span><span class="c2">10</span><span class="c19">1 </span><span class="c2">10</span><span class="c19">2 </span><span class="c2">10</span><span class="c19">3 </span><span class="c2">Matrix shape </span><span class="c19">m</span><span class="c8">k </span></p><p class="c11"><span class="c2">Matrix shape </span><span class="c19">m</span><span class="c8">k </span><span class="c25">10 </span></p><p class="c11"><span class="c1">dspsp gemm </span></p><p class="c11"><span class="c25">10 </span></p><p class="c11"><span class="c1">spspsp gemm </span><span class="c25">886644220</span><span class="c2">10 </span><span class="c19">2 </span><span class="c2">10 </span><span class="c19">1 </span><span class="c2">10</span><span class="c19">0 </span><span class="c2">10</span><span class="c19">1 </span><span class="c2">10</span><span class="c19">2 </span><span class="c25">0</span><span class="c2">10 </span><span class="c19">2 </span><span class="c2">10 </span><span class="c19">1 </span><span class="c2">10</span><span class="c19">0 </span><span class="c2">10</span><span class="c19">1 </span><span class="c2">10</span><span class="c19">2 </span><span class="c2">Matrix shape </span><span class="c19">m</span><span class="c8">k </span></p><p class="c11"><span class="c2">Matrix shape </span><span class="c19">m</span><span class="c8">k </span></p><p class="c11"><span class="c7">measured estimated </span></p><p class="c11"><span class="c23">(b) Varying the matrix shape m/k. N</span><span class="c4">&#8677; </span><span class="c38">is fixed in each plot. </span></p><p class="c11"><span class="c1">Figure 3: Measured runtimes (markers) and time estimates (lines) for different matrix multiplication kernels. </span></p><p class="c10"><span class="c1">a read on colA has a higher average cost than a read on colB, since a complete row of matrix B is touched in-between two consecutive colA-reads, thus, the system has most probably evicted the cache line of colA and has to fetch it again. The exact number of cycles, and hence, the required time per read or write access depends on whether the addressed cache line resides in the system cache. However, since we consider large matrices with sizes that are by factors larger than the last level system cache, we approximate the read and write accesses as fixed time constants in our model. Moreover, instead of determining the individual time constants for the read</span><span class="c5">X</span><span class="c1">/write</span><span class="c5">X </span><span class="c1">access, we abstracted them into few parame- ters, which can then be determined empirically. Therefore, we expand the expression of TM and accumulate the time constants of the read/write accesses into the constant param- eters &crarr;, , , and obtain a simple time approximation for the </span></p><p class="c11"><span class="c1">Wall-clock time </span></p><p class="c11"><span class="c1">T &#8673; &crarr;(m </span><span class="c0">| </span><span class="c1">&middot; </span><span class="c0">{z </span><span class="c1">k &middot; &#8674;</span><span class="c4">A </span><span class="c0">} </span></p><p class="c11"><span class="c5">N</span><span class="c7">nz </span></p><p class="c11"><span class="c16">A</span><span class="c1">) + (m </span><span class="c0">| </span><span class="c1">&middot; k &middot; &#8674;</span><span class="c0">{z </span><span class="c4">A </span><span class="c0">&middot; n &middot; &#8674;</span><span class="c4">B </span><span class="c0">} </span></p><p class="c11"><span class="c1">) + (m </span><span class="c0">| </span><span class="c1">&middot; n </span><span class="c0">{z </span><span class="c1">&middot; &circ;&#8674;</span><span class="c4">C </span><span class="c0">} </span><span class="c4">N</span><span class="c5">&circ;</span><span class="c88">&#8677; </span></p><p class="c11"><span class="c4">N</span><span class="c5">&circ;</span><span class="c18">nz </span></p><p class="c11"><span class="c7">C</span><span class="c1">), </span></p><p class="c11"><span class="c1">which only depends on the constant parameters </span></p><p class="c11"><span class="c1">&crarr; = T(read</span><span class="c4">colA,rowB</span><span class="c0">) </span></p><p class="c11"><span class="c1">= T(read</span><span class="c4">colB,valA,valB </span><span class="c0">+ write</span><span class="c4">colC,valC</span><span class="c0">) </span><span class="c1">= T(write</span><span class="c4">colC,valC</span><span class="c0">) </span><span class="c1">and the derived dimensions: </span></p><p class="c11"><span class="c1">&bull; N</span><span class="c5">nz</span><span class="c9">A</span><span class="c14">: the number of non-zeros in matrix A </span></p><p class="c11"><span class="c1">&bull; N</span><span class="c5">&#8677;</span><span class="c1">: </span><span class="c14">&circ;</span><span class="c1">the estimated number of multiplications </span></p><p class="c11"><span class="c1">&bull; N</span><span class="c14">&circ;</span><span class="c5">nz</span><span class="c9">C</span><span class="c14">: the estimated number of non-zero elements in the </span><span class="c1">result matrix For the other kernels, the cost function can be deduced in a similar manner. Because of space limitations, we will not discuss them in this paper. Table 1 lists the cost function for each kernel. Each cost function is a linear combination of different derived dimensions, weighted by constant pa- rameters &crarr;, , . The constant parameters are estimated for each kernel by a multilinear least-squares fit. Since they are dependent on the system hardware, the fit has to be done once for each system configuration. </span></p><p class="c21"><span class="c1">Fig. 3a shows the scaling behavior of the matrix multiplica- tion kernels spspsp gemm, spspd gemm and ddd gemm with respect to matrix dimension and density. We observe that our cost models (lines) conform well with the actual algorithm runtimes (markers). From the right plot in Fig. 3a we can infer the following example scenarios: if the density &#8674;</span><span class="c4">A </span><span class="c0">has </span><span class="c1">a higher value than about 0.1, then it can be worthwhile to convert A into a dense representation and continue with the dense kernel &ndash; of course only if the dense representation does not exceed the available memory. If the density is below, it is probably best to select the spspd gemm kernel. For &#8674;</span><span class="c5">A </span><span class="c1">&#8999; 0.01 and depending on the following multiplications and the estimated intermediate result densities, it might be best to take with the spspsp gemm kernel. In Fig. 3b, we fixed the densities &#8674;</span><span class="c5">A</span><span class="c1">,&#8674;</span><span class="c5">B </span><span class="c1">and the (m &middot; k &middot; n), and only varied the relative matrix shapes At both edges of each plot the matrices are extremely product </span><span class="c9">m</span><span class="c5">k </span><span class="c1">rectan- </span><span class="c14">&#8984; </span><span class="c5">n</span><span class="c9">k</span><span class="c14">. </span></p><p class="c10"><span class="c1">gular. The deviation of the actual times from our estimates shows that our cost model has limited accuracy in these extreme cases. However, the deviation is still acceptable, since the times are always overestimated. Overestimation is more robust than underestimation, because SpMachO would then just select another multiplication kernel, whereas in the latter case the deviation would propagate into the overall estimation. It is worthwhile mentioning that one conclusion we deduce from Fig. 3b is that simplistic cost models, which solely depend on the number of non-zero multiplications N</span><span class="c4">&#8677;</span><span class="c0">, are not able to describe the shape dependency, since </span><span class="c1">N</span><span class="c5">&#8677; </span><span class="c1">&#8984; const. is fixed in each plot. </span></p><p class="c11"><span class="c20">4. DENSITY ESTIMATION </span></p><p class="c10"><span class="c1">The estimation of the intermediate result matrix densities is a crucial part of the SpMachO optimizer, since the cost models of the sparse multiplication kernels primarily depend on the number of non-zero elements, and hence, the matrix densities &#8674;. Our approach is to encode the non-zero structure into the smallest possible set of values without losing too much information. </span></p><p class="c11"><span class="c26">293 </span></p><p class="c3"><span class="c1">1b) &#10003;</span><span class="c0">&rho;</span><span class="c4">11 </span><span class="c0">&rho;</span><span class="c4">12 </span></p><p class="c11"><span class="c1">&rho;</span><span class="c5">21 </span><span class="c1">&rho;</span><span class="c5">22</span><span class="c0">CCCCA </span></p><p class="c11"><span class="c1">Figure 4: Assignment of non-zero population densities for two different matrices using a) scalar density, b) density map. </span></p><p class="c11"><span class="c20">4.1 Scalar Density </span></p><p class="c10"><span class="c1">A matrix can be considered as a two-dimensional object which has a certain population density &rho; of non-zero matrix elements. As an example, Fig. 4 a) shows a 4 &#8677; 4 matrix, which has five non-zero elements, and thus, a total population density of &rho; = 5/16 &#8673; 0.31. The scalar density value does not reflect any patterns in the matrix, but it contains all relevant information if, and only if, the matrix is uniformly populated with non-zero elements. Then, the probability of a randomly picked matrix element for being non-zero is p((A)</span><span class="c5">ij </span><span class="c1">= 0) = &rho;, which is 0.31 in the example of Fig. 4 a). </span></p><p class="c10"><span class="c1">Lemma 4.1. Under the condition that the non-zero elements are uniformly distributed, the density estimate &circ;&rho; of a product of two matrices C = A&middot;B can be calculated using probabilistic propagation </span></p><p class="c11"><span class="c1">&circ;&rho;</span><span class="c4">C </span><span class="c0">= &circ;&rho;</span><span class="c4">A&middot;B </span><span class="c0">= 1 (1 &rho;</span><span class="c4">A</span><span class="c0">&rho;</span><span class="c4">B</span><span class="c0">)</span><span class="c5">k</span><span class="c0">. (5) </span></p><p class="c11"><span class="c1">The estimate of Eq. (5) is unbiased, i.e. E[&circ;&rho;</span><span class="c4">C</span><span class="c0">] &#8984; &rho;</span><span class="c4">C</span><span class="c0">. </span></p><p class="c21"><span class="c1">For the sake of simplicity, we denote the operation in Eq. (5) with the symbol , thus, &circ;&rho;</span><span class="c5">C </span><span class="c1">= &rho;</span><span class="c5">A </span><span class="c1">&rho;</span><span class="c5">B</span><span class="c1">. Lemma 4.1 can be derived as follows: using elements c</span><span class="c4">ij </span><span class="c0">of the result </span><span class="c1">The probability for a</span><span class="c5">ik </span><span class="c1">being the </span><span class="c0">matrix </span><span class="c1">inner non-zero </span><span class="c0">are </span><span class="c1">product </span><span class="c0">calculated </span><span class="c1">is p(a</span><span class="c5">ik </span><span class="c1">formulation, </span><span class="c0">as </span><span class="c1">= P0) </span><span class="c4">k </span><span class="c1">a= </span><span class="c4">ik</span><span class="c0">b</span><span class="c1">the &rho;</span><span class="c5">A</span><span class="c1">, </span><span class="c4">kj</span><span class="c0">. </span></p><p class="c10"><span class="c1">for b</span><span class="c4">kj </span><span class="c0">accordingly: p(b</span><span class="c4">kj </span><span class="c0">= 0) = &rho;</span><span class="c4">B</span><span class="c0">. Thus, every summand </span><span class="c1">a</span><span class="c4">ik</span><span class="c0">b</span><span class="c4">kj </span><span class="c0">is nonzero with probability p</span><span class="c4">nz</span><span class="c0">(a</span><span class="c4">ik</span><span class="c0">)^p</span><span class="c4">nz</span><span class="c0">(b</span><span class="c4">kj</span><span class="c0">) = &rho;</span><span class="c4">A</span><span class="c0">&rho;</span><span class="c4">B</span><span class="c0">. </span><span class="c1">c</span><span class="c4">ij </span><span class="c0">is non-zero if any of the summands a</span><span class="c4">ik</span><span class="c0">b</span><span class="c4">kj </span><span class="c0">is non-zero. </span><span class="c1">We leverage the inverse probability and obtain p(c</span><span class="c4">ij </span><span class="c0">= 0) = </span><span class="c1">&#8679;</span><span class="c5">k</span><span class="c1">(1 &rho;</span><span class="c5">A</span><span class="c1">&rho;</span><span class="c5">B</span><span class="c1">). Finally, with p(c</span><span class="c5">ij </span><span class="c1">=0)=1 p(c</span><span class="c5">ij </span><span class="c1">= 0) and p(c</span><span class="c4">ij </span><span class="c0">= 0) = &rho;</span><span class="c4">C</span><span class="c0">, equation (5) results. We remark that </span><span class="c1">we are assuming no cancellation, i.e., a sum of products of overlapping non-zero elements never cancel to zero, which is a very common assumption in the mathematical programming literature [10]. </span></p><p class="c11"><span class="c1">Eq. (5) can be used as an O(1) estimator for the result density prediction of a multiplication of two matrices A and B that have uniform non-zero patterns. Hence, the prediction of a chain multiplication of p matrices has a linear time complexity O(p). Moreover, the density prediction is independent of the parenthesization. </span><span class="c20">4.2 Estimation Errors </span></p><p class="c10"><span class="c1">However, the obvious disadvantage of maintaining a scalar density is that &circ;&rho; is only valid for matrices with uniformly distributed non-zero elements. Although the uniform as- sumption holds for many matrices to a certain degree, there </span></p><p class="c11"><span class="c14">&#9670; </span></p><p class="c11"><span class="c1">= </span></p><p class="c11"><span class="c1">0</span><span class="c0">BBBB@ </span><span class="c1">a) </span></p><p class="c11"><span class="c1">0&rho; = </span><span class="c0">BBBB@ </span></p><p class="c11"><span class="c9">5</span><span class="c5">16 </span></p><p class="c11"><span class="c1">0</span><span class="c0">BBBB@ </span></p><p class="c11"><span class="c1">1</span><span class="c0">CCCCA </span><span class="c1">10 2 0 0 5 0 0 8 0 0 8 0 </span></p><p class="c11"><span class="c0">CCCCA </span><span class="c1">4 0 0 0 </span></p><p class="c10"><span class="c1">0 2 0 0 3 9 0 1 0 0 1 3 0 0 2 0 </span></p><p class="c11"><span class="c1">10</span><span class="c0">CCCCA BBBB@ </span></p><p class="c11"><span class="c1">0.75 0.25 </span></p><p class="c11"><span class="c1">0 0.75 </span></p><p class="c11"><span class="c1">&middot; = </span></p><p class="c11"><span class="c1">&middot; = </span></p><p class="c10"><span class="c1">Figure 5: Product density in extreme non-uniform cases. The upper row shows how two half-populated matrices cancel to zero. The lower row shows how two, almost empty sparse matrices produce a full matrix (outer vector product). </span></p><p class="c10"><span class="c1">are many matrices that have distinguishable non-zero pat- terns, i.e. a topology with some regions that are significantly more dense than others. For these matrices, a density predic- tion according to equation 5 does not provide an accurate, unbiased result. </span></p><p class="c21"><span class="c1">In extreme non-uniform cases, equation (5) could produce an asymptotic maximum error of 100%. Fig. 5 shows two example cases where the &circ;&rho; estimate according to (5) fails significantly: In the first case, the product of two n &#8677; n matrices with each &rho; = 0.5 cancel out into an empty matrix with &rho; = 0, whereas &circ;&rho; = 1 (0.75)</span><span class="c9">n n!1 </span><span class="c1">! the 1. naive Hence, estimate the density according value is to maximal Eq. 5 is </span></p><p class="c21"><span class="c1">overestimated. The second example is a multiplication of two in A sparse and the &rho; = matching </span><span class="c5">n </span><span class="c9">1</span><span class="c14">matrices, which are zero except one column </span><span class="c1">row in B. The resulting full matrix has &rho; = 1, whereas the naive prediction gives &circ;&rho; = 1 (1 </span><span class="c4">n</span><span class="c5">1</span><span class="c7">2 </span><span class="c1">In </span><span class="c14">)</span><span class="c9">n </span><span class="c1">order </span><span class="c9">n!1 </span><span class="c1">! to 0. </span></p><p class="c11"><span class="c1">lower the average estimation error, we estimate matrix densities on a finer granularity. SpProdest uses a density map for non-uniform sparse matrices, which is able to reflect a 2D matrix pattern on a configurable, granular level. </span><span class="c20">4.3 Density Map </span><span class="c1">The density map fectively a density values </span><span class="c9">m</span><span class="c5">b </span><span class="c14">&#8677; </span><span class="c1">(&rho;)</span><span class="c5">ij</span><span class="c1">, </span><span class="c9">n</span><span class="c5">b </span><span class="c14">density </span><span class="c22">&#8674;</span><span class="c4">A </span><span class="c1">each of a m &#8677; n sparse matrix A is ef- </span></p><p class="c11"><span class="c14">histogram. It consists of </span><span class="c1">referring to the density of the </span><span class="c9">mn </span></p><p class="c10"><span class="c1">corre- </span><span class="c5">b</span><span class="c16">2 </span><span class="c1">sponding block A</span><span class="c4">ij </span><span class="c0">of size b&#8677; b in matrix A. As an example, </span><span class="c1">Fig. 4b) shows the density map of a 4&#8677; 4 matrix with blocks of size 2 &#8677; 2. </span><span class="c0">result </span><span class="c39">&#8674;</span><span class="c0">glance </span><span class="c4">B </span><span class="c0">In </span><span class="c1">of </span><span class="c0">the matrix </span><span class="c1">its </span><span class="c0">at following </span><span class="c1">factor </span><span class="c0">blocked C </span><span class="c1">matrices. </span><span class="c0">are we matrix estimated sketch </span><span class="c1">Therefore, </span><span class="c0">multiplication. how from the the </span><span class="c1">it </span><span class="c0">density </span><span class="c1">is </span><span class="c0">density </span><span class="c1">necessary </span><span class="c0">Assuming map maps &circ;</span><span class="c39">&#8674;</span><span class="c1">to </span><span class="c4">C </span><span class="c39">&#8674;</span><span class="c0">square </span><span class="c1">take </span><span class="c4">A </span><span class="c1">of and the a </span><span class="c0">blocks, the product matrix C can be represented as </span></p><p class="c11"><span class="c23">C = </span></p><p class="c11"><span class="c34">&#10003;</span><span class="c38">AA</span><span class="c4">11 21 </span><span class="c47">&middot; &middot; </span><span class="c38">BB</span><span class="c4">11 11 </span><span class="c38">+ + AA</span><span class="c4">12 22 </span><span class="c47">&middot; &middot; </span><span class="c38">BB</span><span class="c4">21 21 </span><span class="c38">AA</span><span class="c4">11 21 </span><span class="c47">&middot; &middot; </span><span class="c38">BB</span><span class="c4">12 12 </span><span class="c38">+ + AA</span><span class="c4">12 12 </span><span class="c47">&middot; &middot; </span><span class="c38">BB</span><span class="c4">22 22</span><span class="c34">&#9670; </span></p><p class="c11"><span class="c23">. (6) </span></p><p class="c11"><span class="c1">First, we define an estimator for the addition of two ma- trices: </span></p><p class="c10"><span class="c1">Lemma 4.2. Under the condition that the non-zero ele- ments are uniformly distributed, the density estimate &circ;&rho; of the addition of two matrices C = A &middot; B can be calculated using probabilistic propagation </span></p><p class="c11"><span class="c1">&circ;&rho;</span><span class="c5">A+B </span><span class="c1">= &rho;</span><span class="c5">A </span><span class="c1">+ &rho;</span><span class="c5">B </span><span class="c1">(&rho;</span><span class="c5">A</span><span class="c1">&rho;</span><span class="c5">B</span><span class="c1">) &#8984; &rho;</span><span class="c5">A </span><span class="c1">&rho;</span><span class="c5">B</span><span class="c1">. (7) </span></p><p class="c11"><span class="c1">The derivation of Lemma 4.2 is similar to that of Lemma 4.2, which is why we leave it out for space reasons. </span></p><p class="c11"><span class="c26">294 </span></p><p class="c10"><span class="c1">variance of two random, normally distributed</span><span class="c9">3 </span><span class="c1">variables X and Y . If the test statistic f = V</span><span class="c4">X</span><span class="c0">/V</span><span class="c4">Y </span><span class="c0">(ratio of variances </span><span class="c1">in X and Y ) exceeds a critical value f</span><span class="c4">crit </span><span class="c0">according to an </span><span class="c1">&alpha;-quantile of the F-distribution, then H</span><span class="c5">0 </span><span class="c1">is rejected, mean- ing that X and Y are not of the same distribution with probability 1-&alpha;. </span></p><p class="c11"><span class="c1">For a matrix with N</span><span class="c4">B </span><span class="c0">b &#8677; b blocks, we define </span></p><p class="c11"><span class="c1">V</span><span class="c4">observed </span><span class="c0">= </span><span class="c1">N</span><span class="c4">B </span><span class="c1">1 </span></p><p class="c11"><span class="c0">1 </span></p><p class="c11"><span class="c1">X</span><span class="c4">ij </span></p><p class="c11"><span class="c1">(N</span><span class="c5">nz</span><span class="c1">(ij) E[N</span><span class="c5">nz </span><span class="c9">b&#8677;b </span><span class="c14">])</span><span class="c9">2 </span><span class="c1">(9) </span></p><p class="c11"><span class="c1">f = </span></p><p class="c11"><span class="c1">&#10003;</span><span class="c0">V</span><span class="c4">observed </span><span class="c1">V</span><span class="c4">expected </span></p><p class="c11"><span class="c1">&#9670; </span></p><p class="c11"><span class="c1">= </span></p><p class="c11"><span class="c1">&#10003; </span><span class="c0">V</span><span class="c4">observed </span></p><p class="c11"><span class="c1">E[V (N</span><span class="c5">nz </span><span class="c9">b&#8677;b </span><span class="c1">)]</span><span class="c14">&#9670; </span></p><p class="c11"><span class="c1">(10) </span></p><p class="c11"><span class="c1">For uniformly distributed matrices, f has the expectation value E[f] = 1. The exact choice of the threshold, however, depends on the sample size, i.e. the number of blocks N</span><span class="c4">B </span><span class="c1">and the desired accuracy. </span></p><p class="c11"><span class="c33">4.4.2 Entropy </span></p><p class="c11"><span class="c1">A known measure for the disorder of elements is the entropy </span></p><p class="c11"><span class="c0">X</span><span class="c5">N</span><span class="c4">i </span><span class="c1">Then, combining Eq. (6) with (5) and (7), one obtains </span></p><p class="c11"><span class="c34">&circ;&#8674;</span><span class="c4">C </span><span class="c23">= </span></p><p class="c11"><span class="c1">p</span><span class="c5">i </span><span class="c1">ln p</span><span class="c5">i</span><span class="c1">, (11) </span></p><p class="c10"><span class="c1">which is defined over a space with N entities (or states) i that have a relative probability p</span><span class="c4">i</span><span class="c0">. The entropy is used in </span><span class="c1">a variety of contexts. To name an example, the Shannon entropy [26] is used in information theory to quantify the in- formation content of a message with N characters as entities. In a similar manner, we can define and quantify the informa- tion content of the non-zero pattern of a sparse matrix, by identifying the p</span><span class="c4">i </span><span class="c0">with the local block density &rho;</span><span class="c4">ij</span><span class="c0">. </span></p><p class="c10"><span class="c1">The entropy (11) is maximal if each entity has the same probability. In the terms of sparse matrices, the theoretical disorder is maximized if every block has the same local density &rho;</span><span class="c4">ij </span><span class="c0">&#8984; &rho;. The scaled entropy </span></p><p class="c11"><span class="c0">H </span><span class="c1">&nbsp;&#771;</span><span class="c0">= H</span><span class="c4">max </span><span class="c1">H= </span><span class="c34">&#10003;</span><span class="c47">&rho;&rho;</span><span class="c4">AA</span><span class="c18">11 21 </span><span class="c34">&rho;&rho;</span><span class="c4">BB</span><span class="c18">11 11 </span><span class="c34">&rho;&rho;</span><span class="c4">AA</span><span class="c18">12 22 </span><span class="c34">&rho;&rho;</span><span class="c4">BB</span><span class="c18">21 21 </span><span class="c34">&rho;&rho;</span><span class="c4">AA</span><span class="c18">11 21 </span><span class="c34">&rho;&rho;</span><span class="c4">BB</span><span class="c18">12 12 </span><span class="c34">&rho;&rho;</span><span class="c4">AA</span><span class="c18">12 22 </span><span class="c34">&rho;&rho;</span><span class="c4">BB</span><span class="c18">22 22</span><span class="c34">&#9670; </span></p><p class="c10"><span class="c1">for the density propagation of a 2 &#8677; 2 map. Density maps of a finer granularity, i.e. with more than four blocks, are calculated accordingly. </span></p><p class="c10"><span class="c1">As a result, the average density estimation error is signif- icantly lowered when using density maps compared to the scalar density estimation, which we verified empirically in the evaluation section (Fig. 6.) As a matter of fact, the smaller the block size and the higher the granularity, the more information is stored in the density map and finer struc- tures can be resolved. However, the runtime of the density map estimation also grows with the granularity, since its complexity is in O( </span><span class="c9">n</span><span class="c5">b </span></p><p class="c11"><span class="c5">3</span><span class="c0">), and hence, O(p </span><span class="c5">n</span><span class="c4">b </span></p><p class="c21"><span class="c5">3</span><span class="c0">) for a chain </span><span class="c1">estimation of length p. For infinitesimal block sizes b ! 1&#8677;1, the estimation error vanishes completely, but the determina- tion matrix of &circ;</span><span class="c22">&#8674;</span><span class="c1">multiplication </span><span class="c4">C </span><span class="c1">is then equivalent with the corresponding boolean of A &middot; B, and has the same problem complexity as the actual multiplication. Thus, the block size configuration is generally a trade-off between accuracy and runtime of the prediction. </span></p><p class="c11"><span class="c1">However, we employ a greedy strategy, which reduces the runtime by using density maps only for matrices with a skewed non-zero distribution, and the scalar density for matrices with an approximately uniform distribution. To decide whether a given matrix has an uniformly distributed or a skewed non-zero pattern we define a quantitative disorder measure for sparse matrices. </span><span class="c20">4.4 Matrix Disorder Measures </span></p><p class="c10"><span class="c1">We introduce two measures to quantify how the non-zero pattern of a sparse matrix deviates from an (approximate) uniform distribution. Since we already introduced the density map that involves blocks of different densities, it is natural to approach the problem from same the block-granular level. </span></p><p class="c11"><span class="c33">4.4.1 Variance Analysis </span></p><p class="c10"><span class="c1">One way of deciding whether or not a matrix is approxi- mately uniformly distributed is to make use of a statistical hypothesis testing method. The scalar density propagation formulas as shown in Lemmas 4.1 and 4.2 are based on the assumption that every element of the matrix has the same probability to be populated, and the probability is equal to the overall density &rho;. Using this assumption as the null hypothesis H</span><span class="c5">0</span><span class="c1">, the number of non-zero elements in each b &#8677;b-block would follow a binomial distribution B(N,p) with N = b</span><span class="c9">2 </span><span class="c1">and p = &rho;. This can be deduced in the same way as a coin toss experiment, where the number of experiments N is equal to the number of potential elements in a block, and the success probability p = &rho; equals the global population density. From elementary statistics [7] we get </span></p><p class="c11"><span class="c1">E[N</span><span class="c5">nz </span><span class="c9">b&#8677;b </span><span class="c14">] = b</span><span class="c9">2</span><span class="c1">&rho;, E[V (N</span><span class="c5">nz </span><span class="c9">b&#8677;b </span><span class="c14">)] = b</span><span class="c9">2</span><span class="c1">&rho;(1 &rho;) (8) </span></p><p class="c11"><span class="c1">for the expectation values of for the number of non-zero elements assuming Na </span><span class="c4">nz </span><span class="c1">binomial </span><span class="c0">in one </span><span class="c1">distribution </span><span class="c0">b &#8677; b and the </span><span class="c1">B(b</span><span class="c0">variance </span><span class="c9">2</span><span class="c1">,&rho;). </span></p><p class="c11"><span class="c0">of N </span><span class="c4">nz </span><span class="c5">b&#8677;b </span><span class="c1">when </span></p><p class="c10"><span class="c1">The (dis-)conformance of the null hypothesis H</span><span class="c4">0 </span><span class="c0">with the </span><span class="c1">reality can be determined using a simple one-factorial vari- ance analysis. Therefore, we use the F-test [7], a likelihood quotient test, which checks the conformance of the observed </span></p><p class="c11"><span class="c1">P</span><span class="c4">Nij </span><span class="c1">N</span><span class="c5">B</span><span class="c1">&rho;ln </span><span class="c18">B </span><span class="c1">&rho;</span><span class="c4">ij </span><span class="c0">ln </span><span class="c1">&rho; </span><span class="c0">&rho;</span><span class="c4">ij </span></p><p class="c11"><span class="c14">2 [0,1] (12) </span></p><p class="c10"><span class="c1">is sensitive to the matrix density skew, which we evaluated in section 6. However, in contrast to f, the entropy is rather suited and it for is hard measuring to interpret relative the changes absolute in the value non-zero of H. </span></p><p class="c11"><span class="c14">&nbsp;&#771;</span><span class="c1">disorder, </span></p><p class="c3"><span class="c20">5. SpProdest </span><span class="c1">SpProdest measure &delta; = </span><span class="c14">p</span><span class="c1">is 1/f sketched is retrieved in algorithm (getDisorder) 2. First, the for disorder each matrix, which is a modified version of f according to Eq. (10). Then, &delta; is used to decide whether to store a only scalar density value, or a density map (line 4). If &delta; is lower than a certain threshold &delta;</span><span class="c4">T</span><span class="c0">, then the density map is created, if the </span><span class="c1">disorder is higher, then a scalar density is chosen. Finally, the density estimates are calculated by using the probabilistic density propagation method (EstProdDensity), according to equations (5) and (7). Note that instead of calculating &delta; and &circ;</span><span class="c22">&#8674; </span><span class="c1">for each expression (line 4-7), they can be cached as matrix statistics in the system, and reused for further multiplications. </span></p><p class="c11"><span class="c1">The complexity of SpProdest depends on the actual gran- ularity of the density map. Assuming a chain multiplication </span></p><p class="c11"><span class="c5">3</span><span class="c0">for sufficiently large N and Np ! const., the binomial distribution can be approximated by a normal distribution </span></p><p class="c11"><span class="c26">295 </span></p><p class="c11"><span class="c1">Algorithm 2 SpProdest </span></p><p class="c11"><span class="c1">1: function spProdest(MatrixChain A</span><span class="c4">[1..p]</span><span class="c0">) </span><span class="c1">2: &circ;</span><span class="c22">&#8674;</span><span class="c1">[][] 0 3: for Matrix A</span><span class="c5">i </span><span class="c1">2 A</span><span class="c4">[1..p] </span><span class="c0">do </span><span class="c1">4: &delta; getDisorder(A</span><span class="c4">i</span><span class="c0">) </span><span class="c1">5: if &delta; &lt; &delta;</span><span class="c4">T </span><span class="c0">then </span><span class="c1">6: &circ;</span><span class="c22">&#8674;</span><span class="c1">[i][i] scalarDensity(A</span><span class="c5">i</span><span class="c1">) 7: else 8: &circ;</span><span class="c22">&#8674;</span><span class="c1">[i][i] densityMap(A</span><span class="c5">i</span><span class="c1">) 9: for 1 &lt;j&lt;p do 10: for j&gt;i&gt; 0 do 11: &circ;</span><span class="c22">&#8674;</span><span class="c1">[i][j] estProdDensity(&circ;</span><span class="c22">&#8674;</span><span class="c1">[i][j 1], &circ;</span><span class="c22">&#8674;</span><span class="c1">[j][j]) </span><span class="c0">return &circ;</span><span class="c39">&#8674;</span><span class="c0">[][] </span></p><p class="c10"><span class="c1">of p square matrices, the time complexity would be in the best case O(p) (no map) and in worst case O(p( </span><span class="c9">n</span><span class="c5">b </span><span class="c14">)</span><span class="c9">3</span><span class="c1">), which is equal to the chain multiplication of p </span><span class="c9">n</span><span class="c5">b </span><span class="c14">&#8677; </span><span class="c9">n</span><span class="c5">b </span><span class="c14">matrices, </span><span class="c1">where </span><span class="c9">n</span><span class="c5">b </span><span class="c14">is the dimension of the density grids. Analogously, </span><span class="c1">the space complexity of SpProdest is best case O(p), worst case O(p( </span><span class="c9">n</span><span class="c5">b </span><span class="c14">)</span><span class="c9">2</span><span class="c1">). In practice, the overhead of the SpProdest component is negligible against the potential speedup gained by the SpMachO, which is manifested in our evaluation in section 6.3. </span></p><p class="c11"><span class="c20">6. EVALUATION </span></p><p class="c11"><span class="c1">In this section, we first evaluate the accuracy of spProdest according to the deviation in the result sparse matrix densi- ties. Second, we apply SpMachO on different matrix chains and compare the execution runtime against R and a popular commercial numerical algebra system for matrix computa- tions (called system A here), and two further execution approaches. The platform for our prototype implementation is a two-socket Intel Xeon X5650 CPU with 2 &#8677; 6 cores with 2.66 GHz and a total of 48 GB RAM. </span><span class="c20">6.1 Density Estimate Accuracy </span></p><p class="c10"><span class="c1">As mentioned in section 4, the density &rho;</span><span class="c4">C </span><span class="c0">of a matrix </span><span class="c1">C = A &middot; B depends not only on the densities of the factor matrices &rho;</span><span class="c4">A </span><span class="c0">and &rho;</span><span class="c4">B</span><span class="c0">, but also on their non-zero patterns, </span><span class="c1">especially on the pattern skew. To show the effect of the non- zero pattern skew on the density estimation, we generated a set of matrices with increasing skew. The skew parameter &xi; in our example defines the slope of a linear ascend in the density distribution with increasing row number r: &rho;(r) = &xi; &middot; r. We fixed the total density of A, thus, N</span><span class="c5">nz </span><span class="c1">=const. </span></p><p class="c21"><span class="c1">The left-hand plot in Fig. 6 shows the actual density &rho;</span><span class="c4">C </span><span class="c1">and the estimated densities using the scalar density and the density map approach with different block sizes. In our example, the density of the product matrix &rho;</span><span class="c4">C </span><span class="c0">decreases </span><span class="c1">with increasing pattern skew. This conforms with our notion, that in most cases a higher skew at constant density leads to a lower density of the result matrix, although there are cases which show the opposite behavior, for example, as in Fig. 5. Nevertheless, it is clearly observable that a finer granularity of the density map, and thus, a smaller block size, results in a better density estimation. The idea is to choose the block size as large as possible, since a finer granular density grid negatively influences the runtime performance of spProdest. We chose a block size of 256 &#8677; 256 as a good compromise between accuracy and estimation runtime. The right-hand plot of Fig. 6 confirms that the disorder measures are both </span></p><p class="c11"><span class="c17">0.1</span><span class="c48">1 1 </span></p><p class="c11"><span class="c17">0.8Data skew &xi; </span></p><p class="c11"><span class="c17">Data skew &xi; </span></p><p class="c11"><span class="c17">1 </span></p><p class="c11"><span class="c48">y tisne</span><span class="c17">D0.6 </span></p><p class="c11"><span class="c17">D</span><span class="c7">&#8674;</span><span class="c18">C </span><span class="c7">Scalar Density BS 2048 BS 1024 </span></p><p class="c11"><span class="c7">BS 256 BS 64 </span></p><p class="c11"><span class="c48">r edrosi0.5 </span><span class="c17">0.4 </span></p><p class="c11"><span class="c17">0.2 </span></p><p class="c11"><span class="c17">0 0.2 0.4 0.6 0.8 1 </span></p><p class="c11"><span class="c17">0.01 </span></p><p class="c11"><span class="c17">0 0.2 0.4 0.6 0.8 1 </span></p><p class="c11"><span class="c17">0</span><span class="c7">= </span><span class="c89">p</span><span class="c7">1/f (log scale) </span></p><p class="c21"><span class="c18">H </span><span class="c7">&nbsp;&#771;</span><span class="c18">(linear scale) </span><span class="c1">Figure 6: Left: Estimated density &circ;&rho;</span><span class="c4">C </span><span class="c0">vs. actual density </span><span class="c1">(&rho;</span><span class="c4">C</span><span class="c0">) of the product matrix C = A &middot; B using the scalar </span><span class="c1">and the density map estimation with different grid block sizes (BS). Matrices: A 2 R</span><span class="c9">4096&#8677;2048</span><span class="c1">,B 2 R</span><span class="c9">2048&#8677;4096 </span><span class="c1">and average density h&rho;i = 0.1. Right: Influence of the matrix A,B nonzero skew on the disorder measures &delta;, H. </span><span class="c14">&nbsp;&#771;</span><span class="c1">sensitive to the nonzero skew, but the teststatistic-based disorder the entropy-based measure &delta; is H. </span><span class="c14">&nbsp;&#771;</span><span class="c1">much In particular more sensitive we to the skew than can observe that the trivial scalar density provides a sufficient accuracy for approximately uniform nonzero patterns (&xi; ! 0). </span><span class="c20">6.2 Plan Ranking </span></p><p class="c10"><span class="c1">In this experiment we evaluate the total cost model ac- curacy of SpMachO by comparing the estimated runtime against the actual runtime of each possible plan. Hence, Sp- MachO is optimal if the plan with the lowest actual runtime has also the lowest estimated runtime. However, this experi- mental verification of optimality requires to run all possible execution plans, which is not feasible for longer matrix chains due to the exponentially growing number of plans according to Eq. (4). Thus, we did a &ldquo;brute-force evaluation&rdquo; only for matrix chains of length p = 3: </span></p><p class="c10"><span class="c1">A</span><span class="c9">S</span><span class="c5">1 </span><span class="c14">&middot; A</span><span class="c9">S</span><span class="c5">2 </span><span class="c14">&middot; A</span><span class="c9">S</span><span class="c5">3 </span><span class="c14">. (13) </span><span class="c1">Although there are only two ways of setting the parenthesis in this expression, with all the possible storage type transfor- mations per multiplication node, one obtains 128 different execution plans. Fig. 2 shows some of the possible plans, to illustrate the problem complexity. The execution plans are composed of </span></p><p class="c11"><span class="c1">&bull; multiplication ther produce a sparse operators result matrix </span><span class="c9">D/S</span><span class="c1">&#8677;</span><span class="c9">D/S </span></p><p class="c11"><span class="c5">D/S</span><span class="c1">or a </span><span class="c14">, </span><span class="c1">dense </span><span class="c14">which </span><span class="c1">one. </span><span class="c14">can </span><span class="c1">For </span><span class="c14">ei- </span></p><p class="c11"><span class="c1">a chain of length p there are exactly p 1 multiplication operators. </span></p><p class="c11"><span class="c1">&bull; transformation intermediate result operator from one storage T </span><span class="c5">S/D </span><span class="c9">S/D </span></p><p class="c11"><span class="c14">that </span><span class="c1">representation </span><span class="c14">transforms </span><span class="c1">into </span><span class="c14">the </span></p><p class="c11"><span class="c1">another (which is either dense or sparse.) There can be none or up to 2</span><span class="c9">3p 1 </span><span class="c1">transformation operators. </span></p><p class="c10"><span class="c1">SpMachO estimates the cost for each operator via the cost functions TM,TT described in section 2. As a consequence, each operator estimation potentially contributes to the absolute execution runtime error. </span></p><p class="c10"><span class="c1">In this experiment, we first executed all 128 plans and measured the actual execution runtime. Thereafter, we com- puted the runtime estimations using SpMachO&rsquo;s cost model </span></p><p class="c11"><span class="c26">296 </span></p><p class="c11"><span class="c15">2.6e3 </span><span class="c74">] sm[emiTnoitucex</span><span class="c15">E1.2e3 6.0e2 </span></p><p class="c11"><span class="c15">2.8e2 </span></p><p class="c11"><span class="c15">1.3e2 </span></p><p class="c11"><span class="c15">6.3e1 </span></p><p class="c11"><span class="c15"># plans # plans </span></p><p class="c10"><span class="c1">Figure 7: Vertical histogram of the actual (bars on left-hand side) and estimated (bars on right-hand side) runtimes of all 128 possible execution plans. The edges denote the plan (dis- )placement, the linewidth correlates with the corresponding number of plans. The vertical time axis has a logarithmic scale. </span></p><p class="c10"><span class="c1">for each plan. Fig. 7 shows the histograms for the actually measured (left-hand side) and the estimated (right-hand side) execution runtimes. Note that the vertical axis has a logarith- mic scale, hence, the width of the upper bins refers to a larger time interval than the width of the lower bins. The connect- ing edges in-between the bins of the two histograms show where the plans of the actual runtime histogram are placed in the estimated runtime histogram. If there is an ascending edge, for example from the lowest bin in the left histogram to the second lowest bin of the right histogram, then there is at least one plan, whose runtime was overestimated. If the edge is horizontal, then the estimated runtimes of all plans corresponding to this edge are within the same time interval as their actual runtimes. The width of the edges indicates how many plans are affected. It is worthwhile mentioning that for the selection of the best execution plan, the quan- titative estimation of execution runtimes could potentially differ arbitrarily from the actual runtimes, as long as the esti- mated order of the plans preserves the actual runtime order correctly. This condition is only violated for the edges that are crossing another edge. If the total runtime for a plan is significantly under- or overestimated, its corresponding edge crosses multiple other edges. Indeed, the goodness of the cost model can be defined by the number of edges crossings, weighted by the number of plans per crossing edge. </span></p><p class="c10"><span class="c1">The majority of estimations, which are shown in Fig. 7, are in the correct corresponding time bin. Although there are quite a few crossings, especially in the middle part, most of the edges only span over to the neighboring bin. Moreover, for the selection of the most efficient plan, only the lower part of Fig. 7 is relevant. In particular, the plan with the lowest estimated runtime, which is generated by SpMachO, should be contained in the lowest bin of the actual runtime histogram. Since all edges of the lowest estimated time bin originate from the lowest actual time bin, we observe that the SpMachO selected plan is at least among the top k plans, if not the best. </span></p><p class="c11"><span class="c20">6.3 Performance Comparison </span></p><p class="c10"><span class="c1">We compared the absolute execution runtime of a matrix chain multiplication expression using the optimized plan by SpMachO against R and the commercial system A. Both </span></p><p class="c10"><span class="c1">systems contain classes and algorithms for dense and sparse matrices. In R (V3.0.0) we used the Cran R matrix pack- age[2] (V1.0.12), in system A we used the native sparse matrix representation. In addition,we included the following alternative execution approaches to the measurement: </span></p><p class="c10"><span class="c1">&bull; Left-deep, sparse only: All matrices are multiplied using a sparse-sparse into sparse multiplication using the spspsp gemm kernel (</span><span class="c9">S</span><span class="c1">&#8677;</span><span class="c9">S</span><span class="c5">S</span><span class="c14">), starting with the first </span><span class="c1">left pair and proceeding into the right direction. </span></p><p class="c10"><span class="c1">&bull; Right-deep, sparse-dense-dense: The outermost right pair is multiplied using sparse-sparse into dense multiplication (</span><span class="c9">S</span><span class="c1">&#8677;</span><span class="c9">S</span><span class="c5">D</span><span class="c14">). Then, the matrices on the left </span><span class="c1">are consecutively multiplied with the right-hand dense intermediate result matrix using the sparse-dense into dense multiplication kernel (spdd gemm, </span><span class="c9">S</span><span class="c1">&#8677;</span><span class="c9">D</span><span class="c5">D</span><span class="c14">). </span></p><p class="c10"><span class="c1">Both approaches use the same infrastructure as SpMachO. The reason why we chose exactly these two specific execution strategies is that either of them turned out to be good (or even optimal) for a reasonable large fraction of matrix chains. In particular, they yield good performance if the inter-matrix skew is low, i.e., the dimensions and the densities do not differ, since in these cases, the impact of parenthesization on the optimization is less significant. We also tried other alternatives to the dynamic programming approach of Sp- MachO, for example, a method that picks an execution plan based on the metaheuristic simulated annealing. However, due to the high dimensionality of the search space and the large discrepancy in the runtimes, it turned to be out to be far worse in most cases, hence, we did not include it in the measurements. </span></p><p class="c11"><span class="c33">6.3.1 Data Set </span></p><p class="c10"><span class="c1">Since there are currently no standardized benchmarks for large scale linear algebra expressions, it is generally difficult to provide a comprehensive performance comparison. There- fore, we created two performance experiments: first, we took real world-matrices of different domains and compared the execution runtime of self-multiplication chains (matrix pow- ers). Thereafter, we generated random matrices of different dimension and density skews in order to study the systematic behavior of SpMachO. </span></p><p class="c10"><span class="c1">Table 2: Sparse matrices of different dimensions and pop- ulation densities. The &rho; = N</span><span class="c5">nz</span><span class="c1">/(n &#8677; n) value denotes the population density (rounded) of each matrix. All matrices are square (n &#8677; n.) </span></p><p class="c10"><span class="c1">Name Matrix Domain Dim. N</span><span class="c5">nz </span><span class="c1">&rho; &middot; 10</span><span class="c9">2 </span><span class="c1">[%] NCSM1 Nuclear Physics 3440 2.930 M 24.7 PWNET Power Eng. 8140 2.017 M 3.0 JACO1 Econometric 9129 56 K 0.07 </span></p><p class="c10"><span class="c1">Tab. 2 lists the matrix data sets which we used in the evaluation. The first matrix NCSM1 is taken from a nuclear physics group, the other two (PWNET, JACO1) are from the Florida Sparse Matrix Collection</span><span class="c9">4</span><span class="c1">. </span></p><p class="c10"><span class="c1">In our prototype system, some of the multiplication kernels are implemented single-threaded, whereas other kernels have parallel implementations. Although we emphasize that the </span></p><p class="c11"><span class="c5">4</span><span class="c0">http://www.cise.ufl.edu/research/sparse/matrices/ </span></p><p class="c11"><span class="c26">297 </span></p><p class="c11"><span class="c35">100 </span><span class="c28">] s[emitnuRnoitucex</span><span class="c35">E</span><span class="c14">NCSM1 </span></p><p class="c11"><span class="c28">3 </span></p><p class="c11"><span class="c28">21</span><span class="c35">02 3 4 5 </span></p><p class="c11"><span class="c35">0</span><span class="c28">] s[emitnuRnoitucex</span><span class="c35">E100 </span></p><p class="c11"><span class="c14">PWNET </span></p><p class="c11"><span class="c28">20 </span></p><p class="c11"><span class="c28">10</span><span class="c35">2 3 4 5 </span></p><p class="c11"><span class="c35">E</span><span class="c28">] s[emitnuRnoitucex10 1</span><span class="c35">0.1</span><span class="c14">JACO1 </span></p><p class="c11"><span class="c35">2 4 10 12 Chain length </span></p><p class="c11"><span class="c28">2 </span></p><p class="c11"><span class="c28">1</span><span class="c35">0.01 0</span></p><p class="c11"><span class="c35">10 </span></p><p class="c11"><span class="c35">10 </span></p><p class="c11"><span class="c7">Dynamic Prog. </span></p><p class="c10"><span class="c1">Figure 8: Left Column: Measurement of the execution run- time of sparse matrix chains (matrix powers). Right: The total runtime of SpMachO is visually separated into its components: the plan execution, the SpProdest runtime and the dynamic programming part. </span></p><p class="c10"><span class="c1">conceptual execution plan optimization of SpMachO works orthogonal to the individual performance of the multiplica- tion kernels, the absolute execution runtime does obviously also depend on the low level implementation of each algo- rithm. Hence, there is still potential to reduce the overall runtime further by switching completely to massively par- allelized multiplication kernels. However, although we use mostly sequential kernels in the prototype, our algorithm was still able to outperform R and the commercial system A. </span></p><p class="c11"><span class="c33">6.3.2 Self Multiplications </span></p><p class="c10"><span class="c1">In this part we discuss the performance of matrix self multiplications (matrix powers), which is for example used for the calculation of Markov chains models. </span></p><p class="c10"><span class="c1">The left column of Fig. 8 shows the absolute runtimes using SpMachO versus R, commercial system A, and the right-deep spspsp and left-deep spdd approaches. The first notion is that the relative performance speedup of SpMa- chO becomes more significant with increasing chain length. For matrices with a relatively high density, e.g. NCSM1, SpMachO outperforms the other systems even for a single </span></p><p class="c11"><span class="c35">2 3 4 5 </span></p><p class="c11"><span class="c35">2 3 4 5 </span></p><p class="c11"><span class="c35">2 4 6 8 10 12 </span></p><p class="c11"><span class="c35">Chain length </span></p><p class="c11"><span class="c7">SpMacho Total Right-deep spdd </span></p><p class="c11"><span class="c7">Left-deep spspsp R </span></p><p class="c11"><span class="c7">Commercial Syst. A </span></p><p class="c11"><span class="c7">Plan execution </span></p><p class="c11"><span class="c7">SpProdest </span></p><p class="c10"><span class="c1">multiplication already by several factors. In this case, Sp- MachO recognizes that it is worth to convert the matrix into dense representations prior to the multiplication. For the second matrix chain (PWNET), the performance gap is increasing with the chain length up to a speedup factor of five. Only in the third plot (JACO1), the overhead of SpMachO amortizes not before a chain length of four. In this relatively simple case of matrix self multiplications, the speedup is related to the density evolution of the intermediate results. When the matrix reaches a relatively high density in an early stage of the execution plan, SpMachO is likely to choose dense formats and proceed with dense multiplication kernels, whereas the use of sparse-only kernels will have a poor performance for every additional multiplication. That also explains why the right-deep spdd gemm strategy is often optimal, e.g. for the PWNET matrix chain. </span></p><p class="c10"><span class="c1">The right column in Fig. 8 shows the separate runtimes of each component of SpMachO, which are: the plan execution, the SpProdest runtime, and the dynamic programming loop. The runtime of the dynamic programming part is negligible and only visible for longer chains (10-12). Note that we the SpProdest cost can be further reduced, if we cache the density maps. As of now, they are created once prior to each expression execution, consuming most of SpProdest&rsquo;s runtime. </span></p><p class="c10"><span class="c1">As a side note, the R and commercial system A runtimes show astonishing similarity with our left-deep spspsp gemm approach. We assume that they use a similar way of execu- tion. </span></p><p class="c11"><span class="c33">6.3.3 Random Matrix Chains </span></p><p class="c10"><span class="c1">In the next experiment, we used three different, randomly created matrices. Products of three matrices are very com- mon in many applications, for example in algorithms that contain matrix factorizations. </span></p><p class="c10"><span class="c1">In order to observe the systematic influence of the data skew on the execution runtime, we varied three skew di- mensions: the matrix shape skew, the inter-matrix density skew and the matrix intra-density skew (as of section 6.1). For each skew dimension, we varied a parameter &xi; 2 [0,1] that quantifies the skew in a range from zero (no skew) to one (maximum skew). More precisely, the parameter dimen- sions (m/n)</span><span class="c4">i</span><span class="c0">, &rho;</span><span class="c4">i </span><span class="c0">and the intra-density skews &xi;</span><span class="c4">i</span><span class="c0">, are randomly </span><span class="c1">picked from a &lt;min, max, average&gt; distribution, where &xi; corresponds to the deviation from the average value. </span></p><p class="c10"><span class="c1">Since we created the matrices randomly for each skew parameter configuration, one single configuration can have various random instances, which results in a potentially large variety of different runtimes. This is reasoned by the fact that a skew in the matrices can affect the execution runtime in both directions &ndash; increasingly or decreasingly. Generally speaking, a large skew in the data can dramatically slow down naive execution approaches, but also reveals a large optimization potential for SpMachO by exploiting the skew. In contrast to the previous self-multiplication experiment, a skew in the matrices leads to a higher influence of the parenthesization, and the selection of storage representations and algorithms. </span></p><p class="c10"><span class="c1">To be independent of particular random matrix instances, we repeated the measurement multiple times, hence, we took 25 different randomly created sparse matrix chains of length three per configuration. Fig. 9 shows for each skew configura- tion a box plot with the corresponding median, lower quartile, </span></p><p class="c11"><span class="c26">298 </span></p><p class="c11"><span class="c12">10</span><span class="c13">2 </span></p><p class="c11"><span class="c12">10</span><span class="c13">2 </span></p><p class="c11"><span class="c12">E</span><span class="c31">] s[emitnuRnoitucex</span><span class="c12">10</span><span class="c13">1 </span></p><p class="c11"><span class="c12">10</span><span class="c13">1 </span></p><p class="c11"><span class="c12">10</span><span class="c13">0 </span></p><p class="c11"><span class="c12">10</span><span class="c13">0 </span></p><p class="c11"><span class="c31">10 </span><span class="c13">1 </span></p><p class="c11"><span class="c12">0.3 0.6 1 </span><span class="c31">10 </span><span class="c13">1 </span></p><p class="c11"><span class="c12">0.2 0.5 1 </span></p><p class="c11"><span class="c15">Commercial System A </span></p><p class="c10"><span class="c1">Figure 9: Average (log scale) runtime and variance comparison of SpMachO vs. the right-deep spdd and left-deep spspsp approaches, R, and the commercial system for a multiplication of the expression A</span><span class="c5">1 </span><span class="c1">&middot; A</span><span class="c5">2 </span><span class="c1">&middot; A</span><span class="c5">3</span><span class="c1">. We varied in x-direction: the inter-density skew (left), the shape skew (middle) and the intra-density skew (right). The bounding &lt;min,max,avg&gt; distributions for the data skew are &lt;0.001, 0.5, 0.025&gt; for matrix densities, and &lt;32, 16384, 3072&gt; for the matrix dimensions. The intra-density skew &xi; was chosen according to Fig. 6 with values ranging from 0 to 0.5. </span></p><p class="c10"><span class="c1">upper quartile and whiskers of the execution runtimes. Note that our measured runtime of SpMachO includes the time for the density estimation (SpProdest). We observe the fol- lowing characteristics: First, for low skew parameters, both SpMachO and the right-deep spdd outperform the other approaches for most of the instances. For unskewed matrices, this underlines the result that we already obtained in the previous experiment for matrix chains with similar densi- ties, i.e. that the right-deep spdd multiplication execution is optimal if intermediate results are rather dense. Second, and more interesting, is the development of the runtime distributions with higher data skews. In the inter-density skew experiment (left plot), the median execution time in R, the commercial system A and the left-deep spspsp approach increases, whereas the SpMachO median time stays low and gets even lower for &xi; = 1. Moreover, the variance in time of SpMachO grows notably slower than these of the other systems. In the other two plots, we see a similar picture, al- though most of the median execution times decrease slightly in the shape skew experiment (middle plot), and more sig- nificantly in the intra-density skew experiment (right plot). Here, the increased intra-density skew leads in the majority of cases to a reduced execution runtime, which conforms to our notion. Still, in quite a few cases the runtime of the other systems explodes, leading to the observed high variance and scattering of the execution times. The right-deep spdd approach is more robust, but not optimal for high skews. In contrast, SpMachO is able to reveal the skew and exploit it for optimization. As a result, we observe that SpMachO has a by far better worst-case behavior than the established systems. </span></p><p class="c11"><span class="c20">7. RELATED WORK </span></p><p class="c11"><span class="c1">As this work has overlaps with multiple research areas, we subdivide the discussion into the major subtopics: </span><span class="c20">7.1 Optimization </span><span class="c27">sions </span></p><p class="c10"><span class="c20">of Linear Algebra Expres- </span><span class="c1">Despite the optimization potential, we did not find that common numerical algebra systems optimize the execution of linear algebra expressions based on matrix sparsity and dimension characteristics. In contrast, the idea of optimizing </span></p><p class="c10"><span class="c1">linear algebra operations on system level has been men- tioned in SystemML [14, 6], which describes a Hadoop-based machine-learning framework with an R-like declarative lan- guage. However, the cost model they describe in [6] is based on independent, one-dimensional scaling functions, and they assume full density (&rho; = 1) for intermediate results. In [5] they mention that they optimize by assuming &ldquo;independence with regard to the sparsity of intermediates&rdquo;. In contrast, we observed that particularly in situations with large density differences (inter-density skew) the density of intermediate results influences the optimization significantly. Since simple models are unable to reconstruct the complicated runtime behaviour of matrix multiplication kernels, we also put our focus on accurate cost models from an algorithmic perspec- tive. In addition, our cost analysis revealed that matrix dimensions and the matrix sparsity can not be regarded as independent parameters. </span></p><p class="c11"><span class="c20">7.2 Matrix </span><span class="c27">Estimation </span></p><p class="c21"><span class="c20">Chain Multiplication and Density </span><span class="c1">In contrast to dense matrix chain multiplication, which has been discussed thoroughly in the past decades, e.g. in [12, 16, 21], there is little work about sparse matrix chain multi- plications. Interesting work that should be mentioned in this context is from Cohen [9, 10], who extended the dynamic programming approach idea to sparse matrices. In her work, she minimizes the overall number of floating point operations that are needed to compute the matrix chain product by predicting the non-zero structure for intermediate result ma- trices on row/column-level. The density prediction algorithm of graph, [10] is and based has on a fixed random complexity number &#8677;(propagation </span><span class="c14">P</span><span class="c1">N</span><span class="c4">nz,i</span><span class="c0">). However, </span><span class="c1">in a layered </span><span class="c0">as </span><span class="c1">observed in section 2, the actual runtime cost of sparse matrix multiplication kernels are not just proportional to number of floating point operations. Moreover, coming from a real system perspective, we consider not only pure sparse-sparse matrix multiplications, but leverage sparse-dense transfor- mations and the coexistence of sparse and dense matrices to optimize on a more complete level. </span></p><p class="c11"><span class="c20">7.3 Join Optimization </span></p><p class="c11"><span class="c1">The problem of sparse matrix chain multiplication is re- </span></p><p class="c11"><span class="c26">299 </span></p><p class="c11"><span class="c12">Intra-Density Inter-Density Skew </span></p><p class="c11"><span class="c12">Skew </span></p><p class="c11"><span class="c15">SpMachO </span></p><p class="c11"><span class="c15">Right-deep spdd </span></p><p class="c11"><span class="c15">Left-deep spspsp </span></p><p class="c11"><span class="c12">Shape Skew </span></p><p class="c11"><span class="c15">R </span></p><p class="c11"><span class="c12">10</span><span class="c13">2 </span></p><p class="c11"><span class="c12">10</span><span class="c13">1 </span></p><p class="c11"><span class="c12">10</span><span class="c13">0 </span></p><p class="c11"><span class="c12">0.04 0.12 0.5 </span><span class="c31">10 </span><span class="c13">1 </span></p><p class="c49"><span class="c1">lated to join enumeration and cardinality estimation in a relational database management system RDBMS. This con- nection is more obvious when sparse matrices are represented as hrow,col,vali triple tables [20]. In fact, a multiplication can then be expressed as a join aggregation [3]. The use of dynamic programming in join optimization [25, 22] and join plan generation with respect to physical table properties [17] were inspiring for this work, as well as the use of multidi- mensional histograms [23] for the optimization of queries on multidimensional data. Although the mathematical charac- teristics of matrices and multiplications require a slightly different perspective, it is an interesting aspect that some of the ideas of relational join optimization can be used for linear algebra. </span></p><p class="c81"><span class="c20">8. CONCLUSION </span></p><p class="c67"><span class="c1">In times of emerging analytical and scientific databases, many systems [8, 6] started to deeply integrate linear algebra. This work shows that integrating linear algebra operations, such as matrix multiplications, is not just adding algorithms to the database engine. In fact, due to different matrix representations, algorithms, and the presence of data skew, we observed that a naive execution of sparse matrix products can be up to orders of magnitude slower than an optimized one.</span><span class="c0">In this paper we presented SpMachO, which optimizes </span><span class="c1">sparse, dense and mixed matrix multiplications of arbitrary length, by creating an execution plan that consists of trans- formation and multiplication operators. By using detailed cost functions of different sparse, dense and mixed matrix multiplication kernels, SpMachO leads to a faster and more robust execution compared to widely used algebra systems. Moreover, our density prediction approach SpProdest with an entropy-based skew awareness enables accurate memory consumption and runtime estimates at each stage in the execution plan. </span></p><p class="c63"><span class="c1">To put it in a nutshell, we showed how methods inspired from database technology can improve linear algebra compu- tations, and took a step into the direction of taking complex- ity from data scientists &ndash; who should not be required to have profound knowledge about the connections between math- ematical optimizations, matrix characteristics, algorithmic complexities and the hardware parameters of their system. </span></p><p class="c50"><span class="c20">9. REFERENCES </span></p><p class="c55"><span class="c1">[1] Intel </span><span class="c5">R </span><span class="c1">Math Kernel Library, </span></p><p class="c57"><span class="c1">http://software.intel.com/en-us/intel-mkl. [2] CRAN R Matrix Package, http://cran.r- </span></p><p class="c84"><span class="c1">project.org/web/packages/Matrix/index.html. [3] R. R. Amossen and R. Pagh. Faster Join-Projects and </span></p><p class="c29"><span class="c1">Sparse Matrix Multiplications. In ICDT, 2009. [4] K. Behrend. Dynamical Systems and Matrix Algebra. </span></p><p class="c54"><span class="c1">2008. [5] M. Boehm, R. B. Douglas, A. V. Evfimievski, et al. </span></p><p class="c66"><span class="c1">SystemML&rsquo;s Optimizer: Plan Generation for Large-Scale Machine Learning Programs. IEEE Data Eng. Bull., 37(3), 2014. [6] M. Boehm, S. Tatikonda, B. Reinwald, et al. Hybrid Parallelization Strategies for Large-Scale Machine Learning in SystemML. VLDB, 7(7), 2014. [7] G. E. P. Box, J. S. Hunter, and W. G. Hunter. </span></p><p class="c87"><span class="c1">Statistics for Experimenters: Design, Innovation, and </span></p><p class="c11 c43"><span class="c1">Discovery , 2nd Edition. Wiley-Interscience, 2 edition, May 2005. [8] P. G. Brown. Overview of SciDB: Large Scale Array </span></p><p class="c92"><span class="c1">Storage, Processing and Analysis. In SIGMOD, 2010. [9] E. Cohen. On Optimizing Multiplications of Sparse </span></p><p class="c62"><span class="c1">Matrices. In IPCO, 1996. [10] E. Cohen. Structure Prediction and Computation of </span></p><p class="c70"><span class="c1">Sparse Matrix Products. Journal of Combinatorial Optimization, 2(4), 1998. [11] J. Cohen, B. Dolan, M. Dunlap, J. M. Hellerstein, et al. MAD Skills: New Analysis Practices for Big Data. VLDB, 2(2), Aug. 2009. [12] T. H. Cormen, C. Stein, R. L. Rivest, and C. E. </span></p><p class="c71"><span class="c1">Leiserson. Introduction to Algorithms. McGraw-Hill Higher Education, 2nd edition, 2001. [13] A. Edelman, S. Heller, and S. Lennart Johnsson. Index </span></p><p class="c85"><span class="c1">Transformation Algorithms in a Linear Algebra Framework. IEEE Trans. Parallel Distrib. Syst., 5(12), Dec 1994. [14] A. Ghoting, R. Krishnamurthy, E. Pednault, et al. SystemML: Declarative Machine Learning on MapReduce. In ICDE, 2011. [15] J. R. Gilbert, C. Moler, and R. Schreiber. Sparse </span></p><p class="c79"><span class="c1">Matrices in Matlab: Design and Implementation. SIAM J. Matrix Anal. Appl., 13(1), Jan. 1992. [16] S. S. Godbole. On Efficient Computation of Matrix </span></p><p class="c71 c72"><span class="c1">Chain Products. IEEE Trans. Comput., 22(9), Sept. 1973. [17] G. Graefe and W. J. McKenna. The Volcano Optimizer Generator: Extensibility and Efficient Search. In Proc. Int. Conf. Data Eng., 1993. [18] F. G. Gustavson. Two Fast Algorithms for Sparse </span></p><p class="c86"><span class="c1">Matrices: Multiplication and Permuted Transposition. ACM Trans. Math. Softw., 4(3), Sept. 1978. [19] IBM </span><span class="c5">R </span><span class="c1">Netezza </span><span class="c5">R </span><span class="c1">Analytics. Matrix Engine Developer&rsquo;s </span></p><p class="c56"><span class="c1">Guide. IBM, 1994. [20] D. Kernert, F. K&ouml;hler, and W. Lehner. SLACID - </span></p><p class="c37"><span class="c1">Sparse Linear Algebra in a Column-oriented In-memory Database System. In SSDBM, 2014. [21] H. Lee, J. Kim, S. J. Hong, and S. Lee. Processor </span></p><p class="c91"><span class="c1">Allocation and Task Scheduling of Matrix Chain Products on Parallel Systems. IEEE Trans. Parallel Distrib. Syst., 14(4), Apr. 2003. [22] G. Moerkotte. Constructing Optimal Bushy Trees Possibly Containing Cross Products for Order Preserving Joins is in P. 2003. [23] M. Muralikrishna and D. J. DeWitt. Equi-depth </span></p><p class="c6"><span class="c1">Multidimensional Histograms. SIGMOD Rec., 17(3), June 1988. [24] R. Rebonato and P. J&auml;ckel. The Most General </span></p><p class="c78"><span class="c1">Methodology to Create a Valid Correlation Matrix for Risk Management and Option Pricing Purposes, 1999. [25] P. G. Selinger, M. M. Astrahan, D. D. Chamberlin, </span></p><p class="c44"><span class="c1">R. A. Lorie, and T. G. Price. Access Path Selection in a Relational Database Management System. In SIGMOD, 1979. [26] C. E. Shannon. A mathematical theory of </span></p><p class="c90"><span class="c1">communication. SIGMOBILE Mob. Comput. Commun. Rev., 5(1), Jan. 2001. [27] V. Yegnanarayanan. An application of matrix </span></p><p class="c73"><span class="c1">multiplication. Resonance, 18(4), 2013. </span></p><p class="c42"><span class="c26">300 </span></p></body></html>