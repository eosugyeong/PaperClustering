<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c58{margin-left:-25.7pt;padding-top:12pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c44{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:18.8pt;font-family:"Courier New";font-style:normal}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:7.5pt;font-family:"Arial";font-style:normal}.c25{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Arial";font-style:normal}.c21{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.5pt;font-family:"Arial";font-style:normal}.c33{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.9pt;font-family:"Arial";font-style:normal}.c20{margin-left:-25.7pt;padding-top:23.8pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.9pt;font-family:"Arial";font-style:normal}.c27{margin-left:-0.5pt;padding-top:3.6pt;text-indent:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:23.4pt}.c19{margin-left:-25.7pt;padding-top:12pt;text-indent:36pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c36{margin-left:-16.6pt;padding-top:3.8pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c30{margin-left:-16.6pt;padding-top:4.1pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c28{margin-left:-25.7pt;padding-top:1.7pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c32{margin-left:-16.6pt;padding-top:3.8pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.2pt}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c51{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c45{margin-left:-25.7pt;padding-top:5.8pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-20.2pt}.c9{margin-left:-25.7pt;padding-top:3.8pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c60{margin-left:-25.7pt;padding-top:9.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c61{margin-left:-5.4pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c35{margin-left:218.2pt;padding-top:54pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c39{margin-left:-25.7pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c46{margin-left:34.8pt;padding-top:13.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:43.8pt}.c48{margin-left:-5.4pt;padding-top:7.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c37{margin-left:-16.6pt;padding-top:7.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.4pt}.c38{margin-left:59pt;padding-top:6.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:68.8pt}.c43{margin-left:-25.7pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.8pt}.c49{margin-left:-3.2pt;padding-top:8.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c41{margin-left:-16.6pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.4pt}.c62{margin-left:55.2pt;padding-top:6.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:65pt}.c57{margin-left:43.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:47pt}.c40{margin-left:-16.6pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:115pt}.c53{margin-left:-25.7pt;padding-top:8.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c59{margin-left:-16.6pt;padding-top:17.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19pt}.c14{margin-left:-25.7pt;padding-top:9.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c29{margin-left:-7.5pt;padding-top:3.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.2pt}.c26{margin-left:-16.6pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c56{margin-left:-25.7pt;padding-top:11pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c55{margin-left:-16.6pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.2pt}.c17{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c54{padding-top:18.7pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c16{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c52{padding-top:9.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c42{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c50{margin-left:-8pt;margin-right:-16.6pt}.c47{margin-left:-16.6pt;margin-right:-25.4pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c42"><p class="c17"><span class="c33">Probabilistic Threshold Indexing for Uncertain Strings </span></p><p class="c5"><span class="c7">Sharma Thankachan </span><span class="c2">Georgia Institute of Technology </span><span class="c18">thanks@csc.lsu.edu </span></p><p class="c5"><span class="c2">Georgia, USA </span><span class="c7">Manish Patil </span><span class="c2">Louisiana State University </span><span class="c18">manish.m.patil@gmail.com </span></p><p class="c5"><span class="c2">Louisiana, USA </span><span class="c7">Rahul Shah </span><span class="c2">Louisiana State University </span><span class="c18">rahul@csc.lsu.edu </span><span class="c2">Louisiana, USA </span><span class="c7">Sudip Biswas </span><span class="c2">Louisiana State University </span><span class="c18">sudipid@gmail.com </span><span class="c2">Louisiana, USA </span><span class="c7">ABSTRACT </span><span class="c1">Strings form a fundamental data type in computer systems. String searching has been extensively studied since the inception of com- puter science. Increasingly many applications have to deal with imprecise strings or strings with fuzzy information in them. String matching becomes a probabilistic event when a string contains un- certainty, i.e. each position of the string can have different prob- able characters with associated probability of occurrence for each character. Such uncertain strings are prevalent in various applica- tions such as biological sequence data, event monitoring and auto- matic ECG annotations. We explore the problem of indexing un- certain strings to support efficient string searching. In this paper we consider two basic problems of string searching, namely substring searching and string listing. In substring searching, the task is to find the occurrences of a deterministic string in an uncertain string. We formulate the string listing problem for uncertain strings, where the objective is to output all the strings from a collection of strings, that contain probable occurrence of a deterministic query string. In- dexing solution for both these problems are significantly more chal- lenging for uncertain strings than for deterministic strings. Given a construction time probability value &tau;, our indexes can be con- structed in linear space and supports queries in near optimal time for arbitrary values of probability threshold parameter greater than &tau;. To the best of our knowledge, this is the first indexing solution for searching in uncertain strings that achieves strong theoretical bound and supports arbitrary values of probability threshold pa- rameter. We also propose an approximate substring search index that can answer substring search queries with an additive error in optimal time. We conduct experiments to evaluate the performance of our indexes. </span></p><p class="c8"><span class="c1">c </span><span class="c13">2016, Copyright is with the authors. Published in Proc. 19th Inter- national Conference on Extending Database Technology (EDBT), March 15-18, 2016 - Bordeaux, France: ISBN 978-3-89318-070-7, on OpenPro- ceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0 </span></p><p class="c5"><span class="c7">1. INTRODUCTION </span></p><p class="c8"><span class="c1">String indexing has been one of the key areas of computer sci- ence. Algorithms and data structures of string searching finds ap- plication in web searching, computational biology, natural language processing, cyber security, etc. The classical problem of string in- dexing is to preprocess a string such that query substring can be searched efficiently. Linear space data structures are known for this problem which can answer such queries in optimal O(m + occ) time, where m is the substring length and occ is the number of occurrences reported. </span></p><p class="c8"><span class="c1">Growth of the internet, digital libraries, large genomic projects have contributed to enormous growth of data. As a consequence, noisy and uncertain data has become more prevalent. Uncertain data naturally arises in almost all applications due to unreliability of source, imprecise measurement, data loss, and artificial noise. For example sequence data in bioinformatics is often uncertain and probabilistic. Sensor networks and satellites inherently gather noisy information. </span></p><p class="c8"><span class="c1">Existing research has focused mainly on the study of regular or deterministic string indexing. In this paper we explore the problem of indexing uncertain strings. We begin by describing the uncertain string model, possible world semantics and challenges of searching in uncertain strings. </span></p><p class="c8"><span class="c1">Current literature models uncertain strings in two different ways: the string level model and the character level model. In string level model, we look at the probabilities and enumerate at whole string level, whereas character level model represents each position as a set of characters with associated probabilities. We focus on the character level model which arises more frequently in applications. Let S be an uncertain string of length n. Each character c at po- sition i of S has an associated probability pr(c</span><span class="c12">i</span><span class="c1">). Probabilities at different positions may or may not contain correlation among them. Figure 1(a) shows an uncertain string S of length 5. Note that, the length of an uncertain string is the total number of positions in the string, which can be less than the total number of possible char- acters in the string. For example, in Figure 1(a), total number of characters in string s with nonzero probability is 9, but the total number of positions or string length is only 5. </span></p><p class="c8"><span class="c1">&quot;Possible world semantics&quot; is a way to enumerate all the possi- ble deterministic strings from an uncertain string. Based on pos- sible world semantics, an uncertain string S of length n can gen- erate a deterministic string w by choosing one possible character from each position and concatenating them in order. We call w as one of the possible world for S. Probability of occurrence of </span></p><p class="c5"><span class="c15">Series ISSN: 2367-2005 401 </span><span class="c51">10.5441/002/edbt.2016.37 </span></p><p class="c53"><span class="c1">w = w</span><span class="c0">1</span><span class="c1">w</span><span class="c0">2 </span><span class="c1">...w</span><span class="c0">n </span><span class="c1">is the partial product pr(w</span><span class="c12">1</span><span class="c0">1</span><span class="c6">) &times; pr(w</span><span class="c12">2</span><span class="c0">2</span><span class="c6">) &times;&middot;&middot;&middot;&times; </span><span class="c1">pr(w</span><span class="c12">n</span><span class="c0">n</span><span class="c6">). The number of possible worlds for S increases exponen- </span><span class="c1">tially with n. Figure 1(b) illustrates all the possible worlds for the uncertain string S with their associated probability of occurrence. </span></p><p class="c28"><span class="c1">A meaningful way of considering only a fraction of the possible worlds is based on a probability threshold value &tau;. We consider a generated deterministic string w = w</span><span class="c3">1</span><span class="c11">w</span><span class="c3">2 </span><span class="c11">...w</span><span class="c3">n </span><span class="c11">as a valid oc- </span><span class="c1">currence with respect to &tau;, only if it has probability of occurrence more than &tau;. The probability threshold &tau; effectively removes lower probability strings from consideration. Thus &tau; plays an important role to avoid the exponential blowup of the number of generated deterministic strings under consideration. </span></p><p class="c46"><span class="c10">Character S[1] S[2] S[3] S[4] S[5] </span></p><p class="c57"><span class="c10">a .3 .6 0 .5 1 b .4 0 0 0 0 c 0 .4 0 .5 0 d .3 0 1 0 0 </span></p><p class="c38"><span class="c13">(a) Uncertain string S </span></p><p class="c27"><span class="c10">w Prob(w) w Prob(w) w Prob(w) w[1] aadaa .09 w[5] badaa .12 w[9] dadaa .09 w[2] aadca .09 w[6] badca .12 w[10] dadca .09 w[3] acdaa .06 w[7] badca .08 w[11] dcdaa .06 w[4] acdca .06 w[8] badca .08 w[12] dcdca .06 </span></p><p class="c62"><span class="c13">(b) Possible worlds of S </span></p><p class="c56"><span class="c1">Figure 1: An uncertain string S of length 5 and its all possible worlds with probabilities. </span></p><p class="c20"><span class="c1">Given an uncertain string S and a deterministic query substring p = p</span><span class="c0">1 </span><span class="c1">...p</span><span class="c0">m</span><span class="c1">, we say that p matched at position i of S with re- spect to threshold &tau; if pr(p</span><span class="c12">i</span><span class="c0">1</span><span class="c6">) &times;&middot;&middot;&middot;&times;pr(p</span><span class="c12">i+m&minus;1 </span><span class="c0">m </span><span class="c6">) &ge; &tau;. Note that, </span><span class="c1">O(m + occ) is the theoretical lower bound for substring searching where m is the substring length and occ is the number of occur- rence reported. </span><span class="c7">1.1 Formal problem definition </span></p><p class="c9"><span class="c1">Our goal is to develop efficient indexing solution for searching in uncertain strings. In this paper, we discuss two basic uncertain string searching problems which are formally defined below. </span></p><p class="c19"><span class="c1">P</span><span class="c25">ROBLEM </span><span class="c1">1. Substring Searching: Given an uncertain string S of length n, our task is to index S so that when a deterministic substring p and a probability threshold &tau; come as a query, report all the starting positions of S where p is matched with probability of occurrence greater than &tau;. </span></p><p class="c19"><span class="c1">P</span><span class="c25">ROBLEM </span><span class="c1">2. Uncertain String Listing: Let D={d</span><span class="c0">1</span><span class="c1">,...,d</span><span class="c0">D</span><span class="c1">} be a collection of D uncertain strings of n positions in total. Our task is to index D so that when a deterministic substring p and a probability threshold &tau; come as a query, report all the strings d</span><span class="c0">j </span><span class="c1">such that d</span><span class="c3">j </span><span class="c11">contains atleast one occurrence of p with probability </span><span class="c1">of occurrence greater than &tau;. </span></p><p class="c58"><span class="c1">Note that the string listing problem can be naively solved by run- ning substring searching query in each of the uncertain string from the collection. However, this naive approach will take O(</span><span class="c6">&sum;</span><span class="c0">d</span><span class="c4">i</span><span class="c0">&isin;D </span><span class="c11">search time on d</span><span class="c3">i</span><span class="c11">) time which can be very inefficient if the ac- </span><span class="c1">tual number of documents containing the substring is small. Fig- ure 2 illustrates an example for string listing. In this example, only the string d</span><span class="c3">1 </span><span class="c11">contains query substring &quot;BF&quot; with probability of oc- </span><span class="c1">currence greater than query threshold 0.1. Ideally, the query time should be proportionate to the actual number of documents reported as output. Uncertain strings considered in both these problems can contain correlation among string positions. </span></p><p class="c17 c50"><span class="c1">String collection D = {d</span><span class="c0">1</span><span class="c1">,d</span><span class="c0">2</span><span class="c1">,d</span><span class="c0">3</span><span class="c1">}: </span><span class="c10">d</span><span class="c4">1</span><span class="c10">[1] d</span><span class="c4">1</span><span class="c10">[2] d</span><span class="c4">1</span><span class="c10">[3] d</span><span class="c4">2</span><span class="c10">[1] d</span><span class="c4">2</span><span class="c10">[2] d</span><span class="c4">2</span><span class="c10">[3] d</span><span class="c4">3</span><span class="c10">[1] d</span><span class="c4">3</span><span class="c10">[2] d</span><span class="c4">3</span><span class="c10">[3] A .4 B .3 F .5 A .6 B .5 B .4 A .4 I .3 A 1 B .3 L .3 J .5 C .4 F .3 C .3 F .4 L .3 F .3 F .3 J .2 E .2 P .2 P .3 J .1 F .1 T .3 </span><span class="c1">Output of string listing query (&rdquo;BF&rdquo;,0.1) on D is: d</span><span class="c0">1 </span></p><p class="c47 c54"><span class="c1">Figure 2: String listing from an uncertain string collection D = {d</span><span class="c0">1</span><span class="c1">,d</span><span class="c0">2</span><span class="c1">,d</span><span class="c0">3</span><span class="c1">}. </span></p><p class="c59"><span class="c7">1.2 Challenges in uncertain string searching </span></p><p class="c29"><span class="c1">We summarize some challenges of searching in uncertain strings. </span></p><p class="c49"><span class="c1">&bull; An uncertain string of length n can have multiple charac- ters at each position. As the length of an uncertain string in- creases, the number of possible worlds grows exponentially. This makes a naive technique that exhaustively enumerates all possible worlds infeasible. </span></p><p class="c49"><span class="c1">&bull; Since multiple substrings can be enumerated from the same starting position, care should be taken in substring searching to avoid possible duplication of reported positions. </span></p><p class="c49"><span class="c1">&bull; Enumerating all the possible sequences for arbitrary proba- bility threshold &tau; and indexing them requires massive space for large strings. Also note that, for a specific starting posi- tion in the string, the probability of occurrence of a substring can change arbitrarily (non-decreasing order) with increas- ing length, depending on the probability of the concatenated character This makes it difficult to construct index that can support arbitrary probability threshold &tau;. </span></p><p class="c41"><span class="c1">&bull; Correlated uncertainty among the string positions is not un- common in applications. An index that handles correlation appeals to a wider range of applications. However, handling the correlation can be a bottleneck on space and time. </span><span class="c7">1.3 Related work </span></p><p class="c36"><span class="c1">Although, searching over clean data has been widely researched, indexing uncertain data is relatively new. Below we briefly mention some of the previous works related to uncertain strings. </span></p><p class="c47 c52"><span class="c1">Algorithmic approach: Li et al. [19] tackled the substring search- ing problem where both the query substring and uncertain sequence comes as online query. They proposed a linear time and linear space dynamic programming approach to calculate the probability that a substring is contained in the uncertain string. </span></p><p class="c26"><span class="c1">Approximate substring matching: Given as input a string p, a set of strings {x</span><span class="c0">i</span><span class="c1">|1 &le; i &le; r}, and an edit distance threshold k, the substring matching problem is to find all substrings s of x</span><span class="c3">i </span><span class="c11">such </span><span class="c1">that d(p, s) &le; k, where d(p, s) is the edit distance between p and s. This problem has been well studied on clean texts (see [22] for a survey). Most of the ideas to solve this problem is based on parti- tioning p. Tiangjian et al. [12] extended this problem for uncertain strings. Their index can handle strings of arbitrary lengths. </span></p><p class="c26"><span class="c1">Frequent itemset mining: Some articles discuss the problem of frequent itemset mining in uncertain databases [6, 7, 3], where an itemset is called frequent if the probability of occurrence of the itemset is above a given threshold. </span></p><p class="c55"><span class="c1">Probabilistic database: Several works [5, 26, 25] have devel- oped indexing techniques for probabilistic databases, based on R- trees and inverted indices, for efficient execution of nearest neigh- </span></p><p class="c35"><span class="c15">402 </span></p><p class="c60"><span class="c1">bor queries and probabilistic threshold queries. Dalvi et al. [8] proposed efficient evaluation method for arbitrary complex SQL queries in probabilistic database. Later on efficient index for ranked top-k SQL query answering on a probabilistic database was pro- posed ([24, 18]). Kanagal et al. [16] developed efficient data struc- tures and indexes for supporting inference and decision support queries over probabilistic databases containing correlation. They use a tree data structure named junction tree to represent the cor- relations in the probabilistic database over the tuple-existence or attribute-value random variables. </span></p><p class="c39"><span class="c1">Similarity joins: A string similarity join finds all similar string pairs between two input string collections. Given two collections of uncertain strings R, S, and input (k, &tau;), the task is to find string pairs (r, s) between these collections such that Pr(ed(R, S) &le; k) &gt; &tau; i.e., probability of edit distance between R and S being at most k is more than probability threshold &tau;. There are some works on string joins, e.g., [4, 13, 17], involving approximation, data cleaning, and noisy keyword search, which has been discussed in the probabilistic setting [15]. Patil et al. [23] introduced filtering techniques to give upper and (or) lower bound on Pr(ed(R, S) &le; k) and incorporate such techniques into an indexing scheme with reduced filtering overhead. </span><span class="c7">1.4 Our approach </span></p><p class="c9"><span class="c1">Since uncertain string indexing is more complex than determin- istic string indexing, a general solution for substring searching is challenging. However efficiency can be achieved by tailoring the data structure based on some key parameters, and use the data struc- ture best suited for the purposed application. We consider the fol- lowing parameters for our index design. </span></p><p class="c14"><span class="c1">Threshold parameter &tau;</span><span class="c0">min</span><span class="c1">: The task of substring matching in uncertain string is to find all the probable occurrences, where the probable occurrence is determined by a query threshold parameter &tau;. Although &tau; can have any value between 0 to 1 at query time, real life applications usually prohibits arbitrary small value of &tau;. For example, a monitoring system does not consider a sequence of events as a real threat if the associated probability is too low. We consider a threshold parameter &tau;</span><span class="c0">min</span><span class="c1">, which is a constant known at construction time, such that query &tau; does not fall below &tau;</span><span class="c3">min</span><span class="c11">. Our </span><span class="c1">index can be tailored based on &tau;</span><span class="c0">min </span><span class="c1">at construction time to suit specific application needs. </span></p><p class="c14"><span class="c1">Query substring length: The query substring searched in the un- certain string can be of arbitrary length ranging from 1 to n. How- ever, most often the query substrings are smaller than the indexed string. An example is a sensor system, collecting and indexing big amount of data to facilitate searching for interesting query patterns, which are smaller compared to the data stream. We show that more efficient indexing solution can be achieved based on query sub- string length. </span></p><p class="c43"><span class="c1">Correlation among string positions: Probabilities at different po- sitions in the uncertain string can possibly contain correlation among them. In this paper we consider character level uncertainly model, where a probability of occurrence of a character at a position can be correlated with occurrence of a character at a different position. We formally define the correlation model and show how correlation is handled in our indexes. </span></p><p class="c45"><span class="c1">Our approach involves the use of suffix trees, suffix arrays and range maximum query data structure, which to the best of our knowl- edge, is the first use for uncertain string indexing. Succinct and compressed versions of these data structures are well known to have </span></p><p class="c5 c47"><span class="c1">good practical performance. Previous efforts to index uncertain strings mostly involved dynamic programming and lacked theoret- ical bound on query time. We also formulate the uncertain string listing problem. Practical motivation for this problem is given in Section 6. As mentioned before, for a specific starting position of an uncertain string, the probability of occurrence of a substring can change arbitrarily with increasing length depending on the proba- bility of the concatenated character. We propose an approximate solution by discretizing the arbitrary probability changes with con- junction of a linking structure in the suffix tree. </span><span class="c7">1.5 Our contribution: </span></p><p class="c32"><span class="c1">In this paper, we propose indexing solutions for substring search- ing in a single uncertain string, searching in a uncertain string col- lection, and approximate index for searching in uncertain strings. More specifically, we make the following contributions: </span></p><p class="c61"><span class="c1">1. For the substring searching problem, we propose a linear space solution for indexing a given uncertain string S of length n, such that all the occurrences of a deterministic query string p with probability of occurrence greater than a query threshold &tau; can be reported. We show that for frequent cases our index achieves optimal query time proportional to the substring length and output size. Our index can be de- signed to support arbitrary probability threshold &tau; &ge; &tau;</span><span class="c0">min</span><span class="c1">, where &tau;</span><span class="c3">min </span><span class="c11">is a constant known at index construction time. </span></p><p class="c48"><span class="c1">2. For the uncertain string listing problem, given a collection of uncertain strings D = {d</span><span class="c3">1</span><span class="c11">,...,d</span><span class="c3">D</span><span class="c11">} of total size n, we pro- </span><span class="c1">pose a linear space and near optimal time index for retrieving all uncertain strings that contain a deterministic query string p with probability of occurrence greater than a query thresh- old &tau;. Our index supports queries for arbitrary &tau; &ge; &tau;</span><span class="c0">min</span><span class="c1">, where &tau;</span><span class="c3">min </span><span class="c11">is a constant known at construction time. </span></p><p class="c37"><span class="c1">3. We propose an index for approximate substring searching, which can answer substring searching queries in uncertain strings for arbitrary &tau; &ge; &tau;</span><span class="c3">min </span><span class="c11">in optimal O(m + occ) time, </span><span class="c1">where &tau;</span><span class="c3">min </span><span class="c11">is a constant known at construction time and &epsilon; </span><span class="c1">is the bound on desired additive error in the probability of a matched string, i.e. outputs can have probability of occur- rence &ge; &tau; &minus; &epsilon;. </span><span class="c7">1.6 Outline </span></p><p class="c30"><span class="c1">The rest of the paper is organized as follows. In section 2 we show some practical motivations for our indexes. In section 3 we give a formal definition of the problem, discuss some definitions related to uncertain strings and supporting tools used in our index. In section 4 we build a linear space index for answering a special form of uncertain strings where each position of the string has only one probabilistic character. In section 5 we introduce a linear space index to answer substring matching queries in general uncertain strings for variable threshold. Section 6 discusses searching in an uncertain string collection. In section 7, we discuss approximate string matching in uncertain strings. In section 8, we show the experimental evaluation of our indexes. Finally in section 9, we conclude the paper with a summary and future work direction. </span></p><p class="c40"><span class="c7">2. MOTIVATION </span></p><p class="c36"><span class="c1">Various domains such as bioinformatics, knowledge discovery for moving object database trajectories, web log analysis, text min- ing, sensor networks, data integration and activity recognition gen- erates large amount of uncertain data. Below we show some prac- tical motivation for our indexes. </span></p><p class="c35"><span class="c15">403 </span></p><p class="c8"><span class="c1">Biological sequence data: Sequence data in bioinformatics is of- ten uncertain and probabilistic. For instance, reads in shotgun se- quencing are annotated with quality scores for each base. These quality scores can be understood as how certain a sequencing ma- chine is about a base. Probabilities over long strings are also used to represent the distribution of SNPs or InDels (insertions and dele- tions) in the population of a species. Uncertainty can arise due to a number of factors in the high-throughput sequencing technologies. NC-IUB committee standardized incompletely specified bases in DNA to address this common presence of uncertainty [20]. An- alyzing these uncertain sequences is important and more compli- cated than the traditional string matching problem. </span></p><p class="c8"><span class="c1">We show an example uncertain string generated by aligning ge- nomic sequence of Tree of At4g15440 from OrthologID and deter- ministic substring searching in the sequence. Figure 3 illustrates the example. </span></p><p class="c17"><span class="c10">S[1] S[2] S[3] S[4] S[5] S[6] S[7] S[8] S[9] S[10] S[11] P 1 S .7 F 1 P 1 Q .5 P 1 A .4 I .3 A 1 S .5 A 1 </span></p><p class="c5"><span class="c10">F .3 T .5 F .4 L .3 T .5 </span></p><p class="c5"><span class="c10">P .2 P .3 T .3 </span></p><p class="c5"><span class="c1">Figure 3: Example of an uncertain string S generated by aligning genomic sequence of the tree of At4g15440 from OrthologID. </span></p><p class="c8"><span class="c1">Consider the uncertain string S of Figure 3. A sample query can be {p = &rdquo;AT&rdquo;,&tau; = 0.4}, which asks to find all the occurrences of string AT in S having probability of occurrence more than &tau; = .4. AT can be matched starting at position 7 and starting at position 9. Probability of occurrence for starting position 7 is 0.4&times;0.3=0.12 and for starting position 9 is 1&times;0.5=0.5. Thus position 9 should be reported to answer this query. </span></p><p class="c8"><span class="c1">Automatic ECG annotations: In the Holter monitor application, for example, sensors attached to heart-disease patients send out ECG signals continuously to a computer through a wireless net- work( [9]). For each heartbeat, the annotation software gives a symbol such as N (Normal beat), L (Left bundle branch block beat), and R, etc. However, quite often, the ECG signal of each beat may have ambiguity, and a probability distribution on a few possibilities can be given. A doctor might be interested in locating a pattern such as &acirc; </span><span class="c6">&nbsp;&#774;</span><span class="c1">AIJNNAV&acirc; </span><span class="c6">&nbsp;&#774;</span><span class="c1">A</span><span class="c6">&nbsp;&#775;</span><span class="c1">I indicating two normal beats followed by an atrial premature beat and then a premature ventricular contraction, in order to verify a specific diagnosis. The ECG signal sequence forms an uncertain string, which can be indexed to facilitate deter- ministic substrings searching. </span></p><p class="c5"><span class="c1">Event monitoring: Substring matching over event streams is im- portant in paradigm where continuously arriving events are matched. For example a RFID-based security monitoring system produces stream of events. Unfortunately RFID devices are error prone and associate probability with the gathered events. A sequence of events can represent security threat. The stream of probabilistic events can be modeled with uncertain string and can be indexed so that deter- ministic substring can be queried to detect security threats. </span></p><p class="c5"><span class="c7">3. PRELIMINARIES </span></p><p class="c5"><span class="c7">3.1 Uncertain string and deterministic string </span></p><p class="c8"><span class="c1">An uncertain string S = s</span><span class="c3">1 </span><span class="c11">...s</span><span class="c3">n </span><span class="c11">over alphabet &Sigma; is a sequence </span><span class="c1">of sets s</span><span class="c0">i</span><span class="c1">,i = 1,...,n. Every s</span><span class="c0">i </span><span class="c1">is a set of pairs of the form (c</span><span class="c3">j</span><span class="c11">, pr(c</span><span class="c0">i</span><span class="c3">j</span><span class="c1">)), where every c</span><span class="c3">j </span><span class="c11">is a character in &Sigma; and 0 &le; pr(c</span><span class="c0">i</span><span class="c3">j</span><span class="c1">) &le; 1 is the probability of occurrence of c</span><span class="c3">j </span><span class="c11">at position i in the string. </span><span class="c1">Uncertain string length is the total number of positions in the string, </span></p><p class="c5"><span class="c1">which can be less than the total number of characters in the string. Note that, summation of probability for all the characters at each position should be 1, i.e. </span><span class="c6">&sum;</span><span class="c0">j </span><span class="c6">pr(c</span><span class="c12">i</span><span class="c0">j</span><span class="c6">) = 1. Figure 3 shows an </span><span class="c1">example of an uncertain string of length 11. A deterministic string has only one character at each position with probability 1. We can exclude the probability information for deterministic strings. </span><span class="c7">3.2 Probability of occurrence of a substring in </span><span class="c18">an uncertain string </span><span class="c1">Since each character in the uncertain string has an associated probability, a deterministic substring occurs in the uncertain string with a probability. Let S = s</span><span class="c0">1 </span><span class="c1">...s</span><span class="c0">n </span><span class="c1">is an uncertain string and p is a deterministic string. If the length of p is 1, then probability of occurrence of p at position i of S is the associated probabil- ity pr(p</span><span class="c12">i</span><span class="c1">). Probability of occurrence of a deterministic substring p = p</span><span class="c3">1 </span><span class="c11">...p</span><span class="c3">k</span><span class="c11">, starting at a position i in S is defined as the par- </span><span class="c1">tial product pr(p</span><span class="c12">i</span><span class="c0">1</span><span class="c6">) &times;&middot;&middot;&middot;&times; pr(p</span><span class="c12">i+k&minus;1 </span></p><p class="c5"><span class="c0">k </span><span class="c6">). For example in Figure 3, </span><span class="c1">SFPQ has probability of occurrence 0.7 &times; 1 &times; 1 &times; 0.5=0.35 at position 2. </span><span class="c7">3.3 Correlation among string positions </span></p><p class="c8"><span class="c1">We say that character c</span><span class="c3">k </span><span class="c11">at position i is correlated with character </span><span class="c1">c</span><span class="c3">l </span><span class="c11">at position j, if the probability of occurrence of c</span><span class="c3">k </span><span class="c11">at position </span><span class="c1">i is dependent on the probability of occurrence of c</span><span class="c3">l </span><span class="c11">at position j. </span><span class="c1">We use pr(c</span><span class="c12">i</span><span class="c0">k</span><span class="c6">)</span><span class="c12">+ </span><span class="c1">to denote the probability of c</span><span class="c12">i</span><span class="c0">k </span><span class="c6">when the corre- </span><span class="c1">lated character is present, and pr(c</span><span class="c12">i</span><span class="c0">k</span><span class="c6">)</span><span class="c12">&minus; </span><span class="c1">to denote the probability of c</span><span class="c12">i</span><span class="c0">k </span><span class="c6">when the correlated character is absent. Let x</span><span class="c0">g </span><span class="c1">...x</span><span class="c3">h </span><span class="c11">be a the </span><span class="c1">substring generated from an uncertain string. c</span><span class="c12">i</span><span class="c0">k</span><span class="c6">,g &le; i &le; h is a </span><span class="c1">character within the substring which is correlated with c</span><span class="c12">j</span><span class="c0">l</span><span class="c6">. Depend- </span><span class="c1">ing on the position j, we have 2 cases: </span></p><p class="c5"><span class="c1">Case 1, g &le; j &le; h : The correlated probability of (c</span><span class="c12">i</span><span class="c0">k</span><span class="c6">) is expressed </span></p><p class="c8"><span class="c1">by (c</span><span class="c12">j</span><span class="c0">l </span><span class="c6">=&rArr; a , &not;c</span><span class="c12">j</span><span class="c0">l </span><span class="c6">=&rArr; b), i.e. if c</span><span class="c12">j</span><span class="c0">l </span><span class="c6">is taken as an </span><span class="c1">occurrence, then pr(c</span><span class="c12">i</span><span class="c0">k</span><span class="c6">) = pr(c</span><span class="c12">i</span><span class="c0">k</span><span class="c6">)</span><span class="c12">+</span><span class="c1">, otherwise pr(c</span><span class="c12">i</span><span class="c0">k</span><span class="c6">) = </span><span class="c1">pr(c</span><span class="c12">i</span><span class="c0">k</span><span class="c6">)</span><span class="c12">&minus;</span><span class="c1">. We consider a simple example in Figure 4 to il- lustrate this. In this string, z</span><span class="c12">3 </span><span class="c1">is correlated with e</span><span class="c12">1</span><span class="c1">. For the substring eqz, pr(z</span><span class="c12">3</span><span class="c1">) = .3, and for the substring fqz, pr(z</span><span class="c12">3</span><span class="c1">) = .4. </span></p><p class="c8"><span class="c1">Case 2, j&lt;g or j&gt;h : c</span><span class="c12">j</span><span class="c0">l </span><span class="c6">is not within the substring. In this </span><span class="c1">case, pr(c</span><span class="c12">i</span><span class="c0">k</span><span class="c6">)=pr(c</span><span class="c12">j</span><span class="c0">l</span><span class="c6">)*pr(c</span><span class="c12">i</span><span class="c0">k</span><span class="c6">)</span><span class="c12">+</span><span class="c1">+(1 &minus; pr(c</span><span class="c12">j</span><span class="c0">l</span><span class="c6">))*pr(c</span><span class="c12">i</span><span class="c0">k</span><span class="c6">)</span><span class="c12">+</span><span class="c1">. In Figure 4, for substring qz, pr(z</span><span class="c12">3</span><span class="c1">) = .6 &lowast; .3 + .4 &lowast; .4. </span></p><p class="c5"><span class="c13">S[1] S[2] S[3] e: .6 q: 1 z: e</span><span class="c12">1 </span><span class="c13">=&rArr; .3, &not;e</span><span class="c12">1 </span><span class="c13">=&rArr; .4 f: .4 </span></p><p class="c5"><span class="c1">Figure 4: Uncertain string S with correlated characters. </span></p><p class="c5"><span class="c7">3.4 Suffix tree and generalized suffix tree </span></p><p class="c8"><span class="c1">The suffix tree [28, 21] of a deterministic string t[1 ...n] is a lexicographic arrangement of all these n suffixes in a compact trie structure of O(n) words space, where the i-th leftmost leaf repre- sents the i-th lexicographically smallest suffix of t. For a node i (i.e., node with pre-order rank i), path(i) represents the text ob- tained by concatenating all edge labels on the path from root to node i in a suffix tree. The locus node i</span><span class="c0">p </span><span class="c1">of a string p is the node closest to the root such that the p is a prefix of path(i</span><span class="c3">P </span><span class="c11">). The </span><span class="c1">suffix range of a string p is given by the maximal range [sp, ep] such that for sp &le; j &le; ep, p is a prefix of (lexicographically) j-th </span></p><p class="c5"><span class="c15">404 </span></p><p class="c8"><span class="c1">smallest suffix of t. Therefore, i</span><span class="c0">p </span><span class="c1">is the lowest common ancestor of sp-th and ep-th leaves. Using suffix tree, the locus node as well as the suffix range of p can be computed in O(p) time, where p denotes the length of p. The suffix array A of t is defined to be an array of integers providing the starting positions of suffixes of S in lexicographical order. This means, an entry A[i] contains the position of i-th leaf of the suffix tree in t. For a collection of strings D = {d</span><span class="c0">1</span><span class="c1">,...,d</span><span class="c0">D</span><span class="c1">}, let t = d</span><span class="c0">1</span><span class="c1">d</span><span class="c0">2 </span><span class="c1">...d</span><span class="c0">D </span><span class="c1">be the text obtained by concatenating all the strings in D. Each string is assumed to end with a special character $. The suffix tree of t is called the general- ized suffix tree (GST) of D. </span></p><p class="c5"><span class="c7">4. STRING MATCHING IN </span></p><p class="c5"><span class="c7">SPECIAL UNCERTAIN STRINGS </span><span class="c1">In this section, we construct index for a special form of uncer- tain string which is extended later. Special uncertain string is an uncertain string where each position has only one probable char- acter with associated non-zero probability of occurrence. Special- uncertain string is defined more formally below. </span></p><p class="c8"><span class="c1">D</span><span class="c25">EFINITION </span><span class="c1">1. A special uncertain string X = x</span><span class="c0">1 </span><span class="c1">...x</span><span class="c0">n </span><span class="c1">over alphabet &Sigma; is a sequence of pairs. Every x</span><span class="c3">i </span><span class="c11">is a pair of the form </span><span class="c1">(c</span><span class="c3">i</span><span class="c11">, pr(c</span><span class="c0">i</span><span class="c3">i</span><span class="c1">)), where every c</span><span class="c3">i </span><span class="c11">is a character in &Sigma; and 0 &lt; pr(c</span><span class="c0">i</span><span class="c3">i</span><span class="c1">) &le; 1 is the probability of occurrence of c</span><span class="c0">i </span><span class="c1">at position i in the string. </span></p><p class="c5"><span class="c1">Before we present an efficient index, we discuss a naive solution similar to deterministic substring searching. </span><span class="c7">4.1 Simple index </span></p><p class="c8"><span class="c1">Given a special uncertain string X = x</span><span class="c3">1 </span><span class="c11">...x</span><span class="c3">n</span><span class="c11">, construct the </span><span class="c1">deterministic string t = c</span><span class="c0">1 </span><span class="c1">...c</span><span class="c0">n </span><span class="c1">where c</span><span class="c0">i </span><span class="c1">is the character in x</span><span class="c0">i</span><span class="c1">. We build a suffix tree over t. We build a suffix array A which maps each leaf of the suffix tree to its original position in t. We also build a successive multiplicative probability array C, where C[j] = </span><span class="c11">&prod;</span><span class="c3">ji=1 </span><span class="c1">Pr(c</span><span class="c12">i</span><span class="c0">i</span><span class="c6">), for j = 1,...,n. For a substring x</span><span class="c0">i </span><span class="c1">...x</span><span class="c3">i+j</span><span class="c11">, prob- </span><span class="c1">ability of occurrence can be easily computed by C[i+j]/C[i&minus;1]. Given an input (p, &tau;), we traverse the suffix tree for p and find the locus node and suffix range of p in O(m) time, where m is the length of p. Let the suffix range be [sp, ep]. According to the property of suffix tree, each leaf within the range [sp, ep] con- tains an occurrence of p in t. Original positions of the occur- rence in t can be found using suffix array, i.e., A[sp],...,A[ep]. However, each of these occurrence has an associated probability. We traverse each of the occurrence in the range A[sp],...,A[ep]. For an occurrence A[i], we find the probability of occurrence by C[A[i] + m &minus; 1]/C[A[i] &minus; 1]. If the probability of occurrence is greater than &tau;, we report the position A[i] as an output. Figure 5 illustrates this approach. </span></p><p class="c8"><span class="c1">Handling correlation: Correlation is handled when we check for the probability of occurrence. If A[i] is a possible occurrence, then we need to consider any existing character within the sub- string x</span><span class="c0">i </span><span class="c1">...x</span><span class="c0">i+m&minus;1</span><span class="c1">, that is correlated with another character. Let c</span><span class="c3">k </span><span class="c11">is a character at position j within x</span><span class="c3">i </span><span class="c11">...x</span><span class="c3">i+m&minus;1</span><span class="c11">, which is cor- </span><span class="c1">related with character c</span><span class="c12">j </span><span class="c0">l </span><span class="c6">, i.e. if c</span><span class="c12">j </span><span class="c0">l </span><span class="c6">is included in the substring, </span><span class="c1">then pr(c</span><span class="c12">j</span><span class="c0">k</span><span class="c6">)=pr(c</span><span class="c12">j</span><span class="c0">k</span><span class="c6">)</span><span class="c12">+</span><span class="c1">, or else pr(c</span><span class="c12">j</span><span class="c0">k</span><span class="c6">)=pr(c</span><span class="c12">j</span><span class="c0">k</span><span class="c6">)</span><span class="c12">&minus;</span><span class="c1">. To find the correct probability of (c</span><span class="c12">j</span><span class="c0">k</span><span class="c6">), if j we check the j -th position (j depth char- </span><span class="c1">acter on the root to locus path in the suffix tree) of the substring. If the j -th character is c</span><span class="c3">l</span><span class="c11">, then C[A[i] + m &minus; 1]/C[A[i] &minus; 1] is </span><span class="c1">the correct probability of occurrence for x</span><span class="c3">i </span><span class="c11">...x</span><span class="c3">i+m</span><span class="c11">. Otherwise, </span><span class="c1">C[A[i] + m &minus; 1]/C[A[i] &minus; 1] contains the incorrect probability of c</span><span class="c12">j</span><span class="c0">k</span><span class="c6">. Dividing C[A[i] + m &minus; 1]/C[A[i] &minus; 1] by pr(c</span><span class="c12">j</span><span class="c0">k</span><span class="c6">)</span><span class="c12">+ </span><span class="c1">and multiplying by pr(c</span><span class="c12">j</span><span class="c0">k</span><span class="c6">)</span><span class="c12">&minus; </span><span class="c1">gives the correct probability of occurrence </span></p><p class="c5"><span class="c21">X : (b,0.4),(a,0.7),(n,0.5),(a,0.8),(n,0.9),(a,0.6) Position: </span></p><p class="c5"><span class="c21">1 2 3 4 5 6 </span><span class="c24">T : b a n a n a </span><span class="c21">C : 0.4 0.28 0.14 0.11 0.10 0.06 </span></p><p class="c5"><span class="c21">a b</span><span class="c23">n </span><span class="c21">a </span><span class="c24">ana</span><span class="c21">$ </span><span class="c24">na </span><span class="c21">$ </span></p><p class="c17"><span class="c21">Query: (&rdquo;ana&rdquo;, 0.3) Suffix range: 4 2 </span></p><p class="c5"><span class="c21">Pr</span><span class="c31">occ</span><span class="c21">: </span><span class="c24">0.43 0.28 </span><span class="c21">Output: </span><span class="c34">4 </span></p><p class="c5"><span class="c1">Figure 5: Simple index for special uncertain strings. </span></p><p class="c5"><span class="c1">in this case. If c</span><span class="c3">l </span><span class="c11">falls before or after the substring x</span><span class="c3">i </span><span class="c11">...x</span><span class="c3">i+m&minus;1</span><span class="c11">, </span><span class="c1">pr(c</span><span class="c12">j</span><span class="c0">k</span><span class="c6">)=pr(c</span><span class="c12">j </span><span class="c0">l </span><span class="c6">)*pr(c</span><span class="c1">m &minus; 1]/C[A[i] &minus; 1] the correct probability </span><span class="c12">j</span><span class="c0">k</span><span class="c1">by </span><span class="c6">)</span><span class="c1">of </span><span class="c12">+</span><span class="c1">pr(c+(1&minus;pr(coccurrence. </span><span class="c12">j</span><span class="c0">k</span><span class="c6">)</span><span class="c12">+ j </span><span class="c0">l </span><span class="c1">and </span><span class="c6">))*pr(c</span><span class="c12">j</span><span class="c0">k</span><span class="c6">)</span><span class="c12">&minus;</span><span class="c1">. DividingC[A[i]+ </span></p><p class="c5"><span class="c1">multiplying by pr(cNote that, we can </span><span class="c12">j</span><span class="c0">k</span><span class="c1">identify </span><span class="c6">) gives </span></p><p class="c5"><span class="c1">and group all the characters with existing correlation, and search in the suffix tree in one scan for improved efficiency. </span></p><p class="c5"><span class="c1">The main drawback in this approach is the query time. Within the suffix range [sp, ep], possibly very few number of positions can qualify as output because of &tau;. So spending time on each element of the range [sp, ep] is not justifiable. </span><span class="c7">4.2 Efficient index: </span></p><p class="c8"><span class="c1">Bottleneck of the simple index comes from traversing each ele- ment within the suffix range. For the efficient index, we iteratively retrieve the element with maximum probability of occurrence in the range in constant time. Whenever the next maximum probabil- ity of occurrence falls below &tau;, we conclude our search. We use range maximum query (RMQ) data structure for our index which is briefly explained below. </span></p><p class="c8"><span class="c1">Range Maximum Query: Let B be an array of integers of length n, a range maximum query(RMQ) asks for the position of the maximum value between two specified array indices [i, j]. i.e., the RMQ should return an index k such that i &le; k &le; j and B[k] &ge; B[x] for all i &le; x &le; j. We use the result captured in following lemma for our purpose. </span></p><p class="c8"><span class="c1">L</span><span class="c25">EMMA </span><span class="c1">1. [10, 11] By maintaining a 2n + o(n) bits struc- ture, range maximum query(RMQ) can be answered in O(1) time (without accessing the array). </span></p><p class="c16"><span class="c1">Every leaf of the suffix tree denotes a suffix position in the orig- inal text and a root to leaf path represents the suffix. For uncer- tain string, every character in this root to leaf path has an associ- ated j probability which is not = 1,...,n denote a deterministic stored in substring the suffix which tree. is the Let i-length y</span><span class="c0">j</span><span class="c12">i</span><span class="c6">, for </span></p><p class="c5"><span class="c1">prefix of the j-th suffix,i.e. the substring on the root to i-th leaf path. Let For i probability Y </span><span class="c12">i </span><span class="c1">= 1,...,n, array is the set we of define y</span><span class="c0">j</span><span class="c12">i</span><span class="c6">, </span><span class="c1">for the substrings </span><span class="c6">for j = 1,...,n. </span></p><p class="c16"><span class="c1">C</span><span class="c3">i </span><span class="c11">as the successive multiplicative </span><span class="c1">of Y </span><span class="c12">i</span><span class="c1">. j-th element of C</span><span class="c0">i </span><span class="c1">is the successive th suffix. More multiplicative formally Cprobability </span><span class="c3">i</span><span class="c11">[j] = </span><span class="c1">&prod;</span><span class="c3">A[j]+i&minus;1 </span></p><p class="c16"><span class="c0">k=A[j] </span><span class="c1">of the i-length </span><span class="c6">Pr(c</span><span class="c12">k</span><span class="c0">k</span><span class="c1">prefix </span><span class="c6">) = C[A[j] </span><span class="c1">of the j- </span><span class="c6">+ </span><span class="c1">i&minus;1]/C[A[j]&minus;1](1 &le; j &le; n). For each C</span><span class="c0">i</span><span class="c1">(i = 1,...,log n) we </span></p><p class="c5"><span class="c15">405 </span></p><p class="c5"><span class="c21">n</span><span class="c24">a </span><span class="c21">$ n</span><span class="c24">a$ </span><span class="c23">$ </span></p><p class="c5"><span class="c21">n </span><span class="c24">a </span></p><p class="c5"><span class="c21">$ </span></p><p class="c5"><span class="c21">1 2 3 4 5 6 Suffix array A: 6 4 2 1 5 3 </span></p><p class="c8"><span class="c1">use range maximum query data structure RMQ</span><span class="c3">i </span><span class="c11">of n bits over C</span><span class="c3">i </span><span class="c1">and discard the original array C</span><span class="c3">i</span><span class="c11">. We convert C</span><span class="c3">i </span><span class="c11">into an integer ar- </span><span class="c1">ray by multiplying each element by a sufficiently large number and then build the RMQ</span><span class="c3">i </span><span class="c11">structure over it. We obtain log n number of </span><span class="c1">such RMQ data structures resulting in total space of O(n log n) bits or O(n) words. We also store the global successive multiplica- tive probability array C, where C[j] = </span><span class="c6">&prod;</span><span class="c0">j</span><span class="c3">i=1 </span><span class="c1">Pr(c</span><span class="c12">i</span><span class="c0">i</span><span class="c6">). Given a </span><span class="c1">query (p, &tau;), idea is to use RMQ</span><span class="c0">i </span><span class="c1">for iteratively retrieving maxi- mum probability of occurrence elements in constant time each and validate using C. To maintain linear space, we can support query substring length of m = 0,...,log n in this approach. Algorithm 1 illustrates the index construction phase for short substrings. </span></p><p class="c8"><span class="c1">Query answering for short substrings (m &le; log n): Given an input (p, &tau;), we first retrieve the suffix range [l, r] in O(m) time using suffix tree, where m is the length of p. We can find the max- imum probability occurrence of p in O(1) time by executing query RMQ</span><span class="c0">m</span><span class="c1">(l, r). Let max be the position of maximum probabil- ity occurrence and max = A[max] be the the original position in t. We can find the corresponding probability of occurrence by C[max + i &minus; 1]/C[max &minus; 1]. If the probability is less that &tau;, we conclude our search. If it is greater than &tau;, we report max as an output. For finding rest of the outputs, we recursively search in the ranges [l, max &minus; 1] and [max + 1,r]. Since each call to RMQ</span><span class="c0">m</span><span class="c1">(l, r) takes constant time, validating the probability of oc- currence takes constant time, we spend O(1) time for each output. Total query time is optimal O(m + occ). Algorithm 2 illustrates the query answering for short substrings. Note that, correlation is handled in similar way as described for the naive index, and we omit the details here. </span></p><p class="c5"><span class="c1">Algorithm 1: Special-Short-Substring-Index-Construction </span></p><p class="c5"><span class="c1">input : A special uncertain string X output: suffix tree, suffix array A, successive multiplicative </span></p><p class="c5"><span class="c1">probability array C, RMQ</span><span class="c3">i </span><span class="c11">(i = 1,...,log n) </span><span class="c1">Build deterministic string t from X Build suffix tree over t Build suffix array A over t </span><span class="c44">// Building successive multiplicative </span></p><p class="c5"><span class="c44">probability array </span><span class="c1">C[1] = Pr(c</span><span class="c12">1</span><span class="c0">1</span><span class="c6">) </span><span class="c1">for i = 2; i &le; n;i + + do </span></p><p class="c5"><span class="c1">C[i] = C[i &minus; 1] &times; Pr(c</span><span class="c12">i</span><span class="c0">i</span><span class="c6">) </span><span class="c1">end </span><span class="c44">// Building </span><span class="c1">C</span><span class="c3">i </span><span class="c22">array for </span><span class="c11">i = 1,...,log n </span><span class="c1">for i = 1; i &le; log n;i + + do </span></p><p class="c5"><span class="c1">for j = 1;j &le; n;j + + do </span></p><p class="c5"><span class="c1">C</span><span class="c0">i</span><span class="c1">[j] = C[A[j] + i &minus; 1]/C[A[j] &minus; 1] </span><span class="c44">// Handling correlated characters </span><span class="c1">for all character c</span><span class="c12">k</span><span class="c0">a </span><span class="c6">in t[A[j]...t[A[j] + i &minus; 1] that </span><span class="c1">are correlated with another character c</span><span class="c12">l</span><span class="c0">b </span><span class="c6">do </span></p><p class="c5"><span class="c1">if (A[j] &le; l &le; [A[j] + i &minus; 1] and c</span><span class="c12">l</span><span class="c0">b </span><span class="c6">is not within </span><span class="c1">t[A[j]...t[A[j] + i &minus; 1]) C</span><span class="c0">i</span><span class="c1">[j] = C</span><span class="c0">i</span><span class="c1">[j]/P r(c</span><span class="c12">k</span><span class="c0">a</span><span class="c6">)</span><span class="c12">+ </span><span class="c1">&lowast; Pr(c</span><span class="c12">k</span><span class="c0">a</span><span class="c6">)</span><span class="c12">&minus; </span><span class="c1">else pr(c</span><span class="c12">k</span><span class="c0">a</span><span class="c6">)=pr(c</span><span class="c12">l</span><span class="c0">b</span><span class="c6">)*pr(c</span><span class="c12">k</span><span class="c0">a</span><span class="c6">)</span><span class="c12">+</span><span class="c1">+(1 &minus; pr(c</span><span class="c12">l</span><span class="c0">b</span><span class="c6">))*pr(c</span><span class="c12">k</span><span class="c0">a</span><span class="c6">)</span><span class="c12">&minus; </span><span class="c1">C</span><span class="c3">i</span><span class="c11">[j] = C</span><span class="c3">i</span><span class="c11">[j]/P r(c</span><span class="c0">k</span><span class="c3">a</span><span class="c1">)</span><span class="c12">+ </span><span class="c1">&lowast; Pr(c</span><span class="c12">k</span><span class="c0">a</span><span class="c6">) </span><span class="c1">end </span><span class="c11">end </span><span class="c1">end Build RMQ</span><span class="c3">i </span><span class="c11">over C</span><span class="c3">i </span><span class="c11">end </span></p><p class="c5"><span class="c1">Algorithm 2: Special-Short-Substring-Query-Answering </span></p><p class="c5"><span class="c1">input : Query substring p, probability threshold &tau;) output: Occurrence positions of p in X with probability of </span></p><p class="c5"><span class="c1">occurrence greater than &tau; m = length(p) call RecursiveRmq(m,1,n) </span></p><p class="c5"><span class="c1">function R</span><span class="c25">ECURSIVE</span><span class="c1">R</span><span class="c25">MQ</span><span class="c1">(i, l, r) &#8883; Recursive RMQ method </span></p><p class="c5"><span class="c1">max = RMQ</span><span class="c3">m</span><span class="c11">(l, r) </span><span class="c1">max = A[max] if C[max + i &minus; 1]/C[max &minus; 1] &gt; &tau; then Output max Call RecursiveRmq(m, l, max &minus; 1) Call RecursiveRmq(m, max + 1,r) </span></p><p class="c5"><span class="c1">end </span></p><p class="c8"><span class="c1">Query answering for long substrings (m &gt; log n): We use a blocking scheme for answering long query substrings (m &gt; log n). Since exhaustively enumerating all possible substrings and storing the probabilities for each of them is infeasible, we only store selec- tive probability values at construction time and compute the others at query time. We partition the entire suffix range of suffix array into different size blocks. More formally, for i = log n,...,n, we divide the suffix range [1,n] of suffix array A[1,n] into O(n/i) number of blocks each of size i. Let B</span><span class="c0">i </span><span class="c1">be the set of length i blocks, i.e. B</span><span class="c3">i</span><span class="c11">={[A[1]...A[i]], [A[i+1]...A[2i]],... [A[n&minus;i+1]...A[n]]} </span><span class="c1">and let B={B</span><span class="c3">log n</span><span class="c11">,...,B</span><span class="c3">n</span><span class="c11">}. For a suffix starting at A[j] and </span><span class="c1">for B</span><span class="c3">i</span><span class="c11">, we only consider the length i prefix of that suffix, i.e. </span><span class="c1">A[j...j + i]. The idea is to store only the maximum probability value per block. For B</span><span class="c3">i</span><span class="c11">,i = log n,...,n, we define a probability </span><span class="c1">array PB</span><span class="c3">i </span><span class="c11">containing n/i elements. PB</span><span class="c3">i</span><span class="c11">[j] is the maximum prob- </span><span class="c1">ability of occurrence of all the substrings of length i belonging to the j-th block of B</span><span class="c3">i</span><span class="c11">. We build a range maximum query structure </span><span class="c1">RMQ</span><span class="c0">i </span><span class="c1">for PB</span><span class="c0">i</span><span class="c1">. RMQ</span><span class="c0">i </span><span class="c1">takes O(n/i) bits, total space is bounded by </span><span class="c6">&sum;</span><span class="c0">i </span><span class="c6">O(n/i) = O(n log n) bits or O(n) words. </span></p><p class="c8"><span class="c1">For a query (p, &tau;), we first retrieve the suffix range [l, r]. This suffix range can spread over multiple blocks of B</span><span class="c3">m</span><span class="c11">. We use RMQ</span><span class="c3">m </span><span class="c1">to proceed to next step. Note that RMQ</span><span class="c3">m </span><span class="c11">consists of N/m bits, </span><span class="c1">corresponding to the N/m blocks of B</span><span class="c0">m </span><span class="c1">in order. Our query pro- ceeds by executing range maximum query in RMQ</span><span class="c3">m</span><span class="c11">(l, r), which </span><span class="c1">will give us the index of the maximum probability element of string length m in that suffix range. Let the maximum probability el- ement position in RMQ</span><span class="c3">m </span><span class="c11">is max and the block containing this </span><span class="c1">element is B</span><span class="c0">max</span><span class="c1">. Using C array, we can find out if the proba- bility of occurrence is greater than &tau;. Note that, we only stored one maximum element from each block. If the maximum proba- bility found is greater than &tau;, we check all the other elements in that block in O(m) time. In the next step, we recursively query RMQ</span><span class="c3">m</span><span class="c11">(l, max &minus; 1) and RMQ(max + 1,r) to find out subse- </span><span class="c1">quent blocks. Whenever RMQ query for a range returns an ele- ment having probability less than &tau;, we stop the recursion in that range. Number of blocks visited during query answering is equal to the number of outputs and inside each of those block we check m elements, obtaining total query time of O(m &times; occ). </span></p><p class="c8"><span class="c1">In practical applications, query substrings are rarely longer than log n length. Our index achieves optimal query time for substrings of length less than log n. We show in the experimental section that on average our index achieves efficient query time proportional to substring length and number of outputs reported. </span></p><p class="c5"><span class="c15">406 </span></p><p class="c5"><span class="c7">5. SUBSTRING MATCHING IN </span></p><p class="c5"><span class="c7">GENERAL UNCERTAIN STRING </span><span class="c1">In this section we construct index for general uncertain string based on the index of special uncertain string. The idea is to convert a general uncertain string into a special uncertain string, build the data structure similar to the previous section and carefully eliminate the duplicate answers. Below we show the steps of our solution in details. </span><span class="c7">5.1 Transforming general uncertain string </span></p><p class="c8"><span class="c1">We employ the idea of Amihood et al [1] to transform general uncertain string into a special uncertain string. Maximal factor of an uncertain string is defined as follows. </span></p><p class="c8"><span class="c1">D</span><span class="c25">EFINITION </span><span class="c1">2. A maximal factor of a uncertain string S start- ing at location i with respect to a fixed probability threshold &tau;</span><span class="c0">c </span><span class="c1">is a string of maximal length that when aligned to location i has prob- ability of occurrence at least &tau;</span><span class="c0">c</span><span class="c1">. </span></p><p class="c8"><span class="c1">For example in figure 3, maximal factors of the uncertain string S at location 5 with respect to probability threshold 0.15 are &quot;QPA&quot;, &quot;QPF&quot;, &quot;TPA&quot;, &quot;TPF&quot;. </span></p><p class="c8"><span class="c1">An uncertain string S can be transformed to a special uncertain string by concatenating all the maximal factors of S in order. Suffix tree built over the concatenated maximal factors can answer sub- string searching query for a fixed probability threshold &tau;</span><span class="c3">c</span><span class="c11">. But </span><span class="c1">this method produces a special uncertain string of &Omega;(n</span><span class="c12">2</span><span class="c1">) length, which is practically infeasible. To reduce the special uncertain string length, Amihood et al. [1] employs further transformation to obtain a set of extended maximal factors. Total length of the extended maximal factors is bounded by O(( </span><span class="c12">1</span><span class="c0">&tau;</span><span class="c4">c </span><span class="c1">)</span><span class="c12">2</span><span class="c1">n). </span></p><p class="c8"><span class="c1">L</span><span class="c25">EMMA </span><span class="c1">2. Given a fixed probability threshold value &tau;</span><span class="c0">c</span><span class="c1">(0 &lt; &tau;</span><span class="c3">c </span><span class="c11">&le; 1), an uncertain string S can be transformed into a special </span><span class="c1">uncertain string X of length O(( </span><span class="c12">1</span><span class="c0">&tau;</span><span class="c4">c </span><span class="c1">)</span><span class="c12">2</span><span class="c1">n) such that any determinis- tic substring p of S having probability of occurrence greater than &tau;</span><span class="c3">c </span><span class="c11">is also a substring of X. </span></p><p class="c8"><span class="c1">Simple suffix tree structure for answering query does not work for the concatenated extended maximal factors. A special form of suffix tree, namely property suffix tree is introduced by Ami- hood et al. [1]. Also substring searching in this method works only on a fixed probability threshold &tau;</span><span class="c3">c</span><span class="c11">. A naive way to support arbi- </span><span class="c1">trary probability threshold is to construct special uncertain string and property suffix tree index for all possible value of &tau;</span><span class="c0">c</span><span class="c1">, which is practically infeasible due to space usage. </span></p><p class="c5"><span class="c1">We use the technique of lemma 2 to transform a given general un- certain string to a special uncertain string of length O(( </span><span class="c12">1 </span></p><p class="c5"><span class="c0">&tau;</span><span class="c4">min </span><span class="c1">)</span><span class="c12">2</span><span class="c1">n) based on a probability threshold &tau;</span><span class="c3">min </span><span class="c11">known at construction time, </span><span class="c1">and employ a different indexing scheme over it. Let X be the trans- formed special uncertain string. A running example is shown in Appendix B in the full version of this paper [27]. Following sec- tion elaborates the subsequent steps of the index construction. </span><span class="c7">5.2 Index construction on the </span><span class="c18">transformed uncertain string </span></p><p class="c8"><span class="c1">Our index construction is similar to the index of section 4. We need some additional components to eliminate duplication and po- sition transformation. </span></p><p class="c5"><span class="c1">Let N = |X| be the length of the special uncertain string X. Note that N = O(( </span><span class="c12">1 </span></p><p class="c8"><span class="c0">&tau;</span><span class="c4">min </span><span class="c1">)</span><span class="c12">2</span><span class="c1">n) = O(n), since &tau;</span><span class="c3">min </span><span class="c11">is a constant </span><span class="c1">known in construction time. For transforming the positions of X into the original position in S, we store an array P os of size N, </span></p><p class="c8"><span class="c1">where P os[i]=position of the i-th character of X in the original string S. We construct the deterministic string t = c</span><span class="c3">1 </span><span class="c11">...c</span><span class="c3">N </span><span class="c11">where </span><span class="c1">c</span><span class="c0">i </span><span class="c1">is the character in X</span><span class="c0">i</span><span class="c1">. We build a suffix tree over t. We build a suffix array A which maps each leaf of the suffix tree to its position in t. We also build a successive multiplicative probability array C, where C[j] = </span><span class="c6">&prod;</span><span class="c0">j</span><span class="c3">i=1 </span><span class="c1">Pr(c</span><span class="c12">i</span><span class="c0">i</span><span class="c6">), for 1 &le; j &le; N. For a substring of </span><span class="c1">length j starting at position i, probability of occurrence of the sub- string in X can be easily computed by C[i+j&minus;1]/C[i&minus;1]. For i = 1,...,n, we define C</span><span class="c3">i </span><span class="c11">as the successive multiplicative probability </span><span class="c1">array for substring length i i.e. C</span><span class="c3">i</span><span class="c11">[j] = </span><span class="c1">&prod;</span><span class="c3">A[j]+i&minus;1 </span></p><p class="c8"><span class="c0">k=A[j] </span><span class="c6">Pr(c</span><span class="c12">k</span><span class="c0">k</span><span class="c6">) = </span><span class="c1">C[A[j] + i &minus; 1]/C[A[j] &minus; 1] (1 &le; j &le; n). Appendix B of the full version [27] shows P os array and C array after transformation of an uncertain string. Below we explain how duplicates may arise in outputs and how to eliminate them. </span></p><p class="c8"><span class="c1">Possible duplicate positions in the output arises because of the general to special uncertain string transformation. Note that, dis- tinct positions in X can correspond to the same position in the original uncertain string S, resulting in same position possibly re- ported multiple times. A key observation here is that for two dif- ferent substrings of length m, if the locus nodes are different than the corresponding suffix ranges are disjoint. These disjoint suffix ranges collectively cover all the leaves of the suffix tree. For each such disjoint ranges, we need to store probability values for only the unique positions of S. Without loss of generality we store the value for leftmost unique position in each range. </span></p><p class="c8"><span class="c1">For any node u in the suffix tree, depth(u) is the length of the concatenated edge labels from root to u. We define by L</span><span class="c3">i </span><span class="c11">as the set </span><span class="c1">of nodes u</span><span class="c12">j</span><span class="c0">i </span><span class="c6">such that depth(u</span><span class="c12">j</span><span class="c0">i</span><span class="c6">) &ge; i and depth(parent(u</span><span class="c12">j</span><span class="c0">i</span><span class="c6">)) &le; </span><span class="c1">i. For L</span><span class="c3">i </span><span class="c11">= u</span><span class="c0">1</span><span class="c3">i </span><span class="c1">,...,u</span><span class="c12">k</span><span class="c0">i </span><span class="c6">, we have a set of disjoint suffix ranges </span><span class="c1">[sp</span><span class="c12">1</span><span class="c0">i </span><span class="c6">, ep</span><span class="c12">1</span><span class="c0">i</span><span class="c6">],...,[sp</span><span class="c12">k</span><span class="c0">i </span><span class="c6">, ep</span><span class="c12">k</span><span class="c0">i </span><span class="c6">]. A suffix range [sp</span><span class="c12">j</span><span class="c0">i</span><span class="c6">, ep</span><span class="c12">j</span><span class="c0">i</span><span class="c6">] can contain </span><span class="c1">duplicate positions of S. Using the P os array we can find the unique positions for each range and store only the values corre- sponding to the unique positions in C</span><span class="c0">i</span><span class="c1">. </span></p><p class="c5"><span class="c1">We use range maximum query data structure RMQ</span><span class="c3">i </span><span class="c11">of n bits </span><span class="c1">over C</span><span class="c0">i </span><span class="c1">and discard the original array C</span><span class="c0">i</span><span class="c1">. Note that, RMQ data structure can be built over an integer array. We convert C</span><span class="c3">i </span><span class="c11">into an </span><span class="c1">integer array by multiplying each element by a sufficiently large number and then build the RMQ</span><span class="c0">i </span><span class="c1">structure over it. We obtain log n number of such RMQ data structures resulting in total space of O(nlog n) bits or O(n) words. For long substrings (m &gt; log n), we use the blocking data structure similar to section 4. De- tailed construction phase is shown in Algorithm 3 of Appendix A in the full version of this paper [27]. </span><span class="c7">5.3 Query answering </span></p><p class="c5"><span class="c1">Query answering procedure is almost similar to the query an- swering procedure of section 4. Only difference being the trans- formation of position which is done using the P os array. Detailed query answering Algorithm for short query substrings is included in Appendix A of the full version of this paper [27]. See Appendix B for an illustrative example of query answering. </span><span class="c7">5.4 Space complexity </span></p><p class="c8"><span class="c1">For analyzing the space complexity, we consider each compo- nent of our index. Length of the special uncertain string X and deterministic string t are O(n), where n is the number of posi- tions in S. Suffix tree and suffix tree each takes linear space. We store a successive probability array of size O(n). We build probability array C</span><span class="c3">i </span><span class="c11">for i = 1,...,log n, where each C</span><span class="c3">i </span><span class="c11">takes </span><span class="c1">of O(n). However we build RMQ</span><span class="c0">i </span><span class="c1">of n bits over C</span><span class="c0">i </span><span class="c1">and dis- card the original array C</span><span class="c3">i</span><span class="c11">. We obtain log n number of such RMQ </span><span class="c1">data structures resulting in total space of O(n log n) bits or O(n) words. For the blocking scheme, we build RMQ</span><span class="c0">i </span><span class="c1">data structure for </span></p><p class="c5"><span class="c15">407 </span></p><p class="c5"><span class="c1">i = log n,...,n. RMQ</span><span class="c0">i </span><span class="c1">takes n/i bits, total space is </span><span class="c6">&sum;</span><span class="c0">i </span><span class="c6">n/i = </span><span class="c1">O(nlog n) bits or O(n) words. Since each component of our index takes linear space, total space taken by our index is O(n) words. </span><span class="c7">5.5 Proof of correctness </span></p><p class="c5"><span class="c1">In this section we discuss the correctness of our indexing scheme. Substring conservation property of the transformation: At first we show that any substring of S with probability of occurrence greater than query threshold &tau; can be found in t as well. According to lemma 2, a substring having probability of occurrence greater than &tau;</span><span class="c3">min </span><span class="c11">in S is also a substring of the transformed special uncer- </span><span class="c1">tain string X. Since query threshold value &tau; is greater than &tau;</span><span class="c3">min</span><span class="c11">, </span><span class="c1">and entire character string of X is same as the deterministic string t, a substring having probability of occurrence greater than query threshold &tau; in S will be present in the deterministic string t. Complete set of ocurrences are outputted: For contradiction, we assume that an occurrence position z of substring p in S having probability of occurrence greater than &tau; is not included in the out- put. From the aforementioned property, p is a substring of t. Ac- cording to the property of suffix tree, z must be present in the suffix range [sp, ep] of p. Using RMQ structure, we report all the occur- rence in [sp, ep] in their decreasing order of probability of occur- rence value in S and stop when the probability of occurrence falls below &tau;, which ensures inclusion of z. No incorrect occurrence appears in output: An output z can be incorrect occurrence if it is not present in uncertain string S or its probability of occurrence is less than &tau;. We query only the occur- rences in the suffix range [sp, ep] of p, according to the property of suffix tree all of which are valid occurrences. We also validate the probability of occurrence for each of them using the successive multiplicative probability array C. </span></p><p class="c5"><span class="c7">6. STRING LISTING FROM </span></p><p class="c5"><span class="c7">UNCERTAIN STRING COLLECTION </span><span class="c1">In this section we propose an indexing solution for problem 2. We are given a collection of D uncertain strings D = {d</span><span class="c3">1</span><span class="c11">,...,d</span><span class="c3">D</span><span class="c11">} </span><span class="c1">of n positions in total. Let i denotes the string identifier of string d</span><span class="c3">i</span><span class="c11">. For a query (p, &tau;), we have to report all the uncertain string </span><span class="c1">identifiers j such that d</span><span class="c3">j </span><span class="c11">contains p with probability of occurrence </span><span class="c1">more than &tau;. In other words, we want to list the strings from a col- lection of a string, that are relevant to a deterministic query string based on probability parameter. </span></p><p class="c8"><span class="c1">Relevance metric: For a deterministic string t and an uncer- tain string S, we define a relevance metric, Rel(S, t). If t does not have any occurrence in S, then Rel(S, t)=0. If s has only one occurrence of t, then Rel(S, t) is the probability of occurrence of t in S. If s contains multiple occurrences of t, then Rel(S, t) is a function of the probability of occurrences of t in S. Depending on the application, various functions can be chosen as the appropri- ate relevance metric. A common relevance metric is the maximum probability of occurrence, which we denote by Rel(S, t)</span><span class="c0">max</span><span class="c1">. The OR value of the probability of occurrences is another useful rele- vance metric. More formally, if a deterministic string t has nonzero probable occurrences at positions i</span><span class="c3">1</span><span class="c11">,...,i</span><span class="c3">k </span><span class="c11">of an uncertain string </span><span class="c1">S, then we define the relevance metric of t in S as Rel(S, t)</span><span class="c3">OR </span><span class="c11">= &sum;</span><span class="c3">i</span><span class="c4">k</span><span class="c3">j=i</span><span class="c4">1 </span><span class="c1">pr(t</span><span class="c0">j</span><span class="c1">) &minus; </span><span class="c6">&prod;</span><span class="c0">i</span><span class="c4">k</span><span class="c3">j=i</span><span class="c4">1 </span><span class="c1">pr(t</span><span class="c0">j</span><span class="c1">), where pr(t</span><span class="c0">j</span><span class="c1">) is the probability of occurrence of t in S at position j. Figure 6 shows an example. </span></p><p class="c8"><span class="c1">Practical motivation: Uncertain string listing finds numerous practical motivation. Consider searching for a virus pattern in a collection of text files with fuzzy information. The objective is to </span></p><p class="c17"><span class="c1">Uncertain string S: S[1] S[2] S[3] S[4] S[5] S[6] A .4 B .3 A .5 A .6 B .5 A .4 B .3 L .3 F .5 B .4 F .3 C .3 F .3 F .3 J .2 E .2 J .1 F .1 </span></p><p class="c17"><span class="c1">Rel(S,&rdquo;BFA&rdquo;)</span><span class="c3">max</span><span class="c11">=.09 </span><span class="c1">Rel(S,&rdquo;BFA&rdquo;)</span><span class="c0">OR</span><span class="c1">=(.06 + .09 + .048) &minus; (.06 &lowast; .09 &lowast; .048) </span></p><p class="c5"><span class="c1">=.19786 </span></p><p class="c5"><span class="c1">Figure 6: Relevance metric for string listing. </span></p><p class="c8"><span class="c1">quarantine the files that contain the virus pattern. This problem can be modeled as a uncertain string listing problem, where the collection of text files is the uncertain string collection D, the virus pattern is the query pattern P, and &tau; is the confidence of matching. Similarly, searching for a gene pattern in genomic sequences of different species can be solved using uncertain string listing data structure. </span></p><p class="c8"><span class="c1">The index: As explained before, a naive search on each of the string will result in O(</span><span class="c6">&sum;</span><span class="c0">i </span><span class="c6">search time on d</span><span class="c0">i</span><span class="c1">) which can be much larger than the actual number of strings containing the string. Objective of our index is to spend only one search time and time proportional to the number of output strings. We construct a gener- alized suffix tree so that we have to search for the string only once. We concatenate d</span><span class="c3">1</span><span class="c11">,...,d</span><span class="c3">D </span><span class="c11">by a special symbol which is not con- </span><span class="c1">tained in any of the document and obtain a concatenated general uncertain string S = d</span><span class="c3">1</span><span class="c11">$ ...$d</span><span class="c3">D</span><span class="c11">. Next we use the transformation </span><span class="c1">method described in previous section to obtain deterministic string t, construct suffix tree and suffix array for t. According to the prop- erty of suffix tree, the leaves under the locus of a query substring t contains all the occurrence positions of t. However, these leaves can possibly contain duplicate positions and multiple occurrence of the same document. In the query answering phase, duplicate outputs can arise because of the following two reasons: </span></p><p class="c5"><span class="c1">1. Distinct positions in t can correspond to the same position in </span></p><p class="c5"><span class="c1">the original uncertain string S </span></p><p class="c5"><span class="c1">2. Distinct positions in S can correspond to the same string </span></p><p class="c5"><span class="c1">identifier d</span><span class="c0">j </span><span class="c1">which should be reported only once </span></p><p class="c8"><span class="c1">Duplicate elimination is important to keep the query time propor- tional to the number of output strings. At first we construct the suc- cessive multiplicative probability array C</span><span class="c0">i </span><span class="c1">similar to the substring searching index, then show how to incorporate Rel(S, t) value for the multiple occurrences cases in the same document and duplicate elimination. </span></p><p class="c8"><span class="c1">Let y</span><span class="c12">i</span><span class="c0">j</span><span class="c6">, for j = 1,...,n denote a deterministic substring which </span><span class="c1">is the i-length prefix of the j-th suffix,i.e. the substring on the root to i-th leaf path. Note that, multiple y</span><span class="c12">i</span><span class="c0">j </span><span class="c6">can belong to the same locus </span><span class="c1">node in the suffix tree. Let Y </span><span class="c12">i </span><span class="c1">is the set of y</span><span class="c12">i</span><span class="c0">j</span><span class="c6">, for j = 1,...,n. The </span><span class="c1">i-depth locus nodes in the suffix tree constitutes disjoint partitions in Y </span><span class="c12">i</span><span class="c1">. For i = 1,...,n, we define C</span><span class="c0">i </span><span class="c1">as the successive multi- plicative probability array for the substrings of Y </span><span class="c12">i</span><span class="c1">. j-th element of C</span><span class="c0">i </span><span class="c1">is the successive multiplicative probability of the i-length prefix of the j-th suffix. More formally C</span><span class="c0">i</span><span class="c1">[j] = </span><span class="c6">&prod;</span><span class="c0">A[j]+i&minus;1 </span></p><p class="c5"><span class="c0">k=A[j] </span><span class="c6">Pr(c</span><span class="c12">k</span><span class="c0">k</span><span class="c6">) = </span><span class="c1">C[A[j] + i &minus; 1]/C[A[j] &minus; 1](1 &le; j &le; n). </span></p><p class="c8"><span class="c1">The i-depth locus nodes in the suffix tree constitutes disjoint par- titions in C</span><span class="c3">i</span><span class="c11">. Let u be a i-depth locus node having suffix range </span><span class="c1">[j...k] and root to u substring t. Then the partition C</span><span class="c3">i</span><span class="c11">[j...k] </span><span class="c1">belongs to u. For this partitions, we store only one occurrence of </span></p><p class="c5"><span class="c15">408 </span></p><p class="c8"><span class="c1">a string d</span><span class="c0">j </span><span class="c1">with the relevance metric value Rel(S, t), and discard the other occurrences of d</span><span class="c3">j </span><span class="c11">in that range. We build RMQ structure </span><span class="c1">similar to section 5. </span></p><p class="c8"><span class="c1">Query answering: We explain the query answering for short substrings. Blocking scheme described in previous section can be used for longer query substrings. Given an input (p, &tau;), we first retrieve the suffix range [l, r] in O(m) time using suffix tree, where m is the length of p. We can find the maximum relevant occur- rence of p in O(1) time by executing query RMQ</span><span class="c0">m</span><span class="c1">(l, r). Let max be the position of maximum relevant occurrence and max = A[max] be the the original position in t. For relevance metric Rel(S, t)</span><span class="c0">max</span><span class="c1">, we can find the corresponding probability of occur- rence by C[max +i&minus;1]/C[max &minus;1]. In case of the other com- plex relevance metric, all the occurrences need to be considered to retrieve the actual value of Rel(S, t). If the relevance metric is less that &tau;, we conclude our search. If it is greater than &tau;, we report max as an output. For finding rest of the outputs, we recursively search in the ranges [l, max &minus; 1] and [max + 1,r]. Each call to RMQ</span><span class="c0">m</span><span class="c1">(l, r) takes constant time. For simpler relevance metrics (such as Rel(S, t)</span><span class="c3">max</span><span class="c11">), validating the relevance metric takes con- </span><span class="c1">stant time. Total query time is optimal O(m + occ). However, for more complex relevance metric, all the occurrences of t might need to be considered, query time will be proportionate to the total number of occurrences. </span></p><p class="c5"><span class="c7">7. APPROXIMATE SUBSTRING </span></p><p class="c5"><span class="c7">SEARCHING </span><span class="c1">In this section we introduce an index for approximate substring matching in an uncertain string. As discussed previously, several challenges of uncertain string matching makes it harder to achieve optimal theoretical bound with linear space. We have proposed in- dex for exact matching which performs near optimally in practical scenarios, but achieves theoretical optimal bound only for shorter query strings. To achieve optimal theoretical bounds for any query, we propose an approximate string matching solution. Our approx- imate string matching data structure answers queries with an addi- tive error &epsilon;, i.e. outputs can have probability of occurrence &ge; &tau; &minus;&epsilon;. At first we begin by transforming the uncertain string S into a special uncertain string X of length N = O(( </span><span class="c12">1 </span></p><p class="c8"><span class="c0">&tau;</span><span class="c4">min </span><span class="c1">)</span><span class="c12">2</span><span class="c1">n) using the technique of lemma 2 with respect to a probability threshold value &tau;</span><span class="c0">min</span><span class="c1">. We obtain a deterministic string t from X by concatenating the characters of X. We build a suffix tree for t. Note that, each leaf in the suffix tree has an associated probability of occurrence &ge; &tau;</span><span class="c3">min </span><span class="c11">for the corresponding suffix. Given a query p, substring </span><span class="c1">matching query for threshold &tau;</span><span class="c3">min </span><span class="c11">can now be answered by simply </span><span class="c1">scanning the leafs in subtree of locus node i</span><span class="c0">p</span><span class="c1">. We first describe the framework (based on Hon et. al. [14]) which supports a specific probability threshold &tau; and then extend it for arbitrary &tau; &ge; &tau;</span><span class="c0">min</span><span class="c1">. </span></p><p class="c8"><span class="c1">We begin by marking nodes in the suffix tree with positional in- formation by associating P os</span><span class="c3">id </span><span class="c11">&isin; [1,n]. Here, P os</span><span class="c3">id </span><span class="c11">indicates the </span><span class="c1">starting position in the original string S. A leaf node l is marked with a P os</span><span class="c3">id </span><span class="c11">= d if the suffix represented by l begins at posi- </span><span class="c1">tion d in S. An internal node u is marked with d if it is the low- est common ancestor of two leaves marked with d. Notice that a node can be marked with multiple position ids. For each node u and each of its marked position id d, define a link to be a triplet (origin, target, P os</span><span class="c3">id</span><span class="c11">), where origin = u, target is the lowest </span><span class="c1">proper ancestor of u marked with d, and P os</span><span class="c0">id </span><span class="c1">= d. Two crucial properties of these links are listed below. </span></p><p class="c5"><span class="c1">&bull; Given a substring p, for each position d in S where p matches with probability &ge; &tau;</span><span class="c0">min</span><span class="c1">, there is a unique link whose origin </span></p><p class="c5"><span class="c1">is in the subtree of i</span><span class="c0">p </span><span class="c1">and whose target is a proper ancestor of i</span><span class="c3">p</span><span class="c11">, i</span><span class="c3">p </span><span class="c11">being the locus node of substring p. </span></p><p class="c5"><span class="c1">&bull; The total number of links is bounded by O(N). </span></p><p class="c8"><span class="c1">Thus, substring matching query with probability threshold &tau;</span><span class="c0">min </span><span class="c1">can now be answered by identifying/reporting the links that origi- nate in the subtree of i</span><span class="c3">p </span><span class="c11">and are targeted towards some ancestor of </span><span class="c1">it. By referring to each node using its pre-order rank, we are inter- ested in links that are stabbed by locus node i</span><span class="c3">p</span><span class="c11">. Such queries can </span><span class="c1">be answered in O(m+occ), where |p| = m and occ is the number of answers to be reported (Please refer to [14] for more details). </span></p><p class="c8"><span class="c1">As a first step towards answering queries for arbitrary &tau; &ge; &tau;</span><span class="c0">min</span><span class="c1">, we associate probability information along with each link. Thus each link is now a quadruple (origin, target, P os</span><span class="c3">id</span><span class="c11">, prob) where </span><span class="c1">first three parameters remain same as described earlier and prob is the probability of prefix(u) matching uncertain string S at po- sition P os</span><span class="c3">id </span><span class="c11">= d. It is evident that for substring p and arbitrary </span><span class="c1">&tau; &ge; &tau;</span><span class="c3">min</span><span class="c11">, a link stabbed by locus node i</span><span class="c3">p </span><span class="c11">with prob &ge; &tau; cor- </span><span class="c1">responds to an occurrence of p in S at position d with probability &ge; &tau;. However, a link stabbed by i</span><span class="c0">p </span><span class="c1">with prob &lt; &tau; can still pro- duce an outcome since prefix(i</span><span class="c3">P </span><span class="c11">) contains additional characters </span><span class="c1">not included in p, which may be responsible for matching prob- ability to drop below &tau;. Even though we are interested only in approximate matching this observation leads up the next step to- wards the solution. We partition each link (origin = u, target = v,Pos</span><span class="c3">id </span><span class="c11">= d, prob) into multiple links (or</span><span class="c3">1 </span><span class="c11">= u, tr</span><span class="c3">1</span><span class="c11">, d, prob</span><span class="c3">1</span><span class="c11">), </span><span class="c1">(or</span><span class="c0">2 </span><span class="c1">= tr</span><span class="c0">1</span><span class="c1">, tr</span><span class="c0">2</span><span class="c1">, d, prob</span><span class="c0">2</span><span class="c1">), ..., (or</span><span class="c3">k </span><span class="c11">= tr</span><span class="c3">k&minus;1</span><span class="c11">, tr</span><span class="c3">k </span><span class="c11">= v, d, prob</span><span class="c3">k</span><span class="c11">) </span><span class="c1">such that prob</span><span class="c3">j </span><span class="c11">&minus; prob</span><span class="c3">j&minus;1 </span><span class="c11">&le; &epsilon; for 2 &le; j &le; k. Here or</span><span class="c3">2</span><span class="c11">,...,or</span><span class="c3">k </span><span class="c1">may not refer to the actual node in the suffix tree, rather it can be considered as a dummy node inserted in-between an edge in suffix tree. In essence, we move along the path from node u = or</span><span class="c3">1 </span><span class="c11">to- </span><span class="c1">wards its ancestors one character at a time till the probability differ- ence is bounded by &epsilon; i.e., till we reach node tr</span><span class="c3">1</span><span class="c11">. The process then </span><span class="c1">repeats with tr</span><span class="c0">1 </span><span class="c1">as the origin node and so on till we reach the node v. It can be see that the total number of links can now be bounded by O(N/&epsilon;). In order to answer a substring matching query with threshold &tau; &ge; &tau;</span><span class="c0">min</span><span class="c1">, we need to retrieve all the links stabbed by i</span><span class="c0">p </span><span class="c1">with prob &ge; &tau;. Occurrence of substring p in S corresponding to each such link is then guaranteed to have its matching probability at-least &tau; &minus; &epsilon; due to the way links are generated (for any link with (u, v) as origin and target probability of prefix(v) matching in S can be more than that of prefix(v) only by &epsilon; at the most). </span></p><p class="c5"><span class="c7">8. EXPERIMENTAL EVALUATION </span></p><p class="c5"><span class="c1">In this section we evaluate the performance of our substring search- ing and string listing index. We use a collection of query substrings and observe the effect of varying the key parameters. Our experi- ments show that, for short query substrings, uncertain string length does not affect the query performance. For long query substrings, our index fails to achieve optimal query time. However this does not deteriorate the average query time by big margin, since the probability of match also decreases significantly as substring gets longer. Index construction time is proportional to uncertain string size and probability threshold parameter &tau;</span><span class="c3">min</span><span class="c11">. </span></p><p class="c5"><span class="c1">We have implemented the proposed indexing scheme in C++. The experiments are performed on a 64-bit machine with an Intel Core i5 CPU 3.33GHz processor and 8GB RAM running Ubuntu. We present experiments along with analysis of performance. </span><span class="c7">8.1 Dataset </span></p><p class="c8"><span class="c1">We use a synthetic datasets obtained from their real counterparts. We use a concatenated protein sequence of mouse and human (al- phabet size |&Sigma;| = 22), and break it arbitrarily into shorter strings. </span></p><p class="c5"><span class="c15">409 </span></p><p class="c5"><span class="c1">(a) String size vs time </span></p><p class="c5"><span class="c1">(b) Tau vs time </span></p><p class="c5"><span class="c1">(c) Tau(min) vs time </span></p><p class="c5"><span class="c1">(d) m vs time </span></p><p class="c5"><span class="c10">&theta; = .1 </span></p><p class="c5"><span class="c10">&theta; = .1 </span></p><p class="c5"><span class="c10">&theta; = .1 </span></p><p class="c5"><span class="c10">&theta; = .1 &theta; = .2 </span></p><p class="c5"><span class="c10">&theta; = .2 </span></p><p class="c5"><span class="c10">&theta; = .2 </span></p><p class="c5"><span class="c10">&theta; = .2 &theta; = .3 </span></p><p class="c5"><span class="c10">&theta; = .3 </span></p><p class="c5"><span class="c10">&theta; = .3 </span></p><p class="c5"><span class="c10">&theta; = .3 &theta; = .4 </span></p><p class="c5"><span class="c10">&theta; = .4 </span></p><p class="c5"><span class="c10">&theta; = .4 </span></p><p class="c5"><span class="c10">&theta; = .4 </span></p><p class="c5"><span class="c1">0.10 0.12 0.14 </span></p><p class="c5"><span class="c1">0.05 0.10 0.15 0.20 </span></p><p class="c5"><span class="c1">10.00 20.00 </span></p><p class="c5"><span class="c1">Figure 7: Substring searching query time for different string lengths (n), query threshold value &tau;, construction time threshold parameter &tau;</span><span class="c0">min </span><span class="c1">and query substring length m. </span></p><p class="c5"><span class="c1">100 200 300 </span></p><p class="c5"><span class="c1">1.2 </span></p><p class="c5"><span class="c10">&tau; </span></p><p class="c5"><span class="c1">1.5 </span></p><p class="c5"><span class="c1">150 </span></p><p class="c5"><span class="c6">2 </span></p><p class="c5"><span class="c6">1 </span><span class="c1">0.8</span><span class="c6">1 </span></p><p class="c5"><span class="c1">100 </span></p><p class="c5"><span class="c1">10.6 </span></p><p class="c5"><span class="c1">50 0.4 0.5100 200 300 </span></p><p class="c5"><span class="c10">n &times; 1000 </span></p><p class="c5"><span class="c10">&tau;</span><span class="c4">min </span></p><p class="c5"><span class="c10">m </span></p><p class="c5"><span class="c1">(a) String size vs time </span></p><p class="c5"><span class="c1">(b) Tau vs time </span></p><p class="c5"><span class="c1">(c) Tau(min) vs time </span></p><p class="c5"><span class="c1">(d) m vs time </span></p><p class="c5"><span class="c10">&theta; = .1 </span></p><p class="c5"><span class="c1">0.7 </span><span class="c6">1 </span></p><p class="c5"><span class="c10">&theta; = .2 </span></p><p class="c5"><span class="c10">&theta; = .1 </span></p><p class="c5"><span class="c10">&theta; = .2 </span></p><p class="c5"><span class="c10">&theta; = .1 </span></p><p class="c5"><span class="c10">&theta; = .1 &theta; = .2 </span></p><p class="c5"><span class="c10">&theta; = .2 &theta; = .3 </span></p><p class="c5"><span class="c10">&theta; = .3 </span></p><p class="c5"><span class="c10">&theta; = .3 </span></p><p class="c5"><span class="c10">&theta; = .3 &theta; = .4 </span></p><p class="c5"><span class="c10">&theta; = .4 </span></p><p class="c5"><span class="c10">&theta; = .4 </span></p><p class="c5"><span class="c10">&theta; = .4 </span></p><p class="c5"><span class="c6">0.3 </span></p><p class="c5"><span class="c1">0.10 0.12 0.14 0.05 0.10 0.15 0.20 </span></p><p class="c5"><span class="c1">10.00 20.00 </span></p><p class="c5"><span class="c10">&tau; </span></p><p class="c5"><span class="c1">Figure 8: String listing query time for different string lengths (n), query threshold value &tau;, construction time threshold parameter &tau;</span><span class="c3">min </span><span class="c11">and </span><span class="c1">query substring length m. </span></p><p class="c8"><span class="c1">For each string s in the dataset we first obtain a set A(s) of strings that are within edit distance 4 to s. Then a character-level proba- bilistic string S for string s is generated such that, for a position i, the pdf of S[i] is based on the normalized frequencies of the letters in the i-th position of all the strings in A(s). We denote by &theta; the fraction of uncertain characters in the string. &theta; is varied between 0.1 to 0.5 to generate strings with different degree of uncertainty. The string length distributions in this dataset roughly follows a nor- mal distribution in the range of [20,45]. The average number of choices that each probabilistic character S[i] may have is set to 5. </span></p><p class="c5"><span class="c7">8.2 Query </span><span class="c18">and fraction </span><span class="c7">time for </span><span class="c18">of uncertainty </span><span class="c7">different string </span><span class="c18">(</span><span class="c11">&theta;</span><span class="c18">) </span></p><p class="c8"><span class="c7">lengths (</span><span class="c1">n</span><span class="c7">) </span><span class="c1">We evaluate the query time for different string lengths n, ranging from 2K to 300K and &theta; ranging from 0.1 to 0.5. Figure 7(a) and Figure 8(a), shows the query times for substring searching and string listing. Note that, n is number of positions in the uncertain string where each position can have multiple characters. We take the average time for query lengths of 10,100,500,1000. We use &tau;</span><span class="c0">min </span><span class="c1">= 0.1 and query threshold &tau; = 0.2. As shown in the figures, query times does not show much irregularity in performance when the length of string goes high. This is because for shorter query length, our index achieves optimal query time. Although for longer queries, our index achieves O(m &times; occ) time, longer query strings probability of occurrence gets low as string grows longer resulting in less number of outputs. However when fraction of uncertainty(&theta;) increases in the string, performance shows slight decrease as query time increases slightly. This is because longer query strings are more probable to match with strings with high level of uncertainty. </span></p><p class="c5"><span class="c7">8.3 Query </span><span class="c18">uncertainty </span><span class="c7">time for </span><span class="c18">(</span><span class="c11">&theta;</span><span class="c18">) </span></p><p class="c5"><span class="c7">different </span><span class="c1">&tau; </span><span class="c7">and fraction of </span><span class="c1">In Figure 7(b) and Figure 8(b), we show the average query times for string matching and string listing for probability threshold &tau; = </span></p><p class="c5"><span class="c1">1.2 0.6 </span></p><p class="c5"><span class="c1">100 </span></p><p class="c5"><span class="c1">0.5</span><span class="c6">1 </span></p><p class="c5"><span class="c1">0.8</span><span class="c6">80 </span><span class="c1">0.5 </span></p><p class="c17"><span class="c1">0.6 </span><span class="c6">6040</span><span class="c1">0.4 </span></p><p class="c17"><span class="c1">0.4 20</span><span class="c10">n &times; 1000 </span></p><p class="c5"><span class="c10">&tau;</span><span class="c4">min </span></p><p class="c5"><span class="c10">m </span></p><p class="c8"><span class="c1">0.04,0.06,0.08,0.1, 0.12 for fixed &tau;</span><span class="c0">min </span><span class="c1">= 0.1. In terms of per- formance, query time increases with decreasing &tau;. This is because more matching is probable for smaller &tau;. Larger &tau; reduces the out- put size, effectively reducing the query time as well. </span></p><p class="c5"><span class="c7">8.4 Query </span><span class="c18">of uncertainty </span><span class="c7">time for </span><span class="c18">(</span><span class="c11">&theta;</span><span class="c18">) </span></p><p class="c5"><span class="c7">different </span><span class="c1">&tau;</span><span class="c0">min </span><span class="c7">and fraction </span><span class="c1">In Figure 7(c) and Figure 8(c), we show the average query times for string matching and string listing for probability threshold &tau;</span><span class="c3">min </span><span class="c11">= </span><span class="c1">0.04,0.06,0.08,0.1, 0.12 which shows slight impact of &tau;</span><span class="c0">min </span><span class="c1">over query time. </span></p><p class="c5"><span class="c7">8.5 Query </span><span class="c18">(</span><span class="c11">m</span><span class="c18">) and </span><span class="c7">time </span><span class="c18">fraction </span><span class="c7">for different </span><span class="c18">of uncertainty </span><span class="c7">substring </span><span class="c18">(</span><span class="c11">&theta;</span><span class="c18">) </span></p><p class="c8"><span class="c7">lengths </span><span class="c1">In figure 7(d) and figure Figure 8(d), we show the average query times for string matching and string listing. As it can be seen long pattern length drastically increases the query time. </span></p><p class="c5"><span class="c7">8.6 Construction </span><span class="c18">lengths and fraction </span><span class="c7">time for </span><span class="c18">of </span><span class="c7">different </span><span class="c18">uncertainty </span><span class="c7">string </span><span class="c18">(</span><span class="c11">&theta;</span><span class="c18">) </span></p><p class="c5"><span class="c1">Figure 9(a) shows the index construction times for uncertain string length n ranging from 2K to 300K. We can see that the construc- tion time is proportional to the string length n. Increasing uncer- tainty factor &theta; also impacts the construction time as more permu- tation is possible with increasing uncertain positions. Figure 9(b) shows the impact of &theta; on construction time. </span></p><p class="c5"><span class="c7">8.7 Space usage </span></p><p class="c8"><span class="c1">Theoretical bound for our index is O(n). However, this bound can have hidden multiplicative constant. Here we elaborate more on the actual space used for our index. </span></p><p class="c5"><span class="c1">For our indexes, we construct the regular string t of length N = O(( based </span><span class="c0">&tau;</span><span class="c4">min </span><span class="c12">1 </span></p><p class="c5"><span class="c1">on )threshold </span><span class="c12">2</span><span class="c1">n) by concatenating &tau;</span><span class="c0">min</span><span class="c1">. We do not all the store extended the string maximal t in our factors index. </span></p><p class="c5"><span class="c15">410 </span></p><p class="c5"><span class="c1">c</span><span class="c6">) ces(emitnoitcurtsno</span><span class="c1">C</span><span class="c6">(a) String size vs time </span><span class="c1">1,000 </span></p><p class="c5"><span class="c10">&theta; = .1 </span></p><p class="c5"><span class="c10">&theta; = .2 </span></p><p class="c5"><span class="c10">&theta; = .3 </span></p><p class="c5"><span class="c10">&theta; = .4 </span></p><p class="c5"><span class="c6">) ces(emitnoitcurtsno(b) &tau;</span><span class="c0">min </span><span class="c1">vs time </span><span class="c10">&theta; = .1 </span></p><p class="c5"><span class="c10">&theta; = .2 </span></p><p class="c5"><span class="c10">&theta; = .3 </span></p><p class="c5"><span class="c10">&theta; = .4 </span></p><p class="c5"><span class="c1">5 &middot; 10</span><span class="c12">&minus;2 </span><span class="c1">0.1 0.15 0.2 </span></p><p class="c5"><span class="c1">(c) String size vs index space 1,500 </span></p><p class="c17"><span class="c6">) BM(ecapsxedn</span><span class="c1">I</span><span class="c10">&theta; = .1 &theta; = .2 </span></p><p class="c5"><span class="c10">&theta; = .3 </span></p><p class="c5"><span class="c10">&theta; = .4 </span></p><p class="c5"><span class="c1">100 200 300 n &times; 100 </span></p><p class="c5"><span class="c1">n &times; 100 </span></p><p class="c5"><span class="c1">Figure 9: Construction time and index space for different string lengths (n) and probability threshold &tau;</span><span class="c3">min </span><span class="c11">= .1 </span></p><p class="c8"><span class="c1">We built RMQ structures RMQ</span><span class="c3">i </span><span class="c11">for i = 1,...,log n which takes </span><span class="c1">O(N log n) bits. The practical space usage of RMQ is usually very small with hidden multiplicative constant of 2 &minus; 3. So the average space usage of our RMQ structure in total can be stated as 3N words. For a query string p, we find the suffix range of p in the concatenated extended maximum factor string t. For this purpose, instead of using Generalized Suffix Tree(GST), we use its space efficient version i.e., a compressed suffix array (CSA) of t. There are many versions of CSA&rsquo;s available in literature. For our purpose we use the one in [2] that occupies N log &sigma; + o(N log &sigma;) + O(N) bits space and retrieves the suffix range of query string p in O(p) time. In practice, this structure takes about 2.5N words space. We also store an array D of size N storing the partial probabilities, which takes approximately 4N bytes of space. Finally P os array is used for position transformation, taking N words space. Summing up all the space usage, our index takes approximately 3N+2.5N+ 4N + N = 10.5N = the space usage for different ( </span><span class="c0">&tau;</span><span class="c4">min </span><span class="c12">1 </span></p><p class="c5"><span class="c1">string )</span><span class="c12">2</span><span class="c1">10.5n. length(n) Figure and Figure &theta;. </span></p><p class="c5"><span class="c1">9(c) shows </span></p><p class="c5"><span class="c7">9. CONCLUSIONS </span></p><p class="c8"><span class="c1">In this paper we presented indexing framework for searching in uncertain strings. We tackled the problem of searching a deter- ministic substring in uncertain string and proposed both exact and approximate solution. We also formulated the uncertain string list- ing problem and proposed index for string listing from a uncertain string collection. Our indexes can support arbitrary values of proba- bility threshold parameter. Uncertain string searching is still largely an unexplored area. Constructing more efficient index, variations of the string searching problem satisfying diverse query constraints are some interesting future work direction. </span></p><p class="c5"><span class="c7">10. REFERENCES </span><span class="c11">[1] A. Amir, E. Chencinski, C. S. Iliopoulos, T. Kopelowitz, and </span><span class="c1">H. Zhang. Property matching and weighted matching. Theor. Comput. Sci., 395(2-3):298&ndash;310, 2008. [2] D. Belazzougui and G. Navarro. Alphabet-independent </span></p><p class="c17"><span class="c1">compressed text indexing. In ESA, pages 748&ndash;759, 2011. [3] T. Bernecker, H. Kriegel, M. Renz, F. Verhein, and A. Z&uuml;fle. </span></p><p class="c5"><span class="c1">Probabilistic frequent pattern growth for itemset mining in uncertain databases. In Scientific and Statistical Database Management - 24th International Conference, SSDBM 2012, Chania, Crete, Greece, June 25-27, 2012. Proceedings, pages 38&ndash;55, 2012. [4] S. Chaudhuri, V. Ganti, and R. Kaushik. A primitive operator </span></p><p class="c5"><span class="c1">for </span><span class="c11">22nd </span><span class="c1">similarity </span><span class="c11">International </span><span class="c1">joins in </span><span class="c11">Conference </span><span class="c1">data cleaning. </span><span class="c11">on Data </span><span class="c1">In Proceedings </span><span class="c11">Engineering, </span><span class="c1">of </span><span class="c11">ICDE </span><span class="c1">the 2006, 3-8 April 2006, Atlanta, GA, USA, page 5, 2006. </span></p><p class="c5"><span class="c1">1,000 1,000 </span></p><p class="c5"><span class="c1">500 </span></p><p class="c5"><span class="c1">500 </span></p><p class="c5"><span class="c1">500 </span></p><p class="c5"><span class="c1">100 200 300 </span></p><p class="c5"><span class="c1">n &times; 100 </span></p><p class="c5"><span class="c1">[5] R. Cheng, Y. Xia, S. Prabhakar, R. Shah, and J. S. Vitter. </span></p><p class="c5"><span class="c1">Efficient indexing methods for probabilistic threshold queries over uncertain data. In Proceedings of the Thirtieth international conference on Very large data bases-Volume 30, pages 876&ndash;887. VLDB Endowment, 2004. [6] C. K. Chui and B. Kao. A decremental approach for mining </span></p><p class="c5"><span class="c1">frequent itemsets from uncertain data. In Advances in Knowledge Discovery and Data Mining, 12th Pacific-Asia Conference, PAKDD 2008, Osaka, Japan, May 20-23, 2008 Proceedings, pages 64&ndash;75, 2008. [7] C. K. Chui, B. Kao, and E. Hung. Mining frequent itemsets </span></p><p class="c5"><span class="c1">from uncertain data. In Advances in Knowledge Discovery and Data Mining, 11th Pacific-Asia Conference, PAKDD 2007, Nanjing, China, May 22-25, 2007, Proceedings, pages 47&ndash;58, 2007. [8] N. Dalvi and D. Suciu. Efficient query evaluation on </span></p><p class="c5"><span class="c1">probabilistic databases. The VLDB Journal, 16(4):523&ndash;544, 2007. [9] S. Dash, K. Chon, S. Lu, and E. Raeder. Automatic real time </span></p><p class="c5"><span class="c1">detection of atrial fibrillation. Annals of biomedical engineering, 37(9):1701&ndash;1709, 2009. [10] J. Fischer and V. Heun. A New Succinct Representation of </span></p><p class="c5"><span class="c1">RMQ-Information and Improvements in the Enhanced Suffix Array. In ESCAPE, pages 459&ndash;470, 2007. [11] J. Fischer, V. Heun, and H. M. St&uuml;hler. Practical </span></p><p class="c17"><span class="c1">Entropy-Bounded Schemes for O(1)-Range Minimum Queries. In IEEE DCC, pages 272&ndash;281, 2008. [12] T. Ge and Z. Li. Approximate substring matching over uncertain strings. PVLDB, 4(11):772&ndash;782, 2011. [13] L. Gravano, P. G. Ipeirotis, H. V. Jagadish, N. Koudas, </span></p><p class="c5"><span class="c1">S. Muthukrishnan, and D. Srivastava. Approximate string joins in a database (almost) for free. In VLDB 2001, Proceedings of 27th International Conference on Very Large Data Bases, September 11-14, 2001, Roma, Italy, pages 491&ndash;500, 2001. [14] W.-K. Hon, R. Shah, and J. S. Vitter. Space-efficient </span></p><p class="c17"><span class="c1">framework for top-k string retrieval problems. In Foundations of Computer Science, 2009. FOCS&rsquo;09. 50th Annual IEEE Symposium on, pages 713&ndash;722. IEEE, 2009. [15] J. Jestes, F. Li, Z. Yan, and K. Yi. Probabilistic string </span></p><p class="c5"><span class="c1">similarity joins. In Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2010, Indianapolis, Indiana, USA, June 6-10, 2010, pages 327&ndash;338, 2010. [16] B. Kanagal and A. Deshpande. Indexing correlated </span></p><p class="c5"><span class="c1">probabilistic databases. In Proceedings of the 2009 ACM </span></p><p class="c5"><span class="c15">411 </span></p><p class="c5"><span class="c1">SIGMOD International Conference on Management of data, pages 455&ndash;468. ACM, 2009. [17] C. Li, J. Lu, and Y. Lu. Efficient merging and filtering </span></p><p class="c5"><span class="c1">algorithms for approximate string searches. In Proceedings of the 24th International Conference on Data Engineering, ICDE 2008, April 7-12, 2008, Canc&uacute;n, M&eacute;xico, pages 257&ndash;266, 2008. [18] J. Li, B. Saha, and A. Deshpande. A unified approach to </span></p><p class="c5"><span class="c1">ranking in probabilistic databases. The VLDB Journal </span><span class="c6">&nbsp;&#778;</span><span class="c1">UThe International Journal on Very Large Data Bases, 20(2):249&ndash;275, 2011. [19] Y. Li, J. Bailey, L. Kulik, and J. Pei. Efficient matching of </span></p><p class="c5"><span class="c1">substrings in uncertain sequences. In Proceedings of the 2014 SIAM International Conference on Data Mining, Philadelphia, Pennsylvania, USA, April 24-26, 2014, pages 767&ndash;775, 2014. [20] D. M. Lilley, R. M. Clegg, S. Diekmann, N. C. Seeman, E. Von Kitzing, and P. J. Hagerman. Nomenclature committee of the international union of biochemistry and molecular biology (nc- iubmb) a nomenclature of junctions and branchpoints in nucleic acids recommendations 1994. European Journal of Biochemistry, s. FEBS J, 230(1):1&ndash;2, 1996. [21] E. M. McCreight. A space-economical suffix tree </span></p><p class="c5"><span class="c1">construction algorithm. J. ACM, 23(2):262&ndash;272, 1976. </span></p><p class="c5"><span class="c1">[22] G. Navarro. A guided tour to approximate string matching. </span></p><p class="c5"><span class="c1">ACM Comput. Surv., 33(1):31&ndash;88, 2001. [23] M. Patil and R. Shah. Similarity joins for uncertain strings. </span></p><p class="c5"><span class="c1">In Proceedings of the 2014 ACM SIGMOD international conference on Management of data, pages 1471&ndash;1482. ACM, 2014. [24] C. Re, N. Dalvi, and D. Suciu. Efficient top-k query </span></p><p class="c5"><span class="c1">evaluation on probabilistic data. In Data Engineering, 2007. ICDE 2007. IEEE 23rd International Conference on, pages 886&ndash;895. IEEE, 2007. [25] S. Singh, C. Mayfield, S. Prabhakar, R. Shah, and </span></p><p class="c5"><span class="c1">S. Hambrusch. Indexing uncertain categorical data. In Data Engineering, 2007. ICDE 2007. IEEE 23rd International Conference on, pages 616&ndash;625. IEEE, 2007. [26] Y. Tao, R. Cheng, X. Xiao, W. K. Ngai, B. Kao, and </span></p><p class="c5"><span class="c1">S. Prabhakar. Indexing multi-dimensional uncertain data with arbitrary probability density functions. In Proceedings of the 31st international conference on Very large data bases, pages 922&ndash;933. VLDB Endowment, 2005. [27] S. Thankachan, M. Patil, R. Shah, and S. Biswas. </span></p><p class="c5"><span class="c1">Probabilistic threshold indexing for uncertain strings full version. </span><span class="c44">http://csc.lsu.edu/~sbiswas/papers/ uncertainIndexingFullpaper.pdf</span><span class="c1">. [28] P. Weiner. Linear pattern matching algorithms. In SWAT </span></p><p class="c5"><span class="c1">(FOCS), pages 1&ndash;11, 1973. </span></p><p class="c5"><span class="c15">412 </span></p></body></html>