<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c69{margin-left:-16.6pt;padding-top:1.4pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-32.2pt}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c52{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.5pt;font-family:"Arial";font-style:normal}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Courier New";font-style:normal}.c65{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:13.9pt;font-family:"Arial";font-style:normal}.c10{margin-left:-16.6pt;padding-top:3.8pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c41{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c18{margin-left:-16.6pt;padding-top:4.1pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.4pt}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.5pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:7.1pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.1pt;font-family:"Arial";font-style:normal}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c28{margin-left:-25.7pt;padding-top:1.4pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c26{margin-left:-25.7pt;padding-top:3.8pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.1pt;font-family:"Arial";font-style:normal}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c29{margin-left:-25.7pt;padding-top:3.8pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c72{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c33{margin-left:-25.7pt;padding-top:1.4pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c25{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Courier New";font-style:normal}.c57{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c21{margin-left:-25.7pt;padding-top:1.7pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.2pt;font-family:"Arial";font-style:normal}.c64{margin-left:-25.7pt;padding-top:0.5pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c36{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c44{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c48{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c23{margin-left:220.6pt;padding-top:66.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-33.4pt}.c67{margin-left:-16.6pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:67.9pt}.c9{margin-left:-25.7pt;padding-top:11.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:104.8pt}.c35{margin-left:-16.6pt;padding-top:17.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:44.6pt}.c45{margin-left:-13.8pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:60.5pt}.c61{margin-left:-22.8pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:119.2pt}.c2{margin-left:-22.8pt;padding-top:10.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:62.8pt}.c53{margin-left:-16.6pt;padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:57.1pt}.c14{margin-left:-16.6pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-30.5pt}.c66{margin-left:-25.7pt;padding-top:8.4pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-16.4pt}.c43{margin-left:-16.6pt;padding-top:11.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:24pt}.c47{margin-left:-13.8pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:104.2pt}.c73{margin-left:5.9pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.2pt}.c19{margin-left:-25.7pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.2pt}.c38{margin-left:-22.8pt;padding-top:10.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:78.4pt}.c49{margin-left:-22.8pt;padding-top:10.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:75pt}.c60{margin-left:-25.7pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-16.4pt}.c54{margin-left:-13.8pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:108.5pt}.c70{margin-left:-13.8pt;padding-top:7.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:76.1pt}.c27{margin-left:-25.7pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c59{margin-left:10.2pt;padding-top:12pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:1.4pt}.c42{margin-left:-16.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-34.8pt}.c39{margin-left:-13.8pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:97pt}.c68{margin-left:-25.7pt;padding-top:5.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c50{padding-top:1.4pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c63{padding-top:1.7pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c37{padding-top:4.1pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c46{padding-top:3.8pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c32{padding-top:13.9pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c30{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c58{padding-top:10.1pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c55{padding-top:5.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c34{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c71{margin-left:-22.8pt;margin-right:59.4pt}.c51{margin-left:-16.6pt;margin-right:-27.6pt}.c56{margin-left:-13.8pt;margin-right:-22.6pt}.c22{margin-left:-16.6pt;margin-right:-25.4pt}.c62{margin-left:-16.6pt;margin-right:73.9pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c34"><p class="c15"><span class="c41">Lightweight Data Compression Algorithms: An Experimental Survey </span></p><p class="c0"><span class="c65">Experiments and Analyses </span></p><p class="c0"><span class="c7">Patrick Damme, Dirk Habich, Juliana Hildebrandt, Wolfgang Lehner </span><span class="c36">Database Systems Group Technische Universit&auml;t Dresden </span><span class="c24">{firstname.lastname}@tu-dresden.de </span><span class="c36">01062 Dresden, Germany </span><span class="c7">ABSTRACT </span><span class="c5">Lightweight data compression algorithms are frequently ap- plied in in-memory database systems to tackle the growing gap between processor speed and main memory bandwidth. In recent years, the vectorization of basic techniques such as delta coding and null suppression has considerably en- larged the corpus of available algorithms. As a result, today there is a large number of algorithms to choose from, while different algorithms are tailored to different data charac- teristics. However, a comparative evaluation of these algo- rithms under different data characteristics has never been sufficiently conducted in the literature. To close this gap, we conducted an exhaustive experimental survey by evalu- ating several state-of-the-art compression algorithms as well as cascades of basic techniques. We systematically investi- gated the influence of the data properties on the performance and the compression rates. The evaluated algorithms are based on publicly available implementations as well as our own vectorized reimplementations. We summarize our ex- perimental findings leading to several new insights and to the conclusion, that there is no single-best algorithm. </span></p><p class="c0"><span class="c7">1. INTRODUCTION </span></p><p class="c1"><span class="c5">The continuous growth of data volumes is a major chal- lenge for the efficient data processing. This applies not only to database systems [1, 5] but also to other areas, such as in- formation retrieval [3, 18] or machine learning [8]. With the growing capacity of the main memory, efficient analytical data processing becomes possible [4, 11]. However, the gap between computing power of the CPUs and main memory bandwidth continuously increases, which is now the main bottleneck for an efficient data processing. To overcome this bottleneck, data compression plays a crucial role [1, 22]. Aside from reducing the amount of data, compressed data offers several advantages such as less time spent on load and store instructions, a better utilization of the cache hierarchy, </span></p><p class="c1"><span class="c72">c 2017, Copyright is with the authors. Published in Proc. 20th Inter- national Conference on Extending Database Technology (EDBT), March 21-24, 2017 - Venice, Italy: ISBN 978-3-89318-073-8, on OpenProceed- ings.org. Distribution of this paper is permitted under the terms of the Cre- ative Commons license CC-by-nc-nd 4.0 </span></p><p class="c0"><span class="c5">and less misses in the translation lookaside buffer. </span></p><p class="c1"><span class="c5">This compression solution is heavily exploited in modern in-memory column stores for efficient query processing [1, 22]. Here, relational data is maintained using the decom- position storage model [6]. That is, an n-attribute relation is replaced by n binary relations, each consisting of one at- tribute and a surrogate indicating the record identity. Since the latter contains only virtual ids, it is not stored explicitly. Thus, each attribute is stored separately as a sequence of val- ues. For the lossless compression of sequences of values (in particular integer values), a large variety of lightweight algo- rithms has been developed [1, 2, 3, 9, 12, 15, 16, 17, 18, 22]</span><span class="c20">1</span><span class="c5">. In contrast to heavyweight algorithms like arithmetic cod- ing [19], Huffman [10], or Lempel Ziv [21], lightweight algo- rithms achieve comparable or even better compression rates. Moreover, the computational effort for the (de)compression is lower than for heavyweight algorithms. To achieve these unique properties, each lightweight compression algorithm employs one or more basic compression techniques such as frame-of-reference [9, 22] or null suppression [1, 15], which allow the appropriate utilization of contextual knowledge like value distribution, sorting, or data locality. </span></p><p class="c1"><span class="c5">In recent years, the efficient vectorized implementation of these lightweight compression algorithms using SIMD (Sin- gle Instruction Multiple Data) instructions has attracted a lot of attention [12, 14, 16, 18, 20], since it further reduces the computational effort. To better understand these vec- torized lightweight compression algorithms and to be able to select a suitable algorithm for a given data set, the behav- ior of the algorithms regarding different data characteristics has to be known. In particular, the behavior in terms of performance (compression, decompression, and processing) and compression rate is of interest. In the literature, there are two papers with a considerable evaluation part. First, Adabi et al. [1] evaluated a small number of unvectorized al- gorithms on different data characteristics, but they neither considered a rich set of data distributions nor the explicit combination of different compression techniques. Second, Lemire et al. [12] already evaluated vectorized lightweight data compression algorithms, but considered only null sup- pression with and without differential coding. Furthermore, their focus is on postings lists from the IR domain, which narrows the considered data characteristics. Hence, an ex- haustive comparative evaluation as a foundation has been never sufficiently conducted. To overcome this issue, we have </span></p><p class="c0"><span class="c25">1</span><span class="c12">Without claim of completeness. </span></p><p class="c0"><span class="c17">Series ISSN: 2367-2005 72 </span><span class="c52">10.5441/002/edbt.2017.08 </span></p><p class="c27"><span class="c5">done an experimental survey of a broad range of algorithms with different data characteristics in a systematic way. In our evaluation, we used a set of synthetic data sets as well as one commonly used real data set. Our main findings can be summarized as follows: </span></p><p class="c68"><span class="c5">1. Performance and compression rate of the algorithms vary greatly depending on the data properties. Even algorithms that are based on the same techniques, show a very different behavior. 2. By combining various basic techniques, the compres- sion rate can be improved significantly. The perfor- mance may rise or fall depending on the combination. 3. There is no single-best lightweight algorithm, but the decision depends on the data properties. In order to se- lect an appropriate algorithm, a compromise between performance and compression rate must be defined. The remainder of the paper is organized as follows: In Sec- tion 2, we present more details about the area of lightweight data compression and introduce our evaluated algorithms. The implementation aspects are described in Section 3, while Section 4 covers our evaluation setup. Selected results of our experimental survey are presented in Section 5 and Section 6. Finally, we conclude the paper in Section 7. </span></p><p class="c9"><span class="c7">2. PREREQUISITES </span></p><p class="c29"><span class="c5">The focus of our experimental survey is the large corpus of lossless lightweight integer data compression algorithms which are heavily used in modern in-memory column stores [1, 22]. To better understand the algorithm corpus, this section briefly summarizes the basic concepts and introduces the algorithms which are used in our survey. </span><span class="c7">2.1 Lightweight Data Compression </span></p><p class="c29"><span class="c5">First of all, we have to distinguish between techniques and algorithms, thereby each algorithm implements one or more of these techniques. </span><span class="c11">Techniques. </span><span class="c5">There are five basic lightweight techniques to compress a sequence of values: frame-of-reference (FOR) [9, 22], delta coding (DELTA) [12, 15], dictionary compression (DICT) [1, 22], run-length encoding (RLE) [1, 15], and null suppression (NS) [1, 15]. FOR and DELTA represent each value as the difference to either a certain given reference value (FOR) or to its predecessor value (DELTA). DICT replaces each value by its unique key in a dictionary. The objective of these three well-known techniques is to repre- sent the original data as a sequence of small integers, which is then suited for actual compression using the NS technique. NS is the most studied lightweight compression technique. Its basic idea is the omission of leading zeros in the bit representation of small integers. Finally, RLE tackles un- interrupted sequences of occurrences of the same value, so called runs. Each run is represented by its value and length. Hence, the compressed data is a sequence of such pairs. </span></p><p class="c21"><span class="c5">Generally, these five techniques address different data lev- els. While FOR, DELTA, DICT, and RLE consider the log- ical data level, NS addresses the physical level of bits or bytes. This explains why lightweight data compression al- gorithms are always composed of one or more of these tech- niques. In the following, we also denote the techniques from the logical level as preprocessing techniques for the physi- cal compression with NS. These techniques can be further divided into two groups depending on how the input values </span></p><p class="c0 c22"><span class="c5">are mapped to output values. FOR, DELTA, and DICT map each input value to exactly one integer as output value (1:1 mapping). The objective of these preprocessing tech- niques is to achieve smaller numbers which can be better compressed on the bit level. In RLE, not every input value is necessarily mapped to an encoded output value, because a successive subsequence of equal values is encoded in the output as a pair of run value and run length (N:1 mapping). In this case, a compression is already done at the logical level. The NS technique is either a 1:1 or an N:1 mapping depending on the implementation. </span><span class="c11">Algorithms. </span><span class="c5">The genericity of these techniques is the foun- dation to tailor the algorithms to different data character- istics. Therefore, a lightweight data compression algorithm can be described as a cascade of one or more of these basic techniques. On the level of the lightweight data compression algorithms, the NS technique has been studied most exten- sively. There is a very large number of specific algorithms showing the diversity of the implementations for a single technique. The pure NS algorithms can be divided into the following classes [20]: (i) bit-aligned, (ii) byte-aligned, and (iii) word-aligned.</span><span class="c20">2 </span><span class="c5">While bit-aligned NS algorithms try to compress an integer using a minimal number of bits, byte- aligned NS algorithms compress an integer with a minimal number of bytes (1:1 mapping). The word-aligned NS algo- rithms encode as many integer values as possible into 32-bit or 64-bit words (N:1 mapping). The NS algorithms also dif- fer in their data layout. We distinguish between horizontal and vertical layout. In the horizontal layout, the compressed representation of subsequent values is situated in subsequent memory locations. In the vertical layout, each compressed representation is stored in a separate memory word. </span></p><p class="c22 c50"><span class="c5">The logical-level techniques have not been considered to such an extent as the NS technique on the algorithm level. In most cases, the preprocessing steps have been investigated in connection with the NS technique. For instance, PFOR- based algorithms implement the FOR technique in combina- tion with a bit-aligned NS algorithm [22]. These algorithms usually subdivide the input in subsequences of a fixed length and calculate two parameters per subsequence: a reference value for the FOR technique and a common bit width for NS. Each subsequence is encoded using their specific param- eters, thereby the parameters are data-dependently derived. The values that cannot be encoded with the given bit width are stored separately with a greater bit width. </span></p><p class="c67"><span class="c7">2.2 Considered Algorithms </span></p><p class="c37 c22"><span class="c5">We consider all five basic lightweight techniques in detail. Regarding the selected algorithms, we investigate both, im- plementations of a single technique as well as cascades of one logical-level and one physical-level technique. We decided to reimplement the logical-level techniques on our own (see Section 3) in order to be able to freely combine them with all seven considered NS algorithms (see Table 1). In the following, we briefly sketch each considered NS algorithm. </span></p><p class="c45"><span class="c11">2.2.1 Bit-Aligned NS Algorithms </span></p><p class="c10"><span class="c5">4-Gamma Coding [16] processes four input values (data elements) at a time. All four values are stored in the verti- cal storage layout using the number of bits required for the </span></p><p class="c22 c58"><span class="c25">2</span><span class="c12">[20] also defines a frame-based class, which we omit, as the representatives we consider also match the bit-aligned class. </span></p><p class="c23"><span class="c17">73 </span></p><p class="c19"><span class="c5">largest of them. The unary representation of the bit width is stored in a separate memory area for decompression. </span></p><p class="c28"><span class="c5">SIMD-BP128 [12] processes data in blocks of 128 in- tegers at a time. All 128 integers in the block are stored in the vertical layout using the number of bits required for the largest of them. The used bit width is stored in a sin- gle byte, whereby 16 of these bit widths are followed by 16 compressed blocks. </span></p><p class="c21"><span class="c5">SIMD-FastPFOR [12] is a variant of the original PFOR algorithm [22], whose idea is to classify all data elements as either regular coded values or exceptions depending on if they can be represented with a certain bit width. This bit width is chosen such that the overall compression rate be- comes optimal. All data elements are packed with the chosen bit width using the vertical layout. The exceptions require a special treatment, since that number of bits does not suffice for them. In SIMD-FastPFOR the exceptions are stored in additional packed arrays. The overall input is subdivided into pages which are further subdivided into blocks of 128 integers. SIMD-FastPFOR stores the exceptions at the page level and uses an individual bit width for each block. </span></p><p class="c2"><span class="c11">2.2.2 Byte-Aligned NS Algorithms </span></p><p class="c26"><span class="c5">4-Wise Null Suppression [16] compresses integers by omitting leading zero bytes. For each 32-bit integer, between zero and three bytes might be omitted. 4-Wise NS processes four data elements at a time and combines the corresponding four 2-bit descriptors into a 1-byte mask. In the output, four masks are followed by four compressed blocks in the horizontal layout. </span></p><p class="c28"><span class="c5">Masked-VByte [14] uses the same compressed represen- tation as the VByte algorithm [12] and differs only in imple- mentation details. It subdivides an integer into 7-bit units. Each unit that is required to represent the integer produces one byte in the output. The seven data bits are stored in the lower part of that byte, while the most significant bit is used to indicate whether or not the next byte belongs to the next data element. Subsequent compressed values are stored using the horizontal layout. </span></p><p class="c58 c71"><span class="c11">2.2.3 Word-Aligned NS Algorithms </span></p><p class="c26"><span class="c5">Simple-8b [2] outputs one compressed block of 64 bits for a variable number of uncompressed integers. Within one block, all data elements are stored with a common bit width using the horizontal layout. The bit width is chosen such that as many subsequent input elements as possible can be stored in the compressed block. One compressed block contains 60 data bits and a 4-bit selector specifying the compression mode. There are 16 compression modes: 60 1-bit values, 30 2-bit values, 20 3-bit values, and so on. Additionally, Simple-8b has two special modes indicating that the input consisted of 120 respectively 240 zeroes. </span></p><p class="c28"><span class="c5">SIMD-GroupSimple [20] processes the input in units of so-called quads, i.e., four values at a time. For each quad, it determines the number of bits required for the largest element. Based on the bit widths of subsequent quads, it partitions the input sequence into groups, such that as many quads as possible can be stored in four consecutive 32-bit words using the vertical layout. There are ten compression modes: the four consecutive 32-bit words could be filled with 4 &times; 32 1-bit values, 4 &times; 16 2-bit values, 4 &times; 10 3-bit values, and so on. A four bit selector represents the mode chosen for the compressed block. The selectors are stored in a different </span></p><p class="c30 c56"><span class="c5">Class Algorithm Layout Code origin SIMD bit- 4-Gamma vert. Schlegel et al. yes aligned SIMD-BP128 vert. FastPFor-lib yes SIMD-FastPFOR vert. FastPFor-lib yes byte- 4-Wise NS horiz. Schlegel et al. yes aligned Masked-VByte horiz. FastPFor-lib n/y word- Simple-8b horiz. FastPFor-lib no aligned SIMD-GroupSimple vert. our own code yes </span></p><p class="c59"><span class="c5">Table 1: The considered NS algorithms. </span></p><p class="c35"><span class="c5">memory area than the compressed blocks. </span></p><p class="c43"><span class="c7">3. IMPLEMENTATION ASPECTS </span></p><p class="c37 c51"><span class="c5">As already mentioned, we reimplemented all four logical- level techniques in C++, i.e., DELTA, DICT, FOR, and RLE. Regarding the physical-level, several high-quality open- source implementations of NS are available. We used these existing implementations whenever possible and reimplemen- ted only one of them. Table 1 summarizes the origins of the implementations we employed. We also implemented cache- conscious generic cascades of logical-level techniques and NS. Furthermore, we implemented a decompression with aggre- gation for all algorithms to evaluate a processing of com- pressed data. In this section, we describe some of the most crucial implementation details with respect to performance. </span><span class="c7">3.1 SIMD Instruction Set Extensions </span></p><p class="c22 c46"><span class="c5">Single Instruction Multiple Data (SIMD) instruction set extensions such as Intel&rsquo;s SSE and AVX have been available in modern processors for several years. SIMD instructions apply one operation to multiple elements of so-called vector registers at once. The available operations include parallel arithmetic, logical, and shift operations as well as permu- tations. These are highly relevant to lightweight compres- sion algorithms. In fact the main focus of recent research [12, 14, 16, 18, 20] in this field has been the employment of SIMD instructions to speed up (de)compression. Conse- quently, most algorithms we evaluate in this paper make use of SIMD extensions (see Table 1). Vectorized load and store instructions can be either aligned or unaligned. The former require the accessed memory addresses to be multiples of 16 bytes (SSE) and are usually faster. Although nowadays In- tel&rsquo;s AVX2 offers 256-bit operations, we decided to restrict our evaluation to implementations using 128-bit SSE. This has two reasons: (1) Most of the techniques presented in the literature are designed for 128-bit vector operations</span><span class="c20">3 </span><span class="c5">and (2) The comparison is fairer if only one width of vector regis- ters is considered. Intel&rsquo;s SIMD instructions can be used in C/C++ without writing assembly code via intrinsic func- tions, whose names start with </span><span class="c8">_mm_</span><span class="c5">. </span><span class="c7">3.2 Physical-Level Technique: NS </span></p><p class="c18"><span class="c5">In the following, we describe crucial points regarding ex- isting implementations as well as one reimplementation. </span></p><p class="c70"><span class="c11">3.2.1 Bit-Aligned Algorithms </span></p><p class="c10"><span class="c5">We obtained the implementation of 4-Gamma Coding directly from the authors [16], and those of SIMD-BP128 and SIMD-FastPFor from the FastPFor-library [13]. All </span></p><p class="c22 c55"><span class="c25">3</span><span class="c12">Thereby, a transition to 256-bit operations is not always trivial and could be subject to future research. </span></p><p class="c23"><span class="c17">74 </span></p><p class="c27"><span class="c5">three implementations use vectorized shift and mask oper- ations. SIMD-BP128 and SIMD-FastPFor use a dedicated optimized packing and unpacking routine for each of the 32 possible bit widths. It is worth mentioning, that &ndash; while the original PFOR algorithm is a combination of the FOR and the NS technique &ndash; SIMD-FastPFOR, despite its name, does not include the FOR technique, but only the NS technique. </span></p><p class="c38"><span class="c11">3.2.2 Byte-Aligned Algorithms </span></p><p class="c26"><span class="c5">Regarding 4-Wise Null Suppression, we use the origi- nal implementation by Schlegel et al. [16]. It implements the horizontal packing of the uncompressed values using a vec- torized byte permutation. The 16-byte permutation masks required for this are built once in advance and looked up from a table during the compression. This table is indexed with the 1-byte compression masks, thus there are 256 per- mutation masks in total. The decompression works by using the inverse permutation masks. </span></p><p class="c33"><span class="c5">Masked-VByte vectorizes the decompression of the com- pressed format of the original VByte algorithm. The imple- mentation we use is available in the FastPFor-library [13] and is based on code by the original authors. The crucial point of the vectorization is the execution of a SIMD byte permutation in order to reinsert the leading zero-bytes re- moved by the compression. After 16 bytes of compressed data have been loaded into a vector register, the most sig- nificant bits of all bytes are extracted using a SIMD instruc- tion. The lower 12 bits of this 16-bit mask are used as a key to lookup the required permutation mask in a table. After the permutation, the original 7-bit units need to be stitched together, which is done using vectorized shift and mask op- erations. Masked-VByte also has an optimization for the case of 12 compressed 1-byte integers. </span></p><p class="c49"><span class="c11">3.2.3 Word-Aligned Algorithms </span></p><p class="c26"><span class="c5">We use the implementation of Simple-8b available in the FastPFor-library [13], which is a purely sequential imple- mentation. It uses a dedicated sequential packing routine for each of the possible selectors. </span></p><p class="c21"><span class="c5">We reimplemented SIMD-GroupSimple based on the description in the original paper, since we could not find an available implementation. We employed the two optimiza- tions discussed by the original authors: (1) We calculate the pseudo-quad max values instead of the quad max values to reduce the number of branch instructions. (2) We use one dedicated and vectorized packing routine for each selector, whereby the correct one is chosen by a switch-statement. </span></p><p class="c28"><span class="c5">The original compression algorithm processes the input data in three runs: The first run scans the entire input and materializes the pseudo-quad max array in main memory. The size of this array is one quarter of the input data size. The second run scans the pseudo-quad max array and ma- terializes the selectors array. The third run iterates over the selectors array and calls the respective packing routine to do the actual compression. This procedure results in a subopti- mal cache utilization, since at the end of each run, the data it started with has already been evicted from the caches. Thus, reaccessing it in the next run becomes expensive. </span></p><p class="c21"><span class="c5">In order to overcome this issue, we enhanced the compres- sion part of the algorithm with one more optimization, which was not presented in the original paper: Our reimplementa- tion stores the pseudo-quad max values in a ring buffer of a small constant size (32 32-bit integers) instead of an array </span></p><p class="c1 c22"><span class="c5">proportional to the input size. This is based on the observa- tion that the decision for the next selector can never require more than 32 pseudo-quad max values, since at most 4 &times;32 (1-bit) integers can be packed into four 32-bit words. Due to its small size (128 bytes), the ring buffer fits into the L1 data cache and can thus be accessed at top-speed. Our modified compression algorithm repeats the following steps until the end of the input is reached (in the beginning, the ring buffer is empty): (1) Fill the ring buffer by calculating the next up to 32 pseudo-quad max values. This reads up to 4&times;32 = 128 uncompressed integers. (2) Run the original subroutine for determining the next selector on the ring buffer. (3) Store the obtained selector to the selectors section in the output. (4) Compress the next block using the subroutine belonging to the selector. This will typically reread the uncompressed data touched in Step 1. Note that this data is very likely to still reside in the L1 cache, since only a few bytes of memory have been touched in between. (5) Increase the position in the ring buffer by the number of input quads compressed in the previous step. We observed that using this additional optimization, the compression part of our reimplementation is always faster than without it. Note, that this optimization does not affect the compressed output in any way. </span></p><p class="c53"><span class="c7">3.3 Logical-Level Techniques </span></p><p class="c10"><span class="c5">As previously mentioned, logical-level techniques are usu- ally combined with NS in existing algorithms and are thus hardly available in isolation. In order to be able to freely combine any logical-level technique with any NS algorithm, we reimplemented all four logical-level compression tech- niques as stand-alone algorithms. Thereby, an important goal is the vectorization of those algorithms. </span></p><p class="c39"><span class="c11">3.3.1 Vectorized DELTA </span></p><p class="c10"><span class="c5">Our implementation of DELTA represents each input ele- ment as the difference to its fourth predecessor. This allows for an easy vectorization by processing four integers at a time. The first four elements are always copied from the in- put to the output. During the compression, the next four dif- ferences are calculated at once using </span><span class="c8">_mm_sub_epi32()</span><span class="c5">. The decompression reverses this by employing </span><span class="c8">_mm_add_epi32()</span><span class="c5">. This implementation follows the description in [12] with the difference that we do not overwrite the input data, because we still need it as the input for the other algorithms. </span></p><p class="c47"><span class="c11">3.3.2 Sequential DICT </span></p><p class="c37 c22"><span class="c5">Our implementation of DICT is a purely sequential single- pass algorithm employing a static dictionary, which is built on the uncompressed data before the (de)compression takes place. Thus, building the dictionary is not included in our time measurements and the dictionary itself is not included in the compressed representation. This represents the case of a domain-specific dictionary which is known in advance. The compression uses a C++-STL </span><span class="c8">unordered_map </span><span class="c5">to map values to their keys, whereas the decompression uses the key as the index of a </span><span class="c8">vector </span><span class="c5">to look up the corresponding value. </span></p><p class="c54"><span class="c11">3.3.3 Vectorized FOR </span></p><p class="c10"><span class="c5">We implemented the compression of FOR as a vectorized two-pass algorithm. The first pass iterates over the input and determines the reference value, i.e., the minimum us- ing </span><span class="c8">_mm_min_epu32()</span><span class="c5">. This minimum is then copied into all four elements of one vector register. The second pass iter- </span></p><p class="c23"><span class="c17">75 </span></p><p class="c27"><span class="c5">ates over the input again and subtracts this vector register from four input elements at a time using </span><span class="c8">_mm_sub_epi32()</span><span class="c5">. In the end, the reference value is appended to the output. The decompression adds this reference value to four data elements at a time using </span><span class="c8">_mm_add_epi32()</span><span class="c5">. </span></p><p class="c61"><span class="c11">3.3.4 Vectorized RLE </span></p><p class="c26"><span class="c5">Our implementation of RLE also utilizes SIMD instruc- tions. The compression part is based on parallel compar- isons. It repeats the following steps until the end of the input is reached: (1) One 128-bit vector register is loaded with four copies of the current input element. (2) The next four input elements are loaded. (3) The intrinsic </span><span class="c8">_mm_cmpeq_epi32() </span><span class="c5">is employed for a parallel comparison. The result is stored in a vector register. (4) We obtain a 4-bit comparison mask us- ing </span><span class="c8">_mm_movemask_ps()</span><span class="c5">. Each bit in the mask indicates the (non-)equality of two corresponding vector elements. The number of trailing one-bits in this mask is the number of el- ements for which the run continues. If this number is 4, then we have not seen the run&rsquo;s end yet, and continue at step 2. Otherwise, we have reached the run&rsquo;s end and append the run value and run length to the output and continue with step 1 at the next element after the run&rsquo;s end. </span></p><p class="c21"><span class="c5">The decompression executes the following until the entire input has been consumed: (1) Load the next pair of run value and run length. (2) Load one vector register with four copies of the run value. (3) Store the contents of that register to memory as often as required to match the run length. </span></p><p class="c66"><span class="c7">3.4 Cascades of Logical-Level and Physical- </span><span class="c24">Level Techniques </span><span class="c5">The challenge of implementing cascades, i.e., combina- tions of logical-level and physical-level techniques, is the high implementation effort due to the high number of possible combinations. To address this problem, we implemented a cache-conscious cascade which is generic w.r.t. the employed algorithms. That is, it can be instantiated for any two al- gorithms, without further implementation effort. It takes three parameters: a logical-level algorithm L, a physical- level algorithm P, and an (uncompressed) block size bs</span><span class="c57">u</span><span class="c5">. </span></p><p class="c64"><span class="c5">The output consists of compressed blocks, each of which starts with its size as a 32-bit integer followed by 12 bytes of padding to achieve the 16-byte alignment required by SSE instructions. The body of the block contains the compressed data possibly followed by additional padding bytes. </span></p><p class="c28"><span class="c5">The compression procedure repeats the following steps un- til the end of the input is reached: (1) Skip 16 bytes in the output buffer. (2) Apply the compression of L to the next bs</span><span class="c40">u </span><span class="c12">elements in the input. Store the result in an intermediate </span><span class="c5">buffer. (3) Apply the compression of P to that buffer and store the result to the output buffer. (4) Store the size bs</span><span class="c40">c </span><span class="c5">of the compressed block to the bytes skipped in Step 1. (5) Skip some bytes after the compressed block, if it is necessary to achieve 16-byte alignment. </span></p><p class="c60"><span class="c5">The decompression is the reverse procedure repeatedly ex- ecuting the following steps: (1) Read the size bs</span><span class="c40">c </span><span class="c12">of the </span><span class="c5">current compressed block and skip the padding. (2) Apply the decompression of P to the next bs</span><span class="c40">c </span><span class="c12">bytes in the input. </span><span class="c5">Store the result to an intermediate buffer. (3) Decompress the contents of that buffer using L and append the result to the output. (4) Skip the padding in the input, if necessary. The intermediate buffer is reused for all blocks. Its size is in the order of magnitude of bs</span><span class="c40">u </span><span class="c12">(we chose 4KiB+2&times;bs</span><span class="c40">u </span><span class="c12">as </span></p><p class="c1 c22"><span class="c5">a pessimistic estimation). This algorithm is cache-conscious, if bs</span><span class="c57">u </span><span class="c5">is chosen to fit the Lx cache, since then, the data read by the second algorithm is likely to still reside in that cache. </span><span class="c7">3.5 Decompression with Aggregation </span></p><p class="c22 c37"><span class="c5">We also modified the decompressions of both, our own reimplementations and existing implementations, such that they sum up the decompressed data instead of writing it to memory. The usual case for the vectorized algorithms is that four decompressed 32-bit integers reside in a vector register before they are stored to memory using </span><span class="c8">_mm_store_si128()</span><span class="c5">. We replaced these store instructions by vectorized additions. However, since the sum might require more than 32 bits, we first distribute the four 32-bit elements to the four 64-bit ele- ments of two 128-bit registers using </span><span class="c8">_mm_unpacklo_epi32() </span><span class="c5">and </span><span class="c8">_mm_unpackhi_epi32() </span><span class="c5">and add both to two 64-bit run- ning sums (which are added in the very end) by applying </span><span class="c8">_mm_add_epi64()</span><span class="c5">. In the case of RLE, we add the product of the run length and the run value to the running sum. </span></p><p class="c32 c62"><span class="c7">4. EVALUATION SETUP </span></p><p class="c37 c22"><span class="c5">In this section, we describe our overall evaluation setup. All algorithms are implemented in C/C++ and we compiled them with </span><span class="c8">g++ </span><span class="c5">4.8 using the optimization flag </span><span class="c8">-O3</span><span class="c5">. As the operating system we used Ubuntu 14.04. All experiments have been executed on the same hardware machine in or- der to be able to compare the results. The machine was equipped with an Intel Core i7-4710MQ (Haswell) proces- sor with 4 physical and 8 logical cores running at 2.5 GHz. The L1 data, L2, and L3 caches have a capacity of 32 KB, 256 KB and 6 MB, respectively. We use only one core at any time of our evaluation to avoid competition for the shared L3 cache. The capacity of the DDR3 main memory was 16 GB. We are able to copy data using </span><span class="c8">memcpy() </span><span class="c5">at a rate of 6.15 GiB/s or 1,650 mis (million integers per second). </span></p><p class="c22 c63"><span class="c5">All experiments happened entirely in main memory. The disk was never accessed during the time measurements. The whole evaluation is performed using our benchmark frame- work [7]. The synthetic data generation was performed by our data generator once per configuration of data proper- ties. The data properties were recorded and the algorithms were repeatedly performed on the generated data. During the executions, the runtimes and the compression rates were measured. Furthermore, we emptied the cache before each algorithm execution by copying a 12-MB array (twice as large as the L3 cache) using a loop operation. </span></p><p class="c69"><span class="c5">All time measurements were carried out by means of the wallclock-time (C++-STL </span><span class="c8">high_resolution_clock</span><span class="c5">) and were repeated 12 times to receive stable values, thereby we only report average values. The time measurements include: Compression: Loading uncompressed data from main mem- </span></p><p class="c14"><span class="c5">ory, applying the compression algorithm, storing the compressed data to main memory Decompression: Loading compressed data from main mem- </span></p><p class="c42"><span class="c5">ory, applying the decompression algorithm, storing the uncompressed data to main memory Decompression &amp; Aggregation: Loading compressed data </span></p><p class="c73"><span class="c5">from main memory, applying the decompression and summation, storing 8 bytes in total to main memory </span></p><p class="c22 c32"><span class="c7">5. EXPERIMENTS ON SYNTHETIC DATA </span><span class="c5">In this section, we present selected results of our experi- </span></p><p class="c23"><span class="c17">76 </span></p><p class="c0"><span class="c4">(i) compression rate [bits/int] </span></p><p class="c0"><span class="c4">(j) compression speed [mis] </span></p><p class="c0"><span class="c4">(k) decompression speed [mis] </span></p><p class="c0"><span class="c4">(l) aggregation speed [mis] </span></p><p class="c0"><span class="c4">Simple-8b </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">2500 </span></p><p class="c0"><span class="c4">5000 </span></p><p class="c0"><span class="c4">SIMD-GroupSimple </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">4000 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">3000 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">0 4 8 12 16 20 24 28 32 </span></p><p class="c0"><span class="c4">0 4 8 12 16 20 24 28 32 </span></p><p class="c0"><span class="c4">0 4 8 12 16 20 24 28 32 bit width </span></p><p class="c0"><span class="c4">bit width </span></p><p class="c0"><span class="c4">bit width </span></p><p class="c0"><span class="c5">Figure 1: The general behavior of the three classes of NS algorithms. </span></p><p class="c0"><span class="c5">mental survey. We generate synthetic data sets in order to be able to control the data properties in a systematic way. All uncompressed arrays contain 100 million 32-bit integers, i.e., 400 MB. Thus, only a small portion of the uncom- pressed data fits into the L3 cache. We report speeds in million integers per second (mis) and compression rates in bits per integer (bits/int). We begin with the evaluation of pure NS algorithms in Section 5.1. After that, we investi- gate pure logical-level algorithms in Section 5.2. In Section 5.3, we evaluate cascades of logical-level techniques and NS. Finally, in Section 5.4 we present the conclusions we draw from our evaluation on synthetic data. </span><span class="c7">5.1 Null Suppression Algorithms </span></p><p class="c1"><span class="c5">We start by identifying the characteristics of the three classes of NS algorithms. After that, we compare five se- lected NS algorithms in more detail. </span></p><p class="c0"><span class="c11">5.1.1 Classes of NS Algorithms </span></p><p class="c1"><span class="c5">We generate 32 unsorted datasets, such that all data ele- ments in the i-th dataset have exactly i effective bits, i.e., the value range is [0,1] for i = 1 and [2</span><span class="c48">i</span><span class="c20">&minus;1</span><span class="c5">,2</span><span class="c48">i</span><span class="c5">) for i = 2,...,32. Within these ranges, the values are uniformly distributed. </span></p><p class="c1"><span class="c5">Figure 1 (a-d) show the results for the considered bit- aligned algorithms. These have the finest possible com- pression granularity and thus can perfectly adapt to any bit width. Consequently, the compression rate is a linear function of the bit width. The speeds of compression, de- compression, and aggregation follow the same linear trend. Nevertheless, there are differences between the algorithms. Since SIMD-BP128 and SIMD-FastPFOR store less meta in- formation than 4-Gamma, they achieve better compression rates. They are also better regarding the decompression and aggregation speed. However, in terms of compression speed, SIMD-FastPFOR is by far the slowest, while SIMD-BP128 still shows very good performance. </span></p><p class="c0"><span class="c5">Figure 1 (e-h) present the results for the considered byte- </span></p><p class="c0"><span class="c4">1000 500 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c1"><span class="c5">aligned algorithms. These algorithms compress an inte- ger at the granularity of units of 8 bits (4-Wise NS) or 7 bits (Masked-VByte). As a result, the curves of all four measured variables exhibit a step-shape, whereby the step width is constant and equals the unit size of the algorithm. Since the units of 4-Wise NS and Masked-VByte have dif- ferent sizes, the first and second regarding compression rate change several times when increasing the bit width. Con- cerning the speeds, 4-Wise NS is always at least as fast as Masked-VByte, except for the compression of values with up to 7 bits, in this case Masked-VByte is significantly faster. </span></p><p class="c1"><span class="c5">Finally, Fig. 1 (i-l) provide the results for the word- aligned algorithms. These can adapt only to certain bit widths, which are not the multiples of any unit size. For instance, SIMD-GroupSimple supports 1, 2, 3, 4, 5, 6, 8, 10, 16, and 32 bits. Hence, the measured variables show steps at these bit widths, i.e., the steps do not have a constant width per algorithm. This basic shape is especially clear for the compression rate and the compression speed, but can also be found in the decompression and aggregation speed. SIMD- GroupSimple compresses better for certain bit widths, and for others, Simple-8b does. Since Simple-8b uses 64-bit words in the output, it can still achieve a size reduction for bit widths up to 20, while SIMD-GroupSimple cannot reduce the size anymore if the bit width exceeds 16. SIMD- GroupSimple is faster for bit widths up to 3 and slower in all other situations. However, regarding decompression and aggregation, it is faster for all bit widths. </span></p><p class="c1"><span class="c5">To summarize, each of the three classes exhibits its indi- vidual behavior subject to the bit width. At the same time the differences between the classes are significant. </span></p><p class="c0"><span class="c11">5.1.2 Detailed Comparison of NS Algorithms </span></p><p class="c0"><span class="c5">For the following experiments we pick SIMD-BP128, SIMD- FastPFOR, 4-Wise NS, Masked-VByte, and SIMD-Group- Simple and investigate their behavior in more detail. Note that all three classes of NS are represented in this selection. </span></p><p class="c0"><span class="c17">77 </span></p><p class="c0"><span class="c4">0</span><span class="c16">816243240 </span></p><p class="c0"><span class="c4">0</span><span class="c16">816243240 </span></p><p class="c0"><span class="c4">0</span><span class="c16">816243240485664 </span></p><p class="c0"><span class="c4">6000 (a) compression rate [bits/int] </span></p><p class="c0"><span class="c4">(b) compression speed [mis] </span></p><p class="c0"><span class="c4">(c) decompression speed [mis] </span></p><p class="c0"><span class="c16">(d) aggregation speed [mis] </span></p><p class="c0"><span class="c4">4-Gamma </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">2500 </span></p><p class="c0"><span class="c4">SIMD-BP128 SIMD-FastPFOR </span></p><p class="c0"><span class="c4">0 4 8 12 16 20 24 28 32 </span></p><p class="c0"><span class="c4">0 4 8 12 16 20 24 28 32 </span></p><p class="c0"><span class="c4">0 4 8 12 16 20 24 28 32 bit width </span></p><p class="c0"><span class="c4">bit width </span></p><p class="c0"><span class="c4">bit width </span></p><p class="c15"><span class="c4">0 4 8 12 16 20 24 28 32 bit width </span></p><p class="c0"><span class="c4">5000 1500 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">4000 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">3000 1000 </span></p><p class="c0"><span class="c4">2000 500 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">0 4 8 12 16 20 24 28 32 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">bit width </span></p><p class="c0"><span class="c4">(e) compression rate [bits/int] </span></p><p class="c0"><span class="c4">(f) compression speed [mis] </span></p><p class="c0"><span class="c4">(g) decompression speed [mis] </span></p><p class="c0"><span class="c4">(h) aggregation speed [mis] </span></p><p class="c0"><span class="c4">4-Wise NS </span></p><p class="c0"><span class="c4">3500 </span></p><p class="c0"><span class="c4">Masked-VByte </span></p><p class="c0"><span class="c4">0 4 8 12 16 20 24 28 32 </span></p><p class="c0"><span class="c4">0 4 8 12 16 20 24 28 32 </span></p><p class="c0"><span class="c4">0 4 8 12 16 20 24 28 32 bit width </span></p><p class="c0"><span class="c4">bit width </span></p><p class="c0"><span class="c4">bit width </span></p><p class="c15"><span class="c4">0 4 8 12 16 20 24 28 32 bit width </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">3000 1500 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c30"><span class="c4">2500 2000 1000 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c30"><span class="c4">1000 500 0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">(a) compression rate [bits/int] </span></p><p class="c0"><span class="c4">(b) compression speed [mis] </span></p><p class="c0"><span class="c4">(c) decompression speed [mis] </span></p><p class="c0"><span class="c4">6000 </span><span class="c16">(d) aggregation speed [mis] </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span><span class="c4">2</span><span class="c3">32 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span><span class="c4">2</span><span class="c3">32 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span><span class="c4">2</span><span class="c3">32 </span><span class="c4">maximum </span></p><p class="c0"><span class="c4">maximum </span></p><p class="c0"><span class="c4">maximum </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span></p><p class="c0"><span class="c4">mean </span></p><p class="c0"><span class="c4">0</span><span class="c16">816243240 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">2500 </span></p><p class="c0"><span class="c4">5000 </span></p><p class="c0"><span class="c4">2000 1500 </span></p><p class="c0"><span class="c4">4000 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">3000 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c15"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span><span class="c4">2</span><span class="c3">32 </span><span class="c4">maximum </span></p><p class="c0"><span class="c4">(e) compression rate [bits/int] </span></p><p class="c0"><span class="c4">(f) compression speed [mis] </span></p><p class="c0"><span class="c4">(g) decompression speed [mis] </span></p><p class="c0"><span class="c16">(h) aggregation speed [mis] </span></p><p class="c0"><span class="c16">40 </span></p><p class="c15"><span class="c4">2500 6000 </span><span class="c16">3224168</span><span class="c4">02</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span><span class="c4">mean </span></p><p class="c0"><span class="c4">mean </span></p><p class="c0"><span class="c4">mean </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span></p><p class="c0"><span class="c4">mean of the outliers </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">5000 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">4000 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">3000 </span></p><p class="c0"><span class="c4">2000 500 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">(i) </span><span class="c16">32 </span></p><p class="c0"><span class="c4">compression rate [bits/int] </span></p><p class="c0"><span class="c4">(j) compression speed [mis] </span></p><p class="c0"><span class="c4">2500 </span></p><p class="c0"><span class="c4">(k) decompression speed [mis] </span></p><p class="c0"><span class="c4">6000 </span><span class="c16">(l) aggregation speed [mis] </span></p><p class="c0"><span class="c16">24168</span><span class="c4">02</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span><span class="c4">mean of the outliers </span></p><p class="c0"><span class="c4">mean of the outliers </span></p><p class="c0"><span class="c4">mean of the outliers </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span></p><p class="c0"><span class="c4">mean of the outliers </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">5000 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">4000 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">3000 1000 </span></p><p class="c0"><span class="c4">2000 500 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">(m) </span><span class="c16">32 </span></p><p class="c0"><span class="c4">compression rate [bits/int] </span></p><p class="c0"><span class="c4">(n) compression speed [mis] </span></p><p class="c0"><span class="c4">2500 </span></p><p class="c0"><span class="c4">(o) decompression speed [mis] </span></p><p class="c0"><span class="c4">6000 </span><span class="c16">(p) aggregation speed [mis] </span><span class="c4">2000 </span></p><p class="c0"><span class="c4">5000 </span></p><p class="c0"><span class="c16">24</span><span class="c4">1500 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">4000 </span></p><p class="c0"><span class="c16">16</span><span class="c4">1000 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">3000 1000 </span></p><p class="c0"><span class="c16">8</span><span class="c4">02</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span><span class="c4">mean of the outliers </span></p><p class="c0"><span class="c4">mean of the outliers </span></p><p class="c0"><span class="c4">mean of the outliers </span></p><p class="c0"><span class="c4">SIMD-BP128 SIMD-FastPFOR 4-Wise NS Masked-VByte SIMD-GroupSimple </span></p><p class="c0"><span class="c5">Figure 2: Comparison of NS algorithms of different classes on different data distributions. </span></p><p class="c1"><span class="c5">We generate unsorted data using four distributions D1&ndash;4, whereby we vary one parameter for each of them. D1 is a uniform distribution with a min of 0 and a max varying from 0 to 2</span><span class="c20">32 </span><span class="c5">&minus; 1. D2 is a normal distribution with a standard deviation of 20 and a mean varying from 64 to 2</span><span class="c20">31</span><span class="c5">. For D3, 90% of the values follow a normal distribution with a stan- dard deviation of 2 and a mean of 8, while 10% are drawn from a normal distribution with the same standard devia- tion and a mean varying from 8 to 2</span><span class="c20">31</span><span class="c5">. That is, 90% of the data elements are small integers, while 10% are increasingly large outliers. D4 is like D3, but with a ratio of 50:50. While D1&ndash;2 have a high data locality, D3&ndash;4 do not. </span></p><p class="c0"><span class="c5">The results for D1 can be found in Fig. 2 (a-d). The bit-aligned algorithms SIMD-BP128 and SIMD-FastPFOR always achieve the best compression rates, since they can adapt to any bit width. Masked-VByte is the fastest com- pressor for small values, although it is not even vector- ized. However, for larger values, SIMD-BP128 is the fastest, but comes closer to 4-Wise NS as the values grow. SIMD- GroupSimple yields the highest decompression speed for max- imums up to 32. From there on SIMD-BP128 and SIMD- FastPFOR are the fastest, while SIMD-GroupSimple and 4-Wise NS come quite close to their performance, especially for the values for which they do not waste too many bits due to their coarser granularity. </span></p><p class="c1"><span class="c5">For D2 (Fig. 2 (e-h)) we can make the same general obser- vations. However, the steps in the curves of the byte-aligned algorithms become steeper, since D2 produces values with </span></p><p class="c0"><span class="c4">2000 500 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c5">less distinct bit widths than D1. </span></p><p class="c30"><span class="c5">The results of D3 (Fig. 2 (i-l)) reveal some interesting effects. Regarding the compression rate, SIMD-FastPFOR stays the winner, while SIMD-BP128 is competitive only for small outliers. For large outliers it even yields the worst compression rates of all five algorithms. This is due to the fact that SIMD-BP128 packs blocks of 128 integers with the bit width of the largest element in the block, i.e., one outlier per block affects the compression rate significantly. SIMD- FastPFOR on the other side, can handle this case very well, since it &ndash; like all variants of PFOR &ndash; is explicitly designed to tolerate outliers. The byte-aligned algorithms 4-Wise NS and Masked-VByte are worse than SIMD-FastPFOR, but still quite robust, since they choose an individual byte width for each data element and are, thus, not affected by outliers. SIMD-GroupSimple compresses better than SIMD-BP128 in most cases, since outliers lead to small input blocks, while there can still be large blocks of non-outliers. In terms of compression speed, SIMD-BP128 is still in the top-2, but it is overtaken by 4-Wise NS for large outliers. Concerning de- compression speed, 4-Wise NS overtakes SIMD-BP128 when the outliers need more than 12 bits. SIMD-FastPFOR is nearly as fast as 4-Wise NS, but achieves much better com- pression rates. Regarding the aggregation, SIMD-BP128 is still the fastest algorithm, although SIMD-FastPFOR comes very close for small outliers and 4-Wise NS for large outliers. D4 increases the amount of outliers to 50%. The compres- sion rate of SIMD-BP128 does not change any more, since </span></p><p class="c0"><span class="c17">78 </span></p><p class="c0"><span class="c4">(a) total # data elements 100M </span></p><p class="c0"><span class="c4">Uncompr., FOR, DELTA, DICT RLE </span></p><p class="c0"><span class="c16">s tibe vitceffe32 24168</span><span class="c4">(b) Uncompressed </span></p><p class="c0"><span class="c6">100 % </span></p><p class="c0"><span class="c4">80M </span></p><p class="c0"><span class="c6">75 % </span></p><p class="c0"><span class="c4">60M </span></p><p class="c0"><span class="c5">D5 </span></p><p class="c0"><span class="c4"># 110</span><span class="c3">0 </span><span class="c4">10</span><span class="c3">1 </span><span class="c4">10</span><span class="c3">2 </span><span class="c4">10</span><span class="c3">3 </span><span class="c4">10</span><span class="c3">4 </span><span class="c4">10</span><span class="c3">5 </span><span class="c4">10</span><span class="c3">6 </span><span class="c4">10</span><span class="c3">7 </span><span class="c4">10</span><span class="c3">8 </span></p><p class="c0"><span class="c4">avg. run length </span></p><p class="c0"><span class="c6">50 % </span><span class="c4">40M 20M </span></p><p class="c0"><span class="c6">25 % </span></p><p class="c0"><span class="c4">0 10</span><span class="c3">0 </span><span class="c4">10</span><span class="c3">1 </span><span class="c4">10</span><span class="c3">2 </span><span class="c4">10</span><span class="c3">3 </span><span class="c4">10</span><span class="c3">4 </span><span class="c4">10</span><span class="c3">5 </span><span class="c4">10</span><span class="c3">6 </span><span class="c4">10</span><span class="c3">7 </span><span class="c4">10</span><span class="c3">8 </span></p><p class="c0"><span class="c4">2</span><span class="c3">2 </span><span class="c4">2</span><span class="c3">7 </span><span class="c4">2</span><span class="c3">12</span><span class="c4">2</span><span class="c3">17</span><span class="c4">2</span><span class="c3">22</span><span class="c4">2</span><span class="c3">27 </span></p><p class="c0"><span class="c6">0 % </span></p><p class="c0"><span class="c4">avg. run length </span></p><p class="c0"><span class="c4">avg. run length </span></p><p class="c0"><span class="c4">70K </span><span class="c16">(g) # distinct data elements </span></p><p class="c0"><span class="c4">Uncompr. </span></p><p class="c0"><span class="c4">(h) Uncompressed </span></p><p class="c0"><span class="c6">100 % </span></p><p class="c0"><span class="c5">D2 </span></p><p class="c0"><span class="c5">Figure 3: Logical-level techniques applied to D5 (a-g) and D2 (h-l): Data properties.</span><span class="c20">4 </span></p><p class="c15"><span class="c4">10</span><span class="c3">0 </span><span class="c4">10</span><span class="c3">1 </span><span class="c4">10</span><span class="c3">2 </span><span class="c4">10</span><span class="c3">3 </span><span class="c4">10</span><span class="c3">4 </span><span class="c4">10</span><span class="c3">5 </span><span class="c4">10</span><span class="c3">6 </span><span class="c4">10</span><span class="c3">7 </span><span class="c4">10</span><span class="c3">8 </span><span class="c4">avg. run length </span></p><p class="c30"><span class="c16">s tibe vitceffe32 </span><span class="c4">60K 50K </span></p><p class="c0"><span class="c16">24</span><span class="c6">75 % </span><span class="c4">40K 30K </span></p><p class="c0"><span class="c16">16</span><span class="c6">50 % </span></p><p class="c0"><span class="c4">20K </span></p><p class="c0"><span class="c16">8</span><span class="c4"># 1</span><span class="c6">25 % </span><span class="c4">10K 0 </span></p><p class="c0"><span class="c4">2</span><span class="c3">6 </span><span class="c4">2</span><span class="c3">11 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">21 </span><span class="c4">2</span><span class="c3">26 </span><span class="c4">2</span><span class="c3">31 </span></p><p class="c0"><span class="c6">0 % </span></p><p class="c0"><span class="c4">mean </span></p><p class="c0"><span class="c16">4K (a) compression speed [mis] </span></p><p class="c0"><span class="c16">3K (b) decompression speed [mis] </span></p><p class="c30"><span class="c5">alone suffices to reduce the data size. Figure 3 (b-f)</span><span class="c20">4 </span><span class="c5">show </span><span class="c16">3K2K1K</span><span class="c4">0</span><span class="c5">the data distributions in the uncompressed data as well as </span><span class="c16">2K</span><span class="c5">in the outputs of the logical-level techniques. Most uncom- </span></p><p class="c0"><span class="c16">1K</span><span class="c4">DELTA </span></p><p class="c0"><span class="c5">pressed values have 16 or 15 effective bits. This does not </span><span class="c4">FOR 010</span><span class="c3">0 </span><span class="c4">10</span><span class="c3">1 </span><span class="c4">10</span><span class="c3">2 </span><span class="c4">10</span><span class="c3">3 </span><span class="c4">10</span><span class="c3">4 </span><span class="c4">10</span><span class="c3">5 </span><span class="c4">10</span><span class="c3">6 </span><span class="c4">10</span><span class="c3">7 </span><span class="c4">10</span><span class="c3">8 </span><span class="c4">avg. run length </span></p><p class="c1"><span class="c5">change much with FOR, since the value distribution can pro- duce values close to zero. In contrast, the output of DELTA contains nearly only values of one effective bit for long runs, since these yield long sequences of zeros. Note that there are also outliers having 32 effective bits, resulting from neg- ative differences being represented in the two&rsquo;s complement. With DICT, the values start to get smaller as soon as the run length is high enough to lead to a decrease of the number of distinct values (see Fig. 3 (g)), and thus the maximum key. For RLE there are always two peaks in the distribu- tions: one is at a bit width of 16 and corresponds to the run values and the other one is produced by the increasingly high run lengths. Note that this distribution is quite similar to D4 from the previous section. The distributions might seem to get worse for high run lengths. However, it must be kept in mind that RLE reduces the total number of data ele- ments in those cases. Figure 4 provides the (de)compression speeds. The performance of DELTA and FOR is indepen- dent of the data characteristics, since they execute the same instructions for each group of four values. On the other side, RLE is slow for short runs, but becomes by far the fastest algorithm for long runs, since it has to write(read) less data during the (de)compression. DICT is the slowest compres- sor due to the expensive look ups in the map. Regarding the decompression, it is competitive to DELTA and FOR, but sensitive to the number of distinct values, which influences whether or not the dictionary fits into the Lx cache. </span></p><p class="c30"><span class="c5">The distributions for D2 are visualized in Fig. 3 (h-l). Here, FOR can improve the distribution significantly, since the value range is narrow. The same applies to DICT, since consequently the number of distinct values is small. As the data is unsorted and does not have runs, about half of the values in the output of DELTA have 32 effective bits, i.e., the distributions get worse in most cases. Note that RLE doubles the number of data elements due to the lack of runs. To sum up, logical-level techniques can significantly im- </span></p><p class="c0"><span class="c25">4 </span><span class="c12">How to read Fig. 3 (b-f) and (h-l): The y-axis lists all possible numbers of effective bits a data element could have. Each vertical slice corresponds to one configuration of the data properties. The intensity encodes what portion of the data elements has how many effective bits. That is, the dark pixels show which numbers of effective bits occur most frequently in the dataset. </span></p><p class="c0"><span class="c17">79 </span><span class="c4">RLE DICT </span></p><p class="c0"><span class="c5">Figure 4: Logical-level techniques on D5: Speeds. </span></p><p class="c1"><span class="c5">basically all blocks were affected by outliers in D3 already. However, since the other algorithms compress worse now, the trade-offs have to be reevaluated. Thanks to patched coding, SIMD-FastPFOR still is in the top-2 regarding the compression rate. However, this comes at the cost of (de)- compression and aggregation performance, which heavily de- creases as the outliers grow. Encoding each value individu- ally 4-Wise NS and Masked-VByte come very close to the compression rate of SIMD-FastPFOR and 4-Wise NS de- compresses faster than SIMD-FastPFOR for large outliers. </span></p><p class="c1"><span class="c5">To sum up, the best algorithm regarding compression rate or performance depends on the data distribution. Regarding one measured variable, a certain algorithm can be the best for one distribution and the worst for another distribution. Moreover, for a certain distribution the best algorithm re- garding one measured variable can be the worst for another variable. In addition, there are many points of intersection between the algorithms&rsquo; compression rates and speeds offer- ing many different trade-offs. </span></p><p class="c0"><span class="c7">5.2 Logical-Level Techniques </span></p><p class="c1"><span class="c5">A general trend observable in Figures 1 and 2 is that all NS algorithms get worse as the data elements get larger. Logical-level techniques can be able to change the data prop- erties in favor of NS. To illustrate this, we provide the re- sults of the application of the four logical techniques to two unsorted datasets: D2, already known from the previous sec- tion, and D5, whose data elements are uniformly drawn from the range [0,2</span><span class="c20">16 </span><span class="c5">&minus; 1] while varying the average run length. </span></p><p class="c0"><span class="c5">We start with the discussion of D5. First of all, in Fig. 3 (a) we can see that the total number of data elements after the application of FOR, DELTA, and DICT is the same as in the uncompressed data (1:1 mapping), while with RLE it decreases significantly as the run length increases (N:1 map- ping). This has two consequences: (1) an NS algorithm applied after RLE needs to compress less data and (2) RLE </span></p><p class="c15"><span class="c4">1</span><span class="c16">32 (c) FOR 24168</span><span class="c4">2</span><span class="c3">2 </span><span class="c4">2</span><span class="c3">7 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">17</span><span class="c4">2</span><span class="c3">22</span><span class="c4">2</span><span class="c3">27 </span><span class="c4">avg. run length </span></p><p class="c15"><span class="c4">1</span><span class="c16">32 (i) FOR 24168</span><span class="c4">2</span><span class="c3">6 </span><span class="c4">2</span><span class="c3">11 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">21 </span><span class="c4">2</span><span class="c3">26 </span><span class="c4">2</span><span class="c3">31 </span><span class="c4">mean </span></p><p class="c15"><span class="c4">1</span><span class="c16">32 (d) DELTA 24168</span><span class="c4">2</span><span class="c3">2 </span><span class="c4">2</span><span class="c3">7 </span><span class="c4">2</span><span class="c3">12</span><span class="c4">2</span><span class="c3">17</span><span class="c4">2</span><span class="c3">22</span><span class="c4">2</span><span class="c3">27 </span><span class="c4">avg. run length </span></p><p class="c15"><span class="c4">1</span><span class="c16">32 (j) DELTA 24168</span><span class="c4">2</span><span class="c3">6 </span><span class="c4">2</span><span class="c3">11 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">21 </span><span class="c4">2</span><span class="c3">26 </span><span class="c4">2</span><span class="c3">31 </span><span class="c4">mean </span></p><p class="c15"><span class="c4">1</span><span class="c16">32 (e) DICT 24168</span><span class="c4">2</span><span class="c3">2 </span><span class="c4">2</span><span class="c3">7 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">17</span><span class="c4">2</span><span class="c3">22</span><span class="c4">2</span><span class="c3">27 </span><span class="c4">avg. run length </span></p><p class="c15"><span class="c4">1</span><span class="c16">32 (k) DICT 24168</span><span class="c4">2</span><span class="c3">6 </span><span class="c4">2</span><span class="c3">11 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">21 </span><span class="c4">2</span><span class="c3">26 </span><span class="c4">2</span><span class="c3">31 </span><span class="c4">mean </span></p><p class="c15"><span class="c4">1</span><span class="c16">32 (f) RLE 24168</span><span class="c4">2</span><span class="c3">2 </span><span class="c4">2</span><span class="c3">7 </span><span class="c4">2</span><span class="c3">12</span><span class="c4">2</span><span class="c3">17</span><span class="c4">2</span><span class="c3">22</span><span class="c4">2</span><span class="c3">27 </span><span class="c4">avg. run length </span></p><p class="c15"><span class="c4">1</span><span class="c16">32 (l) RLE 24168</span><span class="c4">2</span><span class="c3">6 </span><span class="c4">2</span><span class="c3">11</span><span class="c4">2</span><span class="c3">16</span><span class="c4">2</span><span class="c3">21</span><span class="c4">2</span><span class="c3">26</span><span class="c4">2</span><span class="c3">31 </span><span class="c4">mean </span></p><p class="c0"><span class="c4">DICT SIMD-BP128 SIMD-FastPFOR 4-Wise NS Masked-VByte SIMD-GroupSimple </span></p><p class="c0"><span class="c5">Figure 5: Comparison of the cascades on dataset D2.</span><span class="c20">5 </span></p><p class="c0"><span class="c5">prove the data distribution in favor of NS. However, the data properties determine which techniques are suitable. In the worst case, the distributions might even become less suited. We also experimented with other data characteristics such as the number of distinct values and sorted datasets, but omit their results due to a lack of space. Those experiments lead to similar conclusions. </span><span class="c7">5.3 Cascades </span><span class="c24">Level Techniques </span></p><p class="c1"><span class="c7">of Logical-Level and Physical- </span><span class="c5">To find out which improvements over the stand-alone NS algorithms the additional use of logical-level techniques can yield, we compare the five stand-alone NS algorithms from Section 5.1.2 to their cascades with the four logical-level techniques. That is, we compare 5 + 5 &times; 4 = 25 algorithms in total. The evaluation is conducted on three datasets: D1 and D5, which are already known, and D6, a sorted dataset for which we vary the number of distinct data elements by uniformly drawing values from the range [0, max], whereby max starts with 0 and is increased until we reach 100 M dis- tinct values, i.e., until all data elements are unique. For all three datasets, we provide a detailed comparison of SIMD- BP128 to its cascaded derivatives as well as a comparison of all 25 algorithms for selected data configurations. For our generic cascade algorithm, we chose a block size of 16 KiB, i.e., 4096 uncompressed integers. This size is a multiple of the block sizes of all considered algorithms and fits into the L1 cache of our machine. We also experimented with larger block sizes, but found that 16 KiB yields the best speeds. </span></p><p class="c1"><span class="c5">Figure 5 (a-d) show the results of SIMD-BP128 and its cascaded variants on D2. The results for the compression rate are consistent with the distributions in Fig. 3 (h-l): Combined with FOR or DICT, SIMD-BP128 always yields equal or better results than without a preprocessing, while DELTA and RLE affect the results negatively. However, the cascades with logical-level techniques decrease the speeds of the algorithm, whereby the slow-down is significant for small data elements, but becomes acceptable for large val- ues at least for DICT (decompression) and FOR. Indeed, the decompression of FOR + SIMD-BP128 is faster than SIMD-BP128 alone for means larger than 2</span><span class="c20">16</span><span class="c5">. A compar- </span></p><p class="c1"><span class="c5">ison of all 25 algorithms can be found in Fig. 5 (e-h) and (i-l) for means of 2</span><span class="c20">6 </span><span class="c5">respectively 2</span><span class="c20">31</span><span class="c5">.</span><span class="c20">5 </span><span class="c5">For the small mean, the cascades with RLE and DELTA achieve the worst com- pression rates, while for DICT, FOR and stand-alone NS, the algorithms are roughly grouped by the employed NS al- gorithm, since DICT and FOR do not change the distribu- tions for the considered mean (see Fig. 3 (h-l)). Regarding the speeds, the top ranks are held by stand-alone NS algo- rithms. When changing the mean to 2</span><span class="c20">31</span><span class="c5">, the cascades with FOR and DICT achieve by far the best compression rates. Stand-alone NS algorithms are still among the top ranks for the speeds. However, none of them achieves an actual size reduction. While depending on the application, many trade-offs between compression rate and speed could be rea- sonable, it generally does not make sense, to accept com- pression rates of more than 32 bits/int, since then, the data could rather be copied or not touched at all, which would be even faster. Keeping this in mind, the cascades with FOR achieve the best results regarding all three speeds, whereby DELTA also makes it into the top-3 for the compression. </span></p><p class="c1"><span class="c5">Figure 6 shows the results on D5. The cascades of any logical-level technique and SIMD-BP128 achieve better com- pression rates than the stand-alone SIMD-BP128 from some run length on (Fig. 6 (a)). Regarding the (de)compression speeds, only RLE + SIMD-BP128 can yield an improve- ment, if the run length exceeds 2</span><span class="c20">5</span><span class="c5">. It is noteworthy that the cascades with DELTA and FOR imply only a slight slow down, while they achieve much better compression rates. The aggregation speed of RLE + SIMD-BP128 gets out of scope for any other cascade for run lengths above 2</span><span class="c20">8</span><span class="c5">, since the aggregation of RLE has to execute only one multiplica- tion and one addition per run. The next three rows of Fig. 6 compare all cascades for average run lengths of 6, 37, and 517. Even for the lowest of these run lengths (Fig. 6 (e-h)), </span></p><p class="c0"><span class="c25">5 </span><span class="c12">The bars in these diagrams are sorted, such that the best algorithm is at the left. We use the color to encode the NS algorithm and the hatch to encode the logical-level tech- nique, whereby (none) means a stand-alone NS algorithm. Furthermore, bars with an X on top mark algorithms which do not achieve a size reduction on the dataset, i.e., require at least 32 bits per integer. </span></p><p class="c0"><span class="c17">80 </span></p><p class="c0"><span class="c4">(a) compression rate [bits/int] </span></p><p class="c0"><span class="c4">2000 2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span></p><p class="c15"><span class="c4">mean </span><span class="c16">(b) compression speed [mis] </span></p><p class="c0"><span class="c4">2500 </span></p><p class="c0"><span class="c4">(c) decompression speed [mis] </span></p><p class="c0"><span class="c16">6K (d) aggregation speed [mis] 5K4K3K2K1K</span><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span></p><p class="c0"><span class="c4">02</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span><span class="c4">mean </span></p><p class="c0"><span class="c4">mean </span></p><p class="c0"><span class="c4">SIMD-BP128 DELTA + SIMD-BP128 FOR + SIMD-BP128 RLE + SIMD-BP128 DICT + SIMD-BP128 </span></p><p class="c0"><span class="c4">0</span><span class="c16">816243240485664 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span></p><p class="c0"><span class="c4">mean </span></p><p class="c0"><span class="c6">0</span><span class="c13">8162432 X X </span><span class="c16">(e) compression rate [bits/int] </span></p><p class="c0"><span class="c6">0</span><span class="c13">16324864 </span></p><p class="c0"><span class="c6">XXXXXX </span><span class="c13">X X XX </span></p><p class="c0"><span class="c4">(none) </span></p><p class="c0"><span class="c4">(i) compression rate [bits/int] </span></p><p class="c0"><span class="c4">DELTA </span></p><p class="c0"><span class="c4">FOR </span></p><p class="c0"><span class="c6">X </span><span class="c13">X </span></p><p class="c0"><span class="c4">(f) compression speed [mis] </span></p><p class="c0"><span class="c6">2500 </span></p><p class="c0"><span class="c6">2000 </span></p><p class="c0"><span class="c6">1500 </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c6">X </span></p><p class="c0"><span class="c6">X </span></p><p class="c0"><span class="c4">(j) compression speed [mis] </span></p><p class="c0"><span class="c6">1250 </span><span class="c13">X </span><span class="c6">X 1000 </span></p><p class="c0"><span class="c6">X </span></p><p class="c0"><span class="c6">X </span></p><p class="c0"><span class="c6">X </span></p><p class="c0"><span class="c6">X X </span><span class="c31">XXX X X </span></p><p class="c0"><span class="c6">500 </span></p><p class="c0"><span class="c6">750 </span></p><p class="c0"><span class="c6">500 </span></p><p class="c0"><span class="c6">250 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c4">RLE </span></p><p class="c0"><span class="c4">(g) decompression speed [mis] </span></p><p class="c0"><span class="c6">2500 </span></p><p class="c0"><span class="c6">2000 </span></p><p class="c0"><span class="c6">1500 </span></p><p class="c0"><span class="c6">X </span><span class="c31">X </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c4">(k) decompression speed [mis] </span></p><p class="c0"><span class="c6">2000 1500 </span></p><p class="c0"><span class="c31">X X X X X X </span><span class="c6">1000 </span></p><p class="c0"><span class="c6">XXXX </span><span class="c31">X X </span></p><p class="c0"><span class="c6">500 </span></p><p class="c0"><span class="c6">500 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c4">(h) aggregation speed [mis] </span></p><p class="c0"><span class="c6">0</span><span class="c13">1K2K3K4K5K </span></p><p class="c0"><span class="c6">X </span><span class="c31">X </span></p><p class="c0"><span class="c4">(l) aggregation speed [mis] </span></p><p class="c0"><span class="c6">0</span><span class="c13">1K2K3K4K </span><span class="c6">X </span><span class="c31">X </span></p><p class="c0"><span class="c6">X </span><span class="c31">X X </span></p><p class="c0"><span class="c6">X </span></p><p class="c0"><span class="c6">X </span><span class="c31">X X X X X </span></p><p class="c0"><span class="c4">DICT SIMD-BP128 SIMD-FastPFOR 4-Wise NS Masked-VByte SIMD-GroupSimple </span></p><p class="c0"><span class="c5">Figure 6: Comparison of the cascades on dataset D5.</span><span class="c20">5 </span></p><p class="c1"><span class="c5">the cascades with RLE yield by far the best compression rates, while those with DELTA are among the last ranks. However, the (de)compression speeds of the cascades with RLE are not competitive to those of the best stand-alone NS algorithms. On the other hand, RLE + SIMD-BP128 has the best aggregation speed. As the run lengths get a little higher (Fig. 6 (i-l)), the cascades with RLE move further to- wards the top-ranks of the speeds and further improve their compression rates. Interestingly, the compression rates of the cascades with DELTA do now achieve the best compres- sion rates after the cascades with RLE, except for DELTA + SIMD-BP128, which still yields the worst compression rate. When the run length is increased further (Fig. 6 (m-p)), these trends continue and the cascades with RLE do now dominate both, the compression rate and all three speeds. </span></p><p class="c1"><span class="c5">Figure 7 (a-d) report the results of SIMD-BP128 and its cascades on D6 subject to the number of distinct data ele- ments. Since D6 is sorted, a low number of distinct values is equal to a high average run length. Consequently, RLE + SIMD-BP128 achieves a better compression rate than stand- alone SIMD-BP128 until the number of distinct values comes close to the total number of values, i.e. 100 M. Although the possible minimum value is zero, FOR + SIMD-BP128 also improves the compression rate. This is due to the fact that within each input block of the cascade, the value range is small as the data is sorted. Apart from that, especially the decompression speed is interesting. For low numbers of dis- tinct values and thus long runs, SIMD-BP128 and its cascade with RLE are nearly equally fast. As the number of distinct values increases, SIMD-BP128 is affected stronger than RLE + SIMD-BP128. However, when the number of distinct val- ues exceeds 2</span><span class="c20">21</span><span class="c5">, the performance of the cascade with RLE deteriorates and from this point on, the cascade with FOR, respectively DELTA is the fastest algorithm. Note that in this case, the decompression of the stand-alone SIMD-BP128 is never the fastest alternative. Figure 7 (e-h) show the com- </span></p><p class="c1"><span class="c5">parison of all 25 algorithms when the dataset contains 128 distinct values. Since the average run length is very high (nearly 800k), the cascades including RLE are the best re- garding both, compression rate and speeds. The extreme case of unique data elements, i.e., 100 M distinct values, is given in Fig. 7 (i-l). Now the cascades of RLE are among the worst algorithms for all four measured variables, since the data contains no runs. The best compression rates are now achieved by the cascades of DELTA, since the data is sorted. While the fastest compressor is stand-alone SIMD-BP128, the next ranks are held by cascades making use of DELTA. Regarding the decompression speed, the top-3 algorithms use SIMD-BP128 for the NS-part and DELTA, FOR, or no preprocessing. In terms of the aggregation speed, the stand- alone NS algorithms SIMD-BP128 and SIMD-FastPFOR are the fastest. However, DELTA + SIMD-BP128 and FOR + SIMD-BP128 also achieve very good aggregation speeds, but much better compression rates. </span></p><p class="c0"><span class="c5">Summing up, the changes to the data distributions achieved by the logical-level techniques do indeed propagate to the compression rates of their cascades with NS. Furthermore, the speeds of the cascades can even exceed those of the cor- responding stand-alone NS algorithms. This is especially true for the cascades including RLE, if the data contains long enough runs. Cascades with the other three logical- level techniques generally lead to less significant speed ups or even slow downs, whereby these often come with an im- provement of the compression rate. Finally, if the logical- level technique is fixed, its cascades with different NS algo- rithms can lead to significantly different results regarding compression rate and speed. This justifies the consideration of multiple different NS algorithms even in cascades. </span></p><p class="c0"><span class="c7">5.4 Lessons Learned </span></p><p class="c0"><span class="c5">In order to employ lightweight compression effectively, it is desirable to know which algorithm is most suitable for a </span></p><p class="c0"><span class="c17">81 </span></p><p class="c0"><span class="c4">(a) compression rate [bits/int] </span></p><p class="c0"><span class="c4">(b) compression speed [mis] </span></p><p class="c0"><span class="c4">3000 </span><span class="c16">(c) decompression speed [mis] </span></p><p class="c0"><span class="c4">200K </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span></p><p class="c0"><span class="c4">avg. run length </span></p><p class="c0"><span class="c4">(d) aggregation speed [mis] 3500 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">avg. run length </span></p><p class="c0"><span class="c4">avg. run length </span></p><p class="c0"><span class="c4">SIMD-BP128 DELTA + SIMD-BP128 FOR + SIMD-BP128 RLE + SIMD-BP128 DICT + SIMD-BP128 </span></p><p class="c0"><span class="c4">0</span><span class="c16">8162432 </span></p><p class="c0"><span class="c4">2500 </span></p><p class="c0"><span class="c4">3000 </span></p><p class="c0"><span class="c4">150K </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">2500 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">100K </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">50K </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span></p><p class="c0"><span class="c4">avg. run length </span></p><p class="c0"><span class="c4">(e) compression rate [bits/int] </span></p><p class="c0"><span class="c6">0</span><span class="c13">8162432 X </span></p><p class="c0"><span class="c6">0</span><span class="c13">8162432 </span></p><p class="c0"><span class="c6">0</span><span class="c13">81624 </span></p><p class="c0"><span class="c4">(none) </span></p><p class="c0"><span class="c4">(m) compression rate [bits/int] </span></p><p class="c0"><span class="c4">(i) compression rate [bits/int] </span></p><p class="c0"><span class="c4">DELTA </span></p><p class="c0"><span class="c4">FOR </span></p><p class="c0"><span class="c4">(f) compression speed [mis] </span></p><p class="c0"><span class="c6">1500 </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c6">X </span></p><p class="c0"><span class="c6">1500 </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c6">3000 </span></p><p class="c0"><span class="c6">2000 </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c6">500 </span></p><p class="c0"><span class="c6">500 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c4">RLE </span></p><p class="c0"><span class="c4">(n) compression speed [mis] </span></p><p class="c0"><span class="c4">(j) compression speed [mis] </span></p><p class="c0"><span class="c4">(g) decompression speed [mis] </span></p><p class="c1"><span class="c6">2500 2000 1500 </span></p><p class="c0"><span class="c6">X </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c6">2500 </span></p><p class="c0"><span class="c6">2000 </span></p><p class="c0"><span class="c6">1500 </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c6">2500 </span></p><p class="c0"><span class="c6">2000 </span></p><p class="c0"><span class="c6">1500 </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c6">500 </span></p><p class="c0"><span class="c6">500 </span></p><p class="c0"><span class="c6">500 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c4">(o) decompression speed [mis] </span></p><p class="c0"><span class="c4">(k) decompression speed [mis] </span></p><p class="c0"><span class="c6">250K </span></p><p class="c0"><span class="c6">200K </span></p><p class="c0"><span class="c6">150K </span></p><p class="c0"><span class="c6">100K </span></p><p class="c0"><span class="c6">30K </span></p><p class="c0"><span class="c6">20K </span></p><p class="c0"><span class="c6">10K </span></p><p class="c0"><span class="c6">50K </span></p><p class="c0"><span class="c4">(h) aggregation speed [mis] </span></p><p class="c0"><span class="c6">0</span><span class="c13">1K2K3K4K5K6K </span></p><p class="c0"><span class="c6">X </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c4">(p) aggregation speed [mis] </span></p><p class="c0"><span class="c4">(l) aggregation speed [mis] </span></p><p class="c0"><span class="c4">DICT SIMD-BP128 SIMD-FastPFOR 4-Wise NS Masked-VByte SIMD-GroupSimple </span></p><p class="c0"><span class="c5">Figure 7: Comparison of the cascades on dataset D6.</span><span class="c20">5 </span></p><p class="c1"><span class="c5">given data set w.r.t. a certain optimization goal as, e.g., the best compression rate, the highest (de)compression speed, or a combination thereof. Regarding the compression tech- niques, we can observe some general trends. For instance, NS usually performs the better, the lower the values are, while RLE profits from long runs, and DICT from few dis- tinct values. However, these facts can be derived from the ideas of the techniques and have already been shown ex- perimentally by other authors, e.g. in [1]. What is more interesting is the level of the compression algorithms. While SIMD-BP128 seems to be a good choice regardless of the op- timization goal if the data exhibits a good locality, the case is more complicated for data with a low locality. What makes a decision even more complex is that the performance of some NS algorithms is not monotonic in the size of the values. This holds, e.g., for the word-aligned NS algorithms (Fig. 1 (i-l)) as well as Masked-VByte (Fig. 2, second column). </span></p><p class="c1"><span class="c5">Moreover, lightweight data compression is still a hot re- search topic. Hence, more algorithms will be published in the future. Therefore, an automatic approach for choosing the best out of a set of algorithms would be welcome. It is self-evident that the na &#776;&#305;ve solution of first executing all con- sidered algorithms on the exact data to be compressed and then choosing the best algorithm is infeasible for efficiency reasons. Instead, a model of the algorithms&rsquo; compression rate and performance &ndash; subject to the data properties &ndash; could be used. While we believe that such a model could be built based on our systematic evaluation, we see the main contribution of this paper in illustrating that this decision is non-trivial and that, thus, further research in the direction of an automatic selection is necessary. However, since this is beyond the scope of this paper, we leave it for future work. </span></p><p class="c0"><span class="c7">6. EXPERIMENTS ON REAL DATA </span></p><p class="c0"><span class="c5">To complement our experiments on synthetic data, we evaluated the algorithms considered in Section 5.3 on a dataset of postings lists of the real-world document collection GOV2</span><span class="c20">6</span><span class="c5">, which is frequently used to evaluate integer compression al- </span></p><p class="c0"><span class="c25">6</span><span class="c12">This data set of postings lists is provided by Lemire et al. at </span><span class="c44">http://lemire.me/data/integercompression2014.html</span><span class="c12">. </span></p><p class="c1"><span class="c5">gorithms [12, 18, 20]. GOV2 is a corpus of 25 M documents found in a crawl of the </span><span class="c8">.gov </span><span class="c5">websites. The dataset contains about 1.1 M postings lists in total, each of which is a sorted array of unique 32-bit document ids. We discarded all lists containing less than 8192 integers, since time measurements are not reliable enough on too short arrays. </span></p><p class="c30"><span class="c5">Figure 8 (a-d) compare SIMD-BP128 to its cascaded deriva- tives subject to the list length. Note that, as the total num- ber of entries in the lists increases, so does the number of distinct entries (uniqueness), while the average difference of two subsequent entries decreases. The compression rate of RLE is noncompetitive to the other algorithms, since unique values imply the absence of runs. Employing any other logical-level technique can yield an improvement of the com- pression rate, whereby DELTA and FOR get better as the lists get longer. Regarding the compression and aggregation speed, pure SIMD-BP128 is the fastest for all list lengths. However, its cascades with DELTA respectively FOR (only aggregation) are not much slower for long lists. Regarding the decompression, using DELTA or FOR yields better re- sults than pure NS for lists longer than 2</span><span class="c20">19 </span><span class="c5">respectively 2</span><span class="c20">20</span><span class="c5">. Figure 8 (e-h) provide the rankings of all 25 algorithms. The reported measurements are averages over all lists lengths weighted by the actual distribution of the lengths in the dataset. The cascades with DELTA yield the best compres- sion rates. The fastest compressors are SIMD-BP128 and 4-Wise NS followed by their cascades with DELTA. Regard- ing the decompression and aggregation speeds, the top-4 are stand-alone NS algorithms, which are followed by cascades with DELTA or FOR. </span></p><p class="c0"><span class="c7">7. CONCLUSION </span></p><p class="c1"><span class="c5">Lightweight data compression is heavily employed by mod- ern in-memory column-stores in order to compensate for the low main memory bandwidth. In recent years, the corpus of available compression algorithms has significantly grown, mainly due to the use of SIMD extensions. In our experi- mental survey, we systematically evaluated recent vectorized algorithms of all five basic techniques of lightweight compres- sion as well as cascades thereof on a multitude of synthetic </span></p><p class="c0"><span class="c17">82 </span></p><p class="c0"><span class="c4">(a) compression rate [bits/int] </span></p><p class="c0"><span class="c4">(b) compression speed [mis] </span></p><p class="c0"><span class="c4">3000 </span><span class="c16">(c) decompression speed [mis] </span></p><p class="c0"><span class="c4">(d) aggregation speed [mis] </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span><span class="c4">2</span><span class="c3">32 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span><span class="c4">2</span><span class="c3">32 </span></p><p class="c0"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span><span class="c4">2</span><span class="c3">32 </span><span class="c4"># distinct data elements </span></p><p class="c0"><span class="c4"># distinct data elements </span></p><p class="c0"><span class="c4"># distinct data elements </span></p><p class="c0"><span class="c4">SIMD-BP128 DELTA + SIMD-BP128 FOR + SIMD-BP128 RLE + SIMD-BP128 DICT + SIMD-BP128 </span></p><p class="c0"><span class="c4">0</span><span class="c16">816243240485664 </span></p><p class="c0"><span class="c4">3500 </span></p><p class="c0"><span class="c4">200K </span></p><p class="c0"><span class="c4">2500 </span></p><p class="c0"><span class="c4">3000 </span></p><p class="c0"><span class="c4">2000 2500 </span></p><p class="c0"><span class="c4">150K </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">100K </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">50K </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c15"><span class="c4">2</span><span class="c3">0 </span><span class="c4">2</span><span class="c3">4 </span><span class="c4">2</span><span class="c3">8 </span><span class="c4">2</span><span class="c3">12 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">2</span><span class="c3">28 </span><span class="c4">2</span><span class="c3">32 </span><span class="c4"># distinct data elements </span></p><p class="c0"><span class="c6">0</span><span class="c13">24681012 </span></p><p class="c0"><span class="c6">0</span><span class="c13">16324864 </span></p><p class="c0"><span class="c6">XXXX </span><span class="c13">X X X X </span></p><p class="c0"><span class="c4">(none) </span></p><p class="c0"><span class="c4">(e) compression rate [bits/int] </span></p><p class="c0"><span class="c6">X </span><span class="c13">X </span><span class="c16">(i) compression rate [bits/int] </span></p><p class="c0"><span class="c4">DELTA </span></p><p class="c0"><span class="c4">FOR </span></p><p class="c0"><span class="c6">3000 </span></p><p class="c0"><span class="c6">2000 </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c4">(j) compression speed [mis] </span></p><p class="c0"><span class="c6">1250 </span><span class="c13">X </span><span class="c6">1000 750 500 </span></p><p class="c0"><span class="c6">X 250 </span></p><p class="c0"><span class="c6">X </span><span class="c31">XXXX X X X </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c4">RLE </span></p><p class="c0"><span class="c4">(f) compression speed [mis] </span></p><p class="c0"><span class="c6">3000 </span></p><p class="c0"><span class="c6">2500 </span></p><p class="c0"><span class="c6">2000 </span></p><p class="c0"><span class="c6">1500 </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c4">(k) decompression speed [mis] </span></p><p class="c0"><span class="c6">2000 </span></p><p class="c0"><span class="c6">1500 </span></p><p class="c0"><span class="c6">X </span><span class="c31">X </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c6">X </span><span class="c31">X </span></p><p class="c0"><span class="c6">X</span><span class="c31">XX X X X </span></p><p class="c0"><span class="c6">500 </span></p><p class="c0"><span class="c6">500 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c4">(g) decompression speed [mis] </span></p><p class="c0"><span class="c6">400K </span></p><p class="c0"><span class="c6">300K </span></p><p class="c0"><span class="c6">200K </span></p><p class="c0"><span class="c6">100K </span></p><p class="c0"><span class="c4">(l) aggregation speed [mis] </span></p><p class="c0"><span class="c6">0</span><span class="c13">1K2K3K </span><span class="c6">X </span></p><p class="c0"><span class="c6">X </span></p><p class="c0"><span class="c6">X </span><span class="c31">X X X X X X X </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c4">(h) aggregation speed [mis] </span></p><p class="c0"><span class="c4">DICT SIMD-BP128 SIMD-FastPFOR 4-Wise NS Masked-VByte SIMD-GroupSimple </span></p><p class="c0"><span class="c5">Figure 8: Comparison of the cascades on the postings lists of the real-world document collection GOV2.</span><span class="c20">5 </span></p><p class="c1"><span class="c5">and one real data set. We have shown that there is no single- best algorithm suitable for all data sets. Instead, making the right choice is non-trivial and always depends on data properties such as value distributions, run lengths, sorting, and the number of distinct data elements. Furthermore, the best algorithm regarding the compression rate is often not the best regarding the (de)compression speed, such that a trade-off must be defined. Even the various null suppression algorithms show significantly different behavior depending on the data distribution. Finally, cascades of two techniques can heavily improve the compression rate, which comes at the cost of a lower speed in some, but not all cases. </span></p><p class="c0"><span class="c7">Acknowledgments </span><span class="c5">This work was partly funded by the German Research Foun- dation (DFG) in the context of the project &rdquo;Lightweight Compression Techniques for the Optimization of Complex Database Queries&rdquo; (LE-1416/26-1). </span></p><p class="c0"><span class="c7">8. REFERENCES </span><span class="c12">[1] D. J. Abadi, S. Madden, and M. Ferreira. Integrating </span></p><p class="c0"><span class="c5">compression and execution in column-oriented database systems. In SIGMOD, 2006. [2] V. N. Anh and A. Moffat. Index compression using 64-bit words. Softw., Pract. Exper., 40(2), 2010. [3] D. Arroyuelo, S. Gonz&aacute;lez, M. Oyarz&uacute;n, and </span></p><p class="c0"><span class="c5">V. Sepulveda. Document identifier reassignment and run-length-compressed inverted indexes for improved search performance. In SIGIR, 2013. [4] P. A. Boncz, M. L. Kersten, and S. Manegold. </span></p><p class="c0"><span class="c5">Breaking the memory wall in monetdb. Commun. ACM, 51(12), 2008. [5] P. A. Boncz, M. Zukowski, and N. Nes. </span></p><p class="c0"><span class="c5">Monetdb/x100: Hyper-pipelining query execution. In CIDR, 2005. [6] G. P. Copeland and S. N. Khoshafian. A </span></p><p class="c0"><span class="c5">decomposition storage model. SIGMOD Rec., 14(4), 1985. [7] P. Damme, D. Habich, and W. Lehner. A benchmark </span></p><p class="c0"><span class="c5">framework for data compression techniques. In TPCTC, 2015. [8] A. Elgohary, M. Boehm, P. J. Haas, F. R. Reiss, and </span></p><p class="c0"><span class="c5">B. Reinwald. Compressed linear algebra for large-scale machine learning. PVLDB, 9(12), 2016. </span></p><p class="c0"><span class="c5">[9] J. Goldstein, R. Ramakrishnan, and U. Shaft. </span></p><p class="c0"><span class="c5">Compressing relations and indexes. In ICDE, 1998. [10] D. A. Huffman. A method for the construction of </span></p><p class="c15"><span class="c5">minimum-redundancy codes. Proceedings of the Institute of Radio Engineers, 40(9), 1952. [11] T. Kissinger, T. Kiefer, B. Schlegel, D. Habich, </span></p><p class="c0"><span class="c5">D. Molka, and W. Lehner. ERIS: A numa-aware in-memory storage engine for analytical workloads. In ADMS, 2014. [12] D. Lemire and L. Boytsov. Decoding billions of </span></p><p class="c0"><span class="c5">integers per second through vectorization. Softw., Pract. Exper., 45(1), 2015. [13] D. Lemire, L. Boytsov, O. Kaser, M. Caron, </span></p><p class="c0"><span class="c5">L. Dionne, M. Lemay, E. Kruus, A. Bedini, M. Petri, and R. B. Araujo. The FastPFOR C++ library: Fast integer compression, </span><span class="c8">https://github.com/lemire/FastPFOR</span><span class="c5">. [14] J. Plaisance, N. Kurz, and D. Lemire. Vectorized </span></p><p class="c0"><span class="c5">vbyte decoding. CoRR, abs/1503.07387, 2015. [15] M. A. Roth and S. J. Van Horn. Database </span></p><p class="c15"><span class="c5">compression. SIGMOD Rec., 22(3), 1993. [16] B. Schlegel, R. Gemulla, and W. Lehner. Fast integer </span></p><p class="c0"><span class="c5">compression using SIMD instructions. In DaMoN, 2010. [17] F. Silvestri and R. Venturini. Vsencoding: Efficient </span></p><p class="c0"><span class="c5">coding and fast decoding of integer lists via dynamic programming. In CIKM, 2010. [18] A. A. Stepanov, A. R. Gangolli, D. E. Rose, R. J. </span></p><p class="c0"><span class="c5">Ernst, and P. S. Oberoi. Simd-based decoding of posting lists. In CIKM, 2011. [19] I. H. Witten, R. M. Neal, and J. G. Cleary. Arithmetic </span></p><p class="c0"><span class="c5">coding for data compression. Commun. ACM, 30(6), 1987. [20] W. X. Zhao, X. Zhang, D. Lemire, D. Shan, J. Nie, </span></p><p class="c0"><span class="c5">H. Yan, and J. Wen. A general simd-based approach to accelerating compression algorithms. ACM Trans. Inf. Syst., 33(3), 2015. [21] J. Ziv and A. Lempel. A universal algorithm for </span></p><p class="c0"><span class="c5">sequential data compression. IEEE Trans. Inf. Theor., 23(3), 1977. [22] M. Zukowski, S. H&eacute;man, N. Nes, and P. A. Boncz. </span></p><p class="c0"><span class="c5">Super-scalar RAM-CPU cache compression. In ICDE, 2006. </span></p><p class="c0"><span class="c17">83 </span></p><p class="c0"><span class="c4">(a) compression rate [bits/int] </span></p><p class="c0"><span class="c4">(b) compression speed [mis] </span></p><p class="c0"><span class="c4">2500 </span><span class="c16">(c) decompression speed [mis] </span></p><p class="c0"><span class="c4">(d) aggregation speed [mis] 5000 </span></p><p class="c0"><span class="c4">2</span><span class="c3">13 </span><span class="c4">2</span><span class="c3">14 </span><span class="c4">2</span><span class="c3">15 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">17 </span><span class="c4">2</span><span class="c3">18 </span><span class="c4">2</span><span class="c3">19 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">21 </span><span class="c4">2</span><span class="c3">22 </span><span class="c4">2</span><span class="c3">23 </span><span class="c4">2</span><span class="c3">24 </span></p><p class="c0"><span class="c4">2</span><span class="c3">13 </span><span class="c4">2</span><span class="c3">14 </span><span class="c4">2</span><span class="c3">15 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">17 </span><span class="c4">2</span><span class="c3">18 </span><span class="c4">2</span><span class="c3">19 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">21 </span><span class="c4">2</span><span class="c3">22 </span><span class="c4">2</span><span class="c3">23 </span><span class="c4">2</span><span class="c3">24 </span></p><p class="c0"><span class="c4">2</span><span class="c3">13 </span><span class="c4">2</span><span class="c3">14 </span><span class="c4">2</span><span class="c3">15 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">17 </span><span class="c4">2</span><span class="c3">18 </span><span class="c4">2</span><span class="c3">19 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">21 </span><span class="c4">2</span><span class="c3">22 </span><span class="c4">2</span><span class="c3">23 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">list length </span></p><p class="c0"><span class="c4">list length </span></p><p class="c0"><span class="c4">list length </span></p><p class="c0"><span class="c4">SIMD-BP128 DELTA + SIMD-BP128 FOR + SIMD-BP128 RLE + SIMD-BP128 DICT + SIMD-BP128 </span></p><p class="c0"><span class="c4">0</span><span class="c16">81624324048 </span></p><p class="c0"><span class="c4">1500 1250 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">4000 </span></p><p class="c0"><span class="c4">1500 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">3000 </span></p><p class="c0"><span class="c4">750 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">2000 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">500 </span></p><p class="c0"><span class="c4">1000 </span></p><p class="c0"><span class="c4">250 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c0"><span class="c4">0 </span></p><p class="c15"><span class="c4">2</span><span class="c3">13 </span><span class="c4">2</span><span class="c3">14 </span><span class="c4">2</span><span class="c3">15 </span><span class="c4">2</span><span class="c3">16 </span><span class="c4">2</span><span class="c3">17 </span><span class="c4">2</span><span class="c3">18 </span><span class="c4">2</span><span class="c3">19 </span><span class="c4">2</span><span class="c3">20 </span><span class="c4">2</span><span class="c3">21 </span><span class="c4">2</span><span class="c3">22 </span><span class="c4">2</span><span class="c3">23 </span><span class="c4">2</span><span class="c3">24 </span><span class="c4">list length </span></p><p class="c0"><span class="c6">0</span><span class="c13">16324864 </span></p><p class="c0"><span class="c6">X X </span><span class="c13">X X </span></p><p class="c0"><span class="c4">(none) </span></p><p class="c0"><span class="c4">(e) compression rate [bits/int] </span></p><p class="c0"><span class="c4">DELTA </span></p><p class="c0"><span class="c4">FOR </span></p><p class="c0"><span class="c6">X </span></p><p class="c0"><span class="c6">X </span></p><p class="c0"><span class="c4">(f) compression speed [mis] </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c6">800 </span></p><p class="c0"><span class="c6">600 </span></p><p class="c0"><span class="c6">400 </span></p><p class="c0"><span class="c6">200 </span></p><p class="c0"><span class="c6">X </span><span class="c31">XXX X X </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c4">RLE </span></p><p class="c0"><span class="c4">(g) decompression speed [mis] </span></p><p class="c0"><span class="c6">1400 X 1200 1000 800 600 400 200 </span></p><p class="c0"><span class="c6">XX</span><span class="c31">X X X </span></p><p class="c0"><span class="c6">0 </span></p><p class="c0"><span class="c4">(h) aggregation speed [mis] </span></p><p class="c0"><span class="c6">1500 X </span></p><p class="c0"><span class="c6">1000 </span></p><p class="c0"><span class="c6">500 </span></p><p class="c0"><span class="c6">X </span><span class="c31">X X </span></p><p class="c0"><span class="c6">X </span><span class="c31">X </span></p><p class="c0"><span class="c6">0 </span></p></body></html>