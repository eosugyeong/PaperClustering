<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c42{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Courier New";font-style:normal}.c30{margin-left:-28.1pt;padding-top:3.8pt;text-indent:37pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-14pt}.c55{margin-left:-19pt;padding-top:1.4pt;text-indent:27.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-23pt}.c53{color:#000000;font-weight:700;text-decoration:none;vertical-align:sub;font-size:13.2pt;font-family:"Arial";font-style:normal}.c57{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:4.4pt;font-family:"Arial";font-style:normal}.c25{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Courier New";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c69{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c18{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:7.3pt;font-family:"Arial";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4pt;font-family:"Arial";font-style:normal}.c38{margin-left:-28.1pt;padding-top:1.7pt;text-indent:37pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-14pt}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:normal}.c45{margin-left:-28.1pt;padding-top:16.3pt;text-indent:37pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-14pt}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.3pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c29{margin-left:-19pt;padding-top:1.4pt;text-indent:37.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-21.8pt}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:7.3pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c14{color:#595959;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.5pt;font-family:"Arial";font-style:normal}.c78{margin-left:-28.1pt;padding-top:1.7pt;text-indent:37pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-15.2pt}.c52{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.9pt;font-family:"Arial";font-style:normal}.c2{color:#595959;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.1pt;font-family:"Arial";font-style:normal}.c39{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c82{margin-left:-28.1pt;padding-top:6.5pt;text-indent:51.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-14pt}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.4pt;font-family:"Arial";font-style:normal}.c67{margin-left:-143.6pt;padding-top:4.6pt;text-indent:266.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:336.2pt}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c63{margin-left:-14.5pt;padding-top:1.4pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-14.4pt}.c56{margin-left:-19pt;padding-top:4.1pt;text-indent:27.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-33.6pt}.c50{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c32{margin-left:-28.1pt;padding-top:1.4pt;text-indent:37pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-14pt}.c41{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.3pt;font-family:"Courier New";font-style:normal}.c37{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Courier New";font-style:normal}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c27{margin-left:-19pt;padding-top:1.7pt;text-indent:27.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-23pt}.c17{margin-left:-147.9pt;padding-top:36.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:345.1pt}.c68{margin-left:-19pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:6pt}.c74{margin-left:218.2pt;padding-top:661.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c59{margin-left:-14.5pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19pt}.c21{margin-left:-81.4pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:263.8pt}.c58{margin-left:-14.5pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:8.6pt}.c44{margin-left:-28.1pt;padding-top:13pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-6.6pt}.c64{margin-left:-14.5pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-22.8pt}.c73{margin-left:-19pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:14.6pt}.c61{margin-left:218.2pt;padding-top:30.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c75{margin-left:-19pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18.7pt}.c48{margin-left:-14.5pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.8pt}.c72{margin-left:-0.3pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.7pt}.c31{margin-left:-19pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-23pt}.c46{margin-left:-19pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:110.9pt}.c81{margin-left:-19pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-17pt}.c70{margin-left:-8.2pt;padding-top:4.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:201pt}.c76{margin-left:-19pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.4pt}.c83{margin-left:-14.5pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-9.8pt}.c80{margin-left:-14.5pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-13.9pt}.c22{margin-left:-200.2pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:382.6pt}.c65{padding-top:11.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c35{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c71{padding-top:3.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c33{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c84{padding-top:3.4pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c40{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c79{margin-left:-14.5pt;text-indent:28.6pt;margin-right:-13.9pt}.c47{margin-left:-19pt;text-indent:27.9pt;margin-right:-22.8pt}.c11{margin-left:-143.6pt;margin-right:298.1pt}.c66{margin-left:-8.2pt;margin-right:162.9pt}.c62{margin-left:-145.8pt;margin-right:336.2pt}.c77{margin-left:-19pt;margin-right:-22.8pt}.c54{margin-left:-99pt;margin-right:281.3pt}.c60{margin-left:-181.8pt;margin-right:366.2pt}.c51{margin-left:-19pt;margin-right:63.6pt}.c24{margin-left:-19pt;margin-right:115pt}.c36{margin-left:-63pt;margin-right:247.4pt}.c43{margin-left:-217.5pt;margin-right:400.1pt}.c49{margin-left:-10.6pt;margin-right:201pt}.c10{margin-left:-14.5pt;margin-right:2.6pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c40"><p class="c33"><span class="c39">Reaching a desired set of users via different paths: an online advertising technique on a micro-blogging platform </span></p><p class="c6"><span class="c5">Milad Eftekhar </span><span class="c50">Department of Computer Science University of Toronto </span><span class="c26">milad@cs.toronto.edu </span><span class="c50">Toronto, ON, Canada </span><span class="c5">Nick Koudas </span><span class="c50">Department of Computer Science University of Toronto </span><span class="c26">koudas@cs.toronto.edu </span></p><p class="c6"><span class="c50">Toronto, ON, Canada </span><span class="c5">Yashar Ganjali </span><span class="c50">Department of Computer Science University of Toronto </span><span class="c26">yganjali@cs.toronto.edu </span></p><p class="c6"><span class="c5">ABSTRACT </span><span class="c3">Social media and micro-blogging platforms have been suc- cessful for communication and information exchange enjoy- ing vast number of user participation. Given their millions of users, it is natural that there is a lot of interest for mar- keting and advertising on these platforms as attested by the introduced advertising platforms on Twitter and Facebook. In this paper, inspired by micro-blogging advertising plat- forms, we introduce two problems to aid ad and marketing campaigns. The first problem identifies topics (called anal- ogous topics) that have approximately the same audience in a micro-blogging platform as a given query topic. The main idea is that by bidding on an analogous topic instead of the original query topic, we reach approximately the same au- dience while spending less of our budget. Then, we present algorithms to identify expert users on a given query topic and categorize these experts to finely understand their di- versified expertise. This is imperative for word of mouth marketing where individuals have to be targeted precisely. </span></p><p class="c15"><span class="c3">We evaluate our algorithms and solutions for both prob- lems on a large dataset from Twitter attesting to their effi- ciency and accuracy compared with alternate approaches. </span></p><p class="c6"><span class="c5">Categories and Subject Descriptors </span><span class="c3">H.2.8 [Database Management]: Database applications&mdash; Data mining; J.4 [Social and Behavioral Sciences]: Eco- nomics </span></p><p class="c6"><span class="c5">Keywords </span><span class="c3">Social media, Micro-blogging advertising platforms, Analo- gous topics, Alternative topics, Expert categorization </span></p><p class="c6"><span class="c5">1. INTRODUCTION </span></p><p class="c15"><span class="c3">Social media and micro-blogging have experienced expo- nential growth in user acquisition and participation over the last decade. Services such as Twitter, Facebook, and Pin- </span></p><p class="c6"><span class="c23">&copy;</span><span class="c69">2015, Copyright is with the authors. Published in Proc. 18th Interna- tional Conference on Extending Database Technology (EDBT), March 23- 27, 2015, Brussels, Belgium: ISBN 978-3-89318-067-7, on OpenProceed- ings.org. Distribution of this paper is permitted under the terms of the Cre- ative Commons license CC-by-nc-nd 4.0 . </span></p><p class="c15"><span class="c3">terest allow millions of people to share billions of content and interact on a daily basis. Social platforms are targets of sophisticated advertising and marketing, mainly because of the large number of users, and the enormous amount of time users spend on them. </span></p><p class="c15"><span class="c3">In micro-blogging platforms (e.g. Twitter), social con- nections get established by &ldquo;following&rdquo; an individual u. By establishing such a connection, you get to receive and view all posts (tweets) produced by u. The set of all posts that are visible by a user v is commonly referred to as the feed (timeline) of v. The act of following someone explicitly ex- presses interest in the information that person produces. </span></p><p class="c15"><span class="c3">Social media and micro-blogging platforms are utilized by many as important marketing vehicles. By amassing a large number of followers, an individual or a company can broad- cast messages targeted to these followers. Such messages vary depending on the type of the account (e.g., celebrity, professional, consulting, corporate) and what one wishes to achieve (e.g., brand/product awareness, sales leads, or gen- eral information dissemination). Typically, one produces in- formation in the field of one&rsquo;s expertise &ndash; which is a topic or a set of topics that one knows well, professes, or is known for as an expert in the community. For example a celebrity (say a singer) will disseminate information of interest to fans, such as tour dates, personal events and announcements, as well as new songs and albums, whereas a company, say a technology startup, shares information related to its prod- ucts, and the overall technology product space. </span></p><p class="c15"><span class="c3">Recently, new advertising platforms have been introduced [10,17]. In contrast to the keyword bidding model, as is popular in the case of search engine advertising, the micro- blogging platform takes a different approach. An advertiser selects a topic q, bids a specific dollar amount, and provides a post (known as a promoted post). The micro-blogging advertising platform identifies all the users that are inter- ested in the topic q based on some internal algorithms and inserts the promoted post in the feed of these users (explic- itly identifying it as a promoted post). The dollar amount is utilized by an auction that determines the winning bidder (for topic q). As an example, if we are interested in show- ing a promoted post to those users that are interested in music, we will bid an amount for the topic music and pro- vide our promoted post. If we win, our promoted post will be inserted in the feed of those accounts interested in topic music. Commonly the amount we bid is per impression or per engagement (i.e., per person seeing or clicking on the promoted post). </span></p><p class="c6"><span class="c7">181 10.5441/002/edbt.2015.17 </span></p><p class="c45"><span class="c3">In such a setting there are numerous opportunities for op- timization. Of immediate interest would be reaching the same or approximately the same set of people with a lower cost. For example, by bidding on the topic public relations we can be successful only if we bid a price of x. What if we knew that if we bid on the topic seo (search engine opti- mization), we can reach the same people (and thus have the same impressions) for a price y&lt;x? The first problem we outline in this paper is to produce a set of topics R analo- gous to a topic q (that we wish to bid on). These topics have the property that if we bid on one of them instead of q, our promoted post will be inserted in the feed of approximately (for a precise definition of approximate) the same people as those in the case of q. Now, by examining the associated cost of each topic, we can make a more informed decision by comparing the savings versus how many interested indi- viduals our posts will reach for each of the analogous topics in R. We propose an algorithm called IAT to address this problem (Section 3). </span></p><p class="c32"><span class="c3">Note that by advertising on a cheaper topic t 2 R instead of q, (1) (approximately) the same people see the ad and (2) expectedly same people engage with the ad. The cost we should spend in case of targeting t instead of q, therefore, would be lower (1) per impression (in cost per impression model) and (2) per engagement (in cost per engagement model). Hence by bidding on t, we reach same audience with a lower cost independent of the cost model in use. </span></p><p class="c78"><span class="c3">Utilizing this technique provides a win-win situation for advertisers and the advertising platforms (e.g., Google, Twit- ter, etc.). Adopting the technique, advertisers who are in- terested in a topic will have more options (more topics with similar audience) to target. This prevents from the existence of a very popular topic that is too expensive to target along- side some cheaper topics that no one targets. In this situa- tion, more advertisers afford to advertise. Hence the revenue of the advertising platform may significantly increase while advertisers also obtain more savings per advertisement. </span></p><p class="c38"><span class="c3">A second popular marketing activity on micro-blogging platforms is to engage experts on specific topics into word of mouth marketing campaigns. By having experts on a topic become advocates of a product or a service, all of their fol- lowers become informed about the product or service. This is a typical form of word of mouth marketing. For exam- ple, if we are interested to market a new cloud computing product by word of mouth on a social media, we can engage cloud computing experts and persuade them to adopt, use, or talk about our product. </span></p><p class="c38"><span class="c3">Finding the right advocates online is always challenging. Commonly, a user&rsquo;s account has a set of topics associated with it highlighting its expertise. Even if we have an a pri- ori knowledge of the specific topics we wish our advocates to have expertise on, it may be impossible to find one that spans all these topics. Thus, a more iterative approach is desirable. Given that we can identify all experts on a single topic q, it would be very useful if we are capable of catego- rizing those experts based on other topics of their expertise. That would enable us to examine them in a more refined fashion and identify those that are closest to our topics of interest. For example, a set of experts in both cloud comput- ing and virtualization may be more suitable for us than a set of experts in cloud computing and data centers. Being able to compute such expert groups algorithmically, given one specific topic of expertise q (cloud computing in our exam- </span></p><p class="c15 c77"><span class="c3">ple), is imperative. We have typically no knowledge of what is the &ldquo;right&rdquo; number of groups and it is expected that some experts belong to many groups. We propose an algorithm (called CTE, Section 4) to group together all experts on any given topic in a varying number of groups (corresponding to high-level topics) based on the collective topics of expertise of all these users. </span></p><p class="c55"><span class="c3">The problems discussed in this paper are inspired by social media and micro-blogging advertising platforms. Since the internal algorithms utilized by these platforms are unknown to public, we have proposed some models (e.g., expert iden- tification, topic bidding model, etc. that are explained in the next sections) and utilized them in this paper as a proof of concept. We note that these models, our assumptions, and our methods are not based on or designed for any specific social media or micro-blogging platform. </span></p><p class="c55"><span class="c3">As we have access to a large dataset from Twitter, we evaluate the algorithms on this dataset for various queries. Both IAT and CTE algorithms operate fast (a few minutes) in all experiments stressing the practicality of our develop- ments. In addition, we deploy a qualitative study demon- strating the goodness of our findings and compare our CTE algorithm with some baseline techniques (Section 5). A lit- erature review is provided in Section 6, followed by Section 7 concluding our discussion. </span></p><p class="c46"><span class="c5">2. THE TARGETS </span></p><p class="c56"><span class="c3">Different social media and micro-blogging platforms such as Twitter, Facebook, and Google+ have introduced the concept of lists (circles in case of Google+). A list is a user- defined set of accounts. Commonly, users create a list group- ing their favourite accounts on a particular topic into that list which they annotate with a descriptive title. For exam- ple, in Twitter a user may create a list with the title of &ldquo;pol- itics&rdquo; that include Twitter accounts @BarakObama, @Ange- laMerkel, @HillaryClinton, @JohnKerry, and @DavidCameron. The utility of a list is to provide quick filtering (by list title) of posts from accounts belonging in the list. It is very typical to group together accounts that profess or depict expertise on a particular topic. A user can create multiple lists and an account can belong to any number of lists. </span></p><p class="c27"><span class="c3">We utilize the infrastructure of the Peckalytics system [2] to associate with each account u, a set of topics T</span><span class="c9">u </span><span class="c3">extracted from the titles of the lists containing that account. The pro- cess of extraction includes tokenization of the title, common word (stop word) and spam filtering, entity extraction, and related word grouping via Wikipedia and WordNet. The end result is, for each account u, a set of topics that best describes the topics associated (by other users) with u. We emphasize however, that any process of mapping an account to a set of topics that best describes the account can be utilized (e.g., machine learning methods). The techniques presented herein will work fine without any modification. </span></p><p class="c27"><span class="c3">A user u 2 U is an expert on topic t 2 T, iff t 2 T</span><span class="c4">u</span><span class="c0">. This </span><span class="c3">means that (for our specific way of extracting topics) other users recognize u as an expert on topic t. We call topic t, a topic of expertise for u. The set of experts on topic t is denoted by E</span><span class="c4">t</span><span class="c0">. A user u 2 U is interested in topic t 2 T iff </span><span class="c3">the probability that u follows (reads) any content (a post, a shared video, a posted link, etc.) that is related to topic t is higher than a given threshold &theta; 2 [0,1]. For a topic t, we refer to the set of all users who are interested in t as the target set of topic t denoted by S</span><span class="c4">t</span><span class="c0">. </span></p><p class="c61"><span class="c7">182 </span></p><p class="c15"><span class="c3">Micro-blogging platforms utilize several factors (content of posts, followers, etc.) to identify the interests of users and subsequently form target sets. However, such factors are largely proprietary. In this paper, we approximate the target set of a topic t by partitioning it into two categories: (1) users interested in t who are also expert on t (E</span><span class="c4">t</span><span class="c0">) and </span><span class="c3">(2) users interested in t who are not expert on t (I</span><span class="c9">t</span><span class="c3">). In other words, users in E</span><span class="c4">t </span><span class="c0">are producers of contents related </span><span class="c3">to t, and users in I</span><span class="c9">t </span><span class="c3">are consumers of contents related to t. Thus, S</span><span class="c4">t </span><span class="c0">= E</span><span class="c4">t </span><span class="c0">[ I</span><span class="c4">t</span><span class="c0">. For any topic t, the set of experts </span><span class="c3">E</span><span class="c4">t </span><span class="c0">is available to us; i.e., E</span><span class="c4">t </span><span class="c0">= {u|t 2 T</span><span class="c4">u</span><span class="c0">}. However, the I</span><span class="c4">t </span><span class="c3">sets are unknown to us (i.e., we do not know which users are interested in a given topic t). One may suggest to retrieve the interests for each user by taking the union of expertise topics of all accounts this user follows. This approach has some drawbacks. A given user (say u) may be an expert on several topics. When another user (say v) follows u, the user v may be interested in any of these topics but not necessarily in all of them. It is not straightforward to determine which topic is of interest to v, given the topics of u&rsquo;s expertise. In section 3.1, we present an approach to resolve this issue. </span></p><p class="c6"><span class="c5">3. ANALOGOUS TOPICS </span></p><p class="c15"><span class="c3">In an advertising scenario on a social media platform, by placing a bid for a particular topic q, assuming that the bid is granted, users in S</span><span class="c4">q </span><span class="c0">will observe the promoted post on their </span><span class="c3">feeds. Naturally, an interesting question is whether there is any other topic t that is cheaper than q (i.e., it is possible for a lower bid to be granted) with a target set S</span><span class="c9">t </span><span class="c3">&ldquo;close&rdquo; to S</span><span class="c4">q</span><span class="c0">. If possible, this would reduce advertising cost. This </span><span class="c3">question is the key component of this Section. To formalize the question, we introduce some definitions. </span></p><p class="c15"><span class="c3">Definition 1. When a promoted post corresponding to a topic q is shown to a user belonging to S</span><span class="c4">t </span><span class="c0">\ S</span><span class="c4">q </span><span class="c0">(for some </span><span class="c3">topic t), we say a true impression is achieved. If the pro- moted post is shown to a user in S</span><span class="c9">t</span><span class="c3">\S</span><span class="c9">q</span><span class="c3">, we call that a false impression. Note that X\Y denotes the set di</span><span class="c23">ff</span><span class="c3">erence be- tween two arbitrary sets X and Y . As users in S</span><span class="c9">t</span><span class="c3">\S</span><span class="c9">q </span><span class="c3">are not interested in q, presenting a promoted post to them is not a desired outcome. </span></p><p class="c15"><span class="c3">Definition 2. The distance between two arbitrary sets X and Y , denoted by D(X, Y ), is the size of the symmetric dif- ference between them: D(X, Y ) = |(X\Y ) [ (Y \X)|. More- over, the distance between two topics q and t is the distance between their target sets S</span><span class="c4">q </span><span class="c0">and S</span><span class="c4">t</span><span class="c0">: i.e., D(q, t) = D(S</span><span class="c4">q</span><span class="c0">,S</span><span class="c4">t</span><span class="c0">). </span></p><p class="c15"><span class="c3">We note that a low distance between topics q and t trans- lates to a high true impression and a low false impression since D(S</span><span class="c9">q</span><span class="c3">,S</span><span class="c9">t</span><span class="c3">) = |(S</span><span class="c9">t</span><span class="c3">\S</span><span class="c9">q</span><span class="c3">)[(S</span><span class="c9">q</span><span class="c3">\S</span><span class="c9">t</span><span class="c3">)| = |S</span><span class="c9">t</span><span class="c3">\S</span><span class="c9">q</span><span class="c3">|+|S</span><span class="c9">q</span><span class="c3">| |S</span><span class="c9">t</span><span class="c3">\ S</span><span class="c4">q</span><span class="c0">|. Note that |S</span><span class="c4">q</span><span class="c0">| is a constant for a fixed query topic q. </span></p><p class="c15"><span class="c3">Definition 3. A topic t is analogous to topic q i</span><span class="c23">ff </span><span class="c3">the distance between q and t is less than a given threshold k 2 </span><span class="c23">N</span><span class="c3">; i.e., D(q, t) &lt; k. That is, t is analogous to topic q i</span><span class="c23">ff </span><span class="c3">the true impression (S</span><span class="c4">t </span><span class="c0">\ S</span><span class="c4">q</span><span class="c0">) is high while the false impression </span><span class="c3">(S</span><span class="c4">t</span><span class="c0">\S</span><span class="c4">q</span><span class="c0">) is low. </span></p><p class="c15"><span class="c3">The goal of this section is to identify a list of topics that are analogous to a query topic q. These topics are ranked subsequently based on a weight function (Equation 10) that involves both true and false impression values. If any of the analogous topics has a bidding cost lower than q, it is a potential alternative for bidding purposes. </span></p><p class="c6"><span class="c3">Problem 1. Let q be a given query topic. Identify all topics t that are analogous (Definition 3) to q. </span></p><p class="c15"><span class="c3">The solution to Problem 1 can be utilized by advertisers to instigate advertising campaigns by choosing the analogous topics instead of query topic q, target (approximately) the same set of audiences, and pay less. </span></p><p class="c6"><span class="c3">Problem 1 could be solved if the target sets for all topics were known. Unfortunately, as explained in Section 2, find- ing the targets sets is not straightforward (since the I</span><span class="c9">t </span><span class="c3">sets are unknown). To address this problem, in the rest of this section, we present an approach to identify analogous topics without calculating the exact target sets. </span><span class="c5">3.1 Properties of analogous topics </span></p><p class="c15"><span class="c3">The target set of a topic t can be partitioned into two sets: the set of experts E</span><span class="c9">t </span><span class="c3">and the set of interested users I</span><span class="c9">t</span><span class="c3">. According to Section 2, the set E</span><span class="c4">t </span><span class="c0">can be readily identified </span><span class="c3">utilizing the lists. However, I</span><span class="c4">t </span><span class="c0">is unknown. We aim to iden- </span><span class="c3">tify topics t such that I</span><span class="c9">t </span><span class="c3">and I</span><span class="c9">q </span><span class="c3">are &ldquo;close&rdquo; (for a suitable definition of close). </span></p><p class="c15"><span class="c3">We reason about approaches to identify these desired top- ics. Through this reasoning, we gain some intuition about the properties of analogous topics. Based on the discussion, we conclude this section by introducing two properties of analogous topics that enables us to identify them without calculating the I</span><span class="c9">t </span><span class="c3">sets. </span></p><p class="c15"><span class="c3">Approach I: A well-known measure of similarity between two arbitrary sets X and Y is the correlation coefficient, de- noted by &rho;(X, Y ). The correlation between two sets can be calculated utilizing the Pearson product-moment cor- relation coefficient [15]: &rho;(X, Y ) = </span><span class="c13">cov(X,Y ) </span></p><p class="c15"><span class="c9">&sigma;</span><span class="c12">X</span><span class="c9">&sigma;</span><span class="c12">Y </span><span class="c3">that is equal to </span><span class="c13">n(</span><span class="c42">P</span><span class="c1">n</span><span class="c12">i=1 </span><span class="c9">x</span><span class="c12">i</span><span class="c9">y</span><span class="c12">i</span><span class="c9">) </span><span class="c42">P</span><span class="c1">n</span><span class="c12">i=1 </span><span class="c9">x</span><span class="c12">i </span><span class="c37">P</span><span class="c12">ni=1 </span><span class="c9">y</span><span class="c12">i </span><span class="c0">p</span><span class="c4">(n </span><span class="c37">P</span><span class="c12">ni=1 </span><span class="c9">x</span><span class="c34">2</span><span class="c1">i </span><span class="c13">(</span><span class="c42">P</span><span class="c1">n</span><span class="c12">i=1 </span><span class="c9">x</span><span class="c12">i</span><span class="c9">)</span><span class="c34">2</span><span class="c9">)(n </span><span class="c42">P</span><span class="c1">n</span><span class="c12">i=1 </span><span class="c9">y</span><span class="c34">2</span><span class="c1">i </span><span class="c13">(</span><span class="c42">P</span><span class="c1">n</span><span class="c12">i=1 </span><span class="c9">y</span><span class="c12">i</span><span class="c9">)</span><span class="c34">2</span><span class="c9">) </span><span class="c8">on a sample, </span><span class="c3">where n is the number of elements, and x</span><span class="c4">i </span><span class="c0">(y</span><span class="c4">i</span><span class="c0">) is 1 if the i</span><span class="c9">th </span><span class="c3">element belongs to X (Y ) and 0 otherwise. </span></p><p class="c15"><span class="c3">In Theorem 1, we show that there exists a direct transla- tion between the correlation coefficient and the distance of two sets. </span></p><p class="c15"><span class="c3">Theorem 1. For any two arbitrary sets X and Y , if the correlation between them is greater than a threshold &delta; 2 [ 1,1], there exist a threshold k 2 </span><span class="c23">N</span><span class="c3">, negatively associated with &delta; (k &#8672; &delta;), such that the distance between X and Y is less than k: </span></p><p class="c33"><span class="c3">8X,Y,8&delta; 2 [ 1,1], 9k 2 </span><span class="c23">N</span><span class="c3">,k &#8672; &delta;,&rho;(X, Y ) &gt; &delta; , D(X, Y ) &lt; k (1) </span></p><p class="c15"><span class="c3">Proof Proof Sketch. An increase in &rho;(X, Y ) is equiv- alent to an increase in </span><span class="c8">P</span><span class="c3">x</span><span class="c9">i</span><span class="c3">y</span><span class="c9">i </span><span class="c3">(number of similar items in both sets) that is equivalent to a decrease in </span><span class="c8">P</span><span class="c3">x</span><span class="c4">i</span><span class="c0">y</span><span class="c4">i </span><span class="c0">hence </span><span class="c3">a decrease in D(X, Y ). Moreover, any increase in &delta; and sub- sequently &rho;(X, Y ) translates to a decrease in D(X, Y ) and subsequently k. </span></p><p class="c15"><span class="c3">Definition 4. We define the correlation between two ar- bitrary topics t and t</span><span class="c13">0</span><span class="c3">, denoted by &rho;(t, t</span><span class="c13">0</span><span class="c3">), as the correlation between their target sets; i.e., &rho;(t, t</span><span class="c13">0</span><span class="c3">) = &rho;(S</span><span class="c9">t</span><span class="c3">,S</span><span class="c4">t</span><span class="c25">0</span><span class="c3">). Further- more, we define the expertise correlation between two topics t and t</span><span class="c13">0</span><span class="c3">, denoted by &rho;</span><span class="c9">E</span><span class="c3">(t, t</span><span class="c13">0</span><span class="c3">), as the correlation between their sets of experts; i.e., &rho;</span><span class="c4">E</span><span class="c0">(t, t</span><span class="c9">0</span><span class="c0">) = &rho;(E</span><span class="c4">t</span><span class="c0">,E</span><span class="c4">t</span><span class="c25">0</span><span class="c3">). </span></p><p class="c6"><span class="c3">According to Theorem 1, for a given query topic q, all top- ics with a high correlation value with q can be reported as </span></p><p class="c6"><span class="c7">183 </span></p><p class="c15"><span class="c3">the analogous topics. Since the target sets are unknown, the correlation between two topics cannot be computed. How- ever, we can compute the expertise correlation as follows (the Pearson product-moment correlation coefficient): </span></p><p class="c6"><span class="c3">&rho;</span><span class="c4">E</span><span class="c0">(t, t</span><span class="c9">0</span><span class="c0">) = &rho;(E</span><span class="c4">t</span><span class="c0">,E</span><span class="c4">q</span><span class="c0">) = </span><span class="c3">cov(E</span><span class="c4">t</span><span class="c0">,E</span><span class="c4">q</span><span class="c0">) </span></p><p class="c6"><span class="c3">&sigma;</span><span class="c9">E</span><span class="c12">t</span><span class="c3">&sigma;</span><span class="c9">E</span><span class="c12">q </span><span class="c3">= </span><span class="c8">n(P</span><span class="c9">n</span><span class="c4">i=1 </span><span class="c3">t</span><span class="c4">i</span><span class="c0">q</span><span class="c4">i</span><span class="c0">) r.s p(n </span><span class="c3">P</span><span class="c4">ni=1 </span><span class="c3">t</span><span class="c13">2</span><span class="c9">i </span><span class="c8">r</span><span class="c13">2</span><span class="c3">)(n </span><span class="c8">P</span><span class="c9">n</span><span class="c4">i=1 </span><span class="c3">q</span><span class="c13">2</span><span class="c9">i </span><span class="c8">s</span><span class="c13">2</span><span class="c3">)</span><span class="c0">(2) </span></p><p class="c15"><span class="c3">where n is the number of users, and r (s) is the number of expert users on topic t (q). Moreover, t</span><span class="c9">i </span><span class="c3">(q</span><span class="c9">i</span><span class="c3">) is 1 if the i</span><span class="c13">th </span><span class="c3">user is expert on topic t (q) and 0 otherwise. The de- nominator is equal to </span><span class="c8">p</span><span class="c3">(nr r</span><span class="c13">2</span><span class="c3">)(ns s</span><span class="c13">2</span><span class="c3">). The correlation coefficient can vary from -1 (negatively correlated) to +1 (positively correlated). </span></p><p class="c15"><span class="c3">A basic approach to approximate the correlation between two topics might be to calculate the expertise correlation between them and to utilize it as a metric to assess the correlation between those topics; i.e., one may claim that &rho;(t, q) &#8672; &rho;</span><span class="c4">E</span><span class="c0">(t, q). </span></p><p class="c15"><span class="c3">Note that a high expertise correlation between t and q suggests that the distance between E</span><span class="c4">t </span><span class="c0">and E</span><span class="c4">q </span><span class="c0">is small (The- </span><span class="c3">orem 1). Thus, among experts, the true impression is large and the false impression is small. The primary idea of Ap- proach I is that if the expertise correlation between t and q is high, one may conclude that the correlation between the whole target sets S</span><span class="c4">t </span><span class="c0">and S</span><span class="c4">q </span><span class="c0">is high; hence, according to </span><span class="c3">Theorem 1, the distance between S</span><span class="c9">t </span><span class="c3">and S</span><span class="c9">q </span><span class="c3">would be small and t would be analogous to q (Definition 3). Unfortunately, this is not correct as clarified by the following example. </span></p><p class="c15"><span class="c3">Example 1. Consider two topics &ldquo;oil&rdquo; and &ldquo;Persian clas- sic dance&rdquo;. Note that as Persians actively argue about both topics (suppose independently in separate posts), many users may place them in lists corresponding to each topic. There- fore, many Persians belong to the expertise sets of both top- ics, creating a high expertise correlation between these topics. In this sense, we may end up concluding that the topic &ldquo;oil&rdquo; is analogous to the topic &ldquo;Persian classic dance&rdquo;. On the other hand, however, the target sets of these two topics can be very di</span><span class="c23">ff</span><span class="c3">erent. A person who is interested in &ldquo;oil&rdquo; is not necessarily interested in &ldquo;Persian classic dance&rdquo;. In other words, by targeting the interested users in one of these top- ics, we do not target the users interested in the other topic. Thus, a high correlation between sets of experts does not im- ply the same for the corresponding sets of interested users; therefore the target sets for these topics are not necessarily related and &rho;(t, q) 6&#8672; &rho;</span><span class="c9">E</span><span class="c3">(t, q). </span></p><p class="c15"><span class="c3">This problem may be resolved by not looking at topics &ldquo;in isolation&rdquo; but in conjunction with other topics. The two topics &ldquo;oil&rdquo; and &ldquo;Persian classic dance&rdquo; have high exper- tise correlation. However, let us consider other topics with high expertise correlation to &ldquo;oil&rdquo; or &ldquo;Persian classic dance&rdquo;. The topic &ldquo;oil&rdquo; has high expertise correlation to topics in S</span><span class="c4">oil </span><span class="c0">= {energy, power, war,&middot;&middot;&middot;} for example, whereas &ldquo;Per- </span><span class="c3">sian classic dance&rdquo; has high expertise correlation to topics in S</span><span class="c4">dance </span><span class="c0">= {art, music, culture,&middot;&middot;&middot;}. The expertise corre- </span><span class="c3">lation between topics in S</span><span class="c4">oil </span><span class="c0">and S</span><span class="c4">dance </span><span class="c0">is extremely low. </span></p><p class="c15"><span class="c3">Example 1 suggests that a holistic view, that considers the expertise correlation of all topics in conjunction rather than individual topics in isolation, might help to determine </span></p><p class="c6"><span class="c52">q </span></p><p class="c15"><span class="c3">Figure 1: Partitioning a graph of topics. Any op- timal clustering algorithm would generate two clus- ters as shown: node q may be assigned to each of the two clusters. Note that an optimal clustering will not generate a cluster {q, c, s}. </span></p><p class="c35"><span class="c3">topics that are analogous to q. It is a natural tendency of users to be interested in high-level topic categories as well as topics under these categories. For example, if user u is interested in Wimbledon (the tennis tournament), it is nat- ural to assume, that with a high probability, u is interested in the bigger category tennis as well as other tennis events such as French Open, US Open, Australian Open, etc. If u is interested in Oscars, it is safe to assume, with a high probability, u is also interested in other film events such as Golden Globe, BAFTA, Cannes film festival, Berlin film fes- tival, etc. Based on this, we can conclude that for topics in the same category (e.g., Wimbledon and US Open which are both tennis events), the sets of interested users are close (i.e., if t</span><span class="c9">1 </span><span class="c3">and t</span><span class="c9">2 </span><span class="c3">are members of the same category of topics, the distance the topics D(Iin </span><span class="c4">t</span><span class="c3">the </span><span class="c12">1</span><span class="c3">,I</span><span class="c4">t</span><span class="c3">category </span><span class="c12">2</span><span class="c3">) is small). that This topic suggests q that identifying belongs would aid in locating the analogous topics. </span></p><p class="c15"><span class="c3">Approach II: One approach to incorporate this holistic view might be to calculate the expertise correlation between all topics and create a correlation graph where nodes repre- sent topics and the weight of an edge between two arbitrary nodes t and t</span><span class="c13">0 </span><span class="c3">is &rho;</span><span class="c4">E</span><span class="c0">(t, t</span><span class="c9">0</span><span class="c0">). Then partition (or classify) this </span><span class="c3">graph and report all topics in the partition containing topic q, as the topics analogous to q. Unfortunately, this approach has its own shortcomings as shown in the following example. </span></p><p class="c15"><span class="c3">Example 2. Consider the graph shown in Figure 1. Each node in this graph represents a topic with node q being the query topic. All the nodes represented by a circle have a high expertise correlation with each other, and all nodes rep- resented by a square have a high expertise correlation with each other. The expertise correlation is small between a cir- cle and a square node. Node q has a high expertise cor- relation with nodes c and s, and there is a high expertise correlation between nodes c and s. </span></p><p class="c15"><span class="c3">If we are looking for topics analogous to q, ideally one should identify c and s. However, any clustering scheme that relies on a global objective function based on the exper- tise correlations will partition this graph into two clusters as shown in Figure 1 without returning {q, c, s} as a separate cluster. We note that for any algorithm that generates a given number of partitions k, one can generalize this exam- ple by creating sets of k di</span><span class="c23">ff</span><span class="c3">erent shapes, without changing the behavior observed. </span></p><p class="c6"><span class="c7">184 </span></p><p class="c6"><span class="c52">c </span><span class="c53">s </span></p><p class="c15"><span class="c3">The problem encountered in Example 2 is a result of the fact that clustering algorithms rely on the optimization of a global objective function that does not take into account the original topic of interest q. Assigning q to a cluster takes place based on the optimality of a global function, and that can lead to poor performance in situations where the focus is solely on q. </span></p><p class="c15"><span class="c3">Conclusion: These two examples suggest that one needs a hybrid approach considering both the direct expertise cor- relation between each topic and q, as well as the expertise correlation amongst the neighbors. </span></p><p class="c6"><span class="c3">A topic t is analogous to topic q if and only if: </span></p><p class="c6"><span class="c3">PROPERTY 1: The expertise correlation between q and </span></p><p class="c6"><span class="c3">t is greater than a threshold &delta;; i.e., &rho;</span><span class="c4">E</span><span class="c0">(q, t) &gt; &delta;. </span></p><p class="c15"><span class="c3">PROPERTY 2: Topic t is in the same category of top- ics as topic q. In other words, the topics having high expertise correlation with topic t should have high ex- pertise correlation with topic q and vice versa: </span></p><p class="c6"><span class="c3">8t</span><span class="c13">0</span><span class="c3">;&rho;</span><span class="c4">E</span><span class="c0">(t, t</span><span class="c9">0</span><span class="c0">) &gt; &delta; , &rho;</span><span class="c4">E</span><span class="c0">(q, t</span><span class="c9">0</span><span class="c0">) &gt; &delta; </span></p><p class="c15"><span class="c3">When property (1) is satisfied, the distance between E</span><span class="c4">t </span><span class="c3">and E</span><span class="c4">q </span><span class="c0">is small (Theorem 1). Moreover, property (2) sug- </span><span class="c3">gests that the distance between I</span><span class="c9">t </span><span class="c3">and I</span><span class="c9">q </span><span class="c3">is small. Therefore, when both properties are satisfied, the distance between the target set of topic t (S</span><span class="c9">t </span><span class="c3">= E</span><span class="c9">t</span><span class="c3">[I</span><span class="c9">t</span><span class="c3">) and the target set of topic q (S</span><span class="c4">q </span><span class="c0">= E</span><span class="c4">q </span><span class="c0">[ I</span><span class="c4">q</span><span class="c0">) is small; thus t is analogous to q. </span></p><p class="c6"><span class="c3">In most real world scenarios, it is impossible to identify topics t that strictly satisfy both properties. Therefore, we introduce a technique in Section 3.2 that considers both properties, by defining a &ldquo;trade-off&rdquo; between them. In other words, our approach assigns weights to any topic t based on the direct expertise correlation between t and q (Property 1), and at the same time penalizes that weight (by associating a cost) if the topics having high expertise correlation with t have low expertise correlation with q, or the topics with low expertise correlation with t have high expertise correlation with q (Property 2). </span><span class="c5">3.2 Computing analogous topics </span></p><p class="c6"><span class="c3">Recall that U = {u</span><span class="c4">1</span><span class="c0">,u</span><span class="c4">2</span><span class="c0">,&middot;&middot;&middot; ,u</span><span class="c4">n</span><span class="c0">} denotes the set of users </span><span class="c3">and T = {t</span><span class="c9">1</span><span class="c3">,t</span><span class="c9">2</span><span class="c3">,&middot;&middot;&middot; ,t</span><span class="c9">m</span><span class="c3">} denotes the set of topics. </span></p><p class="c15"><span class="c3">Definition 5. The expert coverage probability of topic t 2 T, denoted by P(t), is the fraction of users in U that are expert on t. In particular, P(t) = </span><span class="c13">|E</span><span class="c1">t</span><span class="c13">| </span></p><p class="c15"><span class="c9">|U| </span><span class="c8">. Moreover, the </span><span class="c3">expert coverage probability of topic t 2 T given topic q (the conditional expert coverage probability, denoted by P(t|q)) is the fraction of users in U</span><span class="c13">0 </span><span class="c3">= E</span><span class="c9">q </span><span class="c3">that are expert on t. In particular, P(t|q) = </span><span class="c13">|E</span><span class="c1">t</span><span class="c9">\E</span><span class="c12">q</span><span class="c4">| </span></p><p class="c6"><span class="c9">|E</span><span class="c12">q</span><span class="c4">| </span><span class="c3">. </span></p><p class="c15"><span class="c3">As an example, suppose U consists of 1000 users, among them 10 users are expert on &ldquo;drawing&rdquo; and 50 users are ex- pert on &ldquo;music&rdquo;. This leads to P(drawing) = 10/1000 = 0.01 and P(music)=0.05. </span></p><p class="c15"><span class="c3">For any two topics t and q, the probabilities P(t|q) and P(t) may be significantly different. As an example, as- sume we observe that among experts on topic &ldquo;Picasso&rdquo;, 60% are expert on &ldquo;drawing&rdquo; and 5% are expert on &ldquo;music&rdquo;. Thus, P(drawing|P icasso) = 0.6 &gt;&gt; P(drawing) while P(music|P icasso) = P(music)=0.05 (showing that music and Picasso are independent topics). We argue that these </span></p><p class="c15"><span class="c3">changes in the expert coverage probability of different topics given a fixed topic can be utilized as an equivalent measure to Property 1. </span></p><p class="c15"><span class="c3">We utilize a two-state automaton to study whether any topic t is analogous to a given query topic q or not. This automaton has two states N and A corresponding, respec- tively, to the concepts of &ldquo;Not-analogous&rdquo; (t and q are not analogous) and &ldquo;Analogous&rdquo; (t and q are analogous). Given topic q, while considering topic t, the automaton can be in one of the states N or A. The N state corresponds to low conditional expert coverage probability and the A state corresponds to high conditional expert coverage probability. These states determine how far P(t|q) is from the original P(t) assessing whether topic t satisfies Property 1 (Theo- rem 2). For any topic t, we aim to identify the state of the automaton with the maximum likelihood. </span></p><p class="c15"><span class="c3">We deploy a binomial distribution as the basis to realize such measurement. The binomial distribution is a density function that determines the probability that r successes are achieved in a sequence of d independent experiments, when a success is yield with a fixed probability p. In the case of topics and experts, this expresses the probability that among d experts on q, r users are expert on a topic t where p = P(t). Adhering to the binomial distribution, the probability that the automaton is in state N for a topic t 2 T is: </span></p><p class="c6"><span class="c3">P(N</span><span class="c4">t</span><span class="c0">|q) = </span></p><p class="c6"><span class="c9">d</span><span class="c4">r</span><span class="c12">t </span><span class="c3">P(t)</span><span class="c13">r</span><span class="c1">t</span><span class="c0">(1 </span><span class="c3">Z </span><span class="c0">P(t))</span><span class="c9">d r</span><span class="c12">t </span></p><p class="c6"><span class="c8">(3) </span></p><p class="c6"><span class="c3">where r</span><span class="c9">t </span><span class="c3">= |E</span><span class="c9">t </span><span class="c3">\ E</span><span class="c9">q</span><span class="c3">|, d = |E</span><span class="c9">q</span><span class="c3">|. Similarly the probability that the automaton is in state A is: </span></p><p class="c6"><span class="c3">P(A</span><span class="c4">t</span><span class="c0">|q) = </span></p><p class="c6"><span class="c9">d</span><span class="c4">r</span><span class="c12">t </span><span class="c3">(&alpha; &#8677; P(t))</span><span class="c13">r</span><span class="c1">t</span><span class="c0">(1 </span><span class="c3">Z </span><span class="c0">&alpha; &#8677; P(t))</span><span class="c9">d r</span><span class="c12">t </span></p><p class="c6"><span class="c8">(4) </span></p><p class="c35"><span class="c3">where &alpha; &gt; 1 (a constant), and &alpha; &#8677; P(t) is the expected expert coverage probability of t given q in case t is analo- gous to q. P(t))</span><span class="c13">r</span><span class="c1">t</span><span class="c0">(1 </span><span class="c3">Here, Z = </span><span class="c0">&alpha; &#8677; P(t))</span><span class="c9">d </span><span class="c13">d</span><span class="c9">rr</span><span class="c12">t t </span><span class="c3">P(t)</span><span class="c13">r</span><span class="c1">t</span><span class="c0">(1 is a normalizing P(t))</span><span class="c9">d r</span><span class="c12">t </span><span class="c0">+ </span><span class="c9">d</span><span class="c4">r</span><span class="c12">t </span><span class="c3">(&alpha; &#8677; </span><span class="c0">constant. Since </span><span class="c3">the denominator Z is similar in both equations and does not impact the calculations, hereafter, we ignore it and just con- sider the numerators in calculating and comparing P(A</span><span class="c4">t</span><span class="c0">|q) </span><span class="c3">and P(N</span><span class="c9">t</span><span class="c3">|q). </span></p><p class="c6"><span class="c3">Theorem 2. The value of i</span><span class="c23">ff </span><span class="c3">the distance D(E</span><span class="c4">t</span><span class="c0">,E</span><span class="c4">q</span><span class="c0">) decreases </span><span class="c13">P(A</span><span class="c9">P(N</span><span class="c1">t</span><span class="c9">|q) </span></p><p class="c6"><span class="c12">t</span><span class="c9">|q) </span><span class="c0">(increases). </span></p><p class="c6"><span class="c8">increases (decreases) </span></p><p class="c6"><span class="c3">Proof. Let &alpha; be fixed. The value of </span><span class="c13">P(A</span><span class="c1">t</span><span class="c13">|q) </span></p><p class="c6"><span class="c9">P(N</span><span class="c12">t</span><span class="c4">|q) </span><span class="c3">increases (decreases) when &alpha;</span><span class="c13">r</span><span class="c1">t</span><span class="c0">( </span><span class="c9">1 1 &alpha;P(t) </span></p><p class="c6"><span class="c9">P(t) </span><span class="c8">)</span><span class="c13">d r</span><span class="c1">t </span><span class="c0">increases (decreases). The </span><span class="c3">latter increases (decreases) when r</span><span class="c4">t </span><span class="c0">increases (decreases) </span><span class="c3">or P(t) decreases (increases). This is because &alpha; &gt; 1 and 0 &lt; </span><span class="c13">1 </span><span class="c9">1 </span><span class="c13">&alpha;P(t) </span></p><p class="c15"><span class="c9">P(t) </span><span class="c8">&lt; 1. Moreover P(t) decreases (increases) when </span><span class="c3">|E</span><span class="c9">t</span><span class="c3">| decreases (increases). In both cases D(E</span><span class="c9">t</span><span class="c3">,E</span><span class="c9">q</span><span class="c3">) decreases (increases). </span></p><p class="c15"><span class="c3">To incorporate Property 2, we create a correlation graph: a graph G = (M,E) where any topic t 2 T {q} corresponds to a node in G. Moreover, for any two nodes m</span><span class="c9">i</span><span class="c3">,m</span><span class="c9">j </span><span class="c3">2 M representing topics t</span><span class="c4">i </span><span class="c0">and t</span><span class="c4">j</span><span class="c0">, the weight of the edge e </span><span class="c3">connecting m</span><span class="c9">i </span><span class="c3">and m</span><span class="c9">j </span><span class="c3">is w</span><span class="c9">e </span><span class="c3">= w(m</span><span class="c9">i</span><span class="c3">,m</span><span class="c9">j</span><span class="c3">) = &rho;</span><span class="c9">E</span><span class="c3">(t</span><span class="c9">i</span><span class="c3">,t</span><span class="c9">j</span><span class="c3">). </span></p><p class="c15"><span class="c3">Definition 6. Suppose the state of the automaton for any topic is determined. In particular, the automaton is in state s</span><span class="c4">i </span><span class="c0">(N or A) when considering topic t</span><span class="c4">i </span><span class="c0">(t</span><span class="c4">i </span><span class="c0">corresponds </span><span class="c3">to node m</span><span class="c9">i </span><span class="c3">in G). The edge e = (m</span><span class="c9">i</span><span class="c3">,m</span><span class="c9">j</span><span class="c3">) is called inconsis- tent if (w</span><span class="c4">e </span><span class="c0">&gt; 0 and s</span><span class="c4">i </span><span class="c0">= s</span><span class="c4">j</span><span class="c0">) or (w</span><span class="c4">e </span><span class="c0">&lt; 0 and s</span><span class="c4">i </span><span class="c0">= s</span><span class="c4">j</span><span class="c0">). </span></p><p class="c6"><span class="c7">185 </span></p><p class="c6"><span class="c3">the Definition 6 suggests that an edge e = (m</span><span class="c4">i</span><span class="c0">,m</span><span class="c4">j</span><span class="c0">) is in- </span></p><p class="c6"><span class="c3">other endpoint in S</span><span class="c4">2</span><span class="c0">. In this sense, maximizing W is </span><span class="c3">consistent if the automaton is in different states when the </span></p><p class="c6"><span class="c3">equivalent to identifying the maximum cut. expertise correlation between topics t</span><span class="c4">i </span><span class="c0">and t</span><span class="c4">j </span><span class="c0">(correspond- </span><span class="c3">ing to nodes m</span><span class="c9">i </span><span class="c3">and m</span><span class="c9">j</span><span class="c3">) is positive (&rho;</span><span class="c9">E</span><span class="c3">(t</span><span class="c9">i</span><span class="c3">,t</span><span class="c9">j</span><span class="c3">) &gt; 0) or when the automaton is in the same state if &rho;</span><span class="c4">E</span><span class="c0">(t</span><span class="c4">i</span><span class="c0">,t</span><span class="c4">j</span><span class="c0">) &lt; 0. </span></p><p class="c35"><span class="c3">Problem 2 utilizes the automaton and the correlation graph to </span><span class="c0">Q</span><span class="c3">To identify analogous topics, a more general approach would be to model this process by a 3-state automaton. The automaton, for any topic, can be in any of the 3 states &ldquo;Dis- similar&rdquo;, &ldquo;Independent&rdquo;, or &ldquo;Analogous&rdquo;. The conditional s</span><span class="c4">i </span><span class="c3">expert coverage probability of topic t on these states is, re- spectively, a</span><span class="c9">1</span><span class="c3">, a</span><span class="c9">2 </span><span class="c3">and a</span><span class="c9">3 </span><span class="c3">where a</span><span class="c9">1 </span><span class="c3">&lt; a</span><span class="c9">2 </span><span class="c3">&lt; a</span><span class="c9">3</span><span class="c3">. In particu- lar, a</span><span class="c4">2 </span><span class="c0">= P(t) is the expert coverage probability for topic t </span><span class="c3">over U, a</span><span class="c9">1 </span><span class="c3">is a lower conditional expert coverage probabil- ity showing that the topics t and q are dissimilar (perhaps negatively analogous), and a</span><span class="c9">3 </span><span class="c3">is a higher conditional expert coverage probability showing that the topics t and q are analogous. In the present work, we simplify the model and merge the &ldquo;Dissimilar&rdquo; and &ldquo;Independent&rdquo; states to form a &ldquo;Not-analogous&rdquo; state N. This generalization can be con- ducted easily following the developments in the section. </span></p><p class="c6"><span class="c5">3.3 IAT: </span><span class="c26">Topics </span></p><p class="c35"><span class="c5">an algorithm to Identify Analogous </span><span class="c3">According to Theorem 3, Problem 2 is NP-hard. It in- volves coverage and The (2) value two minimizing probabilities for parts: the (1) first the maximizing over cost part all of can nodes; inconsistent be the calculated i.e., log-likelihood </span><span class="c8">P</span><span class="c3">edges; </span><span class="c9">m</span><span class="c12">i</span><span class="c9">2M </span><span class="c3">for i.e., </span><span class="c8">log </span><span class="c3">each of </span><span class="c8">P(sP</span><span class="c3">expert node </span><span class="c9">e2E i</span><span class="c3">|q), </span></p><p class="c6"><span class="c8">c</span><span class="c9">e</span><span class="c3">. </span></p><p class="c6"><span class="c3">independently. Computing the second part, however, needs to be aware of the states of the neighboring nodes. </span></p><p class="c15"><span class="c3">We propose a technique (called IAT) that adopts a heuris- tic approach to reduce the complexity of Problem 2. The main root of the complexity in Problem 2 is the existence of cycles in the graph. In an acyclic graph, we can order nodes of the graph and identify the best state assignment by optimizing both parts of Equation 5 node-to-node based on this ordering. However, when the graph contains a cycle, no ordering can be assumed between nodes in the cycle; the states of all these nodes depend on each other (due to the second part) and should be determined simultaneously. This leads to a complex structure to deal with. The basic idea of IAT is to obtain an acyclic subgraph (a spanning tree) of the original graph. We, then, identify the optimal states based on this tree. Our experiments show that by utilizing this technique, we can effectively locate the analogous topics. </span></p><p class="c15"><span class="c3">This approach raises the question on how to choose the spanning tree. In Problem 2, before determining the state of a node, we consider the states of the neighboring nodes in order to reduce the cost of inconsistent edges. Among all edges connected to an arbitrary node u, some have the highest probability to be inconsistent. We refer to these as inconsistency-prone edges. The goal is to assign the states such that (1) the log-likelihood of expert coverage probabilities over all nodes is maximized and (2) the cost of inconsistency-prone edges is minimized. The idea is that since the inconsistency-prone edges are the most likely edges to induce costs, a state assignment that reduces the cost over these edges, reduces the cost over all edges. </span></p><p class="c35"><span class="c3">To locate the inconsistency-prone edges, we define an ex- pected cost value for each edge. The edges with high ex- pected cost values are considered inconsistency-prone. Let </span><span class="c0">A</span><span class="c3">bility </span><span class="c0">&circ;</span><span class="c4">u </span><span class="c0">= </span><span class="c3">that </span><span class="c9">log P(A</span><span class="c3">u </span><span class="c9">log </span><span class="c3">is </span><span class="c12">u</span><span class="c9">|t)+log P(A</span><span class="c3">associated </span><span class="c12">u</span><span class="c4">|t) </span><span class="c9">P(N</span><span class="c12">u</span><span class="c9">|t) </span><span class="c3">with </span><span class="c8">determine </span><span class="c3">state A </span><span class="c8">the </span><span class="c3">and </span><span class="c8">expected </span><span class="c3">N</span><span class="c9">u </span><span class="c8">&circ;</span><span class="c3">= 1 </span><span class="c8">proba- </span><span class="c3">A</span><span class="c9">u </span><span class="c8">&circ;</span><span class="c3">be the expected probability that u is associated with state N. </span></p><p class="c6"><span class="c7">186 </span><span class="c3">identify the most likely states </span><span class="c4">t</span><span class="c12">i</span><span class="c0">2 </span><span class="c9">2T </span><span class="c0">{N,A} </span><span class="c9">{q} </span><span class="c8">P(s</span><span class="c0">is </span><span class="c9">i</span><span class="c3">|q) </span><span class="c0">the </span><span class="c3">(or </span><span class="c0">state </span><span class="c3">equivalently </span><span class="c0">assigned </span><span class="c3">for </span><span class="c8">P</span><span class="c0">to </span><span class="c9">t</span><span class="c12">i</span><span class="c3">all </span><span class="c9">2T </span><span class="c0">topic </span><span class="c3">topics </span><span class="c9">{q} </span><span class="c0">t</span><span class="c8">log </span><span class="c4">i</span><span class="c0">. </span><span class="c3">maximizing </span><span class="c8">P(s</span><span class="c0">To </span><span class="c9">i</span><span class="c3">|q)) where </span></p><p class="c15"><span class="c0">satisfy </span><span class="c3">Property 2, Problem 2 associates a cost with any inconsis- tent edge. </span></p><p class="c15"><span class="c3">Problem 2. Let G = (M,E) be the correlation graph for topics in T {q}. Identify the state of the automaton for each node m</span><span class="c4">i </span><span class="c0">2 M </span><span class="c3">maximize </span><span class="c0">(s</span><span class="c4">i </span><span class="c0">2 {N,A}) to </span></p><p class="c6"><span class="c8">X</span><span class="c9">m</span><span class="c12">i</span><span class="c9">2M </span></p><p class="c6"><span class="c3">log P(s</span><span class="c9">i</span><span class="c3">|q) </span><span class="c8">X</span><span class="c9">e2E </span></p><p class="c6"><span class="c3">c</span><span class="c9">e </span><span class="c3">(5) </span></p><p class="c6"><span class="c3">Note that c</span><span class="c4">e </span><span class="c0">is the cost of edge e = (m</span><span class="c4">i</span><span class="c0">,m</span><span class="c4">j</span><span class="c0">) that is equal </span><span class="c3">to constant |w</span><span class="c9">e</span><span class="c3">| if factor e is inconsistent </span><span class="c8">P</span><span class="c3">or zero otherwise. By adding a </span></p><p class="c6"><span class="c4">w</span><span class="c9">e2E </span><span class="c12">e</span><span class="c4">&lt;0 </span></p><p class="c6"><span class="c3">w</span><span class="c4">e </span><span class="c0">to Equation 5, we get </span></p><p class="c6"><span class="c3">(5) &#8984; maximize </span><span class="c8">X</span><span class="c9">m</span><span class="c12">i</span><span class="c9">2M </span></p><p class="c6"><span class="c3">log P(s</span><span class="c4">i</span><span class="c0">|q) </span><span class="c3">X </span></p><p class="c33"><span class="c9">e=(m</span><span class="c12">i</span><span class="c9">,m</span><span class="c12">j</span><span class="c9">)2E s</span><span class="c12">i</span><span class="c9">=s</span><span class="c12">j </span></p><p class="c6"><span class="c3">w</span><span class="c4">e </span><span class="c0">(6) </span></p><p class="c15"><span class="c3">Maximizing Equations 5 and 6 is equivalent to maximiz- ing the probability that the correlation graph G is created by a two-state automaton where the probability that the automaton is in state N or A for each node in G is derived by Equations 3 and 4 (corresponding to Property 1) and for each edge e in G, the probability that the automaton main- tains the same state over the two end-points of that edge depends on w</span><span class="c9">e </span><span class="c3">(corresponding to Property 2). </span></p><p class="c6"><span class="c3">Theorem 3. Problem 2 is NP-hard. </span></p><p class="c15"><span class="c3">Proof. We reduce the max-cut problem to Problem 2. In max-cut problem, given a weighted graph G, the goal is to partition vertices of G into two subsets S</span><span class="c4">1 </span><span class="c0">and S</span><span class="c4">2 </span><span class="c0">such </span><span class="c3">that the weight of edges between S</span><span class="c9">1 </span><span class="c3">and S</span><span class="c9">2 </span><span class="c3">is maximized. The max-cut problem is widely known to be NP-hard [12]. </span></p><p class="c35"><span class="c3">The reduction is as follows. Let us assume we want to identify the maximum cut for the graph G. We create a graph G</span><span class="c13">0 </span><span class="c3">where there is a node u</span><span class="c13">0 </span><span class="c3">in G</span><span class="c13">0 </span><span class="c3">for any node u in G. For any in G</span><span class="c13">0 </span><span class="c3">edge (u</span><span class="c13">0</span><span class="c9">i </span><span class="c8">and u</span><span class="c3">e </span><span class="c13">0</span><span class="c9">j </span><span class="c3">= </span><span class="c8">are </span><span class="c3">(u</span><span class="c9">i</span><span class="c3">,u</span><span class="c9">j</span><span class="c3">) </span><span class="c8">the </span><span class="c3">2 G, we add </span><span class="c8">nodes in G</span><span class="c13">0 </span><span class="c3">an edge corresponding e</span><span class="c13">0 </span><span class="c3">= to (uu</span><span class="c4">i </span><span class="c13">0</span><span class="c9">i</span><span class="c8">,u</span><span class="c0">and </span><span class="c13">0</span><span class="c9">j</span><span class="c8">) </span></p><p class="c35"><span class="c3">u</span><span class="c9">j </span><span class="c3">in G) with a weight of w</span><span class="c4">e</span><span class="c25">0 </span><span class="c3">= w</span><span class="c9">e </span><span class="c3">where w</span><span class="c9">e </span><span class="c3">is the weight of edge e in graph G. Moreover, for any two nodes v</span><span class="c4">i </span><span class="c0">and v</span><span class="c4">j </span><span class="c3">in G not connected to each other, we add an edge between their corresponding w</span><span class="c4">e</span><span class="c25">0 </span><span class="c3">= 0. Finally, nodes we set the v</span><span class="c9">i </span><span class="c13">0</span><span class="c3">probability </span><span class="c8">and v</span><span class="c9">j </span><span class="c13">0</span><span class="c3">is in the N or A state for any node in </span><span class="c8">in G</span><span class="c13">0 </span><span class="c3">with a weight of that the automaton G</span><span class="c13">0 </span><span class="c3">to be equal. Identi- fying the maximum cut for graph G reduces to solving Prob- lem 2 for graph G</span><span class="c13">0 </span><span class="c3">by identifying the state of automaton for any to node in G</span><span class="c13">0 </span><span class="c3">maximizing After identifying W that = the maximizes </span><span class="c8">P</span><span class="c9">e</span><span class="c3">optimal </span><span class="c20">0</span><span class="c9">=(u</span><span class="c20">0</span><span class="c1">i</span><span class="c13">,u</span><span class="c20">0</span><span class="c1">j</span><span class="c3">Equation </span><span class="c13">)2E</span><span class="c3">states </span><span class="c20">0</span><span class="c9">;s</span><span class="c12">u</span><span class="c25">0</span><span class="c12">i </span><span class="c9">=s</span><span class="c3">of 6 that </span><span class="c12">u</span><span class="c3">the </span><span class="c25">0</span><span class="c12">j </span><span class="c3">( is equivalent </span></p><p class="c35"><span class="c3">w</span><span class="c4">e</span><span class="c25">0</span><span class="c3">). automaton for each topic, we define the set responding to nodes in G</span><span class="c13">0 </span><span class="c3">S</span><span class="c9">1 </span><span class="c3">containing all nodes in G cor- that are assigned with a N state and G</span><span class="c13">0 </span><span class="c3">S</span><span class="c4">2 </span><span class="c0">containing all nodes in G corresponding </span><span class="c3">that are assigned with a A state. Thus, W where E</span><span class="c13">0 </span><span class="c3">contains all edges in E with an endpoint = </span><span class="c0">to </span><span class="c8">P</span><span class="c3">in </span><span class="c0">nodes </span><span class="c9">e2E</span><span class="c3">S</span><span class="c4">1 </span><span class="c0">in </span><span class="c20">0 </span><span class="c8">w</span><span class="c13">e </span><span class="c0">and </span></p><p class="c6"><span class="c3">The expected cost of an edge e = (u, v) denoted by &circ;c</span><span class="c4">e </span><span class="c0">is: </span></p><p class="c6"><span class="c3">&circ;c</span><span class="c4">e </span><span class="c0">= </span></p><p class="c6"><span class="c3">(</span><span class="c0">| A</span><span class="c3">&circ;</span><span class="c4">u </span><span class="c0">A</span><span class="c3">&circ;</span><span class="c4">v</span><span class="c0">| &#8677; w</span><span class="c4">e </span><span class="c0">if w</span><span class="c4">e </span><span class="c0">0, </span></p><p class="c6"><span class="c3">(1 | A</span><span class="c9">u </span><span class="c8">&circ;</span><span class="c3">A</span><span class="c9">v</span><span class="c3">|) </span><span class="c8">&circ;</span><span class="c3">&#8677; |w</span><span class="c9">e</span><span class="c3">| if w</span><span class="c9">e </span><span class="c3">&lt; 0. </span><span class="c8">(7) </span></p><p class="c35"><span class="c3">where the value | A</span><span class="c9">u </span><span class="c8">&circ;</span><span class="c3">A</span><span class="c9">v</span><span class="c3">| </span><span class="c8">&circ;</span><span class="c3">(a value between 0 and 1) deter- mines the difference in state A for adjacent nodes probability u and v. of High being values associated of | A</span><span class="c8">&circ;</span><span class="c4">u </span><span class="c3">with </span><span class="c0">A</span><span class="c3">&circ;</span><span class="c4">v</span><span class="c0">| </span><span class="c3">suggest that u and v are likely to be assigned with different states. Having the expected cost values, Problem 3 identifies the optimal acyclic subgraph. </span></p><p class="c15"><span class="c3">Problem 3. Considering the expected cost of each edge in the graph G = (M,E), identify an acyclic subgraph T = (M,E</span><span class="c13">&#8676;</span><span class="c3">) with maximum sum of the expected costs over all edges in E</span><span class="c13">&#8676;</span><span class="c3">. </span></p><p class="c15"><span class="c3">Problem 3 is equivalent to the minimum spanning tree problem. We can create a new graph G</span><span class="c13">0 </span><span class="c3">by negating the weights of all edges in G and identifying the minimum span- ning tree in G</span><span class="c13">0</span><span class="c3">. This tree would be the optimal solution for Problem 3 that can be found utilizing any MST algorithm such as Kruskal [11] or Prim [4]. The run time complexity for these algorithms on a dense graph is O(m</span><span class="c13">2</span><span class="c3">) where m is the cardinality of M. </span></p><p class="c15"><span class="c3">Assume tree T is the optimal solution for Problem 3. The IAT algorithm is a dynamic programming approach that calculates two values LP(N</span><span class="c4">u</span><span class="c0">) and LP(A</span><span class="c4">u</span><span class="c0">) for any node u 2 </span><span class="c3">M starting from leaves, going upwards to the root. For each leaf u, LP(N</span><span class="c4">u</span><span class="c0">) = log P(N</span><span class="c4">u</span><span class="c0">|q) and LP(A</span><span class="c4">u</span><span class="c0">) = log P(A</span><span class="c4">u</span><span class="c0">|q) </span><span class="c3">that are calculated based on Equations 3-4. For any inner node u, the values are calculated as follows: </span></p><p class="c6"><span class="c3">LP(A</span><span class="c4">u</span><span class="c0">) = X </span></p><p class="c6"><span class="c0">log P(A</span><span class="c4">u</span><span class="c0">|q)+ (8) </span><span class="c9">v2C(u)</span><span class="c8">max(LP(A</span><span class="c9">v</span><span class="c3">), LP(N</span><span class="c4">v</span><span class="c0">) w(u, v)), </span></p><p class="c6"><span class="c3">LP(N</span><span class="c4">u</span><span class="c0">) = X </span></p><p class="c6"><span class="c0">log P(N</span><span class="c4">u</span><span class="c0">|q)+ (9) </span><span class="c9">v2C(u)</span><span class="c8">max(LP(A</span><span class="c9">v</span><span class="c3">) w(u, v), LP(N</span><span class="c4">v</span><span class="c0">)), </span></p><p class="c6"><span class="c3">where C(u) is the set of u&rsquo;s children and w(u, v) is the weight of edge (u, v). </span></p><p class="c15"><span class="c3">When all values are calculated, IAT identifies the best state assignment to all nodes by locating the chain of states maximizing the value of max(LP(A</span><span class="c9">r</span><span class="c3">), LP(N</span><span class="c9">r</span><span class="c3">)) where r is the root of the T. </span></p><p class="c15"><span class="c3">The pseudo-code for IAT is presented as Algorithm 1. Note that p</span><span class="c9">u </span><span class="c3">is the parent of u and C(u) is the set of u&rsquo;s children in Tree T. The variable AP ointer</span><span class="c4">u </span><span class="c0">(Npointer</span><span class="c4">u</span><span class="c0">) </span><span class="c3">saves the optimal state assigned to u when its parent p</span><span class="c9">u </span><span class="c3">is assigned with a state A (N). The function &ldquo;argmax(a, b)&rdquo; returns A if a &gt; b and returns N otherwise. Finally, s</span><span class="c9">u </span><span class="c3">holds the assigned state of node u. IAT reports all topics that are assigned with state A as the analogous topics. For each analogous topic t, we define a weight as </span></p><p class="c6"><span class="c3">weight(t) = log P(A</span><span class="c9">t</span><span class="c3">|q) log P(N</span><span class="c9">t</span><span class="c3">|q) (10) </span></p><p class="c15"><span class="c3">This weight determines the improvement we achieve when topic q is assigned with state A instead of N. Thus, top- ics with higher weights correspond to more prominent rela- tionships with topic q. Algorithm IAT ranks the analogous </span></p><p class="c6"><span class="c3">topics based on these weight values and returns Q, a ranked list of all nodes assigned with a A state. </span></p><p class="c6"><span class="c3">Algorithm 1: The IAT algorithm </span></p><p class="c6"><span class="c3">input : The correlation graph G = (M,E), Topic q output: A ranked list of analogous topics Q // Identify the optimal spanning tree 1 Calculate the expected cost of edges according to Eq. 7 2 Identify the optimal spanning tree T (e.g., by Prim) </span></p><p class="c6"><span class="c3">// Probability calculations 3 Traverse T bottom-up (from leaves to the root): 4 foreach u 2 M do 5 LP(A</span><span class="c9">u</span><span class="c3">) = log P(A</span><span class="c9">u</span><span class="c3">|q) + </span><span class="c0">P</span><span class="c4">v2C(u) </span><span class="c3">max(LP(A</span><span class="c4">v</span><span class="c0">), LP(N</span><span class="c4">v</span><span class="c0">) w(u, v)) </span><span class="c3">6 LP(N</span><span class="c9">u</span><span class="c3">) = log P(N</span><span class="c9">u</span><span class="c3">|q) + </span><span class="c8">P</span><span class="c9">v2C(u) </span><span class="c8">max(LP(A</span><span class="c13">v</span><span class="c8">) </span></p><p class="c6"><span class="c3">w(u, v), LP(N</span><span class="c9">v</span><span class="c3">)) 7 AP ointer</span><span class="c4">u </span><span class="c0">= arg max(LP(A</span><span class="c4">u</span><span class="c0">), LP(N</span><span class="c4">u</span><span class="c0">) w(u, p</span><span class="c4">u</span><span class="c0">)) </span><span class="c3">8 NPointer</span><span class="c9">u </span><span class="c3">= arg max(LP(A</span><span class="c9">u</span><span class="c3">) w(u, p</span><span class="c9">u</span><span class="c3">), LP(N</span><span class="c9">v</span><span class="c3">)) </span></p><p class="c6"><span class="c3">// Identifying the state of the root 9 s</span><span class="c4">r </span><span class="c0">= arg max(LP(A</span><span class="c4">r</span><span class="c0">), LP(N</span><span class="c4">r</span><span class="c0">)) </span></p><p class="c6"><span class="c3">// Identifying the state for all nodes 10 Traverse F top-down (from roots to leaves): 11 foreach u 2 M do 12 s</span><span class="c9">u </span><span class="c3">= NPointer</span><span class="c9">u</span><span class="c3">; 13 if s</span><span class="c4">p</span><span class="c12">u </span><span class="c3">= &ldquo;A&rdquo; then 14 s</span><span class="c4">u </span><span class="c0">= AP ointer</span><span class="c4">u </span></p><p class="c6"><span class="c3">// Sort the analogous topics 15 Q = ; 16 foreach u 2 M do 17 if s</span><span class="c4">u </span><span class="c0">= &ldquo;A&rdquo; then </span><span class="c3">18 Q = Q [ {u} </span></p><p class="c6"><span class="c3">19 Sort Q based on Eq. 10 </span></p><p class="c15"><span class="c3">Theorem 4. The IAT algorithm identifies the optimal state assignment on the tree. The run time complexity of IAT is &theta;(m</span><span class="c13">2</span><span class="c3">) where m is the number of topics. </span></p><p class="c15"><span class="c3">Proof Proof Sketch. IAT is a standard dynamic pro- gramming approach that solves Problem 2 step by step from leaves to the root. The value LP(A</span><span class="c4">u</span><span class="c0">) is the optimal solution </span><span class="c3">for Problem 2 on the subtree rooted at u when the state of u is A. Similarly LP(N</span><span class="c4">u</span><span class="c0">) is the optimal solution for Problem 2 </span><span class="c3">on the same subtree when the state of u is N. Therefore the value of max(LP(A</span><span class="c4">r</span><span class="c0">), LP(N</span><span class="c4">r</span><span class="c0">)) is the optimal solution for </span><span class="c3">the whole tree. </span></p><p class="c6"><span class="c3">We can calculate the expected cost of each edge in con- stant time. Since there are m</span><span class="c13">2 </span><span class="c3">edges, line 1 takes &#8677;(m</span><span class="c13">2</span><span class="c3">). Prim&rsquo;s algorithm implemented with Fibonacci heap takes &#8677;(m</span><span class="c13">2</span><span class="c3">) to identify the MST. The probability calculation phase takes &#8677;(m) since each edge in the MST can update LP of one node only once. The state identification phase also takes &#8677;(m) to calculate the optimal states for all nodes. Finally it takes &#8677;(mlog m) to sort the analogous topics. Thus, in total IAT takes &#8677;(m</span><span class="c13">2</span><span class="c3">). </span></p><p class="c6"><span class="c5">4. CATEGORIZING THE FOLLOWERS </span></p><p class="c15"><span class="c3">In Section 1 we explained it is very helpful to categorize all experts on a given topic q based on other topics of their ex- pertise in order to engage them in word of mouth campaigns. For example, among all experts on social media, those who are expert on topics such as &ldquo;consumer behavior&rdquo;, &ldquo;distribu- tion channel&rdquo;, &ldquo;market-based pricing&rdquo;, &ldquo;sales&rdquo;, etc. can be </span></p><p class="c6"><span class="c7">187 </span></p><p class="c15"><span class="c3">potentially categorized together (in a big category of &ldquo;Mar- keting&rdquo;); and those expert on topics such as &ldquo;high ranking placement&rdquo;, &ldquo;website visitors&rdquo;, &ldquo;Google results&rdquo;, &ldquo;search en- gine traffic&rdquo;, &ldquo;white hat seo&rdquo;, etc. can form a big category of &ldquo;Search Engine Optimization&rdquo;. </span></p><p class="c15"><span class="c3">By categorizing the experts, we would be able to under- stand them in a more refined fashion and to locate the ex- perts that are the &ldquo;right&rdquo; advocates to instigate a popularity propagation (based on word of mouth effects) in the network. </span></p><p class="c6"><span class="c5">4.1 CTE: an algorithm to Categorize Topics </span><span class="c26">and Experts </span><span class="c3">Let q be a topic, E</span><span class="c4">q </span><span class="c0">be the set of experts on q, and T</span><span class="c4">u </span><span class="c3">be the set of all topics user u is an expert on. We propose an algorithm (called CTE) to categorize users u 2 E</span><span class="c9">q </span><span class="c3">based on the topics of their expertise. We introduce four desirable properties that CTE should have: (1) Soft clustering: Users may be assigned to several cate- gories. This is desirable as users usually have diverse topics of expertise hence they might belong to various categories. (2) Unknown number of categories k: The optimal number of categories is unknown. The algorithm should identify the best number of categories instead of requiring it as an input. (3) Coping with high dimensional data: The number of top- ics is large. On high dimensional datasets, any approach based on distance (e.g., the traditional clustering algorithms) is inaccurate since distances between all pairs converge. (4) Considering the correlation between topics: Topics are correlated; any approach that is based on an assumption that dimensions (topics in this case) are independent is not applicable. </span></p><p class="c15"><span class="c3">In Sections 5 and 6, we argue that traditional clustering algorithms fail to provide useful categorizations. Here, we present an approach (satisfying the properties above) that considers topics and users in two steps: first it categorizes the topics without taking into account the users (topic cate- gorization phase); and then it assigns each user u 2 E</span><span class="c4">q </span><span class="c0">based </span><span class="c3">on T</span><span class="c4">u</span><span class="c0">, to the topic categories (user assignment phase). </span></p><p class="c15"><span class="c3">This separation of topics and users in categorization helps to segment topics into partitions that are representing high- level topic categories. When we utilize an approach that simultaneously categorizes users in E</span><span class="c4">q </span><span class="c0">and topics in </span><span class="c3">S </span></p><p class="c15"><span class="c9">u2E</span><span class="c12">q </span><span class="c3">T</span><span class="c4">u </span><span class="c3">(e.g., the bi-clustering techniques), topics are categorized according to the correlations calculated utilizing the sets T</span><span class="c9">u </span><span class="c3">of users u in E</span><span class="c4">q</span><span class="c0">, instead of utilizing the sets T</span><span class="c4">u </span><span class="c0">of all users </span><span class="c3">in U. Incorporating the users in E</span><span class="c9">q </span><span class="c3">(instead of all users) to capture correlations introduces coverage bias. </span></p><p class="c15"><span class="c3">Coverage bias loosely means that users in E</span><span class="c4">q </span><span class="c0">are not rep- </span><span class="c3">resentative of the population. There are cases where the cor- relation between two topics t</span><span class="c4">1 </span><span class="c0">and t</span><span class="c4">2 </span><span class="c0">is low but the topics are </span><span class="c3">highly correlated in the context of a query topic q (i.e., based on users in E</span><span class="c4">q</span><span class="c0">). For example, consider topics&ldquo;Queen&rsquo;s park&rdquo; </span><span class="c3">and &ldquo;Government&rdquo; in the context of topic &ldquo;Ontario&rdquo;. These topics are not highly correlated in general. However, when the set we consider consists of experts on &ldquo;Ontario&rdquo;, the two topics would be highly correlated, since Queen&rsquo;s park is the home for the Legislative Assembly of Ontario and is usually utilized as a metonym for the Government of Ontario. On the other hand, there are cases where two topics t</span><span class="c4">1 </span><span class="c0">and t</span><span class="c4">2 </span><span class="c3">are highly correlated but when considered in the context of experts on a query topic q, this correlation is small. Con- sider two topics &ldquo;football&rdquo; and &ldquo;rugby&rdquo; given the query topic </span></p><p class="c15"><span class="c3">&ldquo;fifa&rdquo; as an example. In general rugby and football are cor- related due to the relation between rugby and the American football. However, given the topic &ldquo;fifa&rdquo;, the term &ldquo;football&rdquo; would usually refer to the international &ldquo;football&rdquo; that has low correlation with &ldquo;rugby&rdquo;. </span></p><p class="c6"><span class="c41">4.1.1 Topic categorization </span></p><p class="c15"><span class="c3">The CTE algorithm runs in two phases: (1) topic cate- gorization, and (2) user assignment. Topic categorization starts by creating the correlation graph among topics as discussed in Section 3.2 (incorporating all users in U in weight calculations). Subsequently, we aim to segment top- ics (graph nodes) into categories such that topics with posi- tive expertise correlation values are located in the same cat- egory and topics with negative expertise correlation values are located in different categories. </span></p><p class="c6"><span class="c3">Problem topic t 2 </span><span class="c8">S </span></p><p class="c35"><span class="c3">4. Let G = (V,E) be a correlation graph where </span><span class="c9">u2E</span><span class="c12">q </span><span class="c3">T</span><span class="c4">u </span><span class="c0">corresponds to a node in V . Also, the </span><span class="c3">weight of the edge connecting any pair of nodes u</span><span class="c4">i</span><span class="c0">,u</span><span class="c4">j </span><span class="c0">2 V </span><span class="c3">(representing G into topics t</span><span class="c4">i </span><span class="c3">categories such </span><span class="c0">and </span><span class="c3">that </span><span class="c0">t</span><span class="c4">j</span><span class="c0">) </span><span class="c3">the </span><span class="c0">is w</span><span class="c3">sum </span><span class="c4">u</span><span class="c12">i</span><span class="c9">u</span><span class="c12">j </span><span class="c3">of = the &rho;</span><span class="c4">E</span><span class="c0">(t</span><span class="c3">weights </span><span class="c4">i</span><span class="c0">,t</span><span class="c4">j</span><span class="c0">). Segment </span><span class="c3">of edges with positive weights that are cut and edges with negative weights that are uncut is minimized. </span></p><p class="c15"><span class="c3">Bansal et. al. have shown that Problem 4 is NP-hard even for a simple case where the weight of all edges are either 1 or +1 [1]. Demaine et. al. have shown that Problem 4 and the weighted multicut problem are equivalent; Problem 4 is APX-hard; and obtaining any approximation bound better than &theta;(log n) is difficult (n = |V |). Utilizing the linear pro- gramming rounding and &ldquo;region growing&rdquo; techniques, they have proposed an algorithm to approximate Problem 4 with a tight bound of &theta;(log n) [5]. </span></p><p class="c6"><span class="c3">This approach models the problem as a linear program. A zero-one variable x</span><span class="c4">uv </span><span class="c0">is defined for any pair of vertices u and </span><span class="c3">v. The equation x</span><span class="c9">uv </span><span class="c3">= 0 suggests that u and v are in the same category; x</span><span class="c4">uv </span><span class="c0">= 1 declares the opposite. Problem 4 </span><span class="c3">translates to minimize </span><span class="c8">X </span></p><p class="c6"><span class="c9">(u,v): w</span><span class="c12">uv</span><span class="c4">&lt;0 </span></p><p class="c6"><span class="c3">|w</span><span class="c9">uv</span><span class="c3">|(1 x</span><span class="c9">uv</span><span class="c3">)+ </span><span class="c8">X </span></p><p class="c6"><span class="c9">(u,v): w</span><span class="c12">uv</span><span class="c4">&gt;0 </span></p><p class="c6"><span class="c3">|w</span><span class="c9">uv</span><span class="c3">|x</span><span class="c9">uv </span></p><p class="c6"><span class="c3">subject to the following constraints: (1) x</span><span class="c9">uv </span><span class="c3">2 [0,1], (2) x</span><span class="c9">uv </span><span class="c3">= x</span><span class="c9">vu</span><span class="c3">, and (3) x</span><span class="c9">uv </span><span class="c3">+ x</span><span class="c9">vw </span><span class="c3">x</span><span class="c9">uw</span><span class="c3">. A&ldquo;region growing&rdquo;technique is adopted, afterwards, to trans- form the fractional values of x</span><span class="c9">uv </span><span class="c3">to integral values 0 or 1. The basic idea is to grow balls around graph nodes (with a fixed maximum radius). Each ball is reported as a category. Therefore, two nodes u and v with a high value x</span><span class="c9">uv </span><span class="c3">would be assigned to two different balls and finally two different categories (equivalent to setting x</span><span class="c9">uv </span><span class="c3">= 1). </span></p><p class="c35"><span class="c3">The run time complexity of the algorithm proposed by Demaine et. al. is O(n</span><span class="c13">7</span><span class="c3">). This approach is not practical for datasets containing a large number of topics. In our im- plementation of topics based on Twitter lists, we construct millions of topics for Twitter users. Any approach based on an O(n</span><span class="c13">7</span><span class="c3">) algorithm is deemed not practical for our setting. We propose a heuristic approach called MaxMerge to cat- egorize the correlation graph when the graph is large. The CTE algorithm utilizes MaxMerge in the topic categoriza- tion phase. To start, MaxMerge constructs a category for each vertex in G. The algorithm proceeds iteratively. In </span></p><p class="c6"><span class="c7">188 </span></p><p class="c15"><span class="c3">each iteration, it calculates the value &#8710;</span><span class="c4">AB </span><span class="c0">achieved by merg- </span><span class="c3">ing any pair of existing categories A and B. The value &#8710;</span><span class="c9">AB </span><span class="c3">is the average of the weights of edges with one end-point in category A and one end-point in category B. According to Problem 4, the objective is to categorize G such that the edges with positive weights are in the same category and the edges with the negative weights are amongst different categories. The &#8710;</span><span class="c4">AB </span><span class="c0">value expresses our progress towards </span><span class="c3">the objective when the two categories are merged. At each iteration, categories A and B having the maximum positive value of &#8710;</span><span class="c4">AB </span><span class="c0">will be merged; MaxMerge continues as long </span><span class="c3">as this maximum positive value is greater than the average weight of all node pairs in the whole graph (stating that merging the two categories at hand should result in a value that is higher than the average weight of one big category that includes all topics). Pseudo code of MaxMerge is pro- vided as Alg 2. The input is a graph G = (V,E). </span></p><p class="c6"><span class="c3">Algorithm 2: The MAXMERGE algorithm 1 Let avr be the average of the weight of all edges in E 2 Consider each node as a category 3 foreach pair of categories A and B do 4 SUM = </span><span class="c8">P</span><span class="c3">weight of all edges between A and B </span></p><p class="c6"><span class="c3">&#8710;</span><span class="c9">AB </span><span class="c3">= SUM/(|A| &#8676; |B|) 5 Max = max</span><span class="c4">A,B </span><span class="c0">&#8710;</span><span class="c4">AB</span><span class="c0">; A</span><span class="c9">&#8676;</span><span class="c0">,B</span><span class="c9">&#8676; </span><span class="c0">= arg max</span><span class="c4">A,B </span><span class="c0">&#8710;</span><span class="c4">AB </span><span class="c3">6 if Max &gt; avr then 7 Merge A</span><span class="c13">&#8676; </span><span class="c3">and B</span><span class="c13">&#8676; </span><span class="c3">to one category and Goto step 3 8 return all categories </span></p><p class="c6"><span class="c3">Theorem 5. The run time complexity of Algorithm 2 is O(m</span><span class="c13">2 </span><span class="c3">log m) where m = |V |. </span></p><p class="c15"><span class="c3">Proof. Line 1 takes O(m</span><span class="c13">2</span><span class="c3">) since there are m</span><span class="c13">2 </span><span class="c3">edges be- tween the topics. Line 2 takes O(m). At the beginning there are O(m</span><span class="c13">2</span><span class="c3">) pairs of partitions and it takes O(1) to calculate &#8710; values for each pair. Thus it take O(m</span><span class="c13">2</span><span class="c3">) to calculate these values at the first iteration. We can store these values in a priority queue. Based on the implementation it takes O(m</span><span class="c13">2</span><span class="c3">) or O(m</span><span class="c13">2 </span><span class="c3">log m) to create this priority queue. </span></p><p class="c15"><span class="c3">We do several iterations while Max &gt; avr to merge the partitions. The number of iterations is at most m 1. In each iteration, it takes O(log m) to find and delete the max value, O(m) to merge the two partitions, O(m) to up- date the values of &#8710; for the new merged partition, and O(mlog m) to update these values in the priority queue. Note that if we merge two partitions A and B into the new partition C, for any partition D: SUM</span><span class="c4">CD </span><span class="c0">= SUM</span><span class="c4">AD </span><span class="c0">+ </span><span class="c3">SUM</span><span class="c4">BD</span><span class="c0">. </span></p><p class="c6"><span class="c3">Therefore, overall the run time is O(m</span><span class="c13">2 </span><span class="c3">log m). </span></p><p class="c6"><span class="c41">4.1.2 Assigning the experts </span></p><p class="c15"><span class="c3">Once the topic categories are identified, we assign users in E</span><span class="c4">q </span><span class="c0">to these categories. CTE assigns a user u 2 E</span><span class="c4">q </span><span class="c0">based on </span><span class="c3">T</span><span class="c9">u </span><span class="c3">(Algorithm 3): it assigns u to any category containing at least one topic in T</span><span class="c4">u</span><span class="c0">. Note that adhering to this approach, </span><span class="c3">a user u can be a member of several partitions expressing u&rsquo;s diversified expertise on various high-level topics. </span></p><p class="c6"><span class="c5">5. EXPERIMENTS </span></p><p class="c15"><span class="c3">We evaluate the proposed algorithms IAT and CTE on a dataset containing about 4.5 million lists (that is all lists available in Twitter when we collected the data). Each list </span></p><p class="c6"><span class="c3">Algorithm 3: Expert Assignments 1 Create an empty category </span><span class="c8">&nbsp;&#771;</span><span class="c3">C for each topical category C 2 foreach user u in E</span><span class="c9">q </span><span class="c3">do 3 foreach topic category C do 4 if there exist topic t 2 C such that t 2 T</span><span class="c9">u </span><span class="c3">then 5 </span><span class="c8">&nbsp;&#771;</span><span class="c3">C = </span><span class="c8">&nbsp;&#771;</span><span class="c3">C [ {u} </span></p><p class="c6"><span class="c3">6 Output all categories </span><span class="c8">&nbsp;&#771;</span><span class="c3">C </span></p><p class="c6"><span class="c3">Table 1: The Impact of pruning on the number of topics. </span></p><p class="c6"><span class="c3">number number size: number Query topic of of of topics after </span></p><p class="c6"><span class="c3">experts topics the 1% pruning social+media 375809 1360060 551 canada+politics 460 19080 1490 wine+Toronto 1337 27061 938 cloud+computing 56 2769 2769 fashion+trends 1112 36263 886 </span></p><p class="c15"><span class="c3">l</span><span class="c4">i </span><span class="c0">is associated with a topic t</span><span class="c4">i </span><span class="c0">(hence 4.5 million topics)</span><span class="c9">1 </span><span class="c3">For each user u in a list l</span><span class="c4">i</span><span class="c0">, the corresponding topic t</span><span class="c4">i </span><span class="c0">is </span><span class="c3">considered as a topic of expertise for u (i.e., t</span><span class="c4">i </span><span class="c0">2 T</span><span class="c4">u</span><span class="c0">). There </span><span class="c3">are 13.5 million distinct users in these lists. </span></p><p class="c15"><span class="c3">We execute the algorithms on a machine with a 16 core AMD Opteron</span><span class="c13">TM </span><span class="c3">850 Processor. This machine runs Cen- tOS 5.5 (kernel version 2.6.18-194.11.1.e15) and contains 100GB of memory. All algorithms are single-threaded and are implemented in Java. </span></p><p class="c6"><span class="c3">We observe similar trends when evaluating our algorithms with different query topics. Here, we report results for the following 5 queries: (1) canada+politics, (2) cloud+computing, (3) social+media, (4) toronto+wine, and (5) fashion+trends. For each query q (e.g., social+media), we retrieve all users whose topics of expertise match each input query (e.g., all users who are expert on social and also on media). These users form the set of experts E</span><span class="c4">q </span><span class="c0">for the given query q. </span><span class="c5">5.1 Identifying the analogous topics </span></p><p class="c35"><span class="c3">Identifying the analogous topics for the aforementioned queries involves two steps: (1) creating the correlation graph, and (2) assigning A or N states to topics (Algorithm IAT). Figure 2(a) shows the distribution of topics and experts for query social+media. We count how many users in E</span><span class="c4">q </span><span class="c3">(for a given query q) are expert in each topic in </span><span class="c8">S </span></p><p class="c15"><span class="c9">u2E</span><span class="c12">q </span><span class="c3">T</span><span class="c9">u</span><span class="c3">. We see a similar trend for all other queries. This figure sug- gests that the curve displaying the number of experts on each topic has a heavy tail. Thus, pruning the topics with very small frequency can significantly help in improving per- formance. The run time of identifying analogous topics for the query social+media is measured utilizing different prun- ing percentages and reported in Figure 2(b). Here, pruning with a percentage of &alpha; means that the topics that appear in the expertise sets of less than &alpha;% of the experts are removed; i.e., a topic t is pruned iff </span><span class="c13">|{u2E</span><span class="c1">q</span><span class="c9">|t2T</span><span class="c12">u</span><span class="c4">}| </span></p><p class="c6"><span class="c9">|E</span><span class="c12">q</span><span class="c9">| </span><span class="c8">&lt; </span><span class="c13">&alpha;</span><span class="c9">100</span><span class="c8">. </span><span class="c3">We see that a pruning of only 1 2% can significantly decrease the run time. On the other hand, pruning does not have a major impact on the accuracy of the results. The </span></p><p class="c6"><span class="c9">1</span><span class="c0">The precise process we follow to make this association can be found in [2]. </span></p><p class="c6"><span class="c7">189 </span></p><p class="c6"><span class="c3">Table 2: Analogous topics (topics are presented stemmed) social media canada politics toronto wine cloud computing fashion trends 1 busi polit food tech fashion 2 entrepreneur news food wine cloud trendsett 3 pr canada foodi cloud comput blogger 4 polit politicsdemocraci economi food drink technolog blog 5 journalist canadian polit restaur a68 fashion blogger 6 seo media canada cloudcomput fashionista 7 entertain news polit wine cloudyp fashion beauti 8 info cdnpoli toronto food tech news media 9 internet market politico canadian cloud 0 design 10 communic peopl eat cloud virtual fashion blog 11 industri canadian toronto restaur virtual shop 12 advertis local chef restaur news news 13 fav progress media busi creativ 14 brand toronto resto vendor lifestyl 15 engag journalist food toronto techi fashion style 16 communiti blogger toronto foodi work beauti 17 inform interest we like eat drink softwar beauti fashion 18 onlin market cdn polit all cloud saa busi 19 digit market liber ontario cloudcomputingenthusiast inspir 20 cultur govern culinari clouderati entertain </span></p><p class="c15"><span class="c3">topics that are reported as analogous are very similar for all these pruning percentages (e.g., no difference exists in the top 10 topics when we prune the topics with various percentages 0.1%-10%). We observe similar behavior for all other queries. For the rest of this section, we use a pruning of 1% of the topics to improve performance. Table 1 shows the number of topics that are not pruned in this step for the given five queries. According to Table 1, the pruning step with even a very small value of 1% significantly reduces the dimensionality of the problem. Hence, the problem can be solved more efficiently. The only exception here is the query for cloud+computing. We note that this query has 56 experts. A pruning of 1% removes any topic appearing in the expertise set of less than 0.56 users; thus, no topic is removed in this case (all topics appear in the expertise set of at least 1 user). </span></p><p class="c35"><span class="c3">Table 2 reports the top-20 topics for each query as identi- fied by IAT. The analogous topics are sorted based on Equa- tion 10. In Table 2, we observe, for example, that topics such as &ldquo;busi(ness)&rdquo; (topics are presented stemmed), &ldquo;en- trepreneur&rdquo;, &ldquo;journalist&rdquo;, &ldquo;seo&rdquo;, &ldquo;internet market(ing)&rdquo;, and &ldquo;communic(ation)&rdquo;are analogous to the query social+media. The utility of this information is evident: instead of focus- ing on topics such as social+media for advertising campaigns (which due to their popularity could involve a high mone- tary premium), one can focus on peripheral topics, not as popular, but still be able to target an audience close to that of the original query. </span></p><p class="c6"><span class="c3">Running IAT takes about 0.1 seconds on average for queries evaluated. Note that the majority of the run time to iden- tify analogous topics is taken by the first step. The total run time is bound by the time required to calculate the cor- relation between the topics. </span><span class="c5">5.2 Categorizing experts and topics </span></p><p class="c15"><span class="c3">The experts and topics categorization is done in two steps: (1) creating the correlation graph, and (2) executing CTE. As Figure 2(c) shows pruning significantly reduces the run time here as well for query social+media. We report the results when a pruning percentage of 1% is utilized. Similar </span></p><p class="c6"><span class="c3">behavior is observed for the other cases. of a query as the number of topics in </span><span class="c8">S </span></p><p class="c35"><span class="c3">We define the size </span><span class="c9">u2E</span><span class="c12">q </span><span class="c3">T</span><span class="c4">u </span><span class="c0">after prun- </span><span class="c3">ing takes place. Figure 2(d) reports the run time of CTE versus the size for different queries. On average, CTE takes less than 1 minute to run, making CTE practical for most real settings. As Theorem 5 suggests, the run time of CTE increases polynomially when the size increases. </span></p><p class="c15"><span class="c3">We have evaluated the CTE algorithm for many queries and observed similar trends in results of all experiments. In what follows, due to space constraints, we present re- sults using the query social+media. We stress however that these results are typical and consistent across a wide range of queries we experimented with. Thus, the specific query social+media is representative of the results obtained with algorithm CTE. Table 3 presents the categories identified by CTE for social+media. In each case, we insert an expla- nation for the set of topics in each category (in bold). It is evident that the contents of each category are highly related; i.e. from that point of view the results do make sense. </span></p><p class="c15"><span class="c3">Evaluating the output of CTE qualitatively is challeng- ing. To assess the utility of the results of CTE, we need to compare it with other applicable approaches and most importantly obtain confidence that the categories identified are indeed the correct ones. In the absence of ground truth to objectively compare the CTE approach with other appli- cable approaches (such as clustering), we resort to develop a base reference set that is manually constructed and com- pare our results against the base set. Running the CTE algorithm on multiple manually created base sets leads to highly consistent results. </span></p><p class="c15"><span class="c3">To create these base sets, we choose several topics, catego- rize each topic into subsets pre-selected by us, and manually annotate each set with a descriptive name. Hereafter, we call these new datasets, the manually annotated datasets. These manually annotated datasets, present a &ldquo;ground truth&rdquo; in which we know (or expect) a preset number of categories to appear. The goal is to categorize topics and users in the manually annotated datasets utilizing different algorithms (without taking the manual annotations into account) and </span></p><p class="c6"><span class="c7">190 </span></p><p class="c6"><span class="c2">250 500000 </span></p><p class="c6"><span class="c2">1200 </span></p><p class="c6"><span class="c2">1500 </span></p><p class="c6"><span class="c2">400000 </span></p><p class="c6"><span class="c2">1000 </span></p><p class="c6"><span class="c2">200 </span></p><p class="c6"><span class="c2">300000 </span></p><p class="c6"><span class="c2">800 600 </span></p><p class="c6"><span class="c2">1000 </span></p><p class="c6"><span class="c2">150 </span></p><p class="c6"><span class="c2">200000 </span></p><p class="c6"><span class="c2">400 </span></p><p class="c6"><span class="c2">500 </span></p><p class="c6"><span class="c2">100 </span></p><p class="c6"><span class="c2">500 1000 1500 2000 2500 3000 </span><span class="c2">1 10 100 1000 10000 100000 </span></p><p class="c6"><span class="c2">0% 2% 4% 6% 8% 10% </span></p><p class="c6"><span class="c2">0% 2% 4% 6% 8% 10% </span></p><p class="c6"><span class="c14">50 </span><span class="c2">100000 </span></p><p class="c33"><span class="c2">200 </span><span class="c2">0</span><span class="c2">0 </span></p><p class="c6"><span class="c2">0 </span></p><p class="c6"><span class="c2">0 </span></p><p class="c6"><span class="c14">) </span><span class="c14">s </span></p><p class="c6"><span class="c14">c</span><span class="c14">c) ) </span></p><p class="c6"><span class="c14">i</span><span class="c14">e</span><span class="c14">ccp</span><span class="c14">s</span><span class="c14">ee</span><span class="c14">(</span><span class="c14">osst((</span><span class="c14">e </span><span class="c14">f e e </span><span class="c14">m</span><span class="c14">o</span><span class="c14">it</span><span class="c14">mmr iiett</span><span class="c14">n </span><span class="c14">b</span><span class="c14">u</span><span class="c14">n n </span><span class="c2">R</span><span class="c14">muuu</span><span class="c2">RRNNumber of topics with more than 1% Number of experts (logarithmic scale) </span></p><p class="c6"><span class="c2">Pruning percentage </span></p><p class="c6"><span class="c2">Pruning percentage </span></p><p class="c6"><span class="c2">frequency </span></p><p class="c6"><span class="c3">(a) topic-expert distribution (b) Identifying Analogous topics (c) Categorizing experts (d) CTE vs. size </span></p><p class="c6"><span class="c3">Figure 2: Dataset distribution and run time analysis </span></p><p class="c6"><span class="c3">Table 3: Topic categories for the query &ldquo;so- cial+media&rdquo;. Rows represent categories including a description followed by the topics in each category. Tourism in North America (calgari, ottawa, van- couver, toronto, chicago, san fransisco, seattle; hotel, tourism, travel, beer, wine, restaurant, food, ...) Australia (melbourn, sydney, australia, aussi) UK (manchester, europe, uk, london) Sports (tennis, golf, hockey, baseball, nfl, sport, foot- ball) Health (mental heath, health well, pharmacy, health- care, doctor, medic, psychology, ...) Education (edu, edtech, learn, university, science, re- search, academy, ...) </span></p><p class="c35"><span class="c3">The optimal categorization of D is achieved when (1) the topics in S are categorized into 4 categories S</span><span class="c4">1</span><span class="c0">, S</span><span class="c4">2</span><span class="c0">, S</span><span class="c4">3</span><span class="c0">, </span><span class="c3">and S</span><span class="c4">4 </span><span class="c0">(in accordance </span><span class="c3">structed); and (2) </span><span class="c0">with the way </span><span class="c3">the users in U</span><span class="c13">0 </span><span class="c0">the data set was con- </span><span class="c3">are categorized into 4 categories of {users who are expert on a topic in S</span><span class="c4">1</span><span class="c0">}, &middot;&middot;&middot;, </span><span class="c3">{users who are expert on a topic in S</span><span class="c9">4</span><span class="c3">}. </span></p><p class="c15"><span class="c3">Although algorithm CTE does not need a number of cat- egories as input, the baseline clustering techniques do re- quire the number of clusters (categories). Thus, we provide them with the optimal number 4 providing them with an advantage. Algorithm CTE identifies the optimal number of categories without receiving it as an input. </span></p><p class="c6"><span class="c3">To calculate the accuracy of an algorithm, we proceed as follows. Assume algorithm X outputs topic categories Investments (invest, economia, economy, financ, realest, realtor, real estat) </span></p><p class="c35"><span class="c3">C</span><span class="c4">1</span><span class="c0">,C</span><span class="c4">2</span><span class="c0">, &middot;&middot;&middot; ,C</span><span class="c4">r </span><span class="c0">and user categories D</span><span class="c4">1</span><span class="c0">,D</span><span class="c4">2</span><span class="c0">,&middot;&middot;&middot; ,D</span><span class="c4">s</span><span class="c0">. We uti- </span><span class="c3">lize four annotations SCIENCE, POLITICS, SPORTS, and South by South &ldquo;SXSW&rdquo; festivals (sxsw, west texas, </span></p><p class="c6"><span class="c3">TECHNOLOGY to label each category produced by X. A austin, houston, dallas) </span></p><p class="c6"><span class="c3">category C</span><span class="c9">i </span><span class="c3">(D</span><span class="c9">i</span><span class="c3">) is labeled by the annotation having the Law (legal, law, lawyer) </span></p><p class="c6"><span class="c3">maximum number of entities in that category. For exam- Twibes: groups of people with common interests </span></p><p class="c6"><span class="c3">ple, consider a topic category C</span><span class="c9">1 </span><span class="c3">= {physics, soccer, math}. (twibe socialnetwork, twibe journal, twibe blog, twibe </span></p><p class="c6"><span class="c3">This category includes two topics in SCIENCE, one topic writer, twibe travel, twibe photographi, twibe webdesign, </span></p><p class="c6"><span class="c3">in SPORTS, and no topic in POLITICS or TECHNOL- twibe internetmarket, twibe brand, twibe socialmedia, </span></p><p class="c6"><span class="c3">OGY. Thus, we label the category C</span><span class="c9">1 </span><span class="c3">as SCIENCE. More- twibe advertis, twibe entrepreneur, ...) </span></p><p class="c15"><span class="c3">compare how close the results are to the manual annota- tions. We compare CTE with the baseline clustering algo- rithm k-means (denoted by kmeans in Figure 3) and 3 base- line co-clustering algorithms Euclidean distance (denoted by </span></p><p class="c35"><span class="c3">over, assume D</span><span class="c4">1 </span><span class="c0">= {u</span><span class="c4">1</span><span class="c0">,u</span><span class="c4">2</span><span class="c0">,u</span><span class="c3">math}, Tin </span><span class="c4">u</span><span class="c12">4 </span><span class="c3">D= </span><span class="c4">1 </span><span class="c0">are </span><span class="c3">{chemistry, T</span><span class="c9">u</span><span class="c0">experts </span><span class="c12">2 </span><span class="c3">= {soccer}, republican}. T</span><span class="c9">u</span><span class="c12">3 </span><span class="c4">3</span><span class="c0">,u</span><span class="c3">= </span><span class="c4">4</span><span class="c0">}, </span><span class="c3">{math, </span><span class="c0">where </span><span class="c3">democrats}, </span><span class="c0">T</span><span class="c4">u</span><span class="c12">1 </span><span class="c3">= {soccer, and We can observe that 3 users </span><span class="c0">on SCIENCE, 2 users on SPORTS, and 2 </span><span class="c3">users on POLITICS. Therefore, we label the category D</span><span class="c9">1 </span><span class="c3">as SCIENCE. cocluster Euclidean), Information theoretic (denoted by co- cluster IT), and minimum sum-squared residue co-clustering </span></p><p class="c6"><span class="c3">The topic categorization accuracy (user categorization ac- curacy) of an algorithm is the percentage of the topics (users) (denoted by cocluster MSR) [3,6,7]. The following categories form one sample manually annotated dataset: (1) A category S</span><span class="c9">1 </span><span class="c3">including topics {physics, math, chem- istry}. We call this category, SCIENCE. (2) A category S</span><span class="c4">2 </span><span class="c0">including topics {democrats, republican, </span></p><p class="c35"><span class="c3">that are labeled correctly. Note that in the previous exam- ple, one topic is labeled inaccurately in C</span><span class="c4">1 </span><span class="c0">(the topic soccer </span><span class="c3">is labeled as SCIENCE) and one user is labeled inaccurately in D</span><span class="c9">1 </span><span class="c3">(u</span><span class="c9">2 </span><span class="c3">is labeled as SCIENCE without having any ex- pertise on physics, math, or chemistry). Figure 3(a) reports politics}. We call this category, POLITICS. (3) A category S</span><span class="c4">3 </span><span class="c0">including topics {soccer, football, fifa}. </span></p><p class="c6"><span class="c3">the accuracy for topic categories and Figure 3(b) shows the accuracy for user categories for all algorithms. Figure 3 We call this category, SPORTS. (4) A category S</span><span class="c4">4 </span><span class="c0">including topics {google, tablet, android}. </span><span class="c3">We call this category, TECHNOLOGY. </span></p><p class="c6"><span class="c3">demonstrates the superiority of CTE when compared with baseline clustering algorithms. </span></p><p class="c6"><span class="c3">We create a dataset D. The set of topics in D is S = {physics, math, chemistry, democrats, republican, politics, </span></p><p class="c6"><span class="c5">6. RELATED WORKS </span><span class="c3">soccer, football, fifa, google, tablet, android}. Users in D </span></p><p class="c6"><span class="c3">Twitter lists are recently used to address a few questions are all users who are expert in at least one topic in S (U </span><span class="c13">0 </span><span class="c3">= </span></p><p class="c6"><span class="c3">such as identifying users&rsquo; topics of expertise [2,16] and sepa- {u 2 U|T</span><span class="c9">u </span><span class="c3">\ S = ;} where U the Twitter dataset). For each denotes the set of all users in user u 2 U</span><span class="c13">0</span><span class="c3">, T</span><span class="c4">u </span><span class="c0">\ S is the </span></p><p class="c35"><span class="c3">rating elite users (e.g., celebrities) from ordinary users [18]. The problem of identifying a set of topics that can be uti- set of all topics (among topics in D) that u is an expert on. </span></p><p class="c6"><span class="c3">lized as a substitute for an expensive topic is studied for the We compare the results of CTE and the baseline algorithms </span></p><p class="c6"><span class="c3">case that target sets of topics are given and the cost for each when deployed to categorize users and topics in D. </span></p><p class="c6"><span class="c3">topic is known [9]. In many real settings we don&rsquo;t have ac- </span></p><p class="c6"><span class="c7">191 </span></p><p class="c17"><span class="c57">A</span><span class="c18">ccuracy </span></p><p class="c6 c66"><span class="c19">6040</span><span class="c19">20</span><span class="c28">0</span><span class="c16">CTE cocluster Euclidean </span></p><p class="c33 c43"><span class="c16">cocluster IT </span></p><p class="c22"><span class="c16">cocluster MSR </span></p><p class="c22"><span class="c16">cocluster MSR </span></p><p class="c6 c60"><span class="c16">kmeans </span></p><p class="c6 c60"><span class="c16">kmeans </span></p><p class="c6 c60"><span class="c16">kmeans </span></p><p class="c6 c11"><span class="c18">c</span><span class="c19">60</span><span class="c18">arucc</span><span class="c19">40</span><span class="c57">A</span><span class="c19">20</span><span class="c28">0</span><span class="c16">CTE cocluster Euclidean </span></p><p class="c6 c11"><span class="c18">c</span><span class="c19">60</span><span class="c18">arucc</span><span class="c19">40</span><span class="c57">A</span><span class="c19">20</span><span class="c28">0</span><span class="c16">CTE cocluster Euclidean </span></p><p class="c6 c11"><span class="c18">c</span><span class="c19">60</span><span class="c18">arucc</span><span class="c19">40</span><span class="c57">A</span><span class="c19">20</span><span class="c28">0</span><span class="c16">CTE cocluster Euclidean </span></p><p class="c6 c11"><span class="c18">c</span><span class="c19">60</span><span class="c18">arucc</span><span class="c19">40</span><span class="c57">A</span><span class="c19">20</span><span class="c28">0</span><span class="c16">CTE cocluster Euclidean </span></p><p class="c6 c11"><span class="c18">c</span><span class="c19">60</span><span class="c18">arucc</span><span class="c19">40</span><span class="c57">A</span><span class="c19">20</span><span class="c28">0</span><span class="c16">CTE cocluster Euclidean </span></p><p class="c33 c54"><span class="c16">cocluster IT </span></p><p class="c33 c54"><span class="c16">cocluster IT </span></p><p class="c33 c54"><span class="c16">cocluster IT </span></p><p class="c33 c54"><span class="c16">cocluster IT </span></p><p class="c33 c54"><span class="c16">cocluster IT </span></p><p class="c21"><span class="c16">cocluster MSR </span></p><p class="c21"><span class="c16">cocluster MSR </span></p><p class="c21"><span class="c16">cocluster MSR </span></p><p class="c21"><span class="c16">cocluster MSR </span></p><p class="c21"><span class="c16">cocluster MSR </span></p><p class="c21"><span class="c16">cocluster MSR </span></p><p class="c6 c36"><span class="c16">kmeans </span></p><p class="c6 c36"><span class="c16">kmeans </span></p><p class="c6 c36"><span class="c16">kmeans </span></p><p class="c6 c36"><span class="c16">kmeans </span></p><p class="c6 c36"><span class="c16">kmeans </span></p><p class="c6 c36"><span class="c16">kmeans </span></p><p class="c6 c36"><span class="c16">kmeans </span></p><p class="c6 c47"><span class="c3">The authors would like to thank Alex Cheng for his assis- tance with data preparation. </span></p><p class="c6 c47"><span class="c3">The authors would like to thank Alex Cheng for his assis- tance with data preparation. </span></p><p class="c6 c47"><span class="c3">The authors would like to thank Alex Cheng for his assis- tance with data preparation. </span></p><p class="c24 c65"><span class="c5">9. REFERENCES </span></p><p class="c6 c24"><span class="c5">9. REFERENCES </span></p><p class="c6 c24"><span class="c5">9. REFERENCES </span></p><p class="c6 c24"><span class="c5">9. REFERENCES </span></p><p class="c6 c24"><span class="c5">9. REFERENCES </span></p><p class="c6 c24"><span class="c5">9. REFERENCES </span></p><p class="c6 c24"><span class="c5">9. REFERENCES </span></p><p class="c6 c24"><span class="c5">9. REFERENCES </span></p><p class="c10 c84"><span class="c3">[1] N. Bansal, A. Blum, and S. Chawla. Correlation </span></p><p class="c6 c10"><span class="c3">[1] N. Bansal, A. Blum, and S. Chawla. Correlation </span></p><p class="c6 c10"><span class="c3">[1] N. Bansal, A. Blum, and S. Chawla. Correlation </span></p><p class="c6 c10"><span class="c3">[1] N. Bansal, A. Blum, and S. Chawla. Correlation </span></p><p class="c6 c10"><span class="c3">[1] N. Bansal, A. Blum, and S. Chawla. Correlation </span></p><p class="c6 c10"><span class="c3">[1] N. Bansal, A. Blum, and S. Chawla. Correlation </span></p><p class="c82"><span class="c3">(a) Topic categories (b) User categories Figure 3: Comparison between the accuracy of dif- ferent clustering algorithms cess to this information. This paper focuses on the problem when the target sets and costs are unknown. </span></p><p class="c38"><span class="c3">Automatons are utilized in several problems such as iden- tifying bursts of activity in time-series data [13], spatial datasets [14], and subgraphs of social networks&rsquo; graphs [8]. Perhaps the most similar work to our IAT algorithm is the DIBA algorithm [8] that is proposed to identify the bursty subgraphs of users in a social network when the informa- tion burst happens as a result of an external activity (such as an earthquake). We note that there are major differ- ences between our IAT and DIBA algorithms: (1) DIBA is mainly designed for unweighted graphs; (2) DIBA does not consider negative edges. In fact, the optimization problem (Problem 2) in presence of negative edges is NP-hard (The- orem 3) while if all weights are non-negative, the problem would become equivalent to min-cut and can be solved in polynomial time [8]; (3) IAT addresses Problem 2 by locat- ing the optimal cycle-free subgraph, while DIBA utilizes a heuristic approach that randomly orders graph nodes and attempts to find the best label for each node in this order; this approach does not identify the optimal subgraph and may ignore considering several important (costly) edges. </span></p><p class="c38"><span class="c3">The traditional clustering algorithms can be categorized to partitioning methods (e.g., k-means), hierarchical meth- ods (top-down, bottom-up), Density-based (e.g., DBSCAN), model-based (EM), link-based, bi-clustering, and graph par- titioning (e.g., finding cliques or quasi-cliques in the graph, and correlation clustering). These algorithms also suffer from several disadvantages in the case of our problem. To the best of our knowledge none of these clustering algorithms provide all of the four desirable properties (introduced in Section 4.1); hence they are not applicable to categorize ex- perts. For completeness, we compared our proposed algo- rithm with some of these algorithms in Section 5. </span></p><p class="c44"><span class="c5">7. CONCLUSION AND FUTURE WORKS </span></p><p class="c30"><span class="c3">In this paper we introduce two problems. The first prob- lem is to identify topics (called analogous) that have (ap- proximately) the same audience on a micro-blogging plat- forms as a query topic. The idea is that by bidding on an analogous topic instead of the original query topic, we will reach (approximately) the same audience while spending less budget on advertising. This is inspired by the social media advertising platforms. The second problem is to understand the diversified expertise of the experts on the given query topic and categorize these experts. We evaluate the tech- niques proposed for both problems on a large dataset from Twitter attesting their efficiency and accuracy. </span></p><p class="c32"><span class="c3">An important direction for future work is to study the problems when the bids on each topic is known. This exten- sion can assist advertisers to maximize their revenue while minimizing the advertising cost. </span></p><p class="c6 c79"><span class="c3">clustering. Mach. Learn., 56(1-3):89&ndash;113, June 2004. [2] A. Cheng, N. Bansal, and N. Koudas. Peckalytics: </span></p><p class="c48"><span class="c3">Analyzing experts and interests on twitter. SIGMOD Demo Track, 2013. [3] H. Cho, I. Dhillon, Y. Guan, and S. Sra. Minimum </span></p><p class="c80"><span class="c3">sum-squared residue co-clustering of gene expression data. SDM, pages 114&ndash;125, 2004. [4] T. H. Cormen, C. Stein, R. L. Rivest, and C. E. </span></p><p class="c83"><span class="c3">Leiserson. Introduction to Algorithms. McGraw-Hill Higher Education, 3rd edition, 2009. Chapter 23. [5] E. D. Demaine, D. Emanuel, A. Fiat, and </span></p><p class="c58"><span class="c3">N. Immorlica. Correlation clustering in general weighted graphs. Theor. Comput. Sci., 361(2):172&ndash;187, Sept. 2006. [6] I. Dhillon and Y. Guan. Information theoretic </span></p><p class="c64"><span class="c3">clustering of sparse co-occurrence data. ICDM, pages 517&ndash;520, 2003. [7] I. S. Dhillon, S. Mallela, and D. S. Modha. Information </span></p><p class="c63"><span class="c3">theoretic co-clustering. SIGKDD, pages 89&ndash;98, 2003. [8] M. Eftekhar, N. Koudas, and Y. Ganjali. Bursty </span></p><p class="c59"><span class="c3">subgraphs in social networks. WSDM, pages 213&ndash;222, 2013. [9] M. Eftekhar, S. Thirumuruganathan, G. Das, and </span></p><p class="c31"><span class="c3">N. Koudas. Price trade-offs in social media advertising. In Proceedings of the Second Edition of the ACM Conference on Online Social Networks, COSN &rsquo;14, pages 169&ndash;176, New York, NY, USA, 2014. ACM. [10] Facebook. Facebook help centre. </span></p><p class="c68"><span class="c3">https://www.facebook.com/help/ads. [11] J. Joseph B. Kruskal. On the shortest spanning </span></p><p class="c76"><span class="c3">subtree of a graph and the traveling salesman problem. Proceedings of the American Mathematical Society, 7(1):48&ndash;50, June 1956. [12] R. Karp. Reducibility among combinatorial problems. In 50 Years of Integer Programming 1958-2008, pages 219&ndash;241. Springer Berlin Heidelberg, 2010. [13] J. Kleinberg. Bursty and hierarchical structure in </span></p><p class="c73"><span class="c3">streams. SIGKDD, pages 91&ndash;101, 2002. [14] M. Mathioudakis, N. Bansal, and N. Koudas. </span></p><p class="c81"><span class="c3">Identifying, attributing and describing spatial bursts. VLDB Endowment, 3(1-2):1091&ndash;1102, Sept. 2010. [15] J. L. Rodgers and A. W. Nicewander. Thirteen ways </span></p><p class="c75"><span class="c3">to look at the correlation coefficient. The American Statistician, 42(1):59&ndash;66, 1988. [16] N. K. Sharma, S. Ghosh, F. Benevenuto, N. Ganguly, and K. Gummadi. Inferring who-is-who in the twitter social network. In Proceedings of the 2012 ACM workshop on Workshop on online social networks, pages 55&ndash;60, 2012. [17] Twitter. Start Advertising 1 Twitter for Business. </span></p><p class="c29"><span class="c3">https://business.twitter.com/start-advertising. [18] S. Wu, J. M. Hofman, W. A. Mason, and D. J. Watts. </span></p><p class="c72"><span class="c3">Who says what to whom on twitter. In Proceedings of the 20th international conference on World wide web, pages 705&ndash;714, 2011. </span></p><p class="c6 c49"><span class="c28">100 </span></p><p class="c70"><span class="c19">80 </span></p><p class="c6 c62"><span class="c28">100 </span></p><p class="c67"><span class="c19">80 </span><span class="c18">y </span></p><p class="c6 c51"><span class="c5">8. ACKNOWLEDGMENTS </span></p><p class="c6 c51"><span class="c5">8. ACKNOWLEDGMENTS </span></p><p class="c47 c71"><span class="c3">The authors would like to thank Alex Cheng for his assis- tance with data preparation. </span></p><p class="c6 c47"><span class="c3">The authors would like to thank Alex Cheng for his assis- tance with data preparation. </span></p><p class="c74"><span class="c7">192 </span></p></body></html>