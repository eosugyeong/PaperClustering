<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c61{margin-left:-19pt;padding-top:8.6pt;text-indent:19.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c83{margin-left:-18.5pt;padding-top:1pt;text-indent:34.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.4pt}.c82{margin-left:-16.9pt;padding-top:7.4pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.2pt}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c96{margin-left:-16.9pt;padding-top:2.2pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.4pt}.c57{margin-left:-19pt;padding-top:1.9pt;text-indent:28.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c71{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:7.2pt;font-family:"Arial";font-style:normal}.c30{margin-left:-17.4pt;padding-top:2.2pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.2pt}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c16{margin-left:-18.5pt;padding-top:1.9pt;text-indent:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18.8pt}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c66{margin-left:-17.1pt;padding-top:2.2pt;text-indent:27.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.4pt}.c94{margin-left:-19pt;padding-top:7.7pt;text-indent:29pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.8pt}.c86{margin-left:-15.1pt;padding-top:1pt;text-indent:27.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.4pt}.c115{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Courier New";font-style:normal}.c55{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.6pt;font-family:"Arial";font-style:normal}.c68{margin-left:-17.1pt;padding-top:2.2pt;text-indent:27.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19pt}.c76{margin-left:-18.5pt;padding-top:16.1pt;text-indent:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.2pt}.c131{margin-left:-17.1pt;padding-top:1.9pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.4pt}.c10{margin-left:-17.1pt;padding-top:2.2pt;text-indent:27.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.7pt}.c60{margin-left:-17.1pt;padding-top:16.1pt;text-indent:27.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.4pt}.c53{margin-left:-19pt;padding-top:10.3pt;text-indent:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.4pt}.c120{margin-left:-18.5pt;padding-top:1.2pt;text-indent:34.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.1pt}.c114{margin-left:-17.8pt;padding-top:2.2pt;text-indent:27.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.2pt}.c29{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c98{margin-left:-18.7pt;padding-top:1.9pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.2pt}.c21{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:7.6pt;font-family:"Arial";font-style:normal}.c108{margin-left:-16.9pt;padding-top:9.8pt;text-indent:26.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25pt}.c63{margin-left:-19pt;padding-top:11pt;text-indent:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c43{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c129{margin-left:-17.1pt;padding-top:13.2pt;text-indent:17.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18.2pt}.c28{margin-left:-17.1pt;padding-top:7pt;text-indent:27.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.4pt}.c36{margin-left:-18.5pt;padding-top:1.9pt;text-indent:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c116{margin-left:-19pt;padding-top:1.9pt;text-indent:29.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c13{margin-left:-19pt;padding-top:2.2pt;text-indent:28.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Arial";font-style:normal}.c52{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.1pt;font-family:"Courier New";font-style:normal}.c95{margin-left:-16.9pt;padding-top:1.9pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.4pt}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c62{margin-left:-16.9pt;padding-top:7.9pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-20.6pt}.c59{margin-left:-16.9pt;padding-top:2.2pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18pt}.c101{margin-left:-19pt;padding-top:1.9pt;text-indent:29pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c67{margin-left:-16.9pt;padding-top:0.5pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.4pt}.c47{margin-left:-17.4pt;padding-top:22.6pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.4pt}.c42{margin-left:-18.5pt;padding-top:1pt;text-indent:34.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.1pt}.c37{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.6pt;font-family:"Arial";font-style:normal}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:9.1pt;font-family:"Arial";font-style:normal}.c35{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.1pt;font-family:"Arial";font-style:normal}.c90{margin-left:-18.5pt;padding-top:1pt;text-indent:34.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.9pt}.c100{margin-left:-7.8pt;padding-top:2.2pt;text-indent:16.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:112.6pt}.c32{margin-left:-17.1pt;padding-top:2.2pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.2pt}.c81{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:9.1pt;font-family:"Courier New";font-style:normal}.c17{margin-left:-18.7pt;padding-top:1.9pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c127{margin-left:-19pt;padding-top:56.4pt;text-indent:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.2pt}.c45{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.2pt;font-family:"Arial";font-style:normal}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.5pt;font-family:"Arial";font-style:normal}.c25{margin-left:-19pt;padding-top:1.9pt;text-indent:28.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c27{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.3pt;font-family:"Arial";font-style:normal}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.9pt;font-family:"Arial";font-style:normal}.c103{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c89{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:3.4pt;font-family:"Arial";font-style:normal}.c20{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c11{margin-left:-19pt;padding-top:2.2pt;text-indent:28.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.8pt}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.1pt;font-family:"Arial";font-style:normal}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c41{margin-left:-16.9pt;padding-top:1.2pt;text-indent:32.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19pt}.c111{margin-left:-18.5pt;padding-top:8.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.2pt}.c93{margin-left:-19pt;padding-top:22.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.8pt}.c107{margin-left:-18.5pt;padding-top:10.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:142.5pt}.c50{margin-left:-17.3pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-16.1pt}.c122{margin-left:-19pt;padding-top:6.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.2pt}.c121{margin-left:20.4pt;padding-top:14.2pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:83.4pt}.c97{margin-left:-23.8pt;padding-top:737.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24pt}.c92{margin-left:218.6pt;padding-top:51.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c34{margin-left:-2.9pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:138.9pt}.c22{margin-left:-16.9pt;padding-top:16.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-2.6pt}.c15{margin-left:-15.1pt;padding-top:3.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c77{margin-left:-17.1pt;padding-top:10.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19pt}.c88{margin-left:-18.5pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c33{margin-left:-16.9pt;padding-top:16.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.4pt}.c2{margin-left:-18.5pt;padding-top:235.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.2pt}.c99{margin-left:218.6pt;padding-top:50.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c125{margin-left:-8.6pt;padding-top:1.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.8pt}.c126{margin-left:5pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:101pt}.c85{margin-left:-16.9pt;padding-top:13.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.4pt}.c109{margin-left:-75.2pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:178.6pt}.c133{margin-left:-9.6pt;padding-top:1.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:27.5pt}.c31{margin-left:-19pt;padding-top:10.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c69{margin-left:-9.6pt;padding-top:2.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:35.2pt}.c84{margin-left:218.6pt;padding-top:50.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c49{margin-left:-16.9pt;padding-top:21.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:13.7pt}.c117{margin-left:218.6pt;padding-top:64.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c56{margin-left:-17.1pt;padding-top:2.2pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-19.4pt}.c91{margin-left:100.3pt;padding-top:10.3pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:5.4pt}.c119{margin-left:218.6pt;padding-top:51.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c78{margin-left:-0.7pt;padding-top:1.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:42.9pt}.c39{margin-left:-6.8pt;padding-top:4.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-5.3pt}.c54{margin-left:-17.1pt;padding-top:13.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:11.8pt}.c112{margin-left:218.6pt;padding-top:51.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c130{margin-left:-16.9pt;padding-top:19.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.2pt}.c51{margin-left:-17.1pt;padding-top:13.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.2pt}.c58{margin-left:82.2pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:19pt}.c123{margin-left:-13.8pt;padding-top:17.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-14.9pt}.c38{margin-left:-18.5pt;padding-top:565.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c12{margin-left:-18.5pt;padding-top:16.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c113{margin-left:-18.5pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-16.4pt}.c65{padding-top:1pt;text-indent:32.5pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c79{padding-top:1.9pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c72{padding-top:11.3pt;text-indent:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c104{padding-top:2.2pt;text-indent:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c124{padding-top:2.2pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c132{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c70{padding-top:16.3pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c102{padding-top:98.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c44{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c73{padding-top:8.2pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c64{margin-left:-17.1pt;text-indent:17.4pt;margin-right:-18pt}.c87{margin-left:-17.1pt;text-indent:27.2pt;margin-right:-19.4pt}.c106{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c75{margin-left:-16.9pt;margin-right:-18pt}.c128{margin-left:-16.9pt;margin-right:-9.8pt}.c19{margin-left:-16.9pt;margin-right:-19.2pt}.c80{margin-left:237.1pt;margin-right:-273.4pt}.c110{margin-left:8.6pt;margin-right:118.6pt}.c46{margin-left:120.2pt;margin-right:5.8pt}.c74{margin-left:-16.9pt;margin-right:-17.8pt}.c48{margin-left:-18.5pt;margin-right:-17.6pt}.c118{margin-left:-19pt;margin-right:-12.3pt}.c105{margin-left:-16.9pt;margin-right:-19pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c106"><p class="c97"><span class="c1">Series ISSN: 2367-2005 253 </span><span class="c103">10.5441/002/edbt.2018.23 </span></p><p class="c50"><span class="c45">Apollo: Learning Query Correlations for Predictive Caching in Geo-Distributed Systems </span></p><p class="c121"><span class="c23">Brad Glasbergen </span><span class="c26">University of Waterloo bjglasbe@uwaterloo.ca </span></p><p class="c109"><span class="c23">Michael Abebe </span><span class="c26">University of Waterloo mtabebe@uwaterloo.ca </span></p><p class="c58"><span class="c23">Khuzaima Daudjee </span><span class="c26">University of Waterloo kdaudjee@uwaterloo.ca </span></p><p class="c58"><span class="c23">Khuzaima Daudjee </span><span class="c26">University of Waterloo kdaudjee@uwaterloo.ca </span></p><p class="c91"><span class="c23">Scott Foggo </span><span class="c26">University of Waterloo sjfoggo@uwaterloo.ca </span></p><p class="c126"><span class="c23">Anil Pacaci </span><span class="c26">University of Waterloo apacaci@uwaterloo.ca </span></p><p class="c61"><span class="c6">ABSTRACT </span><span class="c3">The performance of modern geo-distributed database applications is increasingly dependent on remote access latencies. Systems that cache query results to bring data closer to clients are gaining popularity but they do not dynamically learn and exploit access patterns in client workloads. We present a novel prediction frame- work that identifies and makes use of workload characteristics obtained from data access patterns to exploit query relationships within an application&rsquo;s database workload. We have designed and implemented this framework as Apollo, a system that learns query patterns and adaptively uses them to predict future queries and cache their results. Through extensive experimentation with two different benchmarks, we show that Apollo provides significant performance gains over popular caching solutions through reduced query response time. Our experiments demonstrate Apollo&rsquo;s ro- bustness to workload changes and its scalability as a predictive cache for geo-distributed database applications. </span></p><p class="c12"><span class="c6">1 INTRODUCTION </span><span class="c3">Modern distributed database systems and applications frequently have to handle large query processing latencies resulting from the geo-distribution of data [11, 13, 41]. Industry reports indicate that even small increases in client latency can result in significant drops in both web traffic [20] and sales [3, 30]. A common solution to this latency problem is to place data closer to clients [38, 39] using caches, thereby avoiding costly remote round-trips to datacenters [27]. Static data, such as images and video content, is often cached on servers geographically close to clients. These caching servers, called edge nodes, are a crucial component in industry architec- tures. To illustrate this, consider Google&rsquo;s datacenter and edge node locations in Figure 1. Google has comparatively few datacen- ter locations relative to edge nodes, and the latency between the edge nodes and datacenters can be quite large. Efficiently caching data on these edge nodes substantially reduces request latency for clients. </span></p><p class="c48 c104"><span class="c3">Existing caching solutions for edge nodes and content deliv- ery networks (CDN) focus largely on static data, necessitating costly round trips to remote data centers for requests relying on dynamic data [21]. Since a majority of webpages today are gener- ated dynamically [5], a large number of requests are not satisfied by cached data, thereby incurring significant latency penalties. We address this concern in Apollo, a system that exploits client access patterns to intelligently prefetch and cache dynamic data on edge nodes. </span></p><p class="c88"><span class="c9">&copy; 2018 Copyright held by the owner/author(s). Published in Proceedings of the 21st International Conference on Extending Database Technology (EDBT), March 26-29, 2018, ISBN 978-3-89318-078-3 on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0. </span></p><p class="c8 c46"><span class="c9">(b) Edge Node Locations </span></p><p class="c123"><span class="c3">Figure 1: Google&rsquo;s datacenter and edge node locations [21]. </span></p><p class="c49"><span class="c14">1. </span><span class="c20">SELECT </span><span class="c14">C_ID </span><span class="c20">FROM </span><span class="c14">CUSTOMER </span><span class="c20">WHERE </span><span class="c14">C_UNAME = @C_UN </span><span class="c20">and </span><span class="c14">C_PASSWD = @C_PAS </span></p><p class="c22"><span class="c14">2. </span><span class="c20">SELECT </span><span class="c14">MAX(O_ID) </span><span class="c20">FROM </span><span class="c14">ORDERS </span><span class="c20">WHERE </span><span class="c14">O_C_ID = @C_ID </span></p><p class="c70 c128"><span class="c14">3. </span><span class="c20">SELECT </span><span class="c14">... </span><span class="c20">FROM </span><span class="c14">ORDER_LINE, ITEM </span><span class="c20">WHERE </span><span class="c14">OL_I_ID = I_ID </span><span class="c20">and </span><span class="c14">OL_O_ID = @O_ID </span></p><p class="c130"><span class="c3">Figure 2: A set of motivating queries in TPC-W&rsquo;s Order Display web interaction. Boxes of the same colour indicate shared values across queries. </span></p><p class="c47"><span class="c3">Database client workloads often exhibit query patterns, cor- responding to application usage patterns. In many workloads [1, 10, 42], queries are highly correlated. That is, the execution of one query determines which query executes next and with what parameters. These dependencies provide opportunities for opti- mization through predictively caching queries. In this paper, we focus on discovering relationships among queries in a workload. We exploit the discovered relationships to predictively execute future dependent queries. Our focus is to reduce the response time of consequent queries by predicting and executing them, caching query results ahead of time. In doing so, clients can avoid contact- ing a database located at a distant datacenter, satisfying queries instead from the cache on a closer edge node. </span></p><p class="c10"><span class="c3">As examples of query patterns, we consider a set of queries from the TPC-W benchmark [42]. In this benchmark&rsquo;s Order Display web interaction, shown in Figure 2, we observe that the second query is dependent upon the result set of the first query. Therefore, given the result set of the first query, we can determine the input set of the second query, predictively execute it, and cache its results. After the second query has executed, we can use its result set as input to the third query, again presenting an </span></p><p class="c8 c110"><span class="c9">(a) Datacenter Locations </span></p><p class="c8 c110"><span class="c9">(a) Datacenter Locations </span></p><p class="c2"><span class="c3">Figure 3: Query flow through components of the predictive framework. </span></p><p class="c93"><span class="c3">opportunity for predictive caching. Similar scenarios abound in the TPC-W and TPC-C benchmarks, such as in the TPC-W Best- Seller web interaction and in the TPC-C Stock level transaction. Examples that benefit from such optimization, including real- world applications, have been previously described [10]. </span></p><p class="c13"><span class="c3">In this paper, we propose a novel prediction framework that uses a query-pattern aware technique to improve performance in geo- distributed database systems through caching. We implement this framework in Apollo, which uses a query transition graph to learn correlations between queries and to predict future queries. In doing so, Apollo determines query results that should be cached ahead of time so that future queries can be satisfied from a cache deployed close to clients. Apollo prioritizes common and expensive queries for caching, eliminating or reducing costly round-trips to remote data without requiring modifications to the underlying database architecture. Apollo&rsquo;s ability to learn allows it to rapidly adapt to workloads in an online fashion. Apollo is designed to enhance an existing caching layer, providing predictive caching capabilities for improved performance. </span></p><p class="c125"><span class="c3">The contributions of this paper are threefold: (1) We propose a novel predictive framework to identify re- lationships among queries and predict consequent ones. Our framework uses online learning to adapt to changing workloads and reduce query response times (Section 2). (2) We design and implement our framework in a system called Apollo, which predictively executes and caches query re- sults on edge nodes close to the client (Section 3). (3) We deploy and extensively test Apollo on Amazon EC2 using the TPC-W and TPC-C benchmark workloads to show that significant performance gains can be achieved for different query workloads (Section 4). </span></p><p class="c63"><span class="c6">2 PREDICTING QUERIES </span><span class="c3">A client&rsquo;s database workload is comprised of a stream of queries and the transitions between them. These queries are synthesized into the query transition graph, which is at the core of our pre- dictive framework. From this query transition graph, we discover </span></p><p class="c5 c80"><span class="c3">query relationships, dependencies and workload characteristics for use in our predictive framework. The predictive framework stores query result sets in a shared local cache, querying the re- mote database if a client submits a query for which the cache does not have the results. </span></p><p class="c96"><span class="c3">Figure 3 gives a high level overview of how incoming queries are executed, synthesized into the query transition graph, and used for future query predictions. Incoming queries are routed to the query processor, which retrieves query results from a shared local query result cache, falling back to a remote database on a cache miss. Query results are immediately returned to the client and, together with their source queries, are mapped to more general- ized query template representations (Section 2.1). These query templates are placed into per-client queues of queries called query streams, which are continuously scanned for relationships among executed queries. Query relationships are synthesized into the query transition graph and then used to detect query correlations, discovering dependencies among executed queries and storing them in a dependency graph. This dependency graph is used by the prediction engine to predict consequent queries given client queries that have executed. </span></p><p class="c32"><span class="c3">Although we focus on geographically distributed edge nodes with remote datacenters, Apollo can also be deployed locally as a middleware cache. Our experiments in Section 4 show that both deployment environments benefit significantly from Apollo&rsquo;s predictive caching. </span></p><p class="c96"><span class="c3">Next, we discuss the abstractions and algorithms of our pre- dictive framework, describing how queries flowing through the system are merged into the underlying models and used to predict future queries. </span></p><p class="c33"><span class="c6">2.1 Query Templates </span><span class="c3">Using a transition graph to reason about query relationships re- quires a mapping from database workloads (queries and query relationships) to transition structures (query templates and tem- plate transitions). We propose a formalization of this mapping through precise definitions, and then show how our model can be used to predict future queries. </span></p><p class="c56"><span class="c3">Queries within a workload are often correlated directly through parameter sharing. Motivated by the Stock Level transaction in the TPC-C benchmark, consider an example of parameter sharing in which an application executes query Q</span><span class="c4">1 </span><span class="c7">to look up a product ID </span><span class="c3">followed by query Q</span><span class="c0">2 </span><span class="c3">to check the stock level of a given product ID. A common usage pattern is to execute Q</span><span class="c4">1</span><span class="c7">, and then use the </span><span class="c3">returned product ID as an input to Q</span><span class="c4">2 </span><span class="c7">to check that product&rsquo;s stock </span><span class="c3">level. In this case, Q</span><span class="c0">2 </span><span class="c3">is directly related to Q</span><span class="c0">1 </span><span class="c3">via a dependency relationship. Specifically, Q</span><span class="c4">2 </span><span class="c7">relies on the output of Q</span><span class="c4">1 </span><span class="c7">to execute. </span><span class="c3">We generalize our model by tracking relationships among query templates rather than among parameterized queries. Two queries Q</span><span class="c4">1 </span><span class="c7">and Q</span><span class="c4">2 </span><span class="c7">have the same query template if they share the same </span><span class="c3">statement text barring constants that could logically be replaced by placeholders for parameters values (&lsquo;?&rsquo;). Each query template is represented by a node in the query transition graph. </span></p><p class="c67"><span class="c3">Below is an example of two queries (Q</span><span class="c4">1</span><span class="c7">,Q</span><span class="c0">&prime;</span><span class="c4">1</span><span class="c3">) and their corre- sponding templates (Qt</span><span class="c4">1</span><span class="c3">, Qt</span><span class="c35">&prime;</span><span class="c0">1</span><span class="c43">): </span></p><p class="c108"><span class="c3">Q</span><span class="c4">1</span><span class="c7">: </span><span class="c29">SELECT C_ID FROM CUSTOMER WHERE C_UNAME </span><span class="c14">= </span><span class="c20">&rsquo;Bob&rsquo; </span><span class="c14">and C_PASSWD = </span><span class="c20">&rsquo;pwd&rsquo; </span><span class="c3">Qt</span><span class="c4">1</span><span class="c3">: </span><span class="c14">SELECT C_ID FROM CUSTOMER WHERE C_UNAME = </span><span class="c20">? </span><span class="c14">and C_PASSWD = </span><span class="c20">? </span></p><p class="c117"><span class="c1">254 </span></p><p class="c8"><span class="c3">Q</span><span class="c20">&rsquo;Alice&rsquo; </span><span class="c0">1</span><span class="c35">&prime;</span><span class="c43">: </span><span class="c40">SELECT C_ID FROM </span><span class="c14">and C_PASSWD </span><span class="c40">CUSTOMER WHERE C_UNAME = </span></p><p class="c8"><span class="c14">= </span><span class="c20">&rsquo;pwd2&rsquo; </span><span class="c3">Qt</span><span class="c20">? </span><span class="c0">1</span><span class="c35">&prime;</span><span class="c14">and </span><span class="c43">: </span><span class="c40">SELECT C_ID FROM </span><span class="c14">C_PASSWD = </span><span class="c20">? </span></p><p class="c8"><span class="c40">CUSTOMER WHERE C_UNAME = </span></p><p class="c8"><span class="c3">Note that although the above two original queries differ, their query templates are the same. Therefore, a node&rsquo;s transitions in the transition graph are based on query relationships from both Q</span><span class="c0">1 </span><span class="c3">and Q</span><span class="c0">1</span><span class="c35">&prime;</span><span class="c43">. </span><span class="c6">2.2 Query Template Relationships </span><span class="c3">To find query template relationships, we implement the transition graph as a frequency-based Markov graph, constructing it in an online fashion. We exploit the memory-less property of Markov models to simplify transition probability computations &mdash; transi- tion probabilities are based solely on the previous query the client executed. </span></p><p class="c44"><span class="c3">We monitor incoming queries, map them to query templates and calculate template transition probabilities. In particular, for any two executed Qtis larger </span><span class="c4">i </span><span class="c3">on templates this than after edge, some QtQt</span><span class="c4">ii</span><span class="c3">, and . QtWe configurable </span><span class="c4">j</span><span class="c3">refer , store we create to the it probability threshold as an P(Qtedge </span><span class="c4">j</span><span class="c3">|Qtfrom &tau;, of </span><span class="c4">i</span><span class="c3">we ). QtQtIf say </span><span class="c4">j i </span><span class="c3">this executing to QtQtprobability </span><span class="c4">j j </span><span class="c3">if Qtafter </span><span class="c4">j </span><span class="c3">is related is </span></p><p class="c8"><span class="c3">to QtThe </span><span class="c4">i</span><span class="c3">. </span></p><p class="c44"><span class="c3">&tau; parameter serves as a configurable confidence threshold for query template relationships. More concretely, the &tau; parameter provides the minimum we Qt</span><span class="c4">i </span><span class="c3">can to infer that they limit the predictive required are probability related. By choosing for Qt</span><span class="c4">j </span><span class="c3">&tau; executing appropriately, after </span></p><p class="c5"><span class="c3">queries executed after seeing Qt</span><span class="c4">i </span><span class="c3">to only those that are highly correlated to it. In doing so, we ensure that our predictions have a high degree of accuracy and avoid inundating the database with predictive executions of unpopular queries. </span></p><p class="c5"><span class="c3">P(Qt</span><span class="c4">j</span><span class="c7">|Qt</span><span class="c4">i</span><span class="c7">) is too broad to capture fine-grained query template </span><span class="c3">relationships. Given enough time, almost all of the query tem- plates in a workload could be considered related under the above definition. Two templates should not be considered related if there is a significant time gap between them, thus motivating a tem- poral constraint. Furthermore, by placing a temporal restriction on the relationship property, we reduce the time needed to look for incoming related templates. Consequently, we define a config- urable duration, </span><span class="c14">&#8710;</span><span class="c3">t, which specifies the maximum allowable time separation between related query templates. </span></p><p class="c44"><span class="c3">Definition 2.1. For any two query templates Qt</span><span class="c14">&#8710;</span><span class="c3">t) </span><span class="c4">j </span><span class="c3">is executed T time units apart from &gt; &tau; for some threshold parameter &tau; &isin; Qt[0, </span><span class="c4">i</span><span class="c3">1], , Qtif we </span><span class="c4">i</span><span class="c3">P(Qt, Qtconsider </span><span class="c4">jj</span><span class="c7">|Qt</span><span class="c3">, in </span><span class="c4">i</span><span class="c7">;T </span><span class="c3">which Qt</span><span class="c7">&le; </span><span class="c4">j </span><span class="c3">to be a related query template of Qt</span><span class="c4">i</span><span class="c7">. </span></p><p class="c5"><span class="c3">To learn a transition graph representing P(Qt</span><span class="c4">j</span><span class="c7">|Qt</span><span class="c4">i</span><span class="c7">; T &le; </span><span class="c29">&#8710;</span><span class="c7">t), </span><span class="c3">we map executed queries to query templates and place them at the tail of per-client queues called query streams. Since each client has its own stream and transition graph, we avoid expensive lock contention when updating the graphs and computing transition probabilities. </span></p><p class="c5"><span class="c3">Algorithm 1 runs continuously over client query streams, updat- ing their corresponding transition graphs. Intuitively, the algorithm scans the query stream, looking for other query templates that exe- cuted within </span><span class="c14">&#8710;</span><span class="c3">t of the first query template, adding counts to their corresponding edges and afterwards incrementing the vertex count indicating number of times the template has been seen. To calcu- late the probability of P(Qt</span><span class="c4">j</span><span class="c7">|Qt</span><span class="c4">i</span><span class="c7">;T &le; </span><span class="c29">&#8710;</span><span class="c7">t), we take the edge count </span><span class="c3">from Qt</span><span class="c0">i </span><span class="c3">to Qt</span><span class="c0">j </span><span class="c3">and divide by the vertex count for Qt</span><span class="c0">i</span><span class="c3">. To use </span></p><p class="c8"><span class="c3">Algorithm 1 Query Transition Graph Construction Input: query (Qttemplate </span><span class="c4">1</span><span class="c3">,t</span><span class="c4">1</span><span class="c7">), (Qt</span><span class="c3">identifiers </span><span class="c4">2</span><span class="c3">,t</span><span class="c4">2</span><span class="c7">),..., </span><span class="c3">and </span><span class="c7">an </span><span class="c3">their </span><span class="c7">infinite </span><span class="c3">execution </span><span class="c7">stream </span><span class="c3">timestamps, </span></p><p class="c8"><span class="c7">of incoming </span></p><p class="c8"><span class="c14">&#8710;</span><span class="c3">t, a fixed time duration, G = (V, E), a directed graph, initially empty, w</span><span class="c52">v </span><span class="c7">: V &rarr; </span><span class="c29">N</span><span class="c7">, vertex counters indicating the number of times </span><span class="c3">we have seen the vertex, initially all zero, w</span><span class="c4">e </span><span class="c7">: V &times;V &rarr; </span><span class="c29">N</span><span class="c7">, edge counters indicating the number of times </span><span class="c3">we&rsquo;ve seen the outgoing vertex followed by the incoming vertex within </span><span class="c14">&#8710;</span><span class="c3">t, initially all zero. </span></p><p class="c8"><span class="c3">i &larr; 1 loop</span><span class="c7">if t</span><span class="c4">i </span><span class="c7">+ </span><span class="c29">&#8710;</span><span class="c7">t &gt;now() then </span></p><p class="c8"><span class="c3">wait until now()&gt; t</span><span class="c4">i </span><span class="c7">+ </span><span class="c29">&#8710;</span><span class="c7">t </span><span class="c3">end if V w</span><span class="c52">v</span><span class="c7">(Qt</span><span class="c3">&larr; V </span><span class="c4">i</span><span class="c3">) &cup; &larr; {Qtw</span><span class="c4">i</span><span class="c52">v</span><span class="c7">(Qt</span><span class="c3">} </span></p><p class="c8"><span class="c4">i</span><span class="c3">) + 1 j &larr; i + 1 loop</span><span class="c7">if t</span><span class="c4">j </span><span class="c7">&gt; t</span><span class="c4">i </span><span class="c7">+ </span><span class="c29">&#8710;</span><span class="c7">t then </span></p><p class="c8"><span class="c3">// too far apart in time break else</span><span class="c7">E &larr; E &cup; {(Qt</span><span class="c4">i</span><span class="c3">, Qt</span><span class="c4">j</span><span class="c3">)} w</span><span class="c0">e</span><span class="c3">(Qtend if </span></p><p class="c8"><span class="c4">i</span><span class="c3">, Qt</span><span class="c4">j</span><span class="c3">) &larr; w</span><span class="c0">e</span><span class="c3">(Qt</span><span class="c4">i</span><span class="c3">, Qt</span><span class="c4">j</span><span class="c3">) + 1 </span></p><p class="c8"><span class="c3">j &larr; j + 1 end loop // advance forward in stream i &larr; i + 1 end loop </span></p><p class="c5"><span class="c3">the variables directly from Algorithm 1, the probability that query template Qt</span><span class="c4">j </span><span class="c3">executes within </span><span class="c14">&#8710;</span><span class="c3">t of a query template Qt</span><span class="c4">i </span><span class="c3">is given by then The </span><span class="c35">w</span><span class="c3">Qt</span><span class="c18">e</span><span class="c0">(Qtw</span><span class="c81">v</span><span class="c4">(Qt</span><span class="c3">choice </span><span class="c4">j </span><span class="c24">i</span><span class="c3">is </span><span class="c115">,</span><span class="c0">Qt</span><span class="c24">i</span><span class="c3">considered </span><span class="c0">) </span><span class="c24">j</span><span class="c0">) </span></p><p class="c8"><span class="c43">. </span><span class="c3">of </span><span class="c43">Per Definition 2.1, if </span><span class="c3">the </span><span class="c14">&#8710;</span><span class="c3">t related parameter to Qtcan </span><span class="c4">i</span><span class="c3">. </span></p><p class="c8"><span class="c43">this probability exceeds &tau; </span></p><p class="c5"><span class="c3">impact prediction efficacy. If </span><span class="c14">&#8710;</span><span class="c3">t is too high, it is possible that relationships will be predicted where there are none; if </span><span class="c14">&#8710;</span><span class="c3">t is too low, we may not discover re- lationships where they are present. Although the choice of </span><span class="c14">&#8710;</span><span class="c3">t is workload dependent, some indicators aid us in choosing an appro- priate value, such as query arrival rate. If P(Qt</span><span class="c4">j</span><span class="c7">|Qt</span><span class="c4">i</span><span class="c7">;T &le; </span><span class="c29">&#8710;</span><span class="c7">t) is </span><span class="c3">high for a fixed Qt</span><span class="c4">i </span><span class="c7">and many different Qt</span><span class="c4">j</span><span class="c7">, then either Qt</span><span class="c4">i </span><span class="c7">is a </span><span class="c3">common query template with many quick-executing related query templates, or </span><span class="c14">&#8710;</span><span class="c3">t is set too high. If this holds for many differentQt</span><span class="c4">i</span><span class="c7">, </span><span class="c3">then </span><span class="c14">&#8710;</span><span class="c3">t can be decreased. A similar argument holds for increasing </span><span class="c14">&#8710;</span><span class="c3">t. We discuss selection of </span><span class="c14">&#8710;</span><span class="c3">t and &tau; values for various workloads in Section 4.7. </span></p><p class="c5"><span class="c3">A key property of our model is that it uses online learning to adapt to changing workloads. As new query templates are ob- served, query template execution frequencies change, or query re- lationships adjust, the transition graph adapts to learn the changed workload. Moreover, online learning precludes the need to un- dergo expensive offline training before deployment. Instead, our model rapidly learns client workloads and takes action immedi- ately. </span></p><p class="c8"><span class="c1">255 </span></p><p class="c38"><span class="c3">Figure 4: An example of pipelining within a dependency hi- erarchy. The arrows represent a mapping from a prior query template&rsquo;s output set to the consequent query template&rsquo;s in- put set. </span></p><p class="c76"><span class="c3">Figure 4 illustrates how pipelining can be used to form extended chains of predictive executions using the TPC-W example from Figure 2. Qt</span><span class="c4">1 </span><span class="c3">has a mapping to Qt</span><span class="c4">2</span><span class="c3">, which in turn has a mapping </span></p><p class="c122"><span class="c18">1</span><span class="c55">If future executions disprove a mapping, we will mark that mapping invalid and </span><span class="c9">preclude the template from predictive execution if its dependencies are no longer met. </span></p><p class="c8 c48"><span class="c6">2.3 Parameter Mappings </span><span class="c3">Predictive query execution requires a stronger relationship be- tween queries than the transition graph provides. In addition to queries being related, they must also exhibit a dependency rela- tionship. </span></p><p class="c98"><span class="c3">To provide predictive execution capabilities, we record the output sets of query templates and match them with the input sets of templates that we have determined are related based on the transition graph. We then confirm each output column to input variable mapping over a verification period, after which only the mappings present in every execution are returned. </span></p><p class="c17"><span class="c3">As a concrete example, consider the TPC-W queries from Fig- ure 2. We will refer to the query template for the first query as Qt</span><span class="c4">1 </span><span class="c7">and the template for the second query as Qt</span><span class="c4">2</span><span class="c3">. In the first stage of tracking, we observe which query templates have executed within </span><span class="c14">&#8710;</span><span class="c3">t of Qt</span><span class="c4">1</span><span class="c3">. Once Qt</span><span class="c4">1 </span><span class="c3">has executed enough times (according to the verification period), we begin to construct mappings among the query templates. After Qt</span><span class="c4">1 </span><span class="c3">finishes an execution, we record its output set. When any of Qt</span><span class="c4">1</span><span class="c3">&rsquo;s related query templates (in this case assume only Qt</span><span class="c4">2</span><span class="c3">) are executed, we record their input sets. We then check if any column&rsquo;s result in Qt</span><span class="c4">1</span><span class="c3">&rsquo;s output set maps to the input parameters of Qt</span><span class="c4">2</span><span class="c3">. If they do, we record the matching output columns with their corresponding input argument position. If the same mappings are observed across the verification period, we infer that these mappings always hold.</span><span class="c35">1 </span><span class="c3">If a query template has mappings for every one of its input arguments from a set of prior templates, we can predict a query by forwarding parameters from its prior template&rsquo;s result sets as soon as they are available. In this case, we say the query template is a candidate for predictive execution given its prior query templates&rsquo; result sets. Similarly, we discover mappings between Qt</span><span class="c4">2 </span><span class="c3">and Qt</span><span class="c4">3 </span><span class="c3">and use them to execute Qt</span><span class="c4">3 </span><span class="c3">given Qt</span><span class="c4">2</span><span class="c3">&rsquo;s result set. </span></p><p class="c48 c73"><span class="c6">2.4 Pipelining Query Predictions </span><span class="c3">Parameter mappings among query templates enable predictive execution of queries as soon as their input sets are available via prior template execution. It may be the case that the prior query templates are also predictable, forming a hierarchical tree of de- pendencies among templates. We exploit these relationships by pipelining query predictions. Pipelining uses result sets from pre- dictively executed queries as input parameters for future predic- tions, thereby enabling predictions several steps in advance. </span></p><p class="c5 c64"><span class="c3">to Qt</span><span class="c4">3</span><span class="c3">. If Qt</span><span class="c4">1 </span><span class="c3">is executed, we can forward its result set as input with which to predictively execute Qt</span><span class="c4">2</span><span class="c3">. Once Qt</span><span class="c4">2 </span><span class="c3">has also been executed, we can predictively execute Qt</span><span class="c4">3</span><span class="c3">. As such, Qt</span><span class="c4">2 </span><span class="c3">is fully defined given the result set of Qt</span><span class="c4">1</span><span class="c3">, and Qt</span><span class="c4">3 </span><span class="c3">is fully defined given the result set of Qt</span><span class="c4">2</span><span class="c3">. We formalize the notion of fully defined queries: </span></p><p class="c82"><span class="c3">Definition 2.2. A fully defined query template (FDQ) Qt</span><span class="c4">j </span><span class="c3">has all of its inputs provided by some set, possibly empty, of prior query templates Qt</span><span class="c4">i</span><span class="c24">1</span><span class="c3">, Qt</span><span class="c4">i</span><span class="c24">2</span><span class="c3">,...,Qt</span><span class="c4">i</span><span class="c24">k </span><span class="c3">where each Qt</span><span class="c4">i</span><span class="c24">m </span><span class="c3">(</span><span class="c14">&forall;</span><span class="c3">m &isin; [1,k]) is either: </span></p><p class="c39"><span class="c3">(1) a fully defined query template, or (2) a dependency query template, required to execute Qt</span><span class="c4">j</span><span class="c3">. </span></p><p class="c28"><span class="c3">Per Definition 2.2, both Qt</span><span class="c4">2 </span><span class="c3">and Qt</span><span class="c4">3 </span><span class="c3">are FDQs, but Qt</span><span class="c4">1 </span><span class="c3">is sim- ply a dependency query. This definition captures the dependency- graph nature of FDQs &mdash; each node in this graph corresponds to a query template, with inbound and outbound edges corresponding to inbound and outbound parameter mappings, respectively. The transition graph induces the dependency graph but is stored and tracked separately. By keeping the dependency graph separate, we reduce contention on it. Once the dependency graph matches the current workload, it will not need to be modified until the workload changes. </span></p><p class="c54"><span class="c3">Algorithm 2 Core Prediction Algorithm Input: executed query template Qt </span><span class="c14">record</span><span class="c3">_</span><span class="c14">query</span><span class="c3">_</span><span class="c14">template</span><span class="c3">(Qt) new_fdqs = </span><span class="c14">find</span><span class="c3">_</span><span class="c14">new</span><span class="c3">_</span><span class="c14">fdqs</span><span class="c3">(Qt) rdy_fdqs = </span><span class="c14">mark</span><span class="c3">_</span><span class="c14">ready</span><span class="c3">_</span><span class="c14">dependency</span><span class="c3">(Qt) rdy_fdqs = rdy_fdqs &cup; new_fdqs ordered_fdqs = </span><span class="c14">find</span><span class="c3">_</span><span class="c14">all</span><span class="c3">_</span><span class="c14">runnable</span><span class="c3">_</span><span class="c14">fdqs</span><span class="c3">(rdy_fdqs) for all rdy_fdq &isin; ordered_fdqs do </span></p><p class="c100"><span class="c14">execute</span><span class="c3">_</span><span class="c14">fdq</span><span class="c3">(rdy_fdq) end for </span></p><p class="c60"><span class="c3">Discovering new FDQs, managing FDQ dependencies, and pipelining predictions comprise the main routine of the predictive framework. The engine executes Algorithm 2 after the execution of a client-provided instance of query template Qt. The engine records Qt&rsquo;s result set and input parameters in the query transition graph (Section 2.3), looks for parameter mappings, and records discovered dependencies in the dependency graph. This query template is then marked as executed so that FDQ pipelines that de- pend on its result set can proceed. Any queries that are determined ready for execution given the result of this query (and previously executed queries) are then executed, forwarding parameters from their dependent queries&rsquo; result sets. The dependencies are then reset, waiting for future invocations with which to predict queries. The dependency graph is stored as a hash map with edges between dependent queries, allowing Apollo to quickly determine which FDQs are ready for execution given an executed query. </span></p><p class="c5 c87"><span class="c3">Discovering new FDQs, managing FDQ dependencies, and pipelining predictions comprise the main routine of the predictive framework. The engine executes Algorithm 2 after the execution of a client-provided instance of query template Qt. The engine records Qt&rsquo;s result set and input parameters in the query transition graph (Section 2.3), looks for parameter mappings, and records discovered dependencies in the dependency graph. This query template is then marked as executed so that FDQ pipelines that de- pend on its result set can proceed. Any queries that are determined ready for execution given the result of this query (and previously executed queries) are then executed, forwarding parameters from their dependent queries&rsquo; result sets. The dependencies are then reset, waiting for future invocations with which to predict queries. The dependency graph is stored as a hash map with edges between dependent queries, allowing Apollo to quickly determine which FDQs are ready for execution given an executed query. </span></p><p class="c114"><span class="c3">Always defined query templates (ADQs) are a subset of FDQs, requiring that all of their prior query templates (recursively) are FDQs. They comprise an important subclass of fully defined queries since their dependencies are always satisfied; they can be executed and cached at any time. As a concrete example, &ldquo;</span><span class="c14">SELECT COUNT(</span><span class="c29">*</span><span class="c14">) FROM shopping_cart</span><span class="c3">&rdquo; is an ADQ because all of its input parameters (the empty set) are always satisfied. </span></p><p class="c75 c79"><span class="c3">It follows from Definition 2.2 that an FDQ is an ADQ if and only if all of its inputs are provided by ADQs. Consequently, ADQ </span></p><p class="c119"><span class="c1">256 </span></p><p class="c8"><span class="c3">hierarchies are discovered by recursively checking the dependency structure of the FDQ. </span></p><p class="c8"><span class="c6">3 APOLLO </span><span class="c3">In this section, we present Apollo, our system that implements the predictive framework described in Section 2. Apollo is a system layer placed between a client application and the database server. Application clients submit queries to the Apollo system, which then interacts with the database system and cache to return query results. </span></p><p class="c5"><span class="c3">Apollo uses Memcached [19], a popular industrial-strength dis- tributed caching system, as the query result cache. Each executed read-only query has its result set placed in Memcached, which employs the popular Least Recently Used (LRU) eviction policy. With predictive caching enabled, Apollo also places predictively executed query results into the cache, increasing the number of cache hits and thereby overall system performance. Apollo&rsquo;s pre- dictive engine operates in a complementary manner where queries are passed unchanged through to the cache and database, preserv- ing the effective workload behaviour. Apollo executes predicted queries and caches them ahead of time, reducing response times through correlated query result caching. </span></p><p class="c5"><span class="c3">Since Apollo is implemented in the Java programming lan- guage, we use the JDBC API to submit queries to the remote MySQL [33] database. The JDBC API [32] makes Apollo data- base agnostic and therefore portable, allowing MySQL to be easily swapped for any other JDBC compliant relational database sys- tem.</span><span class="c7">To efficiently track query templates within Apollo, we identify </span><span class="c3">queries based on a hash of their constant independent parse tree. A background thread processes the SQL query strings placed into the query stream, parsing and then hashing them into a 64-bit identifier. All parameterizable constants are replaced by a fixed string, and therefore share the same hash code. Thus, queries with the same text modulo parameterizable constants have the same hash.</span><span class="c7">Hashes can be computed efficiently and are used internally to </span><span class="c3">refer to query templates. Apollo uses them to look up nodes in the transition graph, and to find statistics and parameters we have stored for each query template. Hash collisions are very rare due to the length of the hash and common structures that SQL statements share. Due to the complementary nature of Apollo, query template hash collisions are guaranteed not to introduce incorrect system behaviour. </span></p><p class="c8"><span class="c6">3.1 Prediction Engine </span><span class="c3">When a client submits a query, it has its results retrieved from the local cache or executed against the remote database, then placed into Apollo&rsquo;s query stream and evaluated by the prediction engine. Background threads use the query stream to construct the transition graph described in Section 2, processing executed queries into query templates. The core prediction routine from Section 2.4 is then invoked: new FDQs are discovered from the underlying transition graph, the dependency graph is updated, and future queries are predicted using pipelining. We now detail each of these subroutines, showing how these operations are carried out efficiently. </span></p><p class="c5"><span class="c3">Algorithm 3 shows how new FDQs are discovered. First, the transition graph is consulted for all related query templates (tem- plates with inbound edges from Qt</span><span class="c4">i</span><span class="c3">) since these are the templates that may have new mappings from Qt</span><span class="c4">i</span><span class="c3">&rsquo;s result set. Qt</span><span class="c4">i </span><span class="c3">itself is also </span></p><p class="c5"><span class="c3">checked since it may be an ADQ (if it has no input parameters). For each query template Qt</span><span class="c4">j </span><span class="c3">that has no recorded dependency in- formation in the dependency graph, the transition graph is checked to see which templates have mappings to them. If each of Qt</span><span class="c4">j</span><span class="c3">&rsquo;s input parameters are satisfied by its prior query templates then by Definition 2.2 we know that it is an FDQ. An FDQ struc- ture is constructed for Qt</span><span class="c4">j </span><span class="c3">and its dependencies are recorded in the dependency graph. For efficiency, we represent the depen- dency graph as a hash map from dependency query templates to dependent templates and their full dependency list. Therefore, determining newly satisfied FDQs can be performed quickly with simple lookup operations. </span></p><p class="c8"><span class="c3">Algorithm 3 find_new_fdqs Input: a query template Qt</span><span class="c4">i </span><span class="c7">Output: a set of newly discovered FDQs </span></p><p class="c8"><span class="c3">queries_to_check = </span><span class="c14">get</span><span class="c3">_</span><span class="c14">related</span><span class="c3">_</span><span class="c14">queries</span><span class="c3">(Qt</span><span class="c4">i</span><span class="c3">) queries_to_check = queries_to_check &cup; {Qt</span><span class="c4">i</span><span class="c3">} new_fdqs = {} for all Qt</span><span class="c4">j </span><span class="c3">&isin; queries_to_check do </span></p><p class="c8"><span class="c3">if !</span><span class="c14">already</span><span class="c3">_</span><span class="c14">seen</span><span class="c3">_</span><span class="c14">deps</span><span class="c3">(Qt</span><span class="c4">j</span><span class="c3">) then </span></p><p class="c8"><span class="c3">p_mappings = </span><span class="c14">get</span><span class="c3">_</span><span class="c14">prior</span><span class="c3">_</span><span class="c14">query</span><span class="c3">_</span><span class="c14">mappings</span><span class="c3">(Qt</span><span class="c4">j</span><span class="c3">) if </span><span class="c14">have</span><span class="c3">_</span><span class="c14">enough</span><span class="c3">_</span><span class="c14">mappings</span><span class="c3">(Qt</span><span class="c4">j</span><span class="c3">) then </span></p><p class="c8"><span class="c3">fdq = </span><span class="c14">construct</span><span class="c3">_</span><span class="c14">fdq</span><span class="c3">(Qt</span><span class="c4">j</span><span class="c3">,p_mappings) unresolved_deps = </span><span class="c14">get</span><span class="c3">_</span><span class="c14">dependencies</span><span class="c3">(fdq) </span><span class="c14">add</span><span class="c3">_</span><span class="c14">to</span><span class="c3">_</span><span class="c14">dep</span><span class="c3">_</span><span class="c14">graph</span><span class="c3">(unresolved_deps, fdq) </span><span class="c14">mark</span><span class="c3">_</span><span class="c14">seen</span><span class="c3">_</span><span class="c14">deps</span><span class="c3">(fdq) new_fdqs = new_fdqs &cup; {fdq} end if end if end for return new_fdqs </span></p><p class="c5"><span class="c3">Apollo ensures that there exists only one instance of an FDQ hierarchy throughout the system so that mapping updates affect both the FDQ and any FDQ structures that contain it. To do so, we track the FDQs that the system has constructed before, returning a previously constructed FDQ if applicable. During FDQ construction, dependency loops are detected and returned as dependency queries in an FDQ hierarchy. If all children of an FDQ are tagged as ADQs, or if an FDQ has no parameters and no children, then it is tagged as an ADQ and stored for use during cache reload (Section 3.4.2). Dependency queries are marked as unresolved dependencies on the FDQ and used to determine when an FDQ is ready for execution. Algorithm 4 shows how dependencies for known FDQs are tracked and used for predictive execution. After the execution of a given query template Qt</span><span class="c4">i</span><span class="c3">, each dependent FDQ marks that dependency as satisfied. If all of an FDQ&rsquo;s dependencies are now satisfied, we add it to a list of &ldquo;ready FDQs&rdquo;, resetting its dependencies so that they must be satisfied again before we determine the FDQ as being ready for future execution. </span></p><p class="c5"><span class="c3">Algorithm 4 is used as part of a breadth-first approach to de- termine all runnable FDQs given the current query state. Apollo determines which FDQs are executable given the current system state and a newly executed query, adding them to the list of ready FDQs. Apollo then determines which other FDQs are executable given this FDQ list, repeating the process as necessary. This fi- nal list of FDQs is then executed in order, feeding result sets as parameters to dependent FDQs. </span></p><p class="c8"><span class="c1">257 </span></p><p class="c73 c118"><span class="c3">Algorithm 4 mark_ready_dependency Input: an executed query Qt</span><span class="c4">i </span><span class="c3">whose result set is now available Output: a set ready_fdqs of FDQs ready for execution </span></p><p class="c133"><span class="c3">ready_fdqs = {} dependency_lists = </span><span class="c14">get</span><span class="c3">_</span><span class="c14">dep</span><span class="c3">_</span><span class="c14">query</span><span class="c3">_</span><span class="c14">dlists</span><span class="c3">(Qt</span><span class="c4">i</span><span class="c3">) for all d_list &isin; dependency_lists do </span></p><p class="c78"><span class="c14">mark</span><span class="c3">_</span><span class="c14">dependency</span><span class="c3">_</span><span class="c14">satisfied</span><span class="c3">(d_list, Qt</span><span class="c4">i</span><span class="c3">) if </span><span class="c14">all</span><span class="c3">_</span><span class="c14">deps</span><span class="c3">_</span><span class="c14">satisfied</span><span class="c3">(d_list) then </span></p><p class="c69"><span class="c3">ready_fdqs=ready_fdqs &cup; </span><span class="c14">get</span><span class="c3">_</span><span class="c14">fdq</span><span class="c3">(d_list) </span><span class="c14">reset</span><span class="c3">_</span><span class="c14">dependencies</span><span class="c3">(d_list) end if end for return ready_fdqs </span></p><p class="c127"><span class="c6">3.2 Client Sessions </span><span class="c3">Apollo uses a client session consistency scheme [15], enabling its predictive cache to share cached results among clients and scale in the presence of write queries. In brief, each client has an independent session that guarantees that it accesses data at least as fresh as data it last read or wrote and that it efficiently shares cached entries with other clients. </span></p><p class="c11"><span class="c3">Each client maintains a version vector (v</span><span class="c4">1</span><span class="c7">,v</span><span class="c4">2</span><span class="c7">,...,v</span><span class="c4">n</span><span class="c7">) indi- </span><span class="c3">cating its most recently accessed version v</span><span class="c0">i </span><span class="c3">for each table R</span><span class="c0">i</span><span class="c3">. Query results are stored in the cache and timestamped with a version vector (c</span><span class="c4">1</span><span class="c7">,c</span><span class="c4">2</span><span class="c7">,...,c</span><span class="c4">n</span><span class="c7">) matching the version vector of the </span><span class="c3">client that wrote it. When a client wants to execute a read query on a set of tables (R</span><span class="c4">1</span><span class="c7">,R</span><span class="c4">2</span><span class="c7">,...,R</span><span class="c4">n</span><span class="c7">), it checks if there exists an </span><span class="c3">entry in the cache for that query with a version vector with (c</span><span class="c0">1 </span><span class="c3">&ge; v</span><span class="c0">1</span><span class="c3">,c</span><span class="c0">2 </span><span class="c3">&ge; v</span><span class="c0">2</span><span class="c3">,...c</span><span class="c0">n </span><span class="c3">&ge; v</span><span class="c0">n</span><span class="c3">). If so, the client will retrieve and return the cached result, updating its client state for each of the tables to match that of the cached entry. If there is no such entry, the client will execute the query against the database, updat- ing its version vector for each of the affected tables to match their versions in the database and storing the result in the cache. Write queries are never predictively executed (to prevent unnecessary rollbacks) and always execute against the database. After a client executes a write query, its version vector is updated to match the state of the database. </span></p><p class="c101"><span class="c3">Since cache misses and write queries update a client&rsquo;s version vector, old cache entries may be stale under the client&rsquo;s new ver- sion vector. Therefore, if it is important to update a client&rsquo;s version vector only when strictly necessary, and by the minimum amount. As such, when a client could read two different versions of a cached key, Apollo will return the value for the cached key with a version vector that minimizes the distance from the client&rsquo;s version vector. Apollo uses a variety of optimizations to reduce the impact of write queries on predictive caching and system performance, discussed in Section 3.4. </span></p><p class="c104 c48"><span class="c3">Since a client&rsquo;s session is independent of the sessions of other clients, Apollo can easily scale horizontally. An individual client must route all of its requests to the same Apollo instance to main- tain its session, but other clients and processes do not affect its session guarantees. Thus, extract, transform, load (ETL) processes, database triggers, and client write requests do not result in mass invalidations of cached data. Furthermore, Apollo instances do not need to communicate with each other to maintain sessions because a client&rsquo;s session is tracked by a single Apollo instance. </span></p><p class="c8 c75"><span class="c6">3.3 Publish&ndash;Subscribe Model </span><span class="c3">Since Apollo handles many concurrent clients, multiple clients may simultaneously try to execute the same read query. In these cases, it is beneficial to execute the query only once and return its result set to the waiting clients. Optimizing these queries is particularly important for predictive execution since a predicted query may not have finished execution before a client requests its result set. </span></p><p class="c95"><span class="c3">Before executing a read query, Apollo consults a hash map to determine if a copy of the query is already executing. If so, Apollo blocks the query until the other query returns, passing along its result set. Otherwise, it will record an entry in the hash map with a semaphore for other clients and predictive pipelines to wait on. In this way, only one copy of a read query is executing at any time, including shared predictive query pipelines for multiple clients. </span></p><p class="c10"><span class="c3">When Apollo determines that a client&rsquo;s query has multiple us- able versions of its results cached, Apollo will use the earliest version regardless of whether another usable version is already be- ing retrieved for a different client. Experimentally, we determined that it is better to retrieve results for earlier versions since reading later versions will result in large version vector updates for the client and may therefore cause misses for other cached results. Similarly, if Apollo must retrieve the result set from the database, Apollo will subscribe to any ongoing database retrievals of the same query. </span></p><p class="c77"><span class="c6">3.4 Session-Aware Caching </span><span class="c3">Since write queries increment client version vectors, they preclude the client from reading any previously cached values. Therefore, if a client executes a write query after a predictive query is issued on that client&rsquo;s behalf, the predicted query results may be stale and unusable. If so, the system will have performed unnecessary work to execute and cache the query. To minimize the effects of writes on system performance, we avoid predictively executing queries whose results are likely to become stale before client queries can use their results (Section 3.4.1). Since ADQs can be executed at any time, we strive to keep valuable ADQs in the cache by reloading them if their results become outdated (Section 3.4.2). </span></p><p class="c62"><span class="c3">3.4.1 Preventing Unusable Predictions. Apollo determines the likelihood of a write query or cache miss occurring using the query transition graph. Recall from Section 2.2 that each client has a single transition graph. However, by maintaining multiple inde- pendent transition graphs with different </span><span class="c14">&#8710;</span><span class="c3">t intervals, we are able to determine the likelihood of a given query being executed by the client in each of these windows. Using this technique, we predict if a client will retrieve the results for a predictively executable query before its results become stale. Apollo will predictively execute and cache only query results that it deems are likely to be used.</span><span class="c7">To determine if predictively executing and caching a query&rsquo;s </span><span class="c3">results will be helpful, Apollo first estimates the time it will take for the query to be executed and cached. Since all predictable queries are by definition FDQs, we use a simple estimate: the time to predictively execute an FDQ is given by the time it will take to execute its dependencies and the time to execute the FDQ itself. We calculate this estimate recursively: for a target FDQ, we return the maximum time to execute its dependency queries and add the time needed to execute the FDQ. In essence, this process returns the longest expected path from the child weighted by mean query runtimes. To provide an approximation of individual query runtimes, we use the mean execution time for each query </span></p><p class="c92"><span class="c1">258 </span></p><p class="c53"><span class="c3">template. Although more sophisticated methods can be used [4, 45] to estimate query runtimes, we found that this method yields enough accuracy to determine the runtime of predicted query while still being performant. </span></p><p class="c116"><span class="c3">Once the runtime for a given FDQ f has been determined (say t), Apollo looks up the client&rsquo;s transition graph with smallest in- terval </span><span class="c14">&#8710;</span><span class="c3">t where </span><span class="c14">&#8710;</span><span class="c3">t &gt; t. It then uses this graph to determine the likelihood of the client executing a query that would cause f &rsquo;s re- sults &mdash; or the results of its dependencies &mdash; to become stale while f is executing. If this likelihood is sufficiently high (given the &tau; threshold), we avoid executing f to save on database execution costs. Therefore, only queries that are likely to be executed and useful to clients are predictively cached. </span></p><p class="c25"><span class="c3">Although increasing the number of transition graphs per client necessitates additional processing of the query stream, we find that the simplicity of the query transition graph construction algorithm (Algorithm 1) combined with a configurable (but small) number of models per client results in low computational overhead for the system. Furthermore, since workloads [1, 42] tend to have a small number of unique query templates, the storage overhead is minimal. </span></p><p class="c94"><span class="c3">3.4.2 Informed ADQ Reload. Write queries update a client&rsquo;s version vector, and therefore provide an opportunity for opti- mization through informed query result reload. As ADQ depen- dencies are always satisfied and can be executed at any time, we immediately reload valuable ADQ hierarchies after a client executes a write query. Since there can be many ADQs and reloading a hierarchy may be expensive for the database to exe- cute, we limit ADQ reload to only those predictions for query templates considered valuable according to the cost function cost(Qt) = P(Qt) &middot; mean_rt(Qt).</span><span class="c35">2 </span><span class="c3">Specifically, the estimated cost of an ADQ on the system is given by the probability of the ADQ executing and the estimated ADQ runtime. If the cost of the ADQ exceeds a predefined threshold &alpha;, we reload it into the cache. We discuss &alpha; and its effects further in Section 4.7. </span></p><p class="c31"><span class="c6">4 PERFORMANCE EVALUATION </span><span class="c3">In this section, we present the system setup used to conduct ex- periments followed by performance results. Apollo is compared against Memcached [19], a popular mid-tier cache used in data- base and storage systems, as well as the Fido predictive cache [34]. We compare these systems using average query response time and tail latencies, which have been observed to contribute significantly to user experience and indicate concurrent interaction responsive- ness [28]. </span></p><p class="c57"><span class="c3">The Fido engine serves as a drop-in replacement for Apollo&rsquo;s prediction engine, and uses Palmer et al.&rsquo;s associative-memory technique [34] for query prediction, scanning client query streams to predict upcoming queries. Fido-like approaches have been em- ployed to prefetch objects in databases [8]. Fido&rsquo;s implementation- independent middleware prediction engine makes it particularly well-suited as a comparison point against Apollo. </span></p><p class="c13"><span class="c3">The remainder of this section is organized as follows. Sec- tion 4.1 describes our experiments&rsquo; setup and Section 4.2 pro- vides performance experiments for TPC-W. In Section 4.3, we use TPC-C to assess Apollo&rsquo;s scalability under increasing client load. Section 4.4 showcases Apollo&rsquo;s ability to adapt to changing workloads using online learning. Geographic latency experiments and multi-Apollo instance experiments are shown in Sections 4.5 </span></p><p class="c111"><span class="c18">2</span><span class="c55">Note that the techniques in Section 3.3 apply; shared query dependencies and </span><span class="c9">overlapping client query submissions will not result in multiple executions of ADQs. </span></p><p class="c8 c74"><span class="c3">and 4.6 respectively, and Section 4.7 presents a sensitivity analysis of Apollo&rsquo;s configurable parameters. </span></p><p class="c51"><span class="c6">4.1 Experimental Setup </span><span class="c3">Our experiments use a geo-distributed setup in which Amazon EC2 nodes are located in the US-East (N. Virginia) region for: (i) Apollo with 16 virtual CPUs, 64 GB of RAM and a 50 GB SSD (ii) Memcached on a machine with 2 virtual CPUs, 4 GB of RAM, and a 50 GB SSD (iii) a node with concurrent clients running our benchmarks with 16 virtual CPUs, 64 GB of RAM, and a 50 GB SSD. We deploy a database machine in the US-West (Oregon) region for our experiments, which has 16 virtual CPUs, 64 GB of RAM, a 250 GB SSD, and uses MySQL v5.6 as the database. For each experiment, Memcached uses a cache size 5% of the size of the remote database to demonstrate that Apollo is effective with limited cache space. All results presented are the average over at least five independent runs, with bars around the means representing 95% confidence intervals. </span></p><p class="c66"><span class="c3">Our experiments have three primary configurations: the Mem- cached configuration (in which the cache has been warmed for 20 minutes prior to benchmarking), the Apollo caching configuration, and the Fido prediction engine configuration [34]. In the Mem- cached configuration, we check for query results in the cache and forward queries on cache misses to the remote database, caching the retrieved query results. The Apollo and Fido configurations also load query results into the cache after they execute a read- only query on the remote database, but Apollo uses the predictive framework from Section 2 and Fido uses its own predictive engine, which is detailed below. </span></p><p class="c30"><span class="c3">Unlike Apollo, Fido functions on an individual query level rather than on query templates. More concretely, if queries Q</span><span class="c4">1</span><span class="c7">,Q</span><span class="c4">2</span><span class="c7">, </span><span class="c3">...,Q</span><span class="c4">n </span><span class="c7">are present in a client&rsquo;s query stream, Fido looks for </span><span class="c3">a stored pattern that is prefixed by them, say Q</span><span class="c0">1</span><span class="c3">,Q</span><span class="c0">2</span><span class="c3">, ...,Q</span><span class="c0">n</span><span class="c3">, P</span><span class="c4">1</span><span class="c7">,P</span><span class="c4">2</span><span class="c7">,...,P</span><span class="c4">m</span><span class="c7">, proceeding to predictively execute P</span><span class="c4">1</span><span class="c7">,P</span><span class="c4">2</span><span class="c7">,...,P</span><span class="c4">m </span><span class="c3">and cache their results. In contrast to Apollo&rsquo;s online learning capabilities, Fido requires offline training to make predictions. We provide Fido with client workload traces twice the length of the experiment interval to serve as its training set for comparison against a cold start Apollo. Additionally, we let Fido make up to 10 predictions for each matched prefix. </span></p><p class="c105 c124"><span class="c3">In all configurations, clients use session guarantees (Section 3.2) and queries executed at the remote database have their result sets immediately cached in Memcached. Thus, the difference in caching performance between the configurations is due to caching benefits provided by the query prediction engines. </span></p><p class="c131"><span class="c3">Our experiments aim to answer three key questions. First, can Apollo analyze incoming queries and learn patterns within a work- load? Second, are Apollo&rsquo;s predictive caching capabilities effec- tive in reducing query round-trip time by avoiding costly database query executions? Third, can Apollo&rsquo;s predictive framework scale with an increasing number of clients? We present performance results in the next sections that include answers to these questions. </span></p><p class="c129"><span class="c6">4.2 TPC-W Benchmark </span><span class="c3">The TPC-W Benchmark [42] generates a web commerce workload by having emulated browsers interact with servlets that serve webpages. The webpages require persistent data from storage so servlets execute database queries against the remote database to generate webpage content. The TPC-W benchmark includes 14 different web interactions for clients (e.g., Best Sellers, Order Inquiry) each with their own distinct set of queries. For a given </span></p><p class="c112"><span class="c1">259 </span></p><p class="c8"><span class="c37">200 </span><span class="c21">130 </span><span class="c37">Apollo </span></p><p class="c8"><span class="c21">900 </span><span class="c37">Apollo </span><span class="c21">120 </span><span class="c37">Memcached </span></p><p class="c8"><span class="c21">800 </span><span class="c37">Memcached Fido </span></p><p class="c8"><span class="c37">Fido </span><span class="c21">700 110 600 100 500 90 400 </span><span class="c37">80 </span><span class="c21">300 </span><span class="c37">20 30 40 50 </span></p><p class="c8"><span class="c37">94 95 96 97 98 99 </span></p><p class="c8"><span class="c9">(a) Client Scalability </span></p><p class="c8"><span class="c9">(b) Tail Latencies </span></p><p class="c8"><span class="c37">Apollo </span><span class="c21">) sm(e miTe snopseRy reu</span><span class="c37">Q</span><span class="c21">) </span></p><p class="c8"><span class="c21">sm(e miTe snopseRy reu</span><span class="c37">QMemcached Fido </span></p><p class="c8"><span class="c37">80 </span><span class="c9">(c) Learning Over Time </span></p><p class="c8"><span class="c3">Figure 5: Experiment results for 20 minute TPC-W runs using Apollo, Fido, and Memcached (no prediction engine). </span></p><p class="c5"><span class="c3">client, the next web interaction is chosen probabilistically based on the previous interaction. We use a popular implementation [35] of the TPC-W Benchmark specification. </span></p><p class="c5"><span class="c3">The TPC-W benchmark represents an important use case for Apollo since even small changes in latency can significantly im- pact web traffic [20] and sales [30]. Further, it serves as a chal- lenging workload for Apollo due to its inherent randomness and large number of different queries. This randomness serves to test the viability of Apollo&rsquo;s predictive framework under a variable workload. </span></p><p class="c5"><span class="c3">We generated a 33 GB TPC-W database with 1,000,000 items. We measured Apollo&rsquo;s performance using the TPC-W benchmark browsing mix executed for 20 minute measurement intervals while scaling-up the number of clients using our default TPC-W param- eters discussed in Section 4.7. </span></p><p class="c5"><span class="c3">4.2.1 Performance Results. Figure 5(a) shows Apollo&rsquo;s performance for an increasing number of clients compared to Memcached and Fido. Apollo significantly outperforms both Fido and Memcached, enjoying a large response time reduction of up to 33% over Memcached and 25% over Fido. Fido has slightly lower response time than Memcached due to query-instance level predictive caching but is unable to recognize query template pat- terns and generalize to unseen queries, precluding it from being competitive with Apollo. In the case of Memcached, we see its warmed cache offers little advantage over Apollo&rsquo;s and Fido&rsquo;s cold starts &mdash; invalidation and randomness limit the effects of cache warming. </span></p><p class="c5"><span class="c3">Each configuration shows a reduction in response time as the number of clients increase, a consequence of the shared cache between clients. However, shared caching is unable to compete with our predictive caching scheme as in a shared cache, a client must incur a cache miss, execute, and then store query results before others can use it. Consequently, Apollo&rsquo;s techniques of query prediction and informed ADQ reload prove superior, even as the client load is scaled up. </span></p><p class="c5"><span class="c3">Figure 5(b) shows the distribution of tail response times for each of the experimental configurations for 50 client TPC-W runs. Apollo&rsquo;s response times are significantly lower than any of the other methods, particularly for the higher percentiles, due to an improvement in cache hits. At the 97th percentile, Apollo reduces tail latencies by 1.8x over Memcached and Fido. Again, Fido tends to perform about as well as Memcached, despite its large training set size, as it cannot generalize its patterns to query templates for FDQ prediction and query reload. </span></p><p class="c5"><span class="c3">Figure 5(c) shows average query response times in 4 minute in- tervals. We see that Apollo exhibits a downward trend in response time from the start of the measurement interval as it effectively </span></p><p class="c44"><span class="c21">) sm(e miTe snopseRy reu</span><span class="c37">Q</span><span class="c21">130120 110 100 90 </span><span class="c89">04:00 08:00 12:00 16:00 20:00 </span><span class="c37">Number of Clients </span></p><p class="c8"><span class="c37">Percentiles </span></p><p class="c8"><span class="c37">Time </span></p><p class="c5"><span class="c3">learns query correlations and parameter mappings, resulting in an improvement of 30% over its average response time during the first four minutes. Although the other systems&rsquo; performance oscillates according to workload patterns, they do not learn query patterns &mdash; their final average query response times are comparable to that incurred in their first few minutes. </span></p><p class="c5"><span class="c3">To ensure that Apollo can provide these response time reduc- tions without undue resource overhead, we added instrumentation to determine the time and memory needed to find and construct new FDQs. On average, it takes less than 1% of response time to discover new FDQs given a newly executed query, and less than 2% of response time to construct an FDQ. We have observed that Apollo uses scant system resources, requiring only 1.5% the amount of memory used by the database for tracking the transi- tion graph and query parameter mappings. Apollo&rsquo;s predictive techniques submit an additional 25% more queries to the remote database compared to the Memcached configuration. Apollo&rsquo;s intelligent query caching techniques place little additional load on the remote database and use meager resources, while still provid- ing substantially lower average query response times than both Fido and Memcached. </span></p><p class="c5"><span class="c3">To answer the performance questions we had posed earlier in Section 4.1, Apollo is indeed able to make accurate and useful predictions for what to cache, predicting and retaining important result sets in the cache for longer without significant computation or memory overhead. </span></p><p class="c8"><span class="c6">4.3 TPC-C Benchmark </span><span class="c3">The TPC-C Benchmark emulates an order-entry environment in which multiple clients execute a mix of transactions against a database system [1]. Each of these clients functions as a store-front terminal, which submits orders for customers, confirms payments, and updates stock levels. In contrast to TPC-W&rsquo;s workload, TPC- C&rsquo;s OLTP workload features many short-running queries which avoid contention by reduced locking of significant parts of the database. As such, the TPC-C benchmark serves to directly test the scalability of Apollo. </span></p><p class="c5"><span class="c3">The TPC-C specification has two read-only transactions, Stock Level and Order Status, both of which present opportunities for predictive execution. Since the goal of our experimentation with TPC-C is to show the scalability of predictive execution under high numbers of clients, we scale up the mix of read-only transactions to 95% with updates making up the remaining 5%. In doing so, Apollo must track, construct, and execute far more opportunities for predictive queries than in the TPC-W experiments. Thus, this experiment&rsquo;s purpose is to show how well Apollo can handle hundreds of clients executing predictive queries simultaneously. </span></p><p class="c8"><span class="c1">260 </span></p><p class="c44"><span class="c27">Apollo Memcached Fido </span></p><p class="c8"><span class="c3">Figure 6: Experiment results for 20 minute TPC-C runs using Apollo, Fido, and Memcached (no prediction engine). </span></p><p class="c5"><span class="c3">Figure 7: Experiment results for changing the workload from TPC-C to TPC-W using Apollo, Fido, and Memcached (no prediction engine). </span></p><p class="c5"><span class="c3">In our experiments, we use the OLTPBench TPC-C implemen- tation from Difallah et al [18]. To properly assess scalability, we modified the read/write mix, with a final percentage of 5% Pay- ments, 47.5% Order Status, and 47.5% Stock Level Transactions. This mix forces the prediction engines to construct and execute significantly more predictive queries. </span></p><p class="c5"><span class="c3">A TPC-C database of size 100 GB with 1000 warehouses was generated and loaded into a US-West MySQL instance using the data generation mechanism of OLTPBench. For the following experiments, we used our default TPC-C parameters (discussed in Section 4.7) with a 5% write mix. We choose the warehouse pa- rameter in our queries according to a uniform distribution, which results in more predictive executions than a skewed Zipf distribu- tion &mdash; recall that Apollo will not predictively execute queries that are already cached (Section 3.3). </span></p><p class="c5"><span class="c3">4.3.1 Performance Results. Figure 6 shows the scalabil- ity of Apollo, Fido, and Memcached for increasing numbers of clients. Apollo exhibits a significantly lower average response time than Memcached and Fido, even as the number of clients, and therefore the number of predictive query executions, increases. Apollo&rsquo;s efficient data structures and algorithms for tracking and prediction allow scaling even with a large number of clients. Fido and Memcached perform about the same, even as we increase the number of clients. With a large database, query parameters are highly variable and rarely repeated, causing Fido&rsquo;s non-template approach to see few queries from its training set and in turn re- ducing prediction accuracy. As the number of clients increase, the positive effects of shared caching dwarf that of Fido&rsquo;s predictions, resulting in similar performance characteristics between Fido and Memcached. </span></p><p class="c5"><span class="c3">These results show that Apollo can deliver significant perfor- mance gains while scaling effectively to hundreds of concurrent clients continuously executing predictive queries. </span></p><p class="c8"><span class="c27">55 </span><span class="c71">95 ) sm(e miTe snopseRy reu</span><span class="c27">Q</span><span class="c71">90 85 80 75 70 65 60 </span><span class="c27">50 100 150 200 Number of Clients </span></p><p class="c8"><span class="c6">4.4 Adapting to Changing Workloads </span><span class="c3">To assess Apollo&rsquo;s ability to adapt to changing workloads, we conducted an experiment in which the workload was changed from our TPC-C workload described in Section 4.3 to TPC-W partway through the experiment (Figure 7). We see that Apollo quickly learns predictions for the TPC-C workload, resulting in the performance gains shown in Figure 6. By contrast. Fido and Memcached have relatively constant performance during the TPC- C run since they are unable to generalize and make effective predictions for upcoming queries (Section 4.3). </span></p><p class="c5"><span class="c3">Once the workload switches, shown by a dashed vertical line in Figure 7, each configuration experiences a brief penalty in performance because the predictive engines cannot make any pre- dictions for queries in the new workload, and no TPC-W queries are cached. However, Apollo quickly returns to its typical perfor- mance on TPC-W (Figure 5(a)) since it uses online learning to discover query patterns. Fido and Memcached perform similarly after the switch since Fido is unable to make predictions for an untrained workload. Although Fido was trained for TPC-C in this experiment, we note that its performance is comparable to an appropriately trained Fido on the TPC-W portion. This obser- vation further highlights the ineffectiveness of Fido&rsquo;s prediction scheme for the correlated query patterns, which Apollo excels at predicting. </span></p><p class="c8"><span class="c6">4.5 Geographic Latency Testing </span><span class="c3">To assess the effects of different geographic latencies between Apollo and the database, we deployed TPC-W databases in the US-East and Canada regions. Because Apollo, the cache and the benchmark machine are all located in the US-East region, the first configuration tests a &ldquo;local&rdquo; deployment, in which latency among the machines is minimal (a few milliseconds). The second configuration tests moderate latencies of 20 ms. </span></p><p class="c5"><span class="c3">In both configurations (Figures 8(a) and 8(b)), we see that Apollo preserves its lead over the other systems despite limited geographic latency. Apollo reduces query response time by up to 50% in the US East region and by up to 40% in the Canada region. This improvement in the performance gap compared to the higher latency experiments is because cache misses in low latency environments have a larger effect on average performance than when latency is high. The reason for this effect is that Apollo&rsquo;s advantage when caching expensive queries becomes even more significant with reduced latency; prioritizing expensive and fre- quently executed queries results in a substantial improvement and failure to predictively cache them (as in Memcached&rsquo;s and Fido&rsquo;s case) results in a relatively larger performance degradation. </span></p><p class="c5"><span class="c3">These results are not to be interpreted as Apollo is &ldquo;best&rdquo; in a local setting with near zero latency &mdash; the total response time savings for the remote settings are larger than that of the local setting. Apollo provides substantial reductions in average and total response time in both settings, resulting in an enhanced user experience. </span></p><p class="c8"><span class="c6">4.6 Multiple Apollo Instances </span><span class="c3">Apollo can scale to high loads by partitioning clients among multi- ple Apollo engine instances and cache stores. Each Apollo engine maintains a consistent session for each client connected to it, with- out interacting with the other instances or a centralized session manager. </span></p><p class="c8"><span class="c3">To determine Apollo&rsquo;s scaling characteristics, we deployed Apollo on less powerful m4.xlarge EC2 instances with 4 vCPUs </span></p><p class="c8"><span class="c1">261 </span></p><p class="c44"><span class="c37">90 </span><span class="c21">60 60 150 </span><span class="c37">1 Instance </span><span class="c21">50 50 140 </span><span class="c37">2 Instances 3 Instances </span></p><p class="c8"><span class="c21">40 40 130 30 30 120 20 20 110 10 10 100 </span><span class="c37">0 20 30 40 50 </span></p><p class="c8"><span class="c37">0 20 30 40 50 </span></p><p class="c8"><span class="c37">20 30 40 50 60 70 80 90 100 </span></p><p class="c8"><span class="c9">(c) Multiple Apollo Instances </span></p><p class="c8"><span class="c3">Figure 8: Experiment results for 20 minute TPC-W runs in different geographic regions and when using multiple Apollo in- stances. </span></p><p class="c5"><span class="c3">and 16 GB of RAM. We use these less powerful machines as they are individually unable to handle large numbers of clients, necessitating a horizontal scale-out for Apollo instances. We test three different Apollo configurations: one with a single Apollo instance, another with two Apollo instances, and a third with three Apollo instances. Each Apollo instance is given its own dedicated cache, thereby avoiding the need to synchronize version vectors across instances to maintain client sessions. Clients are evenly distributed and pinned to Apollo instances. </span></p><p class="c5"><span class="c3">The results of the experiment are shown in Figure 8(c). As the client load increases, we see that it quickly overwhelms the 1-instance Apollo configuration, resulting in a large increase in query response time. The 2-instance and 3-instance Apollo config- urations show significantly improved scalability, though eventually the 2-instance configuration begins to show a similar upward trend in response time. </span></p><p class="c5"><span class="c3">We observed that the 2-instance query response time at 20 clients is slightly lower than that of the 3-instance configuration. This effect is primarily due to splitting the clients across three machines rather than two, resulting in the 3-instance configu- ration receiving fewer queries to learn from. ADQs are shared among clients, which results in longer learning times to reach a steady state with fewer clients. With a larger number of clients, the increase in the amount of data to learn from and the com- puting power available result in improved performance over the 2-instance configuration. </span></p><p class="c5"><span class="c3">Although having multiple Apollo instances share models and training data would reduce the effects of training data splitting, the trade-off is increased synchronization between otherwise indepen- dent nodes. We eschew this approach for two reasons. First, clients should be split across Apollo instances only when a single instance cannot handle the load. As seen in Figure 8(c), Apollo receives enough training data from clients well before it reaches its load capabilities, even on a less powerful machine. Second, slightly increased training times are a small price to pay for horizontal scalability. Addressing scalability issues in production systems is challenging &mdash; learning for a few more minutes is a simple and inexpensive solution to the insufficient data problem. </span></p><p class="c5"><span class="c3">These multi-instance experiments demonstrate Apollo&rsquo;s ability to scale to large numbers of clients through horizontal scaling and client-session consistency semantics. Since separate Apollo instances do not need to communicate, Apollo demonstrates ex- cellent scaling characteristics. </span></p><p class="c132"><span class="c37">Apollo </span><span class="c21">) sm(e miTe snopseRy reu</span><span class="c37">QMemcached Fido </span></p><p class="c8"><span class="c21">) sm(e miTe snopseRy reu</span><span class="c37">QNumber of Clients </span></p><p class="c8"><span class="c37">Number of Clients </span></p><p class="c8"><span class="c9">(a) TPC-W DB in US East Region (local to client) </span></p><p class="c8"><span class="c37">Apollo Fido </span></p><p class="c8"><span class="c21">) sm(e miTe snopseRy reu</span><span class="c37">QMemcached </span></p><p class="c8"><span class="c37">Number of Clients </span></p><p class="c8"><span class="c9">(b) TPC-W DB in Canada Region </span></p><p class="c8"><span class="c6">4.7 Sensitivity Analysis </span><span class="c3">A key feature of Apollo is its ability to be configured to operate under different workloads and system deployments. This config- urability is enabled by the provision of parameters that can be set according to a particular workload and deployment. In this section, we discuss the effects of these parameters on the overall performance of the system as well as our choice of their default settings. </span></p><p class="c5"><span class="c3">For TPC-W, our default parameter choices were: </span><span class="c14">&#8710;</span><span class="c3">t = 15s, &tau; = 0.01, and a reload cost threshold of &alpha; = 0. Per the specifica- tion [42], TPC-W uses randomized think times with a mean of 7s. Each client has its own application state, which is determined through a probabilistic transition matrix. Therefore, a client&rsquo;s web interactions do not generate a pre-determined chain of queries. </span></p><p class="c5"><span class="c3">The maximal time separation </span><span class="c14">&#8710;</span><span class="c3">t and minimum probability threshold &tau; are correlated. As </span><span class="c14">&#8710;</span><span class="c3">t decreases, the probability of correlated query templates executing within this interval also decreases, thereby requiring a lower &tau; value to capture the re- lationship between query templates. Similarly, as </span><span class="c14">&#8710;</span><span class="c3">t increases, the probability of two correlated query templates executing within this interval also increases, so higher &tau; values are sufficient. If </span><span class="c14">&#8710;</span><span class="c3">t is large and &tau; is small then it is likely that spurious relationships between query templates will be discovered, but such spurious relationships are filtered out by Apollo&rsquo;s parameter mapping ver- ification period, and are therefore seldom predictively executed. Since TPC-W&rsquo;s workload bottlenecks on the database, Apollo filters out the spurious correlations in exchange for discovering as many relationships as possible. To do so, we set a high value of </span><span class="c14">&#8710;</span><span class="c3">t = 15s and a low threshold of &tau; = 0.01. These values were empirically confirmed to yield the best results. </span></p><p class="c5"><span class="c3">In Section 3.4.2, we defined &alpha; to be the minimum cost that an ADQ must have to be reloaded. Note that the cost of an ADQ is the mean response time multiplied by the probability of the query executing. Hence as &alpha; is increased, only ADQs that are both popular and expensive are reloaded. We experimented with different values of &alpha; and found that for small values of &alpha; (less than 5% of the mean query response time), there was little change in query response time. However, as &alpha; continued to be increased past this threshold, the mean query response time grew by over 10%, as valuable ADQs were not reloaded. To ensure that all ADQs were reloaded, &alpha; was set to 0 in our experiments. </span></p><p class="c5"><span class="c3">We observed similar trends with </span><span class="c14">&#8710;</span><span class="c3">t and &tau; in TPC-C as in TPC- W; therefore, our default parameter choices for TPC-C were the same. We left </span><span class="c14">&#8710;</span><span class="c3">t large and &tau; small to place additional pressure on Apollo&rsquo;s parameter mapping filtering functionality. These values were empirically confirmed to yield the best results. </span></p><p class="c8"><span class="c1">262 </span></p><p class="c48 c72"><span class="c3">In our experiments, we used a cache 5% the size of the data- base. We observed that increasing the cache beyond this size did not affect the relative performance differences between Apollo, Memcached, and Fido. </span></p><p class="c48 c70"><span class="c6">5 RELATED WORK </span><span class="c3">Fido [34], detailed in Section 4.1, uses an associative memory model for predictive prefetching in a traditional client/server data- base system. Query patterns in Apollo are tracked at the query template level so a single relationship in Apollo can map to many in Fido. Tracking individual data object accesses, or parameterized queries, means that if Fido has not previously seen a particular parameterized query it will not be able to make a prediction. In contrast, if Apollo has seen the query template (regardless of parameters), it can infer correlation between queries and predic- tively execute. As Fido requires offline training, it cannot adapt to changes in object access patterns. As we have shown, the online nature of Apollo&rsquo;s Markov model allows it to dynamically change over time and thus adapt to new query patterns. </span></p><p class="c13"><span class="c3">Keller et al. [24] describe a distributed caching system for databases, which uses approximate cache descriptions to distribute update notifications to relevant sites and execute queries locally on caches. Each site&rsquo;s cached data is tracked using query predicates. Apollo differs from this work in that we focus on the predictive ex- ecution of consequent queries derived from query patterns, which Keller et al. do not consider. </span></p><p class="c36"><span class="c3">Scalpel [10] tracks queries at a database cursor level, inter- cepting open, fetch and close cursor operations within the JDBC protocol. Since the JDBC API is translated to database specific protocols, Scalpel functions as a client-side cache rather than a mid-tier shared cache like Apollo. Unlike Apollo&rsquo;s online learn- ing model, Scalpel requires offline training to find the patterns that it uses for query rewriting and prefetching. Scalpel employs aggressive cache invalidation on writes and at the start of new transactions, which differs from Apollo&rsquo;s client-centric consis- tency model. Given that Apollo supports mid-tier shared caching across multiple clients, this makes Scalpel unsuitable for compari- son against Apollo. </span></p><p class="c36"><span class="c3">Pavlo et al. [7] implement Markov models in H-Store and use them to optimize transaction execution for distributed database physical design. The system constructs a series of Markov models for stored procedures and monitors the execution paths under a set of input parameters. Their model can be leveraged to determine a base partition for stored procedures and to lock only partitions that are predicted to be accessed during procedure execution. Apollo operates beyond this stored procedure context, and provides bene- fits through caching future queries rather than by analyzing query execution paths. </span></p><p class="c13"><span class="c3">DBProxy [5] is a caching system developed by IBM to cache query results on edge nodes. DBProxy uses multi-layered in- dexes and query containment to match queries to results, evicting stale and unused results. Its single session guarantees differ from Apollo&rsquo;s per-client sessions and limit scalability, in addition to not using online learning or predictive caching to improve perfor- mance. </span></p><p class="c16"><span class="c3">Ramachandra et al. [36] propose a method for semantic prefetch- ing by analyzing the control flow and call graph of program binary files. Given the source code for a database application, the system analyzes and modifies it, adding prefetch requests into the code as soon as the parameters are known and query execution guaranteed. Since this work is limited to requiring access to the source code of </span></p><p class="c5 c64"><span class="c3">application binaries, it only works for fixed workloads. Because Apollo analyzes query streams, it is able to adapt to changing query patterns over time. </span></p><p class="c59"><span class="c3">Although proprietary middleware caching solutions have been developed [9, 16, 26], they do not use predictive analytics to identify future queries and preload them in the cache. </span></p><p class="c96"><span class="c3">Scheuermann et al. [40] propose the Watchman cache manage- ment system, which uses query characteristics to improve cache admission and replacement algorithms. Unlike Apollo, Watchman does not discover query patterns for use in predictive execution and instead focuses solely on cache management. </span></p><p class="c10"><span class="c3">Holze et al. [23] have broached the idea of modeling workloads using Markov models, but such work focuses only on determin- ing when an application&rsquo;s workload has been altered rather than relying on statistical models for caching purposes, like Apollo. They suggest a Markov model as a means to achieve an auto- nomic database, enabling features such as self-configuration, self- optimization, self-healing, and self-protection. In contrast, Apollo uses Markov models of user workloads to predict future queries and enables predictive query caching. </span></p><p class="c96"><span class="c3">Promise [37] is a theoretical framework for predicting query be- haviour within an OLAP database. Promise uses Markov models to predict user query behaviour by developing state machines for parameter value changes and transitions between OLAP queries, but does not consider direct parameter mappings, FDQ hierar- chies, or pipelining predictions. Unlike Apollo, Promise does not validate its techniques through a system implementation. </span></p><p class="c96"><span class="c3">Recent research in approximate query processing [6, 44] has proposed using previous query results as a means for approximat- ing the answer to future queries. These works develop statisti- cal methods to provide accurate, approximate answers and error bounds for upcoming queries, which differs from Apollo&rsquo;s focus on learning parameter mappings for predictive caching. </span></p><p class="c68"><span class="c3">In the view selection problem [12], one must decide on a set of views to materialize so that execution of the workload minimizes some cost function and uses fixed amount of space. Most work in this area requires knowledge of the workload ahead of time [2, 22], with the remainder not considering machine learning techniques for uncovering patterns for use in view selection [17, 25]. </span></p><p class="c96"><span class="c3">XML XPath templates have some similarities to query tem- plates [31], but are not used for online learning in predictive execution and caching. Instead, XPath views are selected using offline training in a warm-up period [29, 43], similar to that of Fido [34]. Similar ideas have been explored to cache dynamic HTML fragments [14]. </span></p><p class="c85"><span class="c6">6 CONCLUSION </span><span class="c3">In this paper, we propose a novel method to determine and lever- age hidden relationships within a database workload via a pre- dictive learning model. We present the Apollo system, which exploits query patterns to predictively execute and cache query results. Apollo&rsquo;s online learning method makes it suitable for different workloads and deployments. Experimental evaluation demonstrates that Apollo is a scalable solution that efficiently uses a cache and outperforms both Memcached, an industrial caching solution for different workloads and the popular Fido predictive cache. </span></p><p class="c85"><span class="c6">ACKNOWLEDGMENTS </span><span class="c3">Funding for this project was provided by the Cheriton Gradu- ate Scholarship, Ontario Graduate Scholarship, and the Natural </span></p><p class="c99"><span class="c1">263 </span></p><p class="c48 c102"><span class="c3">Sciences and Engineering Research Council of Canada. We are grateful for compute resource support from the AWS Cloud Cred- its for Research program. </span></p><p class="c107"><span class="c6">REFERENCES </span></p><p class="c15"><span class="c9">[1] February 2010. The Transaction Processing Council. TPC-C Benchmark </span></p><p class="c86"><span class="c9">(Revision 5.11). http://www.tpc.org/tpcc/. (February 2010). [2] Sanjay Agrawal, Surajit Chaudhuri, and Vivek R. Narasayya. 2000. Automated Selection of Materialized Views and Indexes in SQL Databases. In PVLDB (VLDB &rsquo;00). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 496&ndash;505. http://dl.acm.org/citation.cfm?id=645926.671701 [3] Akamai. 2010. New Study Reveals the Impact of Travel Site Performance on Consumers. https://www.akamai.com/us/en/about/news/press/2010-press/ new-study-reveals-the-impact-of-travel-site-performance-on-consumers.jsp. (2010). [4] M. Akdere, U. &Ccedil;etintemel, M. Riondato, E. Upfal, and S. B. Zdonik. 2012. Learning-based Query Performance Modeling and Prediction. In 2012 IEEE 28th International Conference on Data Engineering. 390&ndash;401. https://doi.org/ 10.1109/ICDE.2012.64 [5] K. Amiri, Sanghyun Park, R. Tewari, and S. Padmanabhan. 2003. DBProxy: a dynamic data cache for web applications. In Proceedings 19th International Conference on Data Engineering (Cat. No.03CH37405). 821&ndash;831. https: //doi.org/10.1109/ICDE.2003.1260881 [6] Christos Anagnostopoulos and Peter Triantafillou. 2017. Efficient scalable accurate regression queries in IN-DBMS analytics. Proceedings - International Conference on Data Engineering (2017), 559&ndash;570. https://doi.org/10.1109/ ICDE.2017.111 [7] Stanley Zdonik Andrew Pavlo, Evan P.C. Jones. 2012. On Predictive Modeling for Optimizing Transaction Execution in Parallel OLTP Systems. PVLDB 5, 2 (2012), 85&ndash;96. [8] P.A. Bernstein, S. Pal, and D.R. Shutt. 2009. Prefetching and caching persistent objects. (June 30 2009). https://www.google.com/patents/US7555488 US Patent 7,555,488. [9] Christof Bornh&ouml;vd, Mehmet Altinel, Sailesh Krishnamurthy, C. Mohan, Hamid Pirahesh, and Berthold Reinwald. 2003. DBCache: Middle-tier Database Caching for Highly Scalable e-Business Architectures. In SIGMOD (SIGMOD &rsquo;03). ACM, New York, NY, USA, 662&ndash;662. https://doi.org/10.1145/872757. </span></p><p class="c90"><span class="c9">872849 [10] Ivan Bowman and Kenneth Salem. 2004. Optimization of query streams using </span></p><p class="c42"><span class="c9">semantic prefetching. SIGMOD (2004), 179&ndash;190. [11] Nathan Bronson, Zach Amsden, George Cabrera, et al. 2013. TAO: Facebook&rsquo;s distributed data store for the social graph. Usenix ATC (2013), 49&ndash;60. https: //doi.org/10.1145/2213836.2213957 [12] Rada Chirkova, Alon Y Halevy, and Dan Suciu. 2001. A formal perspective on </span></p><p class="c83"><span class="c9">the view selection problem. In VLDB, Vol. 1. 59&ndash;68. [13] James C Corbett, Jeffrey Dean, Michael Epstein, and Andrew Fikes. 2012. Spanner : Google &rsquo; s Globally-Distributed Database. OSDI (2012), 1&ndash;14. https://doi.org/10.1145/2491245 [14] Anindya Datta, Kaushik Dutta, Helen Thomas, Debra V, Krithi Ramamritham, and Dan Fishman. 2001. A comparative study of alternative middle tier caching solutions to support dynamic web content acceleration. In In International Conference on Very Large Data Bases (VLDB. 25. [15] Khuzaima Daudjee and Kenneth Salem. 2004. Lazy Database Replication with </span></p><p class="c83"><span class="c9">Ordering Guarantees. In ICDE. 424&ndash;435. [16] Louis Degenaro, Arun Iyengar, Ilya Lipkind, and Isabelle Rouvellou. 2000. A Middleware System Which Intelligently Caches Query Results. In IFIP/ACM International Conference on Distributed Systems Platforms (Middleware &rsquo;00). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 24&ndash;44. http://dl.acm.org/ citation.cfm?id=338283.338285 [17] Prasad M Deshpande, Karthikeyan Ramasamy, Amit Shukla, and Jeffrey F Naughton. 1998. Caching multidimensional queries using chunks. In ACM SIGMOD Record, Vol. 27. ACM, 259&ndash;270. [18] Djellel Eddine Difallah, Andrew Pavlo, Carlo Curino, and Philippe Cudre- Mauroux. 2013. OLTP-Bench: An extensible testbed for benchmarking rela- tional databases. PVLDB 7, 4 (2013), 277&ndash;288. [19] Brad Fitzpatrick. 2016. MemCached. (4 2016). https://memcached.org/ </span></p><p class="c120"><span class="c9">Memcached Caching Software. [20] Brady Forest. 2009. Bing and Google Agree - Slow Pages Lose Users. http: </span></p><p class="c113"><span class="c9">//radar.oreilly.com/2009/06/bing-and-google-agree-slow-pag.html. (2009). [21] Google. 2017. Google&rsquo;s Edge Network. https://peering.google.com/ </span></p><p class="c34"><span class="c9">infrastructure. (2017). </span></p><p class="c8 c19"><span class="c9">[22] Himanshu Gupta. 1997. Selection of Views to Materialize in a Data Warehouse. In ICDT (ICDT &rsquo;97). Springer-Verlag, London, UK, UK, 98&ndash;112. http://dl. acm.org/citation.cfm?id=645502.656089 [23] Marc Holze and Norbert Ritter. 2007. Towards Workload Shift Detection and Prediction for Autonomic Databases. In Proceedings of the ACM First Ph.D. Workshop in CIKM (PIKM &rsquo;07). ACM, New York, NY, USA, 109&ndash;116. https://doi.org/10.1145/1316874.1316892 [24] Arthur M. Keller and Julie Basu. 1996. A Predicate-based Caching Scheme for Client-server Database Architectures. VLDBJ 5, 1 (Jan. 1996), 035&ndash;047. https://doi.org/10.1007/s007780050014 [25] Yannis Kotidis and Nick Roussopoulos. 1999. DynaMat: a dynamic view management system for data warehouses. In ACM SIGMOD Record, Vol. 28. </span><span class="c55">ACM, 371&ndash;382. </span><span class="c9">[26] Per-Ake Larson, Jonathan Goldstein, and Jingren Zhou. 2004. MTCache: Transparent Mid-Tier Database Caching in SQL Server. In ICDE (ICDE &rsquo;04). IEEE Computer Society, Washington, DC, USA, 177&ndash;. [27] Ang Li, Xiaowei Yang, Srikanth Kandula, and Ming Zhang. 2010. CloudCmp: Comparing Public Cloud Providers. Proceedings of the 10th annual conference on Internet measurement - IMC &rsquo;10 (2010), 1. https://doi.org/10.1145/1879141. 1879143 [28] Jialin Li, Naveen Kr. Sharma, Dan R. K. Ports, and Steven D. Gribble. 2014. Tales of the Tail: Hardware, OS, and Application-level Sources of Tail Latency. In Proceedings of the ACM Symposium on Cloud Computing (SOCC &rsquo;14). ACM, New York, NY, USA, Article 9, 14 pages. https://doi.org/10.1145/ 2670979.2670988 [29] Kostas Lillis and Evaggelia Pitoura. 2008. Cooperative XPath Caching. In SIGMOD (SIGMOD &rsquo;08). ACM, New York, NY, USA, 327&ndash;338. https://doi. org/10.1145/1376616.1376652 [30] Greg Linden. 2006. Make Data Useful. http://www.gduchamp.com/media/ </span></p><p class="c65 c105"><span class="c9">StanfordDataMining.2006-11-28.pdf. (2006). [31] Bhushan Mandhani and Dan Suciu. 2005. Query Caching and View Selection for XML Databases. In PVLDB (VLDB &rsquo;05). VLDB Endowment, 469&ndash;480. http://dl.acm.org/citation.cfm?id=1083592.1083648 [32] Oracle. 2017. Java SE 8 JDBC API. https://docs.oracle.com/javase/8/docs/ </span></p><p class="c65 c19"><span class="c9">technotes/guides/jdbc/. (2017). [33] Oracle. 2017. MySQL. https://www.mysql.com/. (2017). [34] Mark Palmer and Stanley B. Zdonik. 1991. Fido: A Cache That Learns to Fetch. In VLDB (VLDB &rsquo;91). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 255&ndash;264. [35] Jose Pereira. 2016. TPC-W Implementation. (4 2016). University of Minho&rsquo;s </span></p><p class="c41"><span class="c9">implementation of TPC-W. [36] Karthik Ramachandra and S. Sudarshan. 2012. Holistic Optimization by Prefetching Query Results. In SIGMOD (SIGMOD &rsquo;12). ACM, New York, NY, USA, 133&ndash;144. https://doi.org/10.1145/2213836.2213852 [37] Carsten Sapia. 2000. PROMISE: Predicting query behavior to enable predictive caching strategies for OLAP systems. In International Conference on Data Warehousing and Knowledge Discovery. Springer, 224&ndash;233. [38] Mehadev Satyanarayanan, Paramvir Bahl, Ramon Caceres, and Nigel Davies. 2009. The Case for VM-Base Cloudlets in Mobile Computing. Pervasive Computing 8, 4 (2009), 14&ndash;23. https://doi.org/10.1109/MPRV.2009.82 [39] Mahadev Satyanarayanan, Zhuo Chen, Kiryong Ha, Wenlu Hu, Wolfgang Richter, and Padmanabhan Pillai. 2014. Cloudlets: at the Leading Edge of Mobile-Cloud Convergence. Proceedings of the 6th International Conference on Mobile Computing, Applications and Services (2014), 1&ndash;9. https://doi.org/ 10.4108/icst.mobicase.2014.257757 [40] Peter Scheuermann, Junho Shim, and Radek Vingralek. 1996. WATCHMAN: A Data Warehouse Intelligent Cache Manager. In Proceedings of the 22th International Conference on Very Large Data Bases (VLDB &rsquo;96). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 51&ndash;62. http://dl.acm.org/ citation.cfm?id=645922.758367 [41] Jeff Shute, Mircea Oancea, Stephan Ellner, et al. 2012. F1: the fault-tolerant </span></p><p class="c65 c105"><span class="c9">distributed RDBMS supporting Google&rsquo;s ad business. In SIGMOD. 777&ndash;778. [42] TPC. 2000. TPC Benchmark W (Web Commerce). http://www.tpc.org/tpcw. </span></p><p class="c19 c65"><span class="c9">(2000). [43] Liang Huai Yang, Mong Li Lee, and Wynne Hsu. 2003. Efficient Mining of XML Query Patterns for Caching. In PVLDB (VLDB &rsquo;03). VLDB Endowment, 69&ndash;80. http://dl.acm.org/citation.cfm?id=1315451.1315459 [44] Barzan Mozafari Yongjoo Park, Ahmad Shahab Tajik, Michael Cafarella. 2017. Database Learning: Toward a Database that Becomes Smarter Every Time. SIGMOD (2017), 587&ndash;602. [45] E. E. Yusufoglu, M. Ayyildiz, and E. Gul. 2014. Neural network-based ap- proaches for predicting query response times. In 2014 International Con- ference on Data Science and Advanced Analytics (DSAA). 491&ndash;497. https: //doi.org/10.1109/DSAA.2014.7058117 </span></p><p class="c84"><span class="c1">264 </span></p></body></html>