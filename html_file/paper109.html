<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c100{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Arial";font-style:normal}.c94{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.4pt;font-family:"Arial";font-style:normal}.c109{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:11.4pt;font-family:"Arial";font-style:normal}.c107{color:#ff2600;font-weight:700;text-decoration:none;vertical-align:sub;font-size:7.9pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c30{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c63{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10.8pt;font-family:"Arial";font-style:normal}.c18{color:#c82506;font-weight:400;text-decoration:none;vertical-align:super;font-size:12pt;font-family:"Arial";font-style:normal}.c80{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.8pt;font-family:"Arial";font-style:normal}.c138{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:17.4pt;font-family:"Arial";font-style:normal}.c101{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c36{color:#ff2600;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:4.6pt;font-family:"Arial";font-style:normal}.c98{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.6pt;font-family:"Arial";font-style:normal}.c66{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:4.8pt;font-family:"Arial";font-style:normal}.c112{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.1pt;font-family:"Arial";font-style:normal}.c110{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11pt;font-family:"Arial";font-style:normal}.c21{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.9pt;font-family:"Arial";font-style:normal}.c44{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.7pt;font-family:"Arial";font-style:normal}.c20{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:10.6pt;font-family:"Arial";font-style:normal}.c97{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.8pt;font-family:"Arial";font-style:normal}.c90{color:#ff2600;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.1pt;font-family:"Arial";font-style:normal}.c96{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:12.2pt;font-family:"Arial";font-style:normal}.c84{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:9.6pt;font-family:"Arial";font-style:normal}.c142{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c67{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.9pt;font-family:"Courier New";font-style:italic}.c114{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c50{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:4.6pt;font-family:"Arial";font-style:normal}.c148{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.6pt;font-family:"Courier New";font-style:italic}.c111{margin-left:-25.4pt;padding-top:1.7pt;text-indent:34.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c123{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.6pt;font-family:"Arial";font-style:normal}.c156{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.2pt;font-family:"Arial";font-style:normal}.c120{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Courier New";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c119{color:#ff2600;font-weight:700;text-decoration:none;vertical-align:sub;font-size:11.9pt;font-family:"Arial";font-style:normal}.c141{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:11.3pt;font-family:"Arial";font-style:normal}.c32{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c106{margin-left:-16.6pt;padding-top:19pt;text-indent:104.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.4pt}.c115{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.1pt;font-family:"Arial";font-style:normal}.c71{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:8.2pt;font-family:"Arial";font-style:normal}.c38{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.7pt;font-family:"Arial";font-style:normal}.c147{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.1pt;font-family:"Arial";font-style:normal}.c60{margin-left:-25.4pt;padding-top:1.4pt;text-indent:34.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c39{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.3pt;font-family:"Arial";font-style:normal}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.5pt;font-family:"Arial";font-style:normal}.c122{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.4pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c16{color:#ff2600;font-weight:700;text-decoration:none;vertical-align:sub;font-size:12.1pt;font-family:"Arial";font-style:normal}.c77{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.6pt;font-family:"Arial";font-style:normal}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c25{color:#ff2600;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.4pt;font-family:"Arial";font-style:normal}.c46{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:15.1pt;font-family:"Arial";font-style:normal}.c85{color:#c82506;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Arial";font-style:normal}.c75{color:#ff2600;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:4.7pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c37{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.6pt;font-family:"Arial";font-style:normal}.c154{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:italic}.c126{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.9pt;font-family:"Arial";font-style:normal}.c47{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:11pt;font-family:"Arial";font-style:normal}.c79{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:12pt;font-family:"Arial";font-style:normal}.c145{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Courier New";font-style:normal}.c132{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11.5pt;font-family:"Arial";font-style:normal}.c82{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c121{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.4pt;font-family:"Arial";font-style:normal}.c40{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.8pt;font-family:"Arial";font-style:normal}.c163{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11pt;font-family:"Arial";font-style:normal}.c56{color:#c82506;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Arial";font-style:normal}.c83{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:9.6pt;font-family:"Arial";font-style:normal}.c42{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:17.8pt;font-family:"Arial";font-style:normal}.c102{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.4pt;font-family:"Arial";font-style:normal}.c116{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.2pt;font-family:"Arial";font-style:normal}.c35{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Courier New";font-style:normal}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.3pt;font-family:"Arial";font-style:normal}.c103{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:4.9pt;font-family:"Arial";font-style:normal}.c41{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c62{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.2pt;font-family:"Arial";font-style:normal}.c29{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.8pt;font-family:"Arial";font-style:normal}.c108{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.5pt;font-family:"Arial";font-style:normal}.c162{color:#c82506;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Courier New";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Arial";font-style:normal}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c68{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c45{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.2pt;font-family:"Arial";font-style:normal}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c59{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:8.6pt;font-family:"Arial";font-style:normal}.c130{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.6pt;font-family:"Arial";font-style:normal}.c149{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.2pt;font-family:"Courier New";font-style:normal}.c7{margin-left:-25.4pt;padding-top:3.8pt;text-indent:34.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c92{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.1pt;font-family:"Arial";font-style:normal}.c99{color:#000000;font-weight:700;text-decoration:none;vertical-align:sub;font-size:11.4pt;font-family:"Arial";font-style:normal}.c54{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.6pt;font-family:"Arial";font-style:normal}.c73{margin-left:-16.6pt;padding-top:9.8pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c57{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Arial";font-style:normal}.c61{margin-left:-16.6pt;padding-top:1.4pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c158{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:10.9pt;font-family:"Arial";font-style:normal}.c33{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.6pt;font-family:"Courier New";font-style:normal}.c128{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.5pt;font-family:"Arial";font-style:normal}.c55{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:11.1pt;font-family:"Arial";font-style:normal}.c87{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.1pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:11.6pt;font-family:"Arial";font-style:normal}.c155{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:18.1pt;font-family:"Arial";font-style:normal}.c150{margin-left:-25.4pt;padding-top:4.1pt;text-indent:34.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c127{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.9pt;font-family:"Arial";font-style:normal}.c69{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c14{margin-left:-16.6pt;padding-top:1.4pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.2pt}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c151{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.4pt;font-family:"Arial";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c117{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c34{margin-left:-16.6pt;padding-top:3.8pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c64{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.9pt;font-family:"Arial";font-style:normal}.c11{color:#ff2600;font-weight:700;text-decoration:none;vertical-align:sub;font-size:18.1pt;font-family:"Arial";font-style:normal}.c137{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:9.2pt;font-family:"Arial";font-style:normal}.c93{margin-left:-16.6pt;padding-top:1.7pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.7pt;font-family:"Arial";font-style:normal}.c88{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.7pt;font-family:"Arial";font-style:normal}.c49{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:11.8pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Courier New";font-style:normal}.c78{color:#000000;font-weight:700;text-decoration:none;vertical-align:sub;font-size:11.5pt;font-family:"Arial";font-style:normal}.c159{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:12.1pt;font-family:"Arial";font-style:normal}.c81{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c143{margin-left:-25.4pt;padding-top:7.7pt;text-indent:34.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c118{margin-left:218.2pt;padding-top:35.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c160{margin-left:-16.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9.2pt}.c72{margin-left:75.8pt;padding-top:7.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:72pt}.c134{margin-left:-12.2pt;padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.2pt}.c129{margin-left:-25.4pt;padding-top:17.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c104{margin-left:34pt;padding-top:18pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:157.4pt}.c157{margin-left:37.6pt;padding-top:13.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:155.5pt}.c140{margin-left:-25.4pt;padding-top:10.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c53{margin-left:-16.6pt;padding-top:110.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.2pt}.c89{margin-left:-25.4pt;padding-top:8.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:52pt}.c86{margin-left:-16.6pt;padding-top:17pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.2pt}.c113{margin-left:7.7pt;padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c153{margin-left:-12.2pt;padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c125{margin-left:-22.8pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:116.8pt}.c131{margin-left:-25.4pt;padding-top:17.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c4{margin-left:218.2pt;padding-top:30.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c48{margin-left:-22.8pt;padding-top:9.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:116.8pt}.c52{margin-left:58.5pt;padding-top:13.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:134.6pt}.c91{margin-left:-3.2pt;padding-top:9.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.2pt}.c58{margin-left:-16.6pt;padding-top:13.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:105.1pt}.c65{margin-left:-3.2pt;padding-top:8.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.4pt}.c144{margin-left:-22.8pt;padding-top:9.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:124.7pt}.c139{margin-left:-12.2pt;padding-top:7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.2pt}.c95{margin-left:103.6pt;padding-top:22.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:44.2pt}.c135{padding-top:4.1pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c136{padding-top:21.6pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c74{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c27{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c12{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c70{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c133{margin-left:70pt;margin-right:109pt}.c9{margin-left:233.5pt;margin-right:-194.2pt}.c161{margin-left:166.5pt;margin-right:24.7pt}.c76{margin-left:108.2pt;margin-right:64.8pt}.c124{margin-left:-16.6pt;margin-right:-25.4pt}.c43{margin-left:78.4pt;margin-right:99.1pt}.c152{margin-left:-16.6pt;margin-right:-25.2pt}.c51{margin-left:125.4pt;margin-right:66.5pt}.c105{margin-left:104.6pt;margin-right:77.3pt}.c146{margin-left:107.2pt;margin-right:64.6pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c70"><p class="c2"><span class="c81">Cost Estimation of Spatial k-Nearest-Neighbor Operators</span><span class="c69">&lowast; </span></p><p class="c2"><span class="c22">Ahmed M. Aly </span><span class="c114">Purdue University </span><span class="c101">aaly@cs.purdue.edu </span></p><p class="c2"><span class="c114">West Lafayette, IN </span><span class="c22">Walid G. Aref </span><span class="c114">Purdue University </span><span class="c101">aref@cs.purdue.edu </span></p><p class="c2"><span class="c114">West Lafayette, IN </span><span class="c22">Mourad Ouzzani </span><span class="c114">Qatar Computing Research Institute </span><span class="c101">mouzzani@qf.org.qa </span><span class="c114">Doha, Qatar </span><span class="c22">ABSTRACT </span><span class="c1">Advances in geo-sensing technology have led to an unprecedented spread of location-aware devices. In turn, this has resulted into a plethora of location-based services in which huge amounts of spa- tial data need to be efficiently consumed by spatial query proces- sors. For a spatial query processor to properly choose among the various query processing strategies, the cost of the spatial operators has to be estimated. In this paper, we study the problem of estimat- ing the cost of the spatial k-nearest-neighbor (k-NN, for short) op- erators, namely, k-NN-Select and k-NN-Join. Given a query that has a k-NN operator, the objective is to estimate the number of blocks that are going to be scanned during the processing of this operator. Estimating the cost of a k-NN operator is challenging for several reasons. For instance, the cost of a k-NN-Select operator is directly affected by the value of k, the location of the query focal point, and the distribution of the data. Hence, a cost model that captures these factors is relatively hard to realize. This paper in- troduces cost estimation techniques that maintain a compact set of catalog information that can be kept in main-memory to enable fast estimation via lookups. A detailed study of the performance and accuracy trade-off of each proposed technique is presented. Ex- perimental results using real spatial datasets from OpenStreetMap demonstrate the robustness of the proposed estimation techniques. </span></p><p class="c2"><span class="c22">1. INTRODUCTION </span></p><p class="c12"><span class="c1">The ubiquity of location-aware devices, e.g., smartphones and GPS-devices, has led to a variety of location-based services in which large amounts of geo-tagged information are created every day. This demands spatial query processors that can efficiently pro- cess spatial queries of various complexities. One class of opera- tions that arises frequently in practice is the class of spatial k-NN operations. Examples of spatial k-NN operations include: (i) Find the k-closest hotels to my location (a k-NN-Select), and (ii) Find for each school the k-closest hospitals (a k-NN-Join). </span></p><p class="c12"><span class="c1">The k-NN-Select and k-NN-Join operators can be used along with other spatial or relational operators in the same query. In this &lowast;</span><span class="c26">This research was supported in part by National Science Founda- tion under Grants IIS 0916614, IIS 1117766, and IIS 0964639. </span></p><p class="c2"><span class="c1">c&#8413;</span><span class="c142">2015, Copyright is with the authors. Published in Proc. 18th Inter- national Conference on Extending Database Technology (EDBT), March 23-27, 2015, Brussels, Belgium: ISBN 978-3-89318-067-7, on OpenPro- ceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0 . </span></p><p class="c12"><span class="c1">case, various query-execution-plans (QEPs, for short) for the same query are possible, but with some of the QEPs having better exe- cution times than the others. The role of a query optimizer is to arbitrate among the various QEPs and pick the one with the least processing cost. In this paper, we study the problem of estimating the cost of the k-NN-Select and k-NN-Join operators. </span></p><p class="c12"><span class="c1">To demonstrate the importance of estimating the cost of these op- erators, consider the following example query: &lsquo;Find the k-closest restaurants to my location such that the price of the restaurant is within my budget&rsquo;. This query combines a spatial k-NN-Select with a relational select (price &le; budget). There are two possible QEPs for executing this query: (i) Apply the relational select first, i.e., se- lect the restaurants with price &le; budget and then get the k-closest out of them, or (ii) Apply an incremental k-NN-Select (i.e., dis- tance browsing [14]) and evaluate the relational select on the fly; execution should stop when k restaurants that qualify the relational predicate are retrieved. Clearly, the two QEPs can have different performance. Thus, it is essential to estimate the cost of each pro- cessing alternative in order to choose the cheaper QEP. Observe that distance browsing is also applicable to non-incremental k-NN- Select (i.e., to QEP(i)). [14] proves that for non-incremental k-NN- Select, the number of scanned blocks is optimal in distance brows- ing. Thus, in this paper, we model the cost of distance browsing being the state-of-the-art for k-NN-Select processing. </span></p><p class="c12"><span class="c1">In addition to modeling the cost of the k-NN-Select, we study the cost of the k-NN-Join. The k-NN-Join is a practical spatial op- eration for many application scenarios. For example, consider the following query that combines a relational or a spatial predicate with a k-NN-Join predicate. Assume that a user wants to select for each hotel, its k-closest restaurants (k-NN-Join predicate) such that the restaurant/hotel&rsquo;s price is within the user&rsquo;s budget (rela- tional predicate), or that the restaurant/hotel&rsquo;s location is within a certain downtown district (spatial range predicate). Clearly, esti- mating the cost of a k-NN-Join is important to decide the ordering of the relational, spatial, and k-NN operators in the QEP. A k-NN- Join can also be useful when multiple k-NN-Select queries are to be executed on the same dataset. To share the execution, exploit data locality and the similarities in the data access patterns, and avoid multiple yet unnecessary scans of the underlying data (e.g., as in [11]), all the query points are treated as an outer relation and processing is performed in a single k-NN-Join. In this paper, we in- troduce a cost model for locality-based k-NN-Join processing [22], which is the state-of-the-art in k-NN-Join processing. </span></p><p class="c12"><span class="c1">While several research efforts (e.g., see [2, 3, 4, 5, 7, 15, 17, 18, 23]) estimate the selectivity and cost of the spatial join and range operators, they are not applicable to k-NN operators. For instance, the cost of a spatial range operator is relatively easy to estimate be- cause the spatial region of the operator, in which the query answer </span></p><p class="c2"><span class="c38">457 10.5441/002/edbt.2015.40 </span></p><p class="c129"><span class="c1">resides, is predefined and fixed in the query. In contrast, the spa- tial region that contains the k-nearest-neighbors of a query point, in the case of a k-NN-Select, or a point of the outer relation in the case of a k-NN-Join, is variable since it depends on the value of k, the location of the point, and the density of the data (i.e., its distribution). These three parameters render the problem of k-NN cost-estimation more challenging. </span></p><p class="c111"><span class="c1">In this paper, we introduce the Staircase technique for estimat- ing the cost of k-NN-Select. The Staircase technique distinguishes itself from existing techniques by the ability to quickly estimate the cost of any query using an O(1) lookup. The main idea of the Stair- case technique is to maintain a compact set of catalog information that summarize the cost. We perform various optimizations to limit the size of the catalog such that it can easily fit in main-memory. We empirically compare the performance of the Staircase technique against the state-of-the-art technique [24]. We show that the Stair- case technique has better accuracy for spatial non-uniform data in the two-dimensional space while achieving orders-of-magnitude gain in query estimation time. Having a fast query execution time is vital for location-based services that serve multiple queries at very high rates, e.g., thousands of queries per second. Thus, estimating the cost needs to be extremely fast as it is a preliminary step before the query itself is executed. </span></p><p class="c60"><span class="c1">In addition to estimating the cost of k-NN-Select, we introduce three new techniques for estimating the cost of k-NN-Join. Simi- larly to the Staircase technique, the proposed techniques employ a compact set of catalogs that summarize the cost and enable fast es- timation. First, we present the Block-Sample as our baseline tech- nique. Then, we introduce the Catalog-Merge technique that has better estimation time than the Block-Sample technique, but incurs relatively high storage overhead. Then, we introduce the Virtual- Grid technique that incurs less storage overhead than the Catalog- Merge technique. To the best of our knowledge, estimating the cost of k-NN-Join has not been addressed in previous work. </span></p><p class="c160"><span class="c1">The contributions of this paper can be summarized as follows: </span></p><p class="c139"><span class="c1">&bull; We introduce the Staircase technique for estimating the k- NN-Select cost. </span></p><p class="c134"><span class="c1">&bull; We introduce three novel techniques for estimating the k- NN-Join cost, namely the Block-Sample, Catalog-Merge, and Virtual-Grid techniques. </span></p><p class="c153"><span class="c1">&bull; We conduct extensive experiments to study the performance and accuracy tradeoff that each of the proposed technique offers. Our experimental results demonstrate that: </span></p><p class="c113"><span class="c1">&ndash; the Staircase technique outperforms the techniques in [24] by two orders of magnitude in estimation time and by more than 10% in estimation accuracy, &ndash; the Catalog-Merge technique achieves an error ratio of less than 5% while keeping the estimation time below one microsecond, and &ndash; the Virtual-Grid technique achieves an error ratio of less than 20% while reducing the storage required to maintain the catalogs by an order of magnitude com- pared to the Catalog-Merge technique. </span></p><p class="c143"><span class="c1">The rest of this paper proceeds as follows. Section 2 intro- duces some preliminaries and discusses the related work. Section 3 presents the Staircase technique for estimating the cost of k-NN- Select. Section 4 presents the Block-Sample, Catalog-Merge, and Virtual-Grid techniques for estimating the cost of k-NN-Join. Sec- tion 5 provides an experimental study of the performance of the proposed techniques. Section 6 contains concluding remarks. </span></p><p class="c2 c51"><span class="c1">p </span></p><p class="c104"><span class="c1">A </span></p><p class="c2 c43"><span class="c10">M</span><span class="c83">AX</span><span class="c112">D</span><span class="c83">IST </span></p><p class="c2 c43"><span class="c10">M</span><span class="c83">AX</span><span class="c112">D</span><span class="c83">IST </span></p><p class="c2 c105"><span class="c10">M</span><span class="c83">IN</span><span class="c112">D</span><span class="c83">IST </span></p><p class="c2 c105"><span class="c10">M</span><span class="c83">IN</span><span class="c112">D</span><span class="c83">IST </span></p><p class="c2 c105"><span class="c10">M</span><span class="c83">IN</span><span class="c112">D</span><span class="c83">IST </span></p><p class="c157"><span class="c1">z </span></p><p class="c2 c146"><span class="c10">M</span><span class="c84">AX</span><span class="c115">D</span><span class="c84">IST </span></p><p class="c2 c146"><span class="c10">M</span><span class="c84">AX</span><span class="c115">D</span><span class="c84">IST </span></p><p class="c2 c161"><span class="c1">B </span></p><p class="c2 c161"><span class="c1">B </span></p><p class="c52"><span class="c1">y </span></p><p class="c2 c76"><span class="c10">M</span><span class="c97">IN</span><span class="c10">D</span><span class="c97">IST </span></p><p class="c2 c76"><span class="c10">M</span><span class="c97">IN</span><span class="c10">D</span><span class="c97">IST </span></p><p class="c106"><span class="c46">x </span><span class="c1">C </span><span class="c1">Figure 1: The M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">and M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">metrics. In distance browsing [14], when k = 2 and q is a query focal point of a k-NN-Select, only blocks A and C are scanned, i.e., cost = 2. </span></p><p class="c86"><span class="c22">2. PRELIMINARIES &amp; RELATED WORK </span></p><p class="c124 c135"><span class="c1">We focus on the variants of the k-NN operations given below. Assume that we have two tables, say R and S, that represent two sets of points in the two-dimensional space. For simplicity, we use the Euclidean distance metric. </span></p><p class="c91"><span class="c1">&bull; k-NN-Select: Given a query-point q, </span><span class="c8">&sigma;</span><span class="c15">k,q</span><span class="c26">(R) returns the k- </span><span class="c1">closest to q from the set of points in R. </span></p><p class="c65"><span class="c1">&bull; k-NN-Join: R </span><span class="c8">&#10038;</span><span class="c15">kNN </span><span class="c26">S returns all the pairs (r, s), where </span><span class="c1">r &isin; R, s &isin; S, and s is among the k-closest points to r. </span></p><p class="c73"><span class="c1">Observe that the k-NN-Join is an asymmetric operation, i.e., the two expressions: (R </span><span class="c8">&#10038;</span><span class="c31">kNN </span><span class="c1">S) and (S </span><span class="c8">&#10038;</span><span class="c31">kNN </span><span class="c1">R) are not equiva- lent. In the expression (R </span><span class="c8">&#10038;</span><span class="c15">kNN </span><span class="c26">S), we refer to Relations R and </span><span class="c1">S as the outer and inner relations, respectively. </span></p><p class="c61"><span class="c1">We assume that the data points are organized in a spatial index structure. However, we do not assume a specific indexing struc- ture; our proposed techniques can be applied to a quadtree, an R- tree, or any of their variants, e.g. [12, 20, 13, 6, 16]. These are hierarchical spatial structures that recursively divide the underly- ing space/points into blocks until the number of points inside a block satisfies some criterion (e.g., being less than some threshold). We assume the existence of an auxiliary index, termed the Count- Index. The auxiliary index does not contain any data points, but rather maintains the count of points in each data block. </span></p><p class="c61"><span class="c1">We make extensive use of the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">and M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">met- rics [19]. Refer to Figure 1 for illustration. The M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">(or M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST</span><span class="c1">) between a point, say p, and a block, say b, refers to the minimum (or maximum) possible distance between p and any point in b. Similarly, the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">(or M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST</span><span class="c1">) between two blocks is the minimum (or maximum) possible distance between them. In some scenarios, we process the blocks in a certain order according to their M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">from a certain point (or block). An ordering of the blocks based on the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">from a certain point (or block) is termed M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">ordering. </span></p><p class="c14"><span class="c1">Before describing how to estimate the cost of the k-NN oper- ations, we briefly describe the state-of-the-art algorithms for pro- cessing the k-NN-Select and k-NN-Join. </span></p><p class="c93"><span class="c1">Existing k-NN-Select algorithms prune the search space follow- ing the branch-and-bound paradigm. [19] applies a depth-first al- gorithm to read the index blocks in M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">order with respect to the query point. Once k points are scanned, the distance between q and the k-farthest point encountered is marked. Refer to Figure 1 for illustration. Assume that k = 2. Scanning the blocks starts with Block A (M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">= 0). Two points, y and z, are encountered, so the distance between q and z (the farthest) is marked and scanning </span></p><p class="c4"><span class="c38">458 </span></p><p class="c2 c133"><span class="c63">q</span><span class="c11">D</span><span class="c16">k </span></p><p class="c2 c133"><span class="c63">q</span><span class="c11">D</span><span class="c16">k </span></p><p class="c2 c133"><span class="c63">q</span><span class="c11">D</span><span class="c16">k </span></p><p class="c2 c133"><span class="c63">q</span><span class="c11">D</span><span class="c16">k </span></p><p class="c2 c133"><span class="c63">q</span><span class="c11">D</span><span class="c16">k </span></p><p class="c2 c133"><span class="c63">q</span><span class="c11">D</span><span class="c16">k </span></p><p class="c129"><span class="c1">the blocks continues (Block C then Block B) until the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">of a scanned blocks is greater than the distance between q and z. Thus, the overall number of blocks to be scanned is 3. </span></p><p class="c111"><span class="c1">The above algorithm is suboptimal and cannot be applied for in- cremental k-NN retrieval. The distance browsing algorithm of [14] achieves optimal performance and can be applied for incremental as well as non-incremental k-NN processing. The main idea of this algorithm is that it can incrementally retrieve the nearest-neighbors to a query point through its getNextNearest() method. Two priority queues are maintained: (1) a priority queue for the blocks that have not been scanned yet (blocks-queue for short), and (2) a priority queue for the tuples in the already scanned blocks that have not been returned as nearest-neighbors yet (tuples-queue for short). The entries in the tuples-queue are prioritized based on the distance from the query point, while the entries in the blocks-queue are prioritized based on the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">from the query point. Upon an invocation of the getNextNearest() method, the top, say t, of the tuples-queue is returned if the distance between t and the query point is less than the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">of the top of the blocks-queue. Oth- erwise, the top of the blocks-queue is scanned and all its tuples are inserted into the tuples-queue (ordered based on the distance from the query point). To illustrate, we apply the distance browsing al- gorithm to the example in Figure 1. Assume that k = 2. Block A is scanned first. Points y and z are inserted into the tuples-queue. The M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">of Block C is less than the distance of the top of the tuples-queue, and hence Block C is scanned and Point x is inserted into the tuples-queue. Now, Point x is retrieved as the nearest- neighbor followed by Point y. Observe that the algorithm avoids scanning Block B. Thus, the overall number of scanned blocks is 2 that is less than the number of blocks to be scanned if the algorithm in [19] is applied. </span></p><p class="c111"><span class="c1">In addition to being optimal, the distance browsing algorithm is quite useful when the number of neighbors to be retrieved, i.e., k, is not known in advance. One use case is when a k-NN-Select predicate is combined with a relational predicate within the same query. Consider, for example, a query that retrieves the k-closest restaurants that provide seafood. The distance browsing algorithm gets the nearest restaurant and then examines whether it provides seafood or not. If it is not the case, the algorithm retrieves the next nearest restaurant. This process is repeated until k restaurants satisfying the condition (i.e., provide seafood) are found. </span></p><p class="c111"><span class="c1">Being the state-of-the-art in k-NN-Select processing, we model the cost of the distance browsing algorithm in this paper. Observe that the cost of the distance browsing algorithm is dominated by the number of blocks that get scanned. Thus, given a k-NN-Select, the goal is to estimate the number of blocks to be scanned with- out touching the data points. Observe that this goal is challenging because the cost depends on: (1) the value of k, (2) the location of the query point, and (3) the distribution of the data that directly affects the structure of the index blocks. These factors have direct impact on the cost. Refer to Figure 1 for illustration. If the value of k is relatively large, M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">scanning of the blocks will con- tinue beyond Block C, and thus leading to a larger overall number of scanned blocks. Similarly, if the location of q is different, the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">values will change, and thus leading to different block or- dering during the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">scan, and different overall number of scanned blocks. Also, if the distribution of the data is different, the index blocks will have completely different shapes and locations in space, and this will affect the values of M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST</span><span class="c1">, and hence will affect the overall number of scanned blocks. </span></p><p class="c111"><span class="c1">[8, 9, 24] study the problem of estimating the cost of a k- NN-Select operator for uniformly distributed datasets. The au- thors of [24] further extend their techniques to support non-uniform </span></p><p class="c12 c124"><span class="c1">datasets. The main idea is to estimate the value of D</span><span class="c15">k </span><span class="c26">(Figure 1), </span><span class="c1">i.e., the smallest radius of a circle centered at the query point and that contains k points. Once the value of D</span><span class="c15">k </span><span class="c26">is estimated, the num- </span><span class="c1">ber of blocks that overlap with the circle whose center is the query point and whose radius is D</span><span class="c15">k </span><span class="c26">is determined. This number can be </span><span class="c1">computed by scanning the blocks of the Count-Index in M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">order from q. </span></p><p class="c93"><span class="c1">Given a non-uniform dataset, [24] assumes that the points in each block are uniformly distributed and that each block has a constant density. Histograms are maintained to estimate the density of each block in the index. To estimate the cost, [24] applies the following algorithm. The blocks of the Count-Index are scanned in M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">order from q. Hence, the scanning starts from the block, say b, that is closest (according to M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">) to q. Observe that if q falls within any block, the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">corresponding to that block will be zero, and hence scanning will start from that block. Given the density of Block b, the area of a circle containing k points for that density is computed and then the value of D</span><span class="c15">k </span><span class="c26">is determined. If </span><span class="c1">the circle is fully contained inside Block b, the search terminates; otherwise, further blocks are examined and the combined density of these blocks is computed. Given the combined density, the area of a circle containing k points is determined. This process is repeated until the computed circle is fully contained within the bounds of the examined blocks. We refer to this algorithm as the density-based algorithm. </span></p><p class="c61"><span class="c1">Although the density-based algorithm in [24] achieves good es- timation accuracy, it incurs relatively high overhead in many cases. For instance, if the value of k is high or if the density of the blocks around the query point is low, the algorithm will keep extending its search region by examining further blocks until its search region contains k points. In addition, at each iteration of the algorithm, the combined density of the encountered blocks is computed, which can be a costly operation. The process of estimating the cost of a database operator has to be extremely fast. Typically, a database query optimizer keeps a set of catalog information that summarizes the cost estimates. Then, given a query, it performs quick lookups or simple computations to estimate the corresponding cost. With that goal in mind, we propose a new cost estimation technique that incurs no computational overhead at query time, but rather requires O(1) lookups. </span></p><p class="c61"><span class="c1">Several query processing techniques have been proposed in the literature for processing k-NN-Join operators, e.g., [10, 25, 22]. [22] represents the state-of-the-art technique in k-NN-Join and has proved to achieve better performance than other existing tech- niques. The key idea that distinguishes [22] from other existing techniques is that in any other technique, each point in a block inde- pendently keeps track of its k-nearest-neighbors encountered thus far with no reuse of neighbors of one point as being neighbors of another point in its spatial proximity. In contrast, [22]&rsquo;s approach identifies a region in space (termed locality) that contains all of the k-nearest-neighbors of all the points in a block. Once the best pos- sible locality is built, each point searches only the locality to find its k-nearest-neighbors. This block-by-block processing methodology results in high performance gains. </span></p><p class="c61"><span class="c1">A naive way to estimate the cost of a k-NN-Join operator us- ing the density-based algorithm of [24] is to treat every point from the outer relation as a query point for a k-NN-Select operator and then aggregate the cost across all the points from the outer rela- tion. However, this approach is costly. Furthermore, this approach does not capture the rationale behind the block-by-block process- ing methodology in k-NN-Join processing as stated above. This calls for efficient cost estimation techniques that can represent the cost of the state-of-the-art techniques in k-NN-Join processing. </span></p><p class="c118"><span class="c38">459 </span></p><p class="c2"><span class="c117">Technique </span><span class="c117">Technique </span></p><p class="c2"><span class="c117">Technique </span></p><p class="c2"><span class="c30">st = 1 </span></p><p class="c2"><span class="c30">st = 6 </span></p><p class="c2"><span class="c30">t = 11 </span><span class="c19">q</span><span class="c138">q </span></p><p class="c2"><span class="c19">q </span></p><p class="c2"><span class="c1">(a) Cost = 1 </span></p><p class="c2"><span class="c26">(b) Cost = 6 </span></p><p class="c2"><span class="c1">(c) Cost = 10 </span></p><p class="c12"><span class="c1">Figure 2: Variability of the cost (number of blocks to be scanned) of a query point given its position with respect to the center of the block. Assume that the dashed circle includes exactly k points. The cost tends to increase as the query point gets farther from the center of the block. The maximum cost is at the corners of the block if we assume uniform distribution of the points within the block. </span></p><p class="c2"><span class="c22">3. K-NN-SELECT COST ESTIMATION </span></p><p class="c2"><span class="c22">3.1 The Staircase Technique </span></p><p class="c12"><span class="c1">In this section, we present the Staircase technique; a new tech- nique for estimating the cost (i.e., number of blocks to be scanned) of a k-NN-Select </span><span class="c8">&sigma;</span><span class="c15">k,q</span><span class="c26">(R). The main idea of the Staircase tech- </span><span class="c1">nique is to maintain a set of catalog information that enables quick estimation of the cost via lookups. Conceptually, the catalog should reflect the cost of a k-NN-Select for every possible query location and for every possible value of k. Given a query point, say q, and the value of k, we can search the catalog and determine the cost. However, maintaining a catalog that covers the domains of these two parameters (k and the location of q) is prohibitively expensive in terms of computation cost and storage requirements. The num- ber of possible locations of q is infinite and the value of k can range from 1 to the size of the underlying table. </span></p><p class="c12"><span class="c1">One key insight to improve the above approach is to exploit the spatial locality of the k-NN operation to reduce the size of the cat- alog. We observe that the k-nearest-neighbors of a query point, say q</span><span class="c35">1</span><span class="c1">, are likely to be among the k-nearest-neighbors of another query point, say q</span><span class="c5">2</span><span class="c26">, if q</span><span class="c5">1 </span><span class="c26">and q</span><span class="c5">2 </span><span class="c26">are within the spatial proximity of each </span><span class="c1">other. In addition, any spatial index structure aims at grouping the points that are within spatial proximity in the same block. This means that the k-nearest-neighbors of the points of the same block have high overlap, and hence the query points that fall within the same block are likely to have similar costs. Given a query point, say q, we can estimate the cost corresponding to q by the cost cor- responding to the center of the block in which q is located. </span></p><p class="c12"><span class="c1">Although the above approach yields good estimation accuracy, it is slightly inaccurate because the cost corresponding to a query point, say q, may vary according to the location of q with respect to the center of the block, say b, in which q is located. For a fixed value of k, the cost corresponding to q is minimum if q is near the center of b and tends to increase as q gets far from the center until it reaches its maximum value in the corners of b. Refer to the example in Figure 2 for illustration of this observation. This observation is particularly true if we assume that within a leaf index block, the points are uniformly distributed. Such assumption is practically reasonable. A typical spatial index tends to split the data points (which can be non-uniformly distributed) until the points are almost balanced across the leaf blocks, and hence points that are within the same block tend to have a uniform distribution within that block. </span></p><p class="c12"><span class="c1">Applying the above observation, we estimate the cost corre- sponding to a query point, say q, by combining two values: 1) the cost corresponding to the center of the block, C</span><span class="c15">center</span><span class="c26">, (i.e., the </span><span class="c1">minimum cost), and 2) the cost corresponding to one of the corners, </span></p><p class="c2"><span class="c21">q L </span></p><p class="c2"><span class="c1">Figure 3: Cost estimation with respect to the center of a block. </span></p><p class="c2"><span class="c1">C</span><span class="c31">corner</span><span class="c1">, (i.e., the maximum cost). More precisely, the estimated cost can be computed as: </span></p><p class="c2"><span class="c1">Cost = C</span><span class="c31">center </span><span class="c1">+ </span><span class="c8">&#8710; </span><span class="c1">&middot; </span><span class="c69">2L </span></p><p class="c12"><span class="c1">Diagonal</span><span class="c69">, (1) </span><span class="c1">where Diagonal is the length of the diagonal of the block, L is the distance between q and the center of the block, and </span></p><p class="c2"><span class="c8">&#8710; </span><span class="c1">= C</span><span class="c31">corner </span><span class="c1">&minus; C</span><span class="c31">center</span><span class="c1">. (2) </span></p><p class="c2"><span class="c1">Refer to Figure 3 for illustration. </span></p><p class="c12"><span class="c1">Thus, we do not need to precompute the k-NN cost for every possible query location. Instead, we precompute the cost only for the center and the corners of every block. Although this can reduce the size of the catalog, we still need to precompute the cost for ev- ery possible value of k, i.e., from 1 to the size of the table. This can still be prohibitively expensive because it needs to be performed for every block in the index. </span></p><p class="c12"><span class="c1">We observe that the cost corresponding to any query point tends to be constant for different ranges of values of k. The reason is that the number of points in a block is relatively large, and hence the cost (number of blocks to be scanned) tends to be stable for a range of values of k. To illustrate this idea, consider the example in Figure 1. Assume that Blocks A and B have 1000 points each. Assume further that k</span><span class="c35">1 </span><span class="c1">= 500 tuples in Block A have a distance that is less than the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">betweeb Block B and the query point q. Applying the distance browsing algorithm [14] as explained in Section 2, points in Block A will be inserted in the tuples-queue. The k</span><span class="c5">1 </span><span class="c26">points in Block A will be retrieved from the tuples-queue </span><span class="c1">before Block B is scanned. Thus, the cost (number of scanned blocks) will equal to 1 for k &isin; [1, k</span><span class="c5">1</span><span class="c26">]. For k&gt;k</span><span class="c5">1</span><span class="c26">, Block B will </span><span class="c1">have to be scanned, and thus the cost will equal to 2. However, the cost will remain equal to 2 for k &isin; [k</span><span class="c5">1 </span><span class="c26">+ 1, k</span><span class="c5">2</span><span class="c26">], where k</span><span class="c5">2 </span><span class="c1">equals the number of points in the tuples-queue that have distance less than the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">between Block C and the query point q. </span></p><p class="c12"><span class="c1">To better illustrate the above observation, we use the Open- StreetMap dataset and build a quadtree index on top (as detailed in Section 4), and then measure the cost corresponding to a ran- dom query point. Figure 4 illustrates that the cost is constant for </span></p><p class="c2"><span class="c38">460 </span></p><p class="c2"><span class="c71">[k</span><span class="c103">start</span><span class="c71">, </span><span class="c33">4 </span></p><p class="c2"><span class="c71">k</span><span class="c103">end</span><span class="c71">] Cost </span></p><p class="c74"><span class="c23">15 </span><span class="c33">3</span><span class="c80">[1, 520] </span><span class="c88">3 </span><span class="c80">[521, 675] </span><span class="c88">7 </span><span class="c29">10 </span><span class="c33">2</span><span class="c80">[676, 3496] </span><span class="c88">8 </span><span class="c29">5 </span><span class="c33">1</span><span class="c80">[3497, 6799] </span><span class="c88">12 </span></p><p class="c2"><span class="c29">0 </span></p><p class="c2"><span class="c149">0</span><span class="c23">1 </span><span class="c80">[4700, 5837] </span><span class="c88">13 </span></p><p class="c2"><span class="c23">2000 4000 6000 8000 10000 </span></p><p class="c2"><span class="c116">k </span></p><p class="c2"><span class="c80">[5838, 10000] </span><span class="c88">14 </span></p><p class="c2"><span class="c1">(a) </span></p><p class="c2"><span class="c132">ing can remental </span></p><p class="c2"><span class="c1">(b) </span></p><p class="c2"><span class="c1">Figure 4: Stability of the cost for different values of k. </span></p><p class="c12"><span class="c1">large intervals of k.</span><span class="c6">1 </span><span class="c1">The shape of the graph resembles a staircase diagram (and hence the name Staircase for the technique). As the figure demonstrates, the cost is constant for relatively large inter- vals of k. For instance, when k &isin; [1, 520], the cost is 3 blocks, and when k &isin; [521, 675], the cost is 7 blocks. Observe that this stability increases as the maximum block capacity increases, i.e., the intervals become larger. </span></p><p class="c12"><span class="c1">We leverage the above stability property to reduce the storage size associated with every block in the index. Instead of blindly computing the cost corresponding to the center (and the corners) of a block for every possible value of k, we determine the values of k at which the cost changes. We store a set of intervals and associate with each interval the corresponding cost. We refer to this information as the catalog. The catalog is a set of tuples of the form ([k</span><span class="c31">start</span><span class="c1">, k</span><span class="c31">end</span><span class="c1">], size). Refer to Figure 4 for illustration. </span></p><p class="c2"><span class="c22">3.2 Building the Catalog </span></p><p class="c12"><span class="c1">The process of building a catalog, be it for the center of a block or for one of the corners, is straightforward. Similarly to the dis- tance browsing algorithm in [14], we maintain two priority queues, a tuples-queue and a blocks-queue. The blocks-queue orders the blocks according to a M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">scan. In contrast, the tuples-queue orders the points according to their distance from the query point, say q. We start with the block in which q is located and insert all the block&rsquo;s points into the tuples-queue. At this point, the cost = 1. We keep removing points from the tuples-queue until the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">in the blocks-queue is less than the top of the tuples-queue. The number of points, say k</span><span class="c5">1</span><span class="c26">, removed so far from the tuples-queue </span><span class="c1">represents the first interval in the catalog, i.e., ([1, k</span><span class="c35">1</span><span class="c1">], 1). Then, we scan the next block in the blocks-queue, insert all its points into the tuples-queue, and increment the cost. We repeat this process until all the blocks are scanned or a sufficiently large value of k is encountered. Pseudocode of the process of building the catalog of a query point is illustrated in Procedure 1. </span></p><p class="c12"><span class="c1">For every block in the index, we precompute five catalogs, one for the center and one for each corner. We merge the four catalogs corresponding to the corners into one catalog that stores for each value of k, the maximum cost amongst the four corners. Thus, we store only two catalogs, one for the center (center-catalog, for short), and one that corresponds to the maximum cost at the corners (corners-catalog, for short). </span></p><p class="c2"><span class="c22">3.3 Cost Estimation </span></p><p class="c12"><span class="c1">Given a query with a k-NN-Select at Location q, the cost can be estimated as follows. First, we identify the block that encloses q and then search in the center-catalog and the corners-catalog for the </span></p><p class="c2"><span class="c31">1</span><span class="c26">A similar behaviour occurs for any query point, but with different values. </span></p><p class="c2"><span class="c1">Procedure 1 Building the k-NN-Select-Cost Catalog. </span></p><p class="c2"><span class="c1">Terms: q: The query point to which we need to build the catalog. MAX_K: The maximum possible/maintained value of k. 1: tupleQ &larr; &empty;; blockQ &larr; M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">scan w.r.t. q 2: cost &larr; 0; currentK &larr; 1; catalog &larr; &empty; 3: while (currentK &lt; MAX_K) do 4: currentBlock &larr; blockQ.next() 5: cost + + 6: tupleQ.insert(currentBlock.allP oints) ordered ac- </span></p><p class="c2"><span class="c1">cording to the distance from q 7: startK &larr; currentK 8: while (tupleQ.top.distance &le; blockQ.top.M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">) do 9: tupleQ.removeT op() 10: currentK + + 11: end while 12: catalog.add([startK, currentK], cost) 13: end while 14: </span><span class="c63">Query </span><span class="c1">return </span><span class="c63">Optimization </span><span class="c1">catalog </span></p><p class="c2"><span class="c63">Flow </span></p><p class="c2"><span class="c82">Catalog </span></p><p class="c2"><span class="c1">Query Output </span></p><p class="c27"><span class="c154">k</span><span class="c1">-NN Select </span></p><p class="c2"><span class="c123">26 </span></p><p class="c2"><span class="c123">/ 36 </span></p><p class="c2"><span class="c1">Figure 5: Cost estimation for a k-NN-Select. </span></p><p class="c12"><span class="c1">intervals to which the value of k belongs. Observe that the above process for building a catalog yields a sorted list of ranges of values of k, and hence binary search can be applied to find the enclosing interval and the corresponding cost in logarithmic time w.r.t. the number of intervals. Then, the cost is estimated using Equations 1 and 2. </span></p><p class="c12"><span class="c1">Because the Staircase technique relies on precomputing the esti- mates, the auxiliary index that contains the statistics, e.g., counts and cost estimates, has to be a space-partitioning index, e.g., quadtree or grid, so that the query point always falls inside a block. Observe that the structure of the auxiliary index can be independent of the index that contains the actual data points, i.e., the data-index. If the data-index is a space partitioning index, then the auxiliary index can have the same exact structure as the data-index. If the data-index is a data-partitioning index, e.g., R-Tree, then the struc- ture of the auxiliary index will be different. In either case, the query point will never be outside a block in the auxiliary index, and hence we will always be able to estimate the cost </span></p><p class="c12"><span class="c1">Because the number of blocks in the index can be large, the stor- age overhead of the catalogs can be significant. We hence limit the maximum value of k that a catalog supports to a practically large constant, e.g., k = 10,000. This would result in compact cata- logs that can be practically maintained for each index block. In the case when a k-NN-Select query has a k value that is greater than 10,000, we can estimate its cost by applying the algorithm </span></p><p class="c2"><span class="c38">461 </span></p><p class="c2"><span class="c56">k &lt; 10,000 </span></p><p class="c2"><span class="c56">k </span><span class="c162">&ge; </span><span class="c56">10,000 </span></p><p class="c2"><span class="c1">Estimator </span></p><p class="c27"><span class="c1">Count Index </span></p><p class="c2"><span class="c1">Planner </span><span class="c18">Cost </span><span class="c85">Estimate </span></p><p class="c27"><span class="c1">Execution Pipeline </span></p><p class="c27"><span class="c1">Data Index </span></p><p class="c2"><span class="c1">Y </span></p><p class="c2"><span class="c1">T </span></p><p class="c2"><span class="c1">L </span></p><p class="c12"><span class="c1">Figure 6: Building the locality of a block. The gray block Q is a block from the outer relation; other blocks are from the inner relation. </span></p><p class="c12"><span class="c1">in [24] using the Count-Index. Figure 5 illustrates the typical flow of a k-NN-Select query. Queries with k &gt; 10,000 (that do not arise frequently in practice) are directed to the Count-Index. All other queries (k &le; 10,000) are served through the catalogs. In Section 5, we show that for a real dataset of 0.1 Billion points, the overhead to store all the catalogs is less than 4 MBs. </span></p><p class="c2"><span class="c22">4. K-NN-JOIN COST ESTIMATION </span></p><p class="c12"><span class="c1">As highlighted in Section 2, the state-of-the-art technique [22] in k-NN-Join processing applies a block-by-block mechanism in which, for each block from the outer relation, the locality blocks are determined from the inner relation. The locality blocks of a block, say b</span><span class="c15">o</span><span class="c26">, from the outer relation represent the minimal set of </span><span class="c1">blocks in which the k-nearest-neighbors of any point &isin; bo exist. Thus, each point from the outer relation searches only the locality of its enclosing block to find its k-nearest-neighbors. </span></p><p class="c12"><span class="c1">Before estimating the cost of k-NN-Join, we briefly explain how the locality of a block is computed. Given a block from the outer re- lation, say b</span><span class="c31">o</span><span class="c1">, the corresponding locality blocks in the inner relation are determined as follows. Blocks of the inner relation are scanned in M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">order from b</span><span class="c15">o</span><span class="c26">. The sum of the count of points in the </span><span class="c1">encountered blocks is maintained. Once that sum reaches k, the highest M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST</span><span class="c1">, say M, of an encountered block is marked and scanning of the blocks continues until a block of M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">greater than M is encountered. The encountered blocks represent the lo- cality of b</span><span class="c15">o</span><span class="c26">. For example, consider the process of finding the lo- </span><span class="c1">cality of Block Q in Figure 6 where k = 10. Blocks are scanned in M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">order from Block Q. This means that scanning starts with Block Z. Assume that Block Z contains 700 points. Now, the sum of the count of points in the encountered blocks (in this case, only Block Z) exceeds k. The M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">between Block Q and Block Z is marked, and scanning the blocks continues (Blocks X, Y , and T, respectively) until Block L is encountered. At this point, scanning is terminated because Block L has M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">from Block Q that is greater than the marked M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">(between Blocks Q and Z). Hence, the number of blocks in the locality of Block Q is 4.</span><span class="c26">A naive way to estimate the cost (i.e., the total number of scanned </span><span class="c1">blocks) of a k-NN-Join is to compute the size of the locality blocks for each block in the outer relation and sum these sizes. However, this can be expensive because the number of blocks in the outer re- lation can be arbitrarily large. In the rest of this section, we present three different techniques that address this problem. </span></p><p class="c2"><span class="c1">Q </span></p><p class="c2"><span class="c1">X </span></p><p class="c2"><span class="c1">Z </span></p><p class="c2"><span class="c10">M</span><span class="c84">AX</span><span class="c126">D</span><span class="c84">IST </span></p><p class="c2"><span class="c10">700 points </span></p><p class="c2"><span class="c147">Catalog </span></p><p class="c2"><span class="c57">[k</span><span class="c66">start</span><span class="c57">, k</span><span class="c66">end</span><span class="c57">] </span><span class="c79">Locality </span></p><p class="c2"><span class="c57">Size </span></p><p class="c2"><span class="c31">[1, 313] </span><span class="c24">25 </span></p><p class="c2"><span class="c31">[314, 5380] </span><span class="c24">41 </span></p><p class="c2"><span class="c31">[5381, 6537] </span><span class="c24">54 </span></p><p class="c2"><span class="c31">[6538, 9368] </span><span class="c24">57 </span></p><p class="c2"><span class="c31">[9369, 10882] </span><span class="c24">63 </span></p><p class="c2"><span class="c1">(b) </span></p><p class="c2"><span class="c1">Figure 7: Stability in the size of the locality for different values of k. </span></p><p class="c2"><span class="c22">4.1 The Block-Sample Technique </span></p><p class="c12"><span class="c1">Instead of computing the locality for each block of the outer rela- tion, we pick a random sample of these blocks, compute the locality size of each block in the sample, and then aggregate the total size and scale it to the total number of blocks in the outer relation. We refer to this technique as the Block-Sample technique. </span></p><p class="c74"><span class="c1">Given a set of n</span><span class="c31">o </span><span class="c1">blocks from the outer relation, we pick a ran- dom sample of size s. If the aggregate locality size of the s blocks is ple agg, blocks then are we chosen estimate to be the spatially overall join distributed cost as across </span><span class="c6">agg</span><span class="c120">&times;</span><span class="c6">n</span><span class="c15">s </span><span class="c145">o </span><span class="c1">. The sam- the space in which the blocks of the outer relation reside. To get such sample of blocks, we do either a depth-first or breadth-first index traversal for the blocks of the outer Although the above relation and skip blocks technique can result in high every accuracy </span><span class="c6">n</span><span class="c15">s </span><span class="c145">o</span><span class="c1">. </span></p><p class="c12"><span class="c1">when the sample size increases, it incurs computational overhead upon receiving a k-NN-Join query. As mentioned earlier in Section 2, a typical query optimizer requires fast estimation of the cost, pos- sibly through quick catalog-lookups. With this goal in mind, we introduce next a catalog-based approach. </span></p><p class="c2"><span class="c22">4.2 The Catalog-Merge Technique </span></p><p class="c12"><span class="c1">The main idea of the Catalog-Merge technique is to precompute the size of the locality of each block in the outer relation and store it in a catalog. Given a k-NN-Join query, we can simply aggregate the precomputed values in the catalogs of the blocks of the outer relation. However, the size of the locality depends on the value of k, so we need to precompute it for every possible value of k, which can be prohibitively expensive. </span></p><p class="c12"><span class="c1">Similarly to the case of k-NN-Select, we observe that the size of the locality of a given block tends to be constant (stable) for relatively large intervals of k. To illustrate, consider the example in Figure 6. Assume that Block Z has 700 points. If k has any value between 1 and 700, exactly the same set of blocks will rep- resent the locality of Block Z, i.e., the size of the locality will be the same. To better illustrate this observation, we use the Open- StreetMap dataset and build a quadtree index on top (as detailed in Section 4), and then measure the locality size of a randomly se- lected block.</span><span class="c6">2 </span><span class="c1">Figure 7 illustrates that the size of the locality is stable for large intervals of k. </span></p><p class="c12"><span class="c1">To build the locality-catalog of a block, we identify the inflection points in the range of values of k at which the locality size changes, e.g., k = 313,5380,...,9368 in Figure 7. This can be performed using binary search within the range of values of k. In particular, we start with k = 1 and compute the locality size, say S. Then, we perform a binary search for the smallest value of k at which the </span></p><p class="c2"><span class="c31">2</span><span class="c26">A similar behaviour occurs for any block, but with different val- ues. </span></p><p class="c2"><span class="c38">462 </span></p><p class="c2"><span class="c158">s kcolBy tilacoLf or ebmu</span><span class="c32">N</span><span class="c39">70 56422814</span><span class="c80">0</span><span class="c0">1 2000 4000 6000 8000 10000 </span><span class="c32">k </span><span class="c1">(a) </span></p><p class="c12"><span class="c1">locality size would be greater than S, i.e., the inflection point, say k</span><span class="c15">i</span><span class="c26">. At this moment, we identify the first range of values as [1, k</span><span class="c15">i </span><span class="c26">&minus; </span><span class="c1">1]. Afterwards, we perform another binary search starting from k = k</span><span class="c31">i </span><span class="c1">to get another range of values of k. This process is repeated until no inflection points are found, i.e., when the maximum value of k is reached. </span></p><p class="c12"><span class="c1">A more efficient approach is to build the locality-catalog incre- mentally through two rounds of M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">scan of the Count-Index. These M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">scan rounds can be achieved using two priority queues in which the blocks of the Count-Index are ordered accord- ing to their M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">from the block we need to build the catalog for. The two M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">scan rounds are interleaved. One scan ex- plores the blocks that should contain at least C points. We refer to this scan as Count-Scan. The other scan explores the blocks that have M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">&le; the highest M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">value of the explored blocks so far from Count-Scan. We refer to this queue as Max-Scan. We maintain a counter, say C. Whenever a block from Count-Scan is retrieved, its M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST</span><span class="c1">, say M, is marked and the value of C is incremented by the number of points in the retrieved block. Then, blocks from Max-Scan are scanned until the M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">is greater than the highest value encountered for M. At this point, a new en- try is created in the catalog by aggregating the number of blocks retrieved from Max-Scan thus far. This process is repeated until C reaches the maximum value of k or all the blocks of the inner relation are consumed by Count-Scan.. Pseudocode for the process is given in Procedure 2. Refer to Figure 6 for illustration, where we compute the catalog of Block Q. We start with C = 1. In Count-Scan, we explore Block Z, update C to be 700, and mark the highest M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">encountered. Then, in Max-Scan, we explore Blocks X, Y , and T because their M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">is less than the largest M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">encountered. At this moment, we create a catalog en- try ([1,700], 4) that represents the start and end values of C with the cost of four blocks (namely, Z, X, Y and T). Afterwards, we continue Count-Scan to explore Block X. Assuming that Block X has 500 points, we update C to be 700 + 500 = 1200 and also mark the M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">between Blocks X and Q. Then, in Max-Scan, we explore Block L because its M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">is less than the highest M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">marked thus far. Now, the cost is incremented by 1 due to Block L. We create a new catalog entry ([701,1200], 5). </span></p><p class="c12"><span class="c1">Observe that the above approach is cheap because it relies only on counting (using the Count-Index) with no scan of the data. As- sume that for a given block from the outer relation, the number of blocks in its locality is L. The above approach visits each of the L blocks at most twice, i.e., the running time is O(L) per block. </span></p><p class="c12"><span class="c1">Note that, by definition, the locality conservatively includes all the blocks needed for the k-NN search (see [22] for details), i.e., the locality contains the k-NN for every point in the outer block, say Q. Although it is true that for some k</span><span class="c5">1 </span><span class="c26">&gt; k all the nearest-neighbors of </span><span class="c1">some points in Q may exist in already scanned blocks (by Count- Scan), there will be some points, e.g., near the corners of Q, that might have some of their k-NN in unscanned blocks. Hence, in our approach, we jump into new ranges of k (and new corresponding cost) whenever a block is retrieved through Count-Scan. </span></p><p class="c12"><span class="c1">Observe that if a block retrieved from Count-Scan has M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">that is less than or equal to the highest M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">encountered thus far, it will not lead to any scan in Max-Scan, and hence will lead to a repeated cost in the next entry of the catalog. For instance, in Figure 6, if the M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">between Blocks Q and Z is greater than the M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">between Block Q and Block X (the next in Count- Scan), then the next new entry in the catalog will be ([701,...], 4), i.e., will have the same cost. To get rid of these redundant entries in the catalog, we continue Count-Scan until the value of the highest encountered M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">changes. </span></p><p class="c2"><span class="c1">Procedure 2 Building the locality-catalog of a block. </span></p><p class="c2"><span class="c1">Terms: Q: The block to which we need to build the catalog. MAX_K: The maximum possible/maintained value of k. 1: // Initializations: 2: CountScan &larr; M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">Scan from Q 3: cBlock &larr; CountScan.next() 4: MaxScan &larr; M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">Scan from Q 5: mBlock &larr; MaxScan.next() 6: C &larr; 1; aggCost &larr; 0; 7: Catalog &larr; &empty;; highestMaxDist &larr; 0 8: while (C &lt; MAX_K) do 9: startK &larr; C 10: while (cBlock.M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">&le; highestMaxDist) do 11: C+ = cBlock.count 12: cBlock &larr; CountScan.next() 13: end while 14: highestMaxDist &larr; cBlock.M</span><span class="c3">AX</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">from Q 15: endK &larr; C 16: while (mBlock.M</span><span class="c3">IN</span><span class="c1">D</span><span class="c3">IST </span><span class="c1">&le; highestMaxDist) do 17: aggcost + + 18: mBlock &larr; MaxScan.next() 19: end while 20: Catalog.add([startK, endK], aggCost) </span><span class="c42">Exact </span><span class="c1">21: 22: end return </span><span class="c42">Estimation </span><span class="c1">while Catalog </span><span class="c77">Block 1 </span></p><p class="c2"><span class="c77">Block 2 </span></p><p class="c2"><span class="c77">Block 3 </span></p><p class="c2"><span class="c77">Block b </span></p><p class="c27"><span class="c127">Temporary catalogs of blocks from the outer relation </span></p><p class="c2"><span class="c21">2 </span></p><p class="c2"><span class="c54">[k</span><span class="c50">start</span><span class="c54">, k</span><span class="c50">end</span><span class="c54">] Cost </span><span class="c90">k</span><span class="c75">1</span><span class="c92">[1, </span><span class="c90">k</span><span class="c36">1</span><span class="c92">] </span><span class="c87">17 </span><span class="c21">513 </span></p><p class="c2"><span class="c92">[</span><span class="c90">k</span><span class="c36">1</span><span class="c92">, </span><span class="c90">k</span><span class="c36">2</span><span class="c92">] </span><span class="c87">25 </span><span class="c90">k</span><span class="c75">3 </span></p><p class="c2"><span class="c92">[</span><span class="c90">k</span><span class="c36">2</span><span class="c92">, </span><span class="c90">k</span><span class="c36">3</span><span class="c92">] </span><span class="c87">29 </span><span class="c78">6</span><span class="c21">9 </span></p><p class="c74"><span class="c92">[</span><span class="c90">k</span><span class="c36">3</span><span class="c92">, ...] </span><span class="c87">32 </span><span class="c119">k</span><span class="c107">2</span><span class="c78">4 8</span><span class="c92">... ... [..., MAX_K] ... </span></p><p class="c27"><span class="c67">k</span><span class="c127">-NN-Join Merge-Catalog </span></p><p class="c2"><span class="c1">Figure 8: Flow of the Catalog-Merge process. </span></p><p class="c2"><span class="c30">4.2.1 Preprocessing </span></p><p class="c12"><span class="c1">For each block in the outer relation of the k-NN-Join, we com- pute a temporary catalog that is similar to the one in Figure 7(b). If the number of blocks in the outer relation is n</span><span class="c31">o</span><span class="c1">, then this process requires O(n</span><span class="c15">o </span><span class="c26">&middot; L), where L is the average size of the locality of </span><span class="c1">a block. This can be costly if n</span><span class="c15">o </span><span class="c26">is large. To solve this problem, </span><span class="c1">we take a spatially distributed random sample of the blocks of the outer relation. We compute a temporary catalog only for the sam- ple blocks and not for each block in the outer relation. Afterwards, we merge all the temporary catalogs, and produce a single catalog that contains the aggregate cost of all the temporary catalogs. Each entry in the final catalog has the form ([k</span><span class="c15">start</span><span class="c26">, k</span><span class="c15">end</span><span class="c26">], size), where </span><span class="c1">size is the estimated join cost when k</span><span class="c15">start </span><span class="c26">&le; k &le; k</span><span class="c15">end</span><span class="c26">. </span></p><p class="c12"><span class="c1">Because each temporary catalog is sorted with respect to the ranges of values of k, we apply a plane sweep over the ranges of values of k and aggregate the cost. To illustrate, consider the example in Figure 8. k</span><span class="c5">1 </span><span class="c26">is the smallest value of k in the catalog </span><span class="c1">entries. This means that the aggregate cost for the interval [1, k</span><span class="c5">1</span><span class="c26">] </span><span class="c1">is 2+5+6+4 = 17. k</span><span class="c35">2 </span><span class="c1">is the next smallest value of k, and hence another interval [k</span><span class="c5">1</span><span class="c26">, k</span><span class="c5">2</span><span class="c26">] with aggregate cost = 17 &minus; 5 + 13 = 25 </span><span class="c1">is created in the output catalog. Similarly, for interval [k</span><span class="c35">2</span><span class="c1">,k</span><span class="c35">3</span><span class="c1">], the aggregate cost = 25 &minus; 4+8=29 and for interval [k</span><span class="c5">3</span><span class="c26">, &middot;&middot;&middot; ], the </span><span class="c1">aggregate cost = 29&minus;6+9 = 32. A min-heap is used to efficiently </span></p><p class="c2"><span class="c38">463 </span></p><p class="c2"><span class="c137">Merger </span></p><p class="c131"><span class="c1">determine the next smallest value across all the temporary catalogs in the plane sweep process. </span></p><p class="c111"><span class="c1">To reduce the size of the catalog, we limit the maintained values of k to some practically large constant, e.g., 10,000. In Section 5, we show that for a real dataset of 0.1 Billion points, the size of the catalog is about 1 MB. </span></p><p class="c48"><span class="c30">4.2.2 Cost Estimation </span></p><p class="c7"><span class="c1">Observe that the resulting k-NN-Join catalog is sorted w.r.t. the values of k. Given a k-NN-Join query, we can lookup the estimate cost using a binary search to find the catalog entry corresponding to the given k value. </span></p><p class="c111"><span class="c1">Although the process of building the catalog is performed once, it can be costly if the number of tables in the database schema is large, say n. The k-NN-Join catalog information is required for every possible pair of tables in the database schema, and hence 2 &times; </span><span class="c41">(</span><span class="c31">n</span><span class="c5">2</span><span class="c8">) </span><span class="c26">catalogs need to be built (because the k-NN-Join is asym- </span><span class="c1">metric). Although sampling can speed up the merging process of the temporary catalogs, it is still expensive to compute 2 &times; </span><span class="c41">(</span><span class="c31">n</span><span class="c5">2</span><span class="c8">)</span><span class="c26">, </span><span class="c1">i.e., a quadratic number of catalogs across the database tables. To address this issue, we introduce our third cost estimation technique that requires only a linear number of catalogs. </span></p><p class="c89"><span class="c22">4.3 The Virtual-Grid Technique </span></p><p class="c150"><span class="c1">Similarly to the Catalog-Merge technique, in the Virtual-Grid technique, we maintain a set of catalog information that is built only once before executing any queries. The key idea is to estimate the cost corresponding to a dataset, say D, when D is the inner rela- tion of a k-NN-Join. Given the n relations in the database schema, where each can potentially be an outer relation in a k-NN-Join with D, instead of computing n catalogs corresponding to D, we com- pute only one catalog that corresponds to the join cost between a virtual index and D. </span></p><p class="c144"><span class="c30">4.3.1 Preprocessing </span></p><p class="c7"><span class="c1">Refer to Figure 9 for illustration. Given the index of a dataset (e.g., the red quadtree decomposition in the figure), we assume the existence of a virtual grid that covers the whole space.</span><span class="c6">3 </span><span class="c1">For each block (grid cell) in the virtual-grid, we compute a catalog that is similar to the one in Figure 7(b) with the difference that the locality is computed with respect to the given index. We associate all these virtual-grid catalogs with the given index (e.g., the quadtree). We repeat this process for each relation in the database schema, i.e., associate with every index a virtual-grid-cost. Observe that unlike the Catalog-Merge approach, this requires linear storage (and pre- processing time) overhead. </span></p><p class="c125"><span class="c30">4.3.2 Cost Estimation </span></p><p class="c7"><span class="c1">Given a k-NN-Join query, we retrieve the virtual-grid corre- sponding to the inner relation. Then, we estimate the cost by scal- ing the cost corresponding to the part of the virtual-grid that over- laps with the outer relation. In particular, for each grid cell, say C, in the virtual-grid, we retrieve the locality size, say L, stored in C&rsquo;s catalog. Then, we select the blocks in the outer relation that overlap with C. This can be performed using a range query on the outer relation. For each of the overlapping blocks, say O, in the outer relation, we multiply L by the ratio between the diagonal length of Block O and the diagonal length of Block C. We sum these products across all the cells of the virtual-grid. The overall sum represents the join cost estimate. </span></p><p class="c140"><span class="c31">3</span><span class="c26">This can be achieved for real datasets where the bounds of the earth are fixed. </span></p><p class="c2 c152"><span class="c1">Figure 10: A sample of OpenStreetMap GPS data and the cor- responding region-quadtree decomposition overlaid on top. </span></p><p class="c124 c136"><span class="c1">Assuming that the number of blocks in the outer relation is n</span><span class="c15">o</span><span class="c26">, </span><span class="c1">the estimation process is O(n</span><span class="c31">o</span><span class="c1">). The reason is that eventually, all the blocks of the outer relation get selected (through the range query performed at each grid cell). In other words, regardless of the grid size, all the blocks will be selected and the corresponding products have to be aggregated. In Section 5, we study the estima- tion time for different grid sizes while fixing the size of the outer relation and demonstrate that the estimation time is almost constant for different grid sizes. </span></p><p class="c58"><span class="c22">5. EXPERIMENTS </span></p><p class="c34"><span class="c1">In this section, we evaluate the performance of the proposed esti- mation techniques. We realize a testbed in which we implement the state-of-the-art techniques for k-NN-Select estimation [24] as well as our proposed estimation techniques. To have a ground truth for the actual cost of the k-NN operators, we implement the Distance Browsing algorithm for k-NN-Select as well as the locality based k-NN-Join. Our implementation is based on a region-quadtree in- dex [21], where each node in the quadtree represents a region of space that is recursively decomposed into four equal quadrants, or subquadrants, with each leaf node containing points that cor- respond to a specific subregion. The maximum block capacity in the quadtrees used in our experiments is set to 10,000 points. All implementations are in Java. Experiments are conducted on a ma- chine running Mac OS X on Intel Core i7 CPU at 2.3 GHz and 8 GB of main memory. </span></p><p class="c93"><span class="c1">We use a real spatial dataset from OpenStreetMap [1]. The num- ber of data points in the dataset is 0.1 Billion points. Figure 10 displays a sample of the data that we plot through a visualizer that </span></p><p class="c4"><span class="c38">464 </span></p><p class="c2 c9"><span class="c117">Estimation by Virtual Grid Join </span></p><p class="c72"><span class="c121">Virtual Grid </span></p><p class="c95"><span class="c25">Inner Index </span></p><p class="c53"><span class="c1">Figure 9: The Virtual-Grid technique for k-NN-Join cost esti- mation. </span></p><p class="c2"><span class="c102">35% </span></p><p class="c2"><span class="c141">o itaRr orr</span><span class="c40">E</span><span class="c102">26% 18% </span></p><p class="c2"><span class="c28">9% </span></p><p class="c27"><span class="c102">Staircase (Center+Corners) Staircase (Center Only) Density-Based 0%1 2 3 4 5 6 7 8 9 10 </span></p><p class="c2"><span class="c40">Scale Factor </span></p><p class="c2"><span class="c1">Figure 11: k-NN-Select estimation accuracy. </span></p><p class="c2"><span class="c1">we have built as part of our testbed. The figure also displays a region-quadtree decomposition that is built on top of the data. </span></p><p class="c12"><span class="c1">To test the performance of our techniques at different data scales, we insert portions of the dataset into the index at multiple ratios. For instance, for scale = 1, we insert 10 Million points, for scale = 2, we insert 20 Million points, and so on until scale = 10 in which all the 0.1 Billion points are inserted. Our performance metrics are the estimation accuracy (i.e., error ratio), the estimation time, the preprocessing time, and the storage overhead. We limit the maxi- mum maintained value of k in all the catalogs to 10,000. </span></p><p class="c2"><span class="c22">5.1 KNN-Select Cost Estimation </span></p><p class="c12"><span class="c1">In this section, we present the performance of the Staircase tech- nique in estimating the cost of a k-NN-Select and compare it with the density-based technique of [24]. We evaluate two variants of the Staircase technique, 1) Center-Only, where the cost corresponding to a query point, say q, is estimated as the cost corresponding to the center in which q is located, and 2) Center+Corners, where the cost is estimated using Equations 1 and 2. </span></p><p class="c2"><span class="c30">5.1.1 Estimation Accuracy </span></p><p class="c12"><span class="c1">In this experiment, we measure the average error ratio in estimat- ing the cost of 100,000 queries that are chosen at random. For each query, we compute the actual cost, compare it with the estimated cost, and measure the error ratio. We compute the average error ratio of all the queries. </span></p><p class="c12"><span class="c1">Figure 11 illustrates that the Staircase technique achieves a smaller error ratio than that of the density-based technique. The error ratio reaches less than 20% when the cost is estimated using the Center+Corners variant. </span></p><p class="c2"><span class="c30">5.1.2 Estimation Time </span></p><p class="c12"><span class="c1">In this experiment, we measure the time each estimation tech- nique requires to estimate the cost of a query. Figure 12 illus- trates that the Staircase technique is almost two orders of mag- nitudes faster than the density-based technique. Observe that the Center+Corners variant of the Staircase technique is slightly slower than the Center-Only variant because the former requires two cat- alog lookups, one from the center-catalog and the other from the corners-catalog. Also, observe that the estimation time of the density-based technique increases as the value of k increases. The reason is that the density-based technique keeps scanning the in- dex blocks until the encountered blocks are estimated to contain k points. In contrast, the estimation time of the Staircase technique is constant regardless of the value of k because the Staircase tech- nique relies on just a single catalog lookup (two lookups in case of the Center+Corners variant). </span></p><p class="c2"><span class="c23">Staircase (Center+Corners) </span></p><p class="c2"><span class="c109">) ces(e miTn oitamits</span><span class="c40">E</span><span class="c23">1E-03 1E-04 </span></p><p class="c2"><span class="c23">Staircase (Center Only) Density-Based </span></p><p class="c2"><span class="c23">1E-05 </span></p><p class="c2"><span class="c23">1E-06 </span></p><p class="c2"><span class="c23">1E-07 </span></p><p class="c2"><span class="c23">1 4 16 64 256 1024 4096 </span></p><p class="c2"><span class="c40">k </span></p><p class="c2"><span class="c1">Figure 12: k-NN-Select estimation time. </span></p><p class="c2"><span class="c96">) ces(e miTg nissecorperP</span><span class="c64">200 </span><span class="c130">50 </span><span class="c64">0Staircase (Center+Corners) </span></p><p class="c2"><span class="c64">Staircase (Center Only) Density-Based </span></p><p class="c2"><span class="c64">1 2 3 4 5 6 7 8 9 10 </span><span class="c100">Scale Factor </span></p><p class="c2"><span class="c1">Figure 13: Preprocessing time of the k-NN-Select estimation techniques. </span></p><p class="c2"><span class="c30">5.1.3 Storage Overhead and Preprocessing Time </span></p><p class="c12"><span class="c1">In this experiment, we measure the storage requirement and pre- processing time of each estimation technique. Observe that the density-based technique has no preprocessing time requirements because it precomputes no catalogs. </span></p><p class="c12"><span class="c1">Figure 13 illustrates that the Staircase technique incurs relatively high preprocessing overhead to precompute the catalogs of all the index blocks. Observe that as the scale factor increases, the pre- processing time increases because more blocks will need to be processed. Also, observe that the Center-Only variant incurs less preprocessing overhead than the Center+Corners variant because the former computes only one catalog per block while the latter computes five catalogs and merges four of them. Notice that this preprocessing phase is an offline process that does not affect the performance of the online cost estimation process. </span></p><p class="c12"><span class="c1">Figure 14 illustrates that density-based technique consumes little storage overhead, basically, due to the density values maintained at each block in the index. In contrast, the Staircase technique has higher storage overhead due to the maintained catalogs. Ob- serve that as the scale factor increases, the storage overhead in- creases because more blocks will be present in the index and each of them will have a separate catalog. However, the storage require- ments of the Staircase technique are less than 4 MBs even for the largest scale factor. Also, observe that the Center-Only variant of the Staircase technique incurs less storage overhead than the Cen- ter+Corners variant because the former maintains only one catalog per block while the latter maintains two catalogs. </span></p><p class="c2"><span class="c22">5.2 K-NN-Join Cost Estimation </span></p><p class="c12"><span class="c1">In this section, we study the performance of the proposed tech- niques for estimating the k-NN-Join cost, namely the Block- Sample, Catalog-Merge, and Virtual-Grid techniques. </span></p><p class="c2"><span class="c38">465 </span></p><p class="c2"><span class="c64">150 </span></p><p class="c2"><span class="c64">100 </span></p><p class="c74"><span class="c68">S</span><span class="c13">) BM(d aehrevOe garot</span><span class="c110">4 321</span><span class="c0">0Staircase (Center+Corners) </span></p><p class="c2"><span class="c0">Staircase (Center Only) Density-Based </span></p><p class="c2"><span class="c0">1 2 3 4 5 6 7 8 9 10 </span></p><p class="c2"><span class="c68">Scale Factor </span></p><p class="c2"><span class="c1">Figure 14: Storage requirements of the k-NN-Select estimation techniques. </span></p><p class="c2"><span class="c30">5.2.1 Estimation Accuracy </span></p><p class="c12"><span class="c1">In this experiment, we estimate the cost of a k-NN-Join between two indexes of 0.1 Billion points each for a random value of k, com- pare it with the actual cost, and then calculate the error ratio. We repeat this process for various sampling sizes for both the Block- Sample and Catalog-Merge techniques, and for various grid sizes for the Virtual-Grid technique. Figure 15 illustrates that the Block- Sample and Catalog-Merge techniques can reach an error ratio that is less than 5% for a sample size &gt; 400. Figure 16 illustrates that the Virtual-Grid technique achieves less than 20% error ratio. </span></p><p class="c2"><span class="c17">80% </span></p><p class="c2"><span class="c55">o itaRr orr</span><span class="c44">E</span><span class="c17">60% 40% </span></p><p class="c2"><span class="c108">Catalog-Merge </span><span class="c17">Block-Sample </span></p><p class="c2"><span class="c17">20% </span></p><p class="c2"><span class="c17">0% </span></p><p class="c2"><span class="c17">50 100 150 200 250 300 350 400 450 500 </span><span class="c44">Sample Size</span><span class="c1">Figure 15: k-NN-Join estimation accuracy. </span></p><p class="c2"><span class="c23">50%</span><span class="c109">o itaRr orr</span><span class="c21">E</span><span class="c23">10% </span></p><p class="c2"><span class="c23">Virtual Grid 40% </span></p><p class="c2"><span class="c23">30% </span></p><p class="c2"><span class="c23">20% </span></p><p class="c2"><span class="c29">4x4 8x8 12x12 16x16 20x20 </span></p><p class="c2"><span class="c21">Grid Size </span></p><p class="c2"><span class="c1">Figure 16: k-NN-Join estimation accuracy. </span></p><p class="c2"><span class="c30">5.2.2 Estimation Time </span></p><p class="c12"><span class="c1">In this experiment, we measure the time required to estimate the cost of a k-NN-Join between two indexes of 0.1 Billion points each. Figure 17 gives the performance for different values of k. The number of samples used in the Catalog-Merge and Block-Sample techniques is fixed to 1000. The grid size used in the Virtual-Grid technique is 10 &times; 10. As the figure demonstrates, the Catalog- </span></p><p class="c12"><span class="c1">Merge technique is more than four orders of magnitude faster than the Block-Sample and Virtual-Grid techniques. The reason for this variance in performance is that the Catalog-Merge technique main- tains one catalog for every pair of relations (indexes) in which the estimate cost is maintained; the cost is directly retrieved from the catalog via one lookup. In contrast, the Block-Sample technique computes the locality for a sample of blocks, which is costly. Also, the Virtual-Grid technique aggregates the cost across each of the grid cells after computing the overlap with the outer relation, which is costly as well. </span></p><p class="c2"><span class="c20">) ces(e miTn oitamits</span><span class="c94">E</span><span class="c31">1E+00 1E-02 </span></p><p class="c2"><span class="c31">1E-06 </span></p><p class="c2"><span class="c31">1 4 16 64 256 1024 4096 </span></p><p class="c2"><span class="c94">k </span></p><p class="c2"><span class="c31">Virtual Grid 1E-04 </span></p><p class="c2"><span class="c31">Sampling Catalog-Merge </span></p><p class="c2"><span class="c1">Figure 17: k-NN-Join estimation time. </span></p><p class="c12"><span class="c1">Figure 18 gives the performance of the Block-Sample and Catalog-Merge techniques for different sample sizes. Observe that the estimation time of the Block-Sample technique increases as the sample size increases.</span><span class="c6">4 </span><span class="c1">In contrast, the estimation time of the Catalog-Merge technique is constant irrespective of the sample size because estimation is performed through one lookup through a pre- computed catalog, i.e., the sample size only affects the preprocess- ing time as we show next. </span></p><p class="c2"><span class="c47">) ces(e miTn oitamits</span><span class="c32">E</span><span class="c17">1E+001E-02 1E-06 </span></p><p class="c2"><span class="c128">100 300 500 700 900 </span></p><p class="c2"><span class="c32">Sample Size </span></p><p class="c2"><span class="c17">Block-Sample </span></p><p class="c2"><span class="c17">1E-04 </span></p><p class="c2"><span class="c17">Catalog-Merge </span></p><p class="c2"><span class="c1">Figure 18: k-NN-Join estimation time. </span></p><p class="c2"><span class="c47">) ces(e miTn oitamits</span><span class="c32">E</span><span class="c62">1E-028E-03 5E-03 </span></p><p class="c2"><span class="c62">1E-06 </span></p><p class="c2"><span class="c151">4x4 8x8 12x12 16x16 20x20 </span></p><p class="c2"><span class="c32">Grid Size </span></p><p class="c2"><span class="c62">Virtual Grid </span></p><p class="c2"><span class="c1">Figure 19: k-NN-Join estimation time. </span></p><p class="c2"><span class="c31">4</span><span class="c26">The slope of the curve is low due to the use of a log-scale. </span></p><p class="c2"><span class="c38">466 </span></p><p class="c2"><span class="c62">3E-03 </span></p><p class="c12"><span class="c1">Figure 19 gives the performance of the Virtual-Grid technique for different grid sizes. As the figure demonstrates, the estimation time is almost constant regardless of the grid size. As highlighted in Section 4, the reason is that the time required for estimation de- pends on the number of blocks in the outer relation, not on the number of cells in the grid. For each grid cell, the overlapping blocks from the outer relation have to be retrieved regardless of the size of the grid. </span></p><p class="c2"><span class="c30">5.2.3 Storage Overhead and Preprocessing Time </span></p><p class="c12"><span class="c1">In this experiment, we measure the storage and preprocessing time requirements for maintaining a set of catalogs for the estima- tion of k-NN-Join queries between 10 indexes that we create. We test the performance at different scale factors, i.e., create 10 differ- ent indexes for each scale factor. For instance, if the scale factor is 5, this means that we create 10 indexes and insert 50 Million points into each of them. </span></p><p class="c12"><span class="c1">In Figure 20, we fix the grid size in the Virtual-Grid technique to 10 &times; 10 and the sample size for the Catalog-Merge technique to 1000. As the figure demonstrates, the Virtual-Grid technique re- quires almost an order of magnitude less storage than the Catalog- Merge techniques. The maintains a catalog for reason is that every pair of the indexes, Catalog-Merge i.e, 2 &times; </span><span class="c41">(</span><span class="c35">10</span><span class="c1">technique </span></p><p class="c2"><span class="c5">2 </span></p><p class="c12"><span class="c8">) </span><span class="c26">= 90 </span><span class="c1">catalogs. In contrast, the Virtual-Grid technique maintains a cata- log for every index, i.e., only 10 catalogs. Figure 21 demonstrates that the Virtual-Grid technique requires a constant amount of pre- processing time (about two seconds) regardless of the scale factor. The reason is that the preprocessing time depends on the number of grid cells; for each grid cell, a catalog is computed. </span></p><p class="c2"><span class="c55">) BM(d aehrevOe garot</span><span class="c44">S</span><span class="c17">10.0 1.0 </span></p><p class="c2"><span class="c17">0.0 </span></p><p class="c2"><span class="c98">Catalog-Merge </span><span class="c17">Virtual Grid </span></p><p class="c2"><span class="c17">1 2 3 4 5 6 7 8 9 10 </span><span class="c44">Scale Factor</span><span class="c1">Figure 20: Storage requirements of the k-NN-Join estimation techniques.</span><span class="c13">) ces(e miTg nissecorperP</span><span class="c17">0.1 </span><span class="c110">9 </span></p><p class="c2"><span class="c110">75</span><span class="c0">2</span><span class="c163">0</span><span class="c0">1 2 3 4 5 6 7 8 9 10 </span><span class="c68">Scale Factor </span></p><p class="c2"><span class="c0">Virtual Grid Block-Sample Catalog-Merge </span></p><p class="c2"><span class="c1">Figure 21: Preprocessing time of the k-NN-Join estimation techniques. </span></p><p class="c2"><span class="c1">In Figure 23, we fix the scale factor to 10. As Figs. 22(a) and 23(a) demonstrate, the Catalog-Merge technique requires more </span></p><p class="c27"><span class="c64">0</span><span class="c159">) BM(d aehrevOe garotS</span><span class="c122">21</span><span class="c64">100 300 500 700 900 </span><span class="c57">Sample Size </span><span class="c64">Catalog-Merge </span></p><p class="c2"><span class="c1">(a) </span></p><p class="c74"><span class="c109">) BM(d aehrevOe garot</span><span class="c40">S</span><span class="c23">0.20.1 0 </span><span class="c29">4x4 </span><span class="c23">Virtual Grid </span></p><p class="c2"><span class="c29">8x8 12x12 16x16 20x20 </span><span class="c40">Grid Size </span></p><p class="c2"><span class="c1">(b) </span></p><p class="c2"><span class="c1">Figure 22: Storage requirements of the k-NN-Join estimation techniques.</span><span class="c49">) </span></p><p class="c2"><span class="c49">ces(e miTg nissecorperP</span><span class="c156">10864</span><span class="c24">2</span><span class="c45">0</span><span class="c24">100 300 500 700 900 </span><span class="c87">Sample Size </span><span class="c24">Catalog-Merge </span></p><p class="c2"><span class="c1">(a) </span></p><p class="c2"><span class="c109">) ces(e miTg nissecorpe</span><span class="c40">r</span><span class="c99">P</span><span class="c23">0.70.5 0.4 </span></p><p class="c2"><span class="c23">0 </span></p><p class="c2"><span class="c29">4x4 8x8 12x12 16x16 20x20 </span><span class="c40">Grid Size </span></p><p class="c2"><span class="c23">Virtual Grid </span></p><p class="c2"><span class="c1">(b) </span></p><p class="c2"><span class="c1">Figure 23: Preprocessing time of the k-NN-Join estimation techniques. </span></p><p class="c12"><span class="c1">storage and preprocessing time as the sample size increases. The reason is that, as the sample size increases, more temporary cata- logs get created during the process of merging the catalogs, which are likely to result in more entries in the final merged catalog. Sim- ilarly, Figs. 22(b) and 23(b) demonstrate that the Virtual-Grid tech- nique requires more storage and preprocessing time as the grid size increases because it maintains a catalog for every grid cell. </span></p><p class="c2"><span class="c38">467 </span></p><p class="c2"><span class="c23">0.2 </span></p><p class="c2"><span class="c155">Summary </span></p><p class="c27"><span class="c37">Estimation Time </span></p><p class="c2"><span class="c37">Estimation </span></p><p class="c2"><span class="c37">Storage Accuracy </span></p><p class="c2"><span class="c37">Overhead </span></p><p class="c27"><span class="c37">Preprocessing Time </span></p><p class="c27"><span class="c148">k</span><span class="c37">-NN-</span><span class="c59">Select </span><span class="c37">Cost Estimation </span></p><p class="c2"><span class="c59">Density-Based </span><span class="c37">Medium Medium None None </span></p><p class="c2"><span class="c59">Staircase </span><span class="c64">(Center-Only) </span><span class="c37">Low Medium Low Medium </span></p><p class="c2"><span class="c59">Staircase </span><span class="c64">(Center+Corners) </span><span class="c37">Low High Low High </span></p><p class="c27"><span class="c148">k</span><span class="c37">-NN-</span><span class="c59">Join </span><span class="c37">Cost Estimation </span></p><p class="c2"><span class="c59">Block-Sample </span><span class="c37">High High None None </span></p><p class="c2"><span class="c59">Catalog-Merge </span><span class="c37">Low High Medium Medium </span></p><p class="c2"><span class="c59">Virtual-Grid </span><span class="c37">Medium Medium Low Low </span></p><p class="c2"><span class="c1">Figure 24: Summary of the pros and cons of each estimation technique. </span></p><p class="c2"><span class="c22">6. CONCLUDING REMARKS </span></p><p class="c12"><span class="c1">In this paper, we study the problem of estimating the cost of the k-NN-Select and k-NN-Join operators. We present various estima- tion techniques; Figure 24 summarizes the tradeoffs each technique offers. Performance evaluation using real spatial datasets from OpenStreetMap demonstrates that: 1) the Staircase technique for k-NN-Select cost estimation is faster than the state-of-the-art tech- nique [24] by more than two orders of magnitude and has better estimation accuracy; 2) the Catalog-Merge and Virtual-Grid tech- niques for k-NN-Join cost estimation achieve less than 5% and 20% error ratio, respectively, while keeping the estimation time be- low one microsecond and one millisecond, respectively; and 3) the Virtual-Grid technique reduces the storage required to maintain the catalogs by an order of magnitude compared to the Catalog-Merge technique. </span></p><p class="c2"><span class="c22">7. REFERENCES </span></p><p class="c2"><span class="c1">[1] OpenStreetMap bulk gps point data. </span></p><p class="c2"><span class="c1">http://blog.osmfoundation.org/2012/04/ 01/bulk-gps-point-data/. [2] S. Acharya, V. Poosala, and S. Ramaswamy. Selectivity </span></p><p class="c2"><span class="c1">estimation in spatial databases. In SIGMOD Conference, pages 13&ndash;24, 1999. [3] N. An, Z.-Y. Yang, and A. Sivasubramaniam. Selectivity </span></p><p class="c2"><span class="c1">estimation for spatial joins. In ICDE, pages 368&ndash;375, 2001. [4] W. G. Aref and H. Samet. Estimating selectivity factors of </span></p><p class="c2"><span class="c1">spatial operations. In FMLDO, pages 31&ndash;43, 1993. [5] W. G. Aref and H. Samet. A cost model for query </span></p><p class="c2"><span class="c1">optimization using R-Trees. In ACM-GIS, pages 60&ndash;67, 1994. [6] N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger. </span></p><p class="c2"><span class="c1">The R*-tree: An efficient and robust access method for points and rectangles. In SIGMOD Conference, pages 322&ndash;331, 1990. [7] A. Belussi and C. Faloutsos. Estimating the selectivity of </span></p><p class="c2"><span class="c1">spatial queries using the &lsquo;correlation&rsquo; fractal dimension. In VLDB, pages 299&ndash;310, 1995. [8] S. Berchtold, C. B&ouml;hm, D. A. Keim, and H.-P. Kriegel. A </span></p><p class="c2"><span class="c1">cost model for nearest neighbor search in high-dimensional data space. In PODS, pages 78&ndash;86, 1997. [9] C. B&ouml;hm. A cost model for query processing in high </span></p><p class="c2"><span class="c1">dimensional data spaces. ACM Trans. Database Syst., 25(2):129&ndash;178, 2000. </span></p><p class="c2"><span class="c1">[10] C. B&ouml;hm and F. Krebs. The k-nearest neighbour join: Turbo </span></p><p class="c2"><span class="c1">charging the kdd process. Knowl. Inf. Syst., 6(6):728&ndash;749, 2004. [11] S. Chandrasekaran and M. J. Franklin. Streaming queries over streaming data. In VLDB 2002, Proceedings of 28th International Conference on Very Large Data Bases, August 20-23, 2002, Hong Kong, China, pages 203&ndash;214, 2002. [12] M. Y. Eltabakh, R. Eltarras, and W. G. Aref. </span></p><p class="c2"><span class="c1">Space-partitioning trees in postgresql: Realization and performance. In ICDE, page 100, 2006. [13] A. Guttman. R-trees: A dynamic index structure for spatial </span></p><p class="c27"><span class="c1">searching. In SIGMOD Conference, pages 47&ndash;57, 1984. [14] G. R. Hjaltason and H. Samet. Distance browsing in spatial </span></p><p class="c2"><span class="c1">databases. ACM Trans. Database Syst., 24(2):265&ndash;318, 1999. [15] Y.-W. Huang, N. Jing, and E. A. Rundensteiner. A cost </span></p><p class="c2"><span class="c1">model for estimating the performance of spatial joins using R-trees. In SSDBM, pages 30&ndash;38, 1997. [16] H.-P. Kriegel, P. Kunath, and M. Renz. R*-tree. In Encyclopedia of GIS, pages 987&ndash;992. 2008. [17] N. Mamoulis and D. Papadias. Selectivity estimation of </span></p><p class="c74"><span class="c1">complex spatial queries. In SSTD, pages 155&ndash;174, 2001. [18] V. Poosala, Y. E. Ioannidis, P. J. Haas, and E. J. Shekita. Improved histograms for selectivity estimation of range predicates. In SIGMOD Conference, pages 294&ndash;305, 1996. [19] N. Roussopoulos, S. Kelley, and F. Vincent. Nearest neighbor </span></p><p class="c27"><span class="c1">queries. In SIGMOD Conference, pages 71&ndash;79, 1995. [20] H. Samet. Foundations of Multidimensional and Metric Data </span></p><p class="c27"><span class="c1">Structures. Morgan Kaufmann Publishers Inc., 2006. [21] H. Samet and R. E. Webber. Storing a collection of polygons </span></p><p class="c74"><span class="c1">using quadtrees. ACM Trans. Graph., 4(3):182&ndash;222, 1985. [22] J. Sankaranarayanan, H. Samet, and A. Varshney. A fast all nearest neighbor algorithm for applications involving large point-clouds. Computers &amp; Graphics, 31(2):157&ndash;174, 2007. [23] C. Sun, D. Agrawal, and A. El Abbadi. Selectivity estimation </span></p><p class="c2"><span class="c1">for spatial joins with geometric selections. In EDBT, pages 609&ndash;626, 2002. [24] Y. Tao, J. Zhang, D. Papadias, and N. Mamoulis. An efficient </span></p><p class="c2"><span class="c1">cost model for optimization of nearest neighbor search in low and medium dimensional spaces. IEEE Trans. Knowl. Data Eng., 16(10):1169&ndash;1184, 2004. [25] C. Xia, H. Lu, B. C. Ooi, and J. Hu. Gorder: An efficient </span></p><p class="c2"><span class="c1">method for KNN join processing. In VLDB, pages 756&ndash;767, 2004. </span></p><p class="c2"><span class="c38">468 </span></p></body></html>