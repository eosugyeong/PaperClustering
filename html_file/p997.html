<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Times New Roman";font-style:normal}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c78{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Times New Roman";font-style:italic}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:italic}.c46{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Times New Roman";font-style:normal}.c43{color:#100f0d;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.1pt;font-family:"Arial";font-style:normal}.c20{color:#100f0d;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.1pt;font-family:"Arial";font-style:normal}.c79{margin-left:-18.2pt;padding-top:3.8pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-6.8pt}.c51{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:italic}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:normal}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Times New Roman";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:italic}.c67{color:#231f20;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.1pt;font-family:"Arial";font-style:normal}.c69{margin-left:-9.2pt;padding-top:1.4pt;text-indent:27.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-2.9pt}.c27{color:#100f0d;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.7pt;font-family:"Arial";font-style:normal}.c65{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Courier New";font-style:italic}.c76{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c10{color:#100f0d;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.8pt;font-family:"Arial";font-style:normal}.c55{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c90{margin-left:-9.2pt;padding-top:1.7pt;text-indent:27.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:19.9pt}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Times New Roman";font-style:normal}.c5{color:#231f20;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.1pt;font-family:"Arial";font-style:normal}.c35{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:italic}.c66{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:normal}.c44{color:#231f20;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.9pt;font-family:"Arial";font-style:normal}.c59{margin-left:-18.2pt;padding-top:1.4pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c68{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.3pt;font-family:"Courier New";font-style:italic}.c31{color:#231f20;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.2pt;font-family:"Arial";font-style:normal}.c32{color:#000000;font-weight:700;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Times New Roman";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c52{color:#231f20;font-weight:400;text-decoration:none;vertical-align:super;font-size:7.9pt;font-family:"Arial";font-style:normal}.c82{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c33{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:italic}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Times New Roman";font-style:normal}.c16{color:#231f20;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.1pt;font-family:"Arial";font-style:normal}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c25{color:#231f20;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.8pt;font-family:"Arial";font-style:normal}.c45{color:#100f0d;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.2pt;font-family:"Arial";font-style:normal}.c91{margin-left:-9.2pt;padding-top:1.4pt;text-indent:28.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:16.6pt}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:italic}.c56{margin-left:-13.4pt;padding-top:1.7pt;text-indent:27.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:3.3pt}.c36{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c41{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Times New Roman";font-style:normal}.c29{color:#100f0d;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.1pt;font-family:"Arial";font-style:normal}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:italic}.c84{margin-left:-18.2pt;padding-top:1.7pt;text-indent:37pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3pt}.c17{color:#100f0d;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.9pt;font-family:"Arial";font-style:normal}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c21{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:italic}.c94{margin-left:-9.2pt;padding-top:1.4pt;text-indent:27.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c7{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Times New Roman";font-style:normal}.c39{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c57{color:#231f20;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.7pt;font-family:"Arial";font-style:normal}.c34{color:#100f0d;font-weight:400;text-decoration:none;vertical-align:super;font-size:7.9pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:italic}.c37{color:#231f20;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.1pt;font-family:"Arial";font-style:normal}.c19{color:#231f20;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.1pt;font-family:"Arial";font-style:normal}.c58{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.3pt;font-family:"Arial";font-style:normal}.c62{color:#231f20;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.2pt;font-family:"Arial";font-style:normal}.c64{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:18.3pt;font-family:"Times New Roman";font-style:italic}.c47{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c93{margin-left:-13.7pt;padding-top:1.4pt;text-indent:27.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:1.4pt}.c85{margin-left:-18.2pt;padding-top:1.7pt;text-indent:27.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c54{color:#231f20;font-weight:400;text-decoration:none;vertical-align:sub;font-size:7.9pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c74{margin-left:-18.2pt;padding-top:13pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:125.2pt}.c26{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:3.1pt}.c81{margin-left:-13.7pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:4.2pt}.c60{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:9.8pt}.c48{margin-left:-18.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:4.2pt}.c71{margin-left:-9.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:0.7pt}.c38{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:1.7pt}.c72{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-11pt}.c80{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:13.4pt}.c88{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12.7pt}.c77{margin-left:-18.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3.2pt}.c63{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-2.9pt}.c50{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-13.2pt}.c42{margin-left:9.5pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.1pt}.c61{margin-left:0.5pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:20.1pt}.c92{margin-left:-18.2pt;padding-top:13pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:123.5pt}.c30{margin-left:-9.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9.4pt}.c70{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:12.7pt}.c73{margin-left:-9.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-14.2pt}.c53{margin-left:-13.4pt;padding-top:5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:11.7pt}.c87{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-7.7pt}.c75{margin-left:225.1pt;padding-top:63.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-29.1pt}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c49{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c24{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c89{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c86{margin-left:-9.2pt;margin-right:10.3pt}.c83{margin-left:-18.2pt;margin-right:-6.8pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c89"><p class="c49"><span class="c55">ATLAS: A Probabilistic Algorithm for High Dimensional Similarity Search </span></p><p class="c2"><span class="c47">Jiaqi Zhai </span><span class="c13">Cornell University </span><span class="c28">jz392@cornell.edu </span></p><p class="c2"><span class="c13">Ithaca, NY </span><span class="c47">Yin Lou </span><span class="c13">Cornell University </span><span class="c28">yinlou@cs.cornell.edu </span></p><p class="c2"><span class="c13">Ithaca, NY </span><span class="c47">Johannes Gehrke </span><span class="c13">Cornell University </span><span class="c28">johannes@cs.cornell.edu </span><span class="c13">Ithaca, NY </span><span class="c23">ABSTRACT </span><span class="c0">Given a set of high dimensional binary vectors and a simi- larity function (such as Jaccard and Cosine), we study the problem of finding all pairs of vectors whose similarity ex- ceeds a given threshold. The solution to this problem is a key component in many applications with feature-rich objects, such as text, images, music, videos, or social networks. In particular, there are many important emerging applications that require the use of relatively low similarity thresholds. </span></p><p class="c8"><span class="c0">We propose ATLAS, a probabilistic similarity search al- gorithm that in expectation finds a 1 </span><span class="c4">&minus;&delta; </span><span class="c0">fraction of all sim- ilar vector pairs. ATLAS uses truly random permutations both to filter candidate pairs of vectors and to estimate the similarity between vectors. At a 97.5% recall rate, ATLAS consistently outperforms all state-of-the-art approaches and achieves a speed-up of up to two orders of magnitude over both exact and approximate algorithms. </span></p><p class="c2"><span class="c23">Categories and Subject Descriptors </span><span class="c0">H.3.3 [</span><span class="c7">Information Storage and Retrieval</span><span class="c0">]: Information Search and Retrieval&mdash;</span><span class="c4">search process, clustering </span></p><p class="c2"><span class="c23">General Terms </span><span class="c0">Algorithms, Experimentation, Performance </span></p><p class="c2"><span class="c23">Keywords </span><span class="c0">Similarity Search, Set Similarity Join, Data Mining </span></p><p class="c2"><span class="c23">1. INTRODUCTION </span></p><p class="c8"><span class="c0">Finding similar objects is a key component in many ap- plications, such as document and image clustering [14, 23], plagiarism detection [30], near duplicate documents detec- tion [24], 3D scene reconstruction [5], similar music and video retrieval [19, 32], community mining [31], and per- sonalized recommendations [17]. </span></p><p class="c2"><span class="c0">A natural way to describe complex objects is to repre- sent them as vectors in a high dimensional space, where the </span></p><p class="c2"><span class="c46">Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. </span><span class="c78">SIGMOD&rsquo;11, </span><span class="c46">June 12&ndash;16, 2011, Athens, Greece. Copyright 2011 ACM 978-1-4503-0661-4/11/06 ...$10.00. </span></p><p class="c8"><span class="c0">dimensions correspond to the features extracted from the objects. For example, there are hundreds of thousands of words in an unabridged English dictionary, and usually each word is considered a feature. Standard content-based image retrieval algorithms preprocess images by extracting local features [26] and quantizing them into visual words [27]; re- search shows that retrieval systems using a million visual words tend to outperform those using a smaller visual vo- cabulary [27]. Large-scale recommendation systems need to find the similarities of users over millions of items [17]. In many important applications, these vectors are either bi- nary, or can be approximated by binary vectors. In this paper, we focus on the special case of similarity search on binary vectors where the dimensionality of the vectors is on the order of 10</span><span class="c6">5 </span><span class="c0">to 10</span><span class="c6">8 </span><span class="c0">dimensions. </span></p><p class="c8"><span class="c0">Many emerging applications need to measure similarities and differences between objects with low similarity thresh- olds. For example, consider the problem of grouping web pages. Haveliwala et al. proposed a method to efficiently cluster 12 million web pages [23] by first representing each web page as a set of words, then finding all pairs of web pages whose similarities are above a specific threshold, and finally applying a link-based clustering algorithm to compute the clusters. The similarity threshold in their application is only 0.2</span><span class="c6">1</span><span class="c0">. As a second example, consider the problem addressed by Agarwal et al. that reconstructs 3D scenes from a very large collection of photos by finding geometrically consis- tent images [5]. Each image is represented by a set of visual words, and similarities between images are computed in or- der to discard unpromising image pairs. However, most ge- ometrically consistent images have similarity between 0</span><span class="c4">.</span><span class="c0">025 and 0</span><span class="c4">.</span><span class="c0">1! As a third example, consider the problem of find- ing plagiarism in large document collections. Sorokina et al. designed a mechanism to find plagiarism in research doc- ument collections [30]; they represented each document as a set of 7-grams, i.e., subsequences containing seven consecu- tive words. The authors found that most plagiarism cases only have four sentences in common. As each document contains hundreds of sentences, this corresponds to a very low similarity threshold. However, previous research on high dimensional similarity search has mainly focused on similar- ity thresholds above 0</span><span class="c4">.</span><span class="c0">5 [7, 8, 35, 33]. These thresholds are primarily useful for tasks such as finding near-duplicate documents and retrieving similar text snippets [35, 28]. </span></p><p class="c2"><span class="c0">Beyond reasons that are motivated by specific applica- tions, the use of low similarity thresholds also has an intu- </span></p><p class="c2"><span class="c14">1</span><span class="c22">We assume that similarity is normalized in the interval [0,1]. </span></p><p class="c2"><span class="c0">997 </span></p><p class="c8"><span class="c0">itive justification based on the geometrical properties of a high dimensional space [10]. Consider the volume </span><span class="c4">V</span><span class="c11">d </span><span class="c22">of a </span><span class="c1">d</span><span class="c22">- </span><span class="c0">dimensional hypersphere with radius </span><span class="c4">R</span><span class="c0">. It can be computed using the following formula: </span></p><p class="c2"><span class="c4">V</span><span class="c11">d </span><span class="c22">= </span><span class="c4">&pi; </span></p><p class="c49"><span class="c33">d </span><span class="c14">2 </span><span class="c9">+ 1)</span><span class="c21">R</span><span class="c12">d</span><span class="c4">, </span><span class="c0">where &Gamma; is the Gamma function. Note that the fraction </span><span class="c4">V</span><span class="c11">d</span><span class="c1">/R</span><span class="c3">d </span><span class="c22">goes to zero as </span><span class="c1">d </span><span class="c22">goes to infinity. To put these facts </span><span class="c0">into the context of similarity search, assume that we select two points, </span><span class="c4">A </span><span class="c0">and </span><span class="c4">B</span><span class="c0">, uniformly at random from the unit hypercube. We are interested in the Euclidean distance be- tween </span><span class="c4">A </span><span class="c0">and </span><span class="c4">B</span><span class="c0">. Consider a hypersphere of radius 0</span><span class="c4">.</span><span class="c0">99 cen- tered on </span><span class="c4">A</span><span class="c0">. When </span><span class="c4">d </span><span class="c0">= 25, the probability that </span><span class="c4">B </span><span class="c0">is inside the hypersphere is at most 7</span><span class="c4">.</span><span class="c0">45 </span><span class="c4">&times; </span><span class="c0">10</span><span class="c12">&minus;</span><span class="c6">4</span><span class="c0">. However, when </span><span class="c4">d </span><span class="c0">= 100, this probability is at most 8</span><span class="c4">.</span><span class="c0">67</span><span class="c4">&times;</span><span class="c0">10</span><span class="c12">&minus;</span><span class="c6">41</span><span class="c0">, and it is at most 7</span><span class="c4">.</span><span class="c0">45 </span><span class="c4">&times; </span><span class="c0">10</span><span class="c12">&minus;</span><span class="c6">110 </span><span class="c0">when </span><span class="c4">d </span><span class="c0">= 200. This simple example il- lustrates a counterintuitive fact: two objects with a very low similarity value could still be very similar since the volume of a </span><span class="c4">d</span><span class="c0">-dimensional hypersphere becomes vanishingly small relative to that of a </span><span class="c4">d</span><span class="c0">-dimensional hypercube as </span><span class="c4">d </span><span class="c0">increases. Traditional methods, such as </span><span class="c4">k</span><span class="c0">d trees and R-trees, only work well in low dimensions (typically less than one hun- dred). Recent attempts to speed up similarity search fo- cus on reducing the search space of candidate pairs whose similarity needs to be compared. These algorithms are ei- ther exact, in that their outputs contain all pairs of vectors whose similarity is above the threshold, or approximate, in that they only give probabilistic guarantees on their out- puts. The fastest exact algorithms, such as All-Pairs [8] and PPJoin [35], use an inverted index and apply various heuris- tics (such as prefix-filtering or ordering of the dataset) to reduce the size of the search space. These approaches re- quire the user to preprocess the data, and in our experi- ments, their performance degrades quickly as the size of the index increases. Approximate algorithms have been devel- oped based on locality sensitive hashing (LSH) [25], a prob- abilistic scheme that hashes similar vectors into the same buckets with high probability. LSH-based algorithms are often much faster than exact algorithms, but they do not work well for low similarity thresholds. Bayardo et al. state that their approach is faster than LSH but did not evaluate Min-Hash [8]. Other experimental evaluations also did not include Min-Hash [35, 33]. </span></p><p class="c8"><span class="c0">In this paper, we propose a novel concept for similarity search that is based on the use of </span><span class="c4">random filters</span><span class="c0">. Algo- rithms built on this idea repeatedly apply random filters to discard irrelevant vector pairs; thus, we call them </span><span class="c4">filtering- based </span><span class="c0">algorithms. We show that we can use truly random permutations to cheaply generate each random filter, then use inverted indexes to compute the intersection between the outputs of all filters to arrive at the candidate set. In addition, we demonstrate that truly random permutations can also be used to reliably estimate the similarity between vector pairs. This leads to a new candidate filtering algo- rithm, which works especially well for objects whose vector representation typically contains many non-zero elements, such as text, images, and videos. Finally, we observe that cluster-like structures abound in real data; text documents can be described with a set of topics, and photographs are of- ten taken near objects of interests. We propose a clustering algorithm that exploits this property for further speed-up. </span></p><p class="c2"><span class="c0">The main contributions of this paper are as follows: </span></p><p class="c2"><span class="c0">&Gamma;( </span><span class="c12">d </span></p><p class="c2"><span class="c66">2 </span></p><p class="c8"><span class="c4">&bull; </span><span class="c0">We propose a filtering-based algorithm for generating vector pairs whose similarities are likely to be above the given threshold. This algorithm significantly out- performs other candidate generation algorithms for low similarity thresholds, supports incremental queries, and can be easily parallelized. </span></p><p class="c8"><span class="c4">&bull; </span><span class="c0">We present an efficient algorithm for estimating the similarities between vector pairs. We show analytically that we only need to examine a fixed fraction of the dimensions in order to discard irrelevant vector pairs. </span></p><p class="c8"><span class="c4">&bull; </span><span class="c0">We present a fast approximate clustering algorithm that exploits cluster structures in real data. Our algo- rithm uses random sampling and probabilistic assign- ments, and can reduce the search space by up to 92% while discarding less than 1% of the true positives. </span></p><p class="c8"><span class="c4">&bull; </span><span class="c0">Based on these three ideas, we develop a novel proba- bilistic similarity search algorithm called ATLAS, and we provide experimental results on several real-world datasets. At a 97.5% recall rate, ATLAS is up to 210 times faster than previous exact algorithms and up to 80 times faster than previous approximate algorithms. </span></p><p class="c8"><span class="c0">The rest of the paper is organized as follows. We first give the problem definition and introduce some notation in Section 2. Then, we present our permutation-based algo- rithms for similarity search in Section 3, and our approxi- mate clustering algorithm in Section 4. We consider how to integrate these algorithms and discuss scalability issues in Section 5. In Section 6, we evaluate our algorithm against current state-of-the-art algorithms. Based on our experi- mental results, we discuss the strengths and weaknesses of existing similarity search algorithms in Section 7. We dis- cuss related work in Section 8 and conclude in Section 9. </span></p><p class="c2"><span class="c23">2. PRELIMINARIES </span></p><p class="c8"><span class="c0">Given a set of </span><span class="c4">n </span><span class="c0">binary vectors </span><span class="c4">V </span><span class="c0">= </span><span class="c4">{v</span><span class="c15">1</span><span class="c1">,v</span><span class="c15">2</span><span class="c1">,&middot;&middot;&middot; ,v</span><span class="c11">n</span><span class="c1">} </span><span class="c22">in </span><span class="c4">{</span><span class="c0">0</span><span class="c4">,</span><span class="c0">1</span><span class="c4">}</span><span class="c12">d </span><span class="c0">(we number the </span><span class="c4">d </span><span class="c0">dimensions 1</span><span class="c4">,</span><span class="c0">2</span><span class="c4">,...,d</span><span class="c0">), a similarity function </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">), and a similarity threshold </span><span class="c4">t</span><span class="c0">, we wish to find all vector pairs </span><span class="c4">{x, y} </span><span class="c0">such that </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&gt; t</span><span class="c0">. We assume that </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) is normalized in the interval [0</span><span class="c4">,</span><span class="c0">1], is symmetric, and monotonically increases with </span><span class="c4">dot</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) when we hold the number of non-zero dimensions in both </span><span class="c4">x </span><span class="c0">and </span><span class="c4">y </span><span class="c0">constant. To trade off accuracy for efficiency, we allow the algorithm to only output a (1</span><span class="c4">&minus;&delta;</span><span class="c0">) fraction of all such vector pairs in expectation. We call this problem </span><span class="c4">&delta;-approximate similarity search</span><span class="c0">. </span></p><p class="c8"><span class="c0">For high-dimensional similarity search problems, the vec- tors in </span><span class="c4">V </span><span class="c0">are usually sparse in that they have few non-zero entries. We represent a sparse vector </span><span class="c4">v</span><span class="c3">i </span><span class="c0">as a list by enumer- ating the indices of its non-zero entries in increasing order: </span><span class="c4">v</span><span class="c3">i </span><span class="c0">= [</span><span class="c4">v</span><span class="c6">0</span><span class="c3">i </span><span class="c21">,v</span><span class="c6">1</span><span class="c3">i </span><span class="c21">,v</span><span class="c6">2</span><span class="c3">i </span><span class="c21">,...,v</span><span class="c12">|v</span><span class="c33">i</span><span class="c12">|&minus;</span><span class="c6">1 </span></p><p class="c8"><span class="c3">i </span><span class="c9">]. We call </span><span class="c21">|v</span><span class="c12">i</span><span class="c21">| </span><span class="c9">the </span><span class="c21">length </span><span class="c9">of </span><span class="c21">v</span><span class="c12">i</span><span class="c9">, </span><span class="c0">and use </span><span class="c4">L </span><span class="c0">to denote the average length of the vectors in </span><span class="c4">V </span><span class="c0">, </span><span class="c4">L </span><span class="c0">= </span><span class="c18">&sum;</span><span class="c3">n</span><span class="c11">i</span><span class="c15">=1 </span><span class="c4">|v</span><span class="c3">i</span><span class="c4">|/n</span><span class="c0">. </span></p><p class="c8"><span class="c0">A permutation </span><span class="c4">&pi; </span><span class="c0">: </span><span class="c4">{</span><span class="c0">1</span><span class="c4">, </span><span class="c0">2</span><span class="c4">,...,d}&rarr;{</span><span class="c0">1</span><span class="c4">,</span><span class="c0">2</span><span class="c4">,...,d} </span><span class="c0">defines a reordering of the </span><span class="c4">d </span><span class="c0">dimensions. We denote the result of applying a permutation </span><span class="c4">&pi; </span><span class="c0">to </span><span class="c4">v</span><span class="c3">i </span><span class="c0">by </span><span class="c4">&pi;</span><span class="c0">(</span><span class="c4">v</span><span class="c3">i</span><span class="c0">). </span><span class="c4">&pi;</span><span class="c0">(</span><span class="c4">v</span><span class="c3">i</span><span class="c0">)</span><span class="c12">j </span><span class="c0">is the </span><span class="c4">j</span><span class="c0">- th smallest element in the list </span><span class="c4">v</span><span class="c11">i </span><span class="c22">under the permutation </span><span class="c0">(0 </span><span class="c4">&le; j &le; |v</span><span class="c3">i</span><span class="c4">| &minus; </span><span class="c0">1). We represent the set of </span><span class="c4">v</span><span class="c3">i</span><span class="c0">&rsquo;s </span><span class="c4">K </span><span class="c0">smallest elements by </span><span class="c4">F </span><span class="c12">&pi;,K </span><span class="c3">v</span><span class="c35">i </span><span class="c0">= </span><span class="c4">{&pi;</span><span class="c0">(</span><span class="c4">v</span><span class="c11">i</span><span class="c22">)</span><span class="c14">0</span><span class="c1">,&pi;</span><span class="c22">(</span><span class="c1">v</span><span class="c11">i</span><span class="c22">)</span><span class="c14">1</span><span class="c1">,...,&pi;</span><span class="c22">(</span><span class="c1">v</span><span class="c11">i</span><span class="c22">)</span><span class="c3">K&minus;</span><span class="c14">1</span><span class="c1">}</span><span class="c22">. </span></p><p class="c2"><span class="c0">Similarity search algorithms usually consist of two stages: </span><span class="c4">candidate generation </span><span class="c0">and </span><span class="c4">candidate filtering</span><span class="c0">. In the first </span></p><p class="c2"><span class="c0">998 </span></p><p class="c2"><span class="c0">Candidate pairs of vectors from </span><span class="c4">V </span></p><p class="c2"><span class="c7">Table 1: Notation Summary </span></p><p class="c8"><span class="c0">stage, the algorithm selects candidate pairs of vectors from </span><span class="c4">V </span><span class="c0">that are likely to have similarity above </span><span class="c4">t</span><span class="c0">. In the second stage, the algorithm verifies if the similarity of each candidate pair is indeed above </span><span class="c4">t</span><span class="c0">. We follow that general framework in this paper. We use </span><span class="c4">C</span><span class="c12">V </span><span class="c0">to denote the set of unordered candidate pairs, Our and algorithm </span><span class="c4">C</span><span class="c3">x </span><span class="c12">V</span><span class="c9">to denote </span><span class="c21">{y | {x, y} &isin; C</span><span class="c12">V </span><span class="c4">}</span><span class="c0">. </span></p><p class="c2"><span class="c0">may further split </span><span class="c4">V </span><span class="c0">into several subsets, </span><span class="c4">V</span><span class="c14">1</span><span class="c4">,V</span><span class="c14">2</span><span class="c4">,...,V</span><span class="c3">m</span><span class="c0">. the set of candidate When this is the case, we use </span><span class="c4">C</span><span class="c12">V</span><span class="c33">i </span><span class="c0">of candidate pairs in </span><span class="c4">V</span><span class="c11">i</span><span class="c22">, and </span><span class="c0">vectors for </span><span class="c4">x </span><span class="c0">in </span><span class="c4">V</span><span class="c11">i</span><span class="c22">, respectively. </span></p><p class="c2"><span class="c1">C</span><span class="c11">x </span><span class="c3">V</span><span class="c35">i</span><span class="c0">to denote </span><span class="c22">to denote </span><span class="c0">the set </span></p><p class="c2"><span class="c0">We focus on the two most commonly used similarity func- tions, Jaccard and Cosine, which are defined as follows: </span></p><p class="c2"><span class="c0">Jaccard(</span><span class="c4">x, y</span><span class="c0">) Cosine(</span><span class="c4">x, y</span><span class="c0">) = = </span><span class="c4">dot</span><span class="c0">(</span><span class="c4">x, dot</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">y</span><span class="c0">) </span><span class="c4">/ / </span><span class="c0">(</span><span class="c4">|x| </span><span class="c18">&radic; </span></p><p class="c2"><span class="c4">|x|&middot;|y| </span></p><p class="c2"><span class="c0">+ </span><span class="c4">|y| &minus; dot</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">)) </span></p><p class="c2"><span class="c0">In Table 1, we summarize the most important notation. </span></p><p class="c24"><span class="c23">3. PERMUTATION-BASED ALGORITHMS </span><span class="c0">Many similarity functions are computed based on the num- ber of non-zero elements shared by the two vectors, </span><span class="c4">x </span><span class="c0">and </span><span class="c4">y</span><span class="c0">. Recall that in our problem setting, </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) increases with </span><span class="c4">dot</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) when we hold both </span><span class="c4">|x| </span><span class="c0">and </span><span class="c4">|y| </span><span class="c0">constant. For Jaccard similarity, </span></p><p class="c2"><span class="c4">dot</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span></p><p class="c2"><span class="c0">1 + </span><span class="c4">t </span><span class="c9">(</span><span class="c21">|x| </span><span class="c9">+ </span><span class="c21">|y|</span><span class="c9">);</span><span class="c0">(1) and for Cosine similarity, </span></p><p class="c2"><span class="c4">dot</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">)</span><span class="c4">/</span><span class="c18">&radic; </span></p><p class="c2"><span class="c4">|x|&middot;|y|. </span><span class="c0">(2) </span></p><p class="c8"><span class="c0">We can use Equations (1) and (2) by applying a truly random permutation to the </span><span class="c4">d </span><span class="c0">dimensions. This allows us to develop efficient methods for the </span><span class="c4">candidate generation </span><span class="c0">step and for the </span><span class="c4">candidate filtering </span><span class="c0">step in a similarity search algorithm. We define the following subproblems: </span></p><p class="c2"><span class="c0">Problem 1. </span><span class="c4">(Candidate Generation) Given V , sim, and t, return {x, y} with C</span><span class="c12">V </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, such y</span><span class="c0">) </span><span class="c4">that &gt; t </span><span class="c0">(1 </span><span class="c4">are &minus; &delta;in </span><span class="c14">1</span><span class="c12">&lowast;</span><span class="c9">) </span><span class="c4">C</span><span class="c21">fraction </span><span class="c12">V </span><span class="c4">. </span></p><p class="c2"><span class="c21">of all vector pairs </span></p><p class="c8"><span class="c0">Problem 2. </span><span class="c4">(Candidate Filtering) Given V , sim, t, and C</span><span class="c12">V </span><span class="c4">, determine whether each vector pair {x, y} &isin; C</span><span class="c12">V </span><span class="c4">is likely to satisfy sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&gt; t in time O</span><span class="c0">(</span><span class="c4">|V |L</span><span class="c0">) + </span><span class="c4">o</span><span class="c0">(</span><span class="c4">|C</span><span class="c12">V </span><span class="c4">|L</span><span class="c0">) </span><span class="c4">with </span><span class="c0">(1 </span><span class="c4">&minus; &delta;</span><span class="c14">2</span><span class="c12">&lowast;</span><span class="c9">) </span><span class="c21">probability of success. </span></p><p class="c2"><span class="c0">and present our solutions in Sections 3.1 and 3.2. </span></p><p class="c2"><span class="c4">|x| </span><span class="c0">+ </span><span class="c4">|y| &minus; dot</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c21">&gt; t &hArr; dot</span><span class="c9">(</span><span class="c21">x, y</span><span class="c9">) </span><span class="c21">&gt; t </span></p><p class="c2"><span class="c0">Notation </span></p><p class="c2"><span class="c0">The </span><span class="c4">V </span></p><p class="c2"><span class="c0">set of binary vectors </span><span class="c4">n </span></p><p class="c2"><span class="c4">n </span><span class="c0">= </span><span class="c4">|V |</span><span class="c0">, the number of vectors in </span><span class="c4">V d </span></p><p class="c2"><span class="c0">The dimensionality of the vectors in </span><span class="c4">V t </span></p><p class="c2"><span class="c0">The similarity threshold </span><span class="c4">sim </span></p><p class="c2"><span class="c0">The similarity function </span><span class="c4">&delta; </span></p><p class="c2"><span class="c0">The error rate </span><span class="c4">&pi; </span></p><p class="c2"><span class="c0">A permutation of </span><span class="c4">{</span><span class="c0">1</span><span class="c4">, </span><span class="c0">2</span><span class="c4">,...,d} x </span></p><p class="c2"><span class="c0">A vector in </span><span class="c4">V |x| </span></p><p class="c2"><span class="c0">The number of non-zero entries in </span><span class="c4">x L </span></p><p class="c2"><span class="c0">The average length of vectors in </span><span class="c4">V C</span><span class="c12">V </span></p><p class="c2"><span class="c0">Description </span></p><p class="c2"><span class="c4">|x|&middot;|y| &gt; t &hArr; dot</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&gt; t</span><span class="c18">&radic; </span></p><p class="c2"><span class="c23">3.1 Candidate Generation </span></p><p class="c8"><span class="c0">To illustrate the intuition behind our filtering-based can- didate generation algorithm, assume that </span><span class="c4">A</span><span class="c14">1</span><span class="c4">,A</span><span class="c14">2</span><span class="c4">,...,A</span><span class="c3">n </span><span class="c0">is a list of random variables such that each one of them can only take one of two values, 0 and 1, and that exactly </span><span class="c4">m </span><span class="c0">among them have value 1. Because these </span><span class="c4">m </span><span class="c0">non-zero ele- ments divide the list into </span><span class="c4">m</span><span class="c0">+1 blocks, the expected position of the first non-zero element </span><span class="c4">A</span><span class="c12">&lowast; </span><span class="c0">among them, </span><span class="c4">P</span><span class="c3">A</span><span class="c68">*</span><span class="c9">, should </span><span class="c0">be approximately </span><span class="c4">n/</span><span class="c0">(</span><span class="c4">m</span><span class="c0">+1). Furthermore, </span><span class="c4">P</span><span class="c11">A</span><span class="c65">* </span><span class="c0">is extremely unlikely to be very large. For </span><span class="c4">n </span><span class="c0">= 1000 and </span><span class="c4">m </span><span class="c0">= 100, the probability that </span><span class="c4">P</span><span class="c3">A</span><span class="c68">* </span><span class="c9">is larger than 50 is less than 0</span><span class="c21">.</span><span class="c9">005. </span></p><p class="c8"><span class="c0">This motivates us to use a simple procedure for candidate generation: apply a random permutation to the </span><span class="c4">d </span><span class="c0">dimen- sions, build an inverted index that contains each vector&rsquo;s smallest </span><span class="c4">K </span><span class="c0">non-zero elements, and finally look up the in- verted index to find the candidate pairs. Here, </span><span class="c4">K </span><span class="c0">is chosen such that for two vectors </span><span class="c4">x </span><span class="c0">and </span><span class="c4">y</span><span class="c0">, if </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&gt; t</span><span class="c0">, then with high probability, the intersection of their first </span><span class="c4">K </span><span class="c0">non- zero dimensions is non-empty. </span></p><p class="c2"><span class="c0">Example 1. </span><span class="c4">Consider four vectors: a </span><span class="c0">= [1</span><span class="c4">, </span><span class="c0">3</span><span class="c4">,</span><span class="c0">5</span><span class="c4">,</span><span class="c0">7]</span><span class="c4">, b </span><span class="c0">= [2</span><span class="c4">,</span><span class="c0">3</span><span class="c4">,</span><span class="c0">5</span><span class="c4">, </span><span class="c0">7]</span><span class="c4">, c </span><span class="c0">= [2</span><span class="c4">, </span><span class="c0">3</span><span class="c4">,</span><span class="c0">4</span><span class="c4">,</span><span class="c0">6]</span><span class="c4">, and d </span><span class="c0">= [1</span><span class="c4">,</span><span class="c0">2</span><span class="c4">, </span><span class="c0">4]</span><span class="c4">, and a permutation </span></p><p class="c2"><span class="c4">&pi;</span><span class="c15">1 </span><span class="c22">= </span></p><p class="c2"><span class="c36">( </span><span class="c22">1 2 3 4 5 6 7 </span><span class="c0">4 2 6 5 7 3 1 </span></p><p class="c2"><span class="c36">)</span><span class="c1">. Under this permutation, </span></p><p class="c8"><span class="c4">&pi;</span><span class="c14">1</span><span class="c0">(</span><span class="c4">a</span><span class="c0">) = [7</span><span class="c4">,</span><span class="c0">1</span><span class="c4">,</span><span class="c0">3</span><span class="c4">, </span><span class="c0">5]</span><span class="c4">, &pi;</span><span class="c14">1</span><span class="c0">(</span><span class="c4">b</span><span class="c0">) = [7</span><span class="c4">, </span><span class="c0">2</span><span class="c4">,</span><span class="c0">3</span><span class="c4">,</span><span class="c0">5]</span><span class="c4">, &pi;</span><span class="c14">1</span><span class="c0">(</span><span class="c4">c</span><span class="c0">) = [2</span><span class="c4">,</span><span class="c0">6</span><span class="c4">, </span><span class="c0">4</span><span class="c4">,</span><span class="c0">3]</span><span class="c4">, and &pi;</span><span class="c15">1</span><span class="c22">(</span><span class="c1">d</span><span class="c22">) = [2</span><span class="c1">,</span><span class="c22">1</span><span class="c1">,</span><span class="c22">4]</span><span class="c1">. For K </span><span class="c22">= 1</span><span class="c1">, this procedure will in- </span><span class="c4">dex the element </span><span class="c0">7 </span><span class="c4">in a, the element </span><span class="c0">7 </span><span class="c4">in b, the element </span><span class="c0">2 </span><span class="c4">in c, and the element </span><span class="c0">2 </span><span class="c4">in d. The candidate set is C</span><span class="c14">1 </span><span class="c0">= </span><span class="c4">{{a, b},{c, d}}. If K </span><span class="c0">= 2</span><span class="c4">, then the candidate set will be C</span><span class="c14">2 </span><span class="c0">= </span><span class="c4">{{a, b},{a, d},{b, c},{b, d},{c, d}} instead. </span></p><p class="c8"><span class="c0">This procedure only needs to index a small fraction of the non-zero elements in each vector even when </span><span class="c4">t </span><span class="c0">is low, and thus it is extremely fast. The candidate filtering step may become the bottleneck in the algorithm if we have too many candidates. We can further reduce the size of the candidate set (and also the time needed for the candidate filtering step) by running the above procedure </span><span class="c4">S </span><span class="c0">times and taking the intersection of all candidate sets. Each run of the above procedure can be regarded as the use of a </span><span class="c4">random filter</span><span class="c0">, and each candidate set as the output of a random filter. The algorithm works because pairs of vectors with high similarity scores are more likely to be included in the output than pairs with low similarity scores. </span></p><p class="c2"><span class="c0">Example 2. </span><span class="c4">Consider again the four vectors from Ex- </span></p><p class="c2"><span class="c4">ample 1. We apply &pi;</span><span class="c15">2 </span><span class="c22">= </span></p><p class="c2"><span class="c36">( </span><span class="c22">1 2 3 4 5 6 7 </span><span class="c0">5 3 1 4 2 7 6 </span></p><p class="c2"><span class="c36">) </span></p><p class="c2"><span class="c4">, and </span></p><p class="c24"><span class="c4">again index the first two non-zero elements in each vector. The We observe candidate that set C </span><span class="c15">2</span><span class="c4">is &cap;Cthen </span><span class="c15">2 </span><span class="c22">= </span><span class="c4">C </span><span class="c1">{{a, </span><span class="c15">2 </span><span class="c0">= </span><span class="c1">b},{b, </span><span class="c4">{{a, b}, </span><span class="c1">c},{c, </span><span class="c4">{a, c},{b, </span><span class="c1">d}}, which </span><span class="c4">c}, {c, </span><span class="c1">hap- </span><span class="c4">d}}. </span></p><p class="c2"><span class="c4">pens to contain exactly all vector pairs that have at least two elements in common. </span></p><p class="c8"><span class="c0">If we repeat this procedure </span><span class="c4">S </span><span class="c0">times, the probability that a pair of vectors </span><span class="c4">{x, y} </span><span class="c0">is present in the final candidate set </span><span class="c4">C</span><span class="c12">V </span><span class="c0">is </span></p><p class="c2"><span class="c7">Pr </span></p><p class="c2"><span class="c36">(</span><span class="c22">1 </span><span class="c1">&minus; </span></p><p class="c2"><span class="c36">)</span><span class="c11">S </span></p><p class="c24"><span class="c4">.</span><span class="c22">(3) </span><span class="c0">Our algorithm can be implemented with </span><span class="c4">S </span><span class="c0">inverted in- dexes. The </span><span class="c4">s</span><span class="c0">-th inverted </span><span class="c4">...,I</span><span class="c3">s</span><span class="c12">d</span><span class="c9">. We give the </span><span class="c0">index, </span><span class="c4">I</span><span class="c3">s</span><span class="c0">, consists of </span><span class="c4">d </span><span class="c0">lists, </span><span class="c4">I</span><span class="c3">s</span><span class="c6">1</span><span class="c21">,I</span><span class="c3">s</span><span class="c6">2</span><span class="c21">, </span></p><p class="c2"><span class="c9">pseudo code in Algorithm 1. </span></p><p class="c2"><span class="c0">999 </span></p><p class="c2"><span class="c36">[</span><span class="c1">|F </span><span class="c11">x </span><span class="c3">&pi;</span><span class="c35">s</span><span class="c11">,K </span><span class="c4">&cap; F </span><span class="c11">y </span><span class="c12">&pi;</span><span class="c33">s</span><span class="c3">,K </span><span class="c4">| &gt; </span><span class="c0">0</span><span class="c4">, &forall;s</span><span class="c18">] </span></p><p class="c2"><span class="c0">= </span></p><p class="c2"><span class="c36">(</span><span class="c11">|x&cup;y|&minus;|x&cap;y| </span></p><p class="c2"><span class="c36">) </span></p><p class="c2"><span class="c3">K </span></p><p class="c49"><span class="c36">(</span><span class="c11">|x&cup;y| </span><span class="c3">K </span></p><p class="c2"><span class="c36">) </span></p><p class="c2"><span class="c7">Algorithm 1 </span><span class="c0">ATLAS: Candidate Generation </span></p><p class="c2"><span class="c7">Input: </span><span class="c4">V </span><span class="c0">; </span><span class="c4">S</span><span class="c0">; </span><span class="c4">K </span><span class="c7">Output: </span><span class="c4">C</span><span class="c12">V </span></p><p class="c2"><span class="c0">1: </span><span class="c4">C</span><span class="c12">V </span><span class="c4">&larr; </span><span class="c0">all possible vector pairs in </span><span class="c4">V </span><span class="c0">2: </span><span class="c7">for </span><span class="c4">s </span><span class="c0">= 1 to </span><span class="c4">S </span><span class="c7">do </span><span class="c0">3: </span><span class="c4">&pi;</span><span class="c11">s </span><span class="c1">&larr; </span><span class="c22">a random permutation of the </span><span class="c1">d </span><span class="c22">dimensions </span><span class="c0">4: </span><span class="c4">&forall; x &isin; V </span><span class="c0">, apply </span><span class="c4">&pi;</span><span class="c3">s </span><span class="c0">to get </span><span class="c4">F </span><span class="c12">&pi;</span><span class="c33">s</span><span class="c12">,K </span><span class="c3">x </span><span class="c22">5: </span><span class="c1">I</span><span class="c11">s </span><span class="c1">&larr; </span><span class="c22">an inverted index built from all </span><span class="c1">F </span><span class="c3">&pi;</span><span class="c35">s</span><span class="c11">,K x </span><span class="c0">where </span></p><p class="c2"><span class="c0">(</span><span class="c4">x &isin; V </span><span class="c0">) 6: </span><span class="c4">C &larr; </span><span class="c0">all </span><span class="c4">{x, y} </span><span class="c0">that share at least one indexed feature 7: </span><span class="c4">C</span><span class="c12">V </span><span class="c4">&larr; C</span><span class="c12">V </span><span class="c4">&cap; C </span><span class="c0">8: </span><span class="c7">end for </span><span class="c0">9: </span><span class="c7">return </span><span class="c4">C</span><span class="c12">V </span></p><p class="c8"><span class="c0">A naive implementation of Algorithm 1 uses </span><span class="c4">O</span><span class="c0">(</span><span class="c4">n</span><span class="c6">2</span><span class="c0">) mem- ory. We can avoid this by computing the candidate set, </span><span class="c4">C</span><span class="c12">V</span><span class="c3">x </span><span class="c9">, </span><span class="c0">for each </span><span class="c4">x </span><span class="c0">separately. We give the pseudo code in Algo- rithm 2. This procedure only requires linear space and can be easily parallelized. </span></p><p class="c2"><span class="c7">Algorithm 2 </span><span class="c0">Finding Candidate Vectors For Vector </span><span class="c4">x </span></p><p class="c2"><span class="c7">Input: </span><span class="c4">x &isin; V </span><span class="c0">; </span><span class="c4">I</span><span class="c14">1</span><span class="c4">,I</span><span class="c14">2</span><span class="c4">,...,I</span><span class="c3">S</span><span class="c0">; </span><span class="c4">F </span><span class="c12">&pi;</span><span class="c66">1</span><span class="c12">,K </span><span class="c3">x </span><span class="c21">,F </span><span class="c12">&pi;</span><span class="c66">2</span><span class="c12">,K </span><span class="c3">x </span><span class="c21">,...,F </span><span class="c12">&pi;</span><span class="c33">S</span><span class="c12">,K </span><span class="c3">x </span><span class="c7">Output: </span><span class="c4">C</span><span class="c12">V</span><span class="c3">x </span><span class="c22">1: </span><span class="c1">C</span><span class="c3">V</span><span class="c11">x </span><span class="c4">&larr; {</span><span class="c0">1</span><span class="c4">, </span><span class="c0">2</span><span class="c4">,...,|V |} </span></p><p class="c2"><span class="c0">2: </span><span class="c7">for </span><span class="c4">s </span><span class="c0">= 1 to </span><span class="c4">S </span><span class="c7">do </span><span class="c0">3: </span><span class="c4">C </span><span class="c11">x </span><span class="c4">&larr; &empty; </span><span class="c0">4: </span><span class="c7">for all </span><span class="c4">p &isin; F </span><span class="c12">&pi;</span><span class="c33">s</span><span class="c12">,K </span><span class="c3">x </span><span class="c41">do </span><span class="c0">5: </span><span class="c4">C </span><span class="c11">x </span><span class="c4">&larr; C </span><span class="c11">x </span><span class="c4">&cup; I</span><span class="c12">p</span><span class="c3">s </span><span class="c22">6: </span><span class="c32">end for </span><span class="c0">7: </span><span class="c4">C</span><span class="c12">V</span><span class="c3">x </span><span class="c21">&larr; C</span><span class="c3">x </span><span class="c4">&cap; C </span><span class="c11">x </span><span class="c22">8: </span><span class="c32">end for </span><span class="c0">9: </span><span class="c7">return </span><span class="c4">C</span><span class="c12">V</span><span class="c3">x </span></p><p class="c8"><span class="c0">Note that the probability of </span><span class="c4">{x, y} &isin; C</span><span class="c12">V </span><span class="c0">is closely re- lated to the ratio of </span><span class="c4">|x &cap; y| </span><span class="c0">to </span><span class="c4">|x &cup; y|</span><span class="c0">. For Cosine similarity, this probability depends on both </span><span class="c4">|x| </span><span class="c0">and </span><span class="c4">|y|</span><span class="c0">, so we cannot directly bound the recall rate. Nevertheless, in many prac- tical applications, the lengths of the vectors do not vary too much. In such cases, we can use this algorithm for Cosine similarity at a cost of longer running times. </span></p><p class="c8"><span class="c0">The space complexity of the algorithm is </span><span class="c4">O</span><span class="c0">(</span><span class="c4">nKS</span><span class="c0">+</span><span class="c4">d</span><span class="c0">). The main overhead comes from the space needed to store the </span><span class="c4">S </span><span class="c0">inverted indexes. The time complexity is </span><span class="c4">O</span><span class="c0">(</span><span class="c4">n</span><span class="c6">2</span><span class="c4">S </span><span class="c0">+ </span><span class="c4">nS</span><span class="c0">(</span><span class="c4">L </span><span class="c0">+ </span><span class="c4">K</span><span class="c0">)+</span><span class="c4">Sd</span><span class="c0">+</span><span class="c4">nL </span><span class="c0">log </span><span class="c4">L</span><span class="c0">). Here, </span><span class="c4">O</span><span class="c0">(</span><span class="c4">nS</span><span class="c0">(</span><span class="c4">L</span><span class="c0">+</span><span class="c4">K</span><span class="c0">)) is the time needed for generating the inverted indexes, and </span><span class="c4">O</span><span class="c0">(</span><span class="c4">n</span><span class="c6">2</span><span class="c4">S</span><span class="c0">) is the time needed for generating the candidate pairs. In practice, the number of vector pairs with similarity above </span><span class="c4">t </span><span class="c0">is on the or- der of </span><span class="c4">o</span><span class="c0">(</span><span class="c4">n</span><span class="c6">2</span><span class="c0">), so for suitable parameters, the time needed for candidate generation will never reach </span><span class="c4">O</span><span class="c0">(</span><span class="c4">n</span><span class="c6">2</span><span class="c4">S</span><span class="c0">). In our ex- periments, we find that we can obtain a good estimation of the parameter values via random sampling; we discuss our sampling procedure in Section 5.1. </span></p><p class="c24"><span class="c51">3.1.1 Comparison with Existing Candidate Genera- </span><span class="c64">tion Algorithms </span><span class="c0">The efficiency of a similarity search algorithm is mostly determined by its candidate generation algorithm. Existing candidate generation algorithms can be categorized into two groups: </span><span class="c4">signature-based schemes</span><span class="c0">, and </span><span class="c4">inverted index-based schemes</span><span class="c0">. A signature-based algorithm first generates a set of signatures for each vector. Then, two vectors are considered </span></p><p class="c8"><span class="c0">as a candidate if they share at least one common signature. Algorithms that use this idea include Min-Hash [11], random projections-based LSH [12], and PartEnum [7]. In contrast, an inverted index-based algorithm first selects some elements from each vector to index, then computes the candidate set while traversing the inverted index. The fastest algorithms among them are All-Pairs [8] and PPJoin [35]. </span></p><p class="c8"><span class="c0">Our approach is best viewed as a </span><span class="c4">filtering-based scheme</span><span class="c0">, because our core idea is to repeatedly apply a series of ran- dom filters to the </span><span class="c4">O</span><span class="c0">(</span><span class="c4">n</span><span class="c6">2</span><span class="c0">) vector pairs. To efficiently apply these filters, we combine insights from both the signature- based algorithms and the inverted index-based algorithms. Our selection of the first </span><span class="c4">K </span><span class="c0">non-zero dimensions in each vector can be viewed as generating </span><span class="c4">K </span><span class="c0">signatures for each vector. Then, we use inverted indexes to efficiently compute the intersection of candidate sets. </span></p><p class="c8"><span class="c7">Comparison with Min-Hash: </span><span class="c0">Another probabilistic similarity search algorithm, Min-Hash, also uses random permutations [11]. Min-Hash is a LSH-based algorithm. Its hash function hashes two vectors into the same bucket with a probability that increases with their Jaccard similarity. If </span><span class="c4">&pi; </span><span class="c0">is a min-wise independent permutation, then </span></p><p class="c2"><span class="c7">Pr</span><span class="c0">[</span><span class="c4">&pi;</span><span class="c0">(</span><span class="c4">x</span><span class="c0">)</span><span class="c6">0 </span><span class="c0">= </span><span class="c4">&pi;</span><span class="c0">(</span><span class="c4">y</span><span class="c0">)</span><span class="c6">0</span><span class="c0">] = </span><span class="c21">|x </span><span class="c4">|x </span><span class="c21">&cap; </span><span class="c4">&cup; </span><span class="c21">y| </span></p><p class="c2"><span class="c4">y| </span><span class="c9">= Jaccard(</span><span class="c21">x, y</span><span class="c9">)</span><span class="c21">. </span><span class="c9">(4) </span></p><p class="c8"><span class="c0">The minimum non-zero dimension under </span><span class="c4">&pi; </span><span class="c0">is called a </span><span class="c4">token</span><span class="c0">. A </span><span class="c4">signature </span><span class="c0">(or hash key) is formed by combining </span><span class="c4">R </span><span class="c0">tokens. The candidate set is then the vector pairs sharing the same signatures. This process is repeated </span><span class="c4">J </span><span class="c0">times in order to achieve a high recall rate. The resulting probability that we keep </span><span class="c4">{x, y} </span><span class="c0">is </span></p><p class="c2"><span class="c7">Pr </span></p><p class="c2"><span class="c36">[ </span><span class="c39">&#8899;</span><span class="c11">J</span><span class="c36">( </span><span class="c39">&#8898;</span><span class="c11">R</span><span class="c18">)] </span></p><p class="c2"><span class="c18">(</span><span class="c4">&pi;</span><span class="c3">r</span><span class="c12">j</span><span class="c9">(</span><span class="c21">x</span><span class="c9">)</span><span class="c6">0 </span><span class="c0">= </span><span class="c4">&pi;</span><span class="c3">r</span><span class="c12">j</span><span class="c9">(</span><span class="c21">y</span><span class="c9">)</span><span class="c6">0</span><span class="c0">= 1</span><span class="c4">&minus;</span><span class="c0">1 </span><span class="c4">&minus; </span><span class="c11">j</span><span class="c15">=1 </span></p><p class="c2"><span class="c11">r</span><span class="c15">=1</span><span class="c18">)</span><span class="c3">R</span><span class="c18">)</span><span class="c3">J </span></p><p class="c2"><span class="c4">|x &cup; y|. </span></p><p class="c8"><span class="c0">(5) Note that each </span><span class="c4">&pi;</span><span class="c3">r </span><span class="c12">j</span><span class="c9">in (5) only needs to be a min-wise inde- </span><span class="c0">pendent permutation. In our algorithm, for (3) to hold, we need each </span><span class="c4">&pi;</span><span class="c11">s </span><span class="c22">to be a </span><span class="c1">K </span><span class="c22">min-wise independent permutation. </span><span class="c0">Directly evaluating a </span><span class="c4">K </span><span class="c0">min-wise independent hash func- tion can be very costly for large </span><span class="c4">K</span><span class="c0">s; however, in practical applications, </span><span class="c4">d </span><span class="c0">is usually far less than 10</span><span class="c6">9</span><span class="c0">. This allows us to use a random permutation in order to avoid calculating </span><span class="c4">K </span><span class="c0">min-wise independent hash functions. </span></p><p class="c2"><span class="c0">We now discuss the performance differences between Min- Hash and our candidate generation algorithm. Note that </span></p><p class="c2"><span class="c36">(</span><span class="c22">1 </span><span class="c1">&minus; </span></p><p class="c2"><span class="c36">(</span><span class="c1">|x &cap; y| </span></p><p class="c2"><span class="c36">(</span><span class="c11">|x&cup;y|&minus;|x&cap;y| </span></p><p class="c2"><span class="c36">) </span></p><p class="c2"><span class="c3">K </span></p><p class="c2"><span class="c36">(</span><span class="c22">1 </span><span class="c1">&minus; </span><span class="c22">(1 </span><span class="c1">&minus; t</span><span class="c22">)</span><span class="c3">K</span><span class="c18">)</span><span class="c3">S </span><span class="c1">, </span></p><p class="c8"><span class="c0">and the differences between the left-hand side and the right- hand side of the above equation is negligible when </span><span class="c4">K </span><span class="c0">is small relative to the lengths of the vectors. We select </span><span class="c4">K</span><span class="c0">, </span><span class="c4">S</span><span class="c0">, </span><span class="c4">J</span><span class="c0">, and </span><span class="c4">R </span><span class="c0">such that both ATLAS&rsquo;s candidate generation algorithm and pairs Min-Hash of vectors have </span><span class="c4">{x, y} </span><span class="c0">an with expected Jaccard(</span><span class="c4">x, </span><span class="c0">recall </span><span class="c4">y</span><span class="c0">) rate = </span><span class="c4">t</span><span class="c0">: </span></p><p class="c2"><span class="c0">of 1 </span><span class="c4">&minus; &delta;</span><span class="c14">1 </span><span class="c12">&lowast;</span><span class="c9">for all </span></p><p class="c2"><span class="c0">(1 </span><span class="c4">&minus; </span><span class="c0">(1 </span><span class="c4">&minus; t</span><span class="c0">)</span><span class="c12">K</span><span class="c0">)</span><span class="c12">S </span><span class="c0">= 1 </span><span class="c4">&minus; </span><span class="c0">(1 </span><span class="c4">&minus; t</span><span class="c12">R</span><span class="c0">)</span><span class="c12">J </span></p><p class="c8"><span class="c0">We take logs on both sides of the above equation. Because </span><span class="c4">&delta;</span><span class="c14">1 </span><span class="c12">&lowast;</span><span class="c9">is usually very small in practice, we can apply the ap- </span><span class="c0">proximation ln(1 + </span><span class="c4">x</span><span class="c0">) </span><span class="c4">&asymp; x </span><span class="c0">for </span><span class="c4">|x| &lt; </span><span class="c0">1: </span></p><p class="c2"><span class="c4">S</span><span class="c0">(1 </span><span class="c4">&minus; t</span><span class="c0">)</span><span class="c12">K </span><span class="c0">= (1 </span><span class="c4">&minus; t</span><span class="c12">R</span><span class="c0">)</span><span class="c12">J</span><span class="c4">. </span></p><p class="c2"><span class="c0">1000 </span></p><p class="c2"><span class="c36">(</span><span class="c11">|x&cup;y| </span></p><p class="c2"><span class="c3">K </span></p><p class="c2"><span class="c36">) </span></p><p class="c2"><span class="c36">)</span><span class="c11">S </span></p><p class="c2"><span class="c4">&le; </span></p><p class="c2"><span class="c0">It directly follows that </span></p><p class="c2"><span class="c4">K </span><span class="c0">= </span><span class="c9">ln(1 </span><span class="c21">&minus; t</span><span class="c12">R</span><span class="c0">) </span></p><p class="c2"><span class="c0">ln(1 </span><span class="c4">&minus; t</span><span class="c0">)</span><span class="c21">. </span><span class="c9">(6) </span></p><p class="c8"><span class="c0">We immediately observe that </span><span class="c4">r </span><span class="c0">= ln(1 </span><span class="c4">&minus; t</span><span class="c12">R</span><span class="c0">)</span><span class="c4">/ </span><span class="c0">ln(1 </span><span class="c4">&minus; t</span><span class="c0">) goes to 0 as </span><span class="c4">t </span><span class="c0">approaches 0. For </span><span class="c4">R </span><span class="c0">= 3, </span><span class="c4">r &asymp; </span><span class="c0">0</span><span class="c4">.</span><span class="c0">078 when </span><span class="c4">t </span><span class="c0">= 0</span><span class="c4">.</span><span class="c0">3, 0</span><span class="c4">.</span><span class="c0">0095 when </span><span class="c4">t </span><span class="c0">= 0</span><span class="c4">.</span><span class="c0">1, and 0</span><span class="c4">.</span><span class="c0">00062 when </span><span class="c4">t </span><span class="c0">= 0</span><span class="c4">.</span><span class="c0">025, and </span><span class="c4">r </span><span class="c0">becomes even smaller as </span><span class="c4">R </span><span class="c0">increases. Since the efficiency of the overall algorithm is strongly related to the number of false positive candidates, and the probability of generating false positives is controlled by </span><span class="c4">R </span><span class="c0">in Min-Hash and </span><span class="c4">S </span><span class="c0">in AT- LAS, we can interpret Equation (6) to mean that Min-Hash requires a significant increase in </span><span class="c4">J </span><span class="c0">to compensate for a small increase in </span><span class="c4">R</span><span class="c0">. This suggests that ATLAS is more efficient than Min-Hash for lower similarity thresholds. In Section 6, we experimentally show that ATLAS is generally faster than Min-Hash when </span><span class="c4">t &le; </span><span class="c0">0</span><span class="c4">.</span><span class="c0">5. </span></p><p class="c8"><span class="c0">We show these effects in a simulation-based analysis that uses datasets from Section 6. Consider the probability of selecting a pair </span><span class="c4">{x, y} </span><span class="c0">as a candidate. We set the expected value of </span><span class="c7">Pr</span><span class="c0">[</span><span class="c4">{x, y} &isin; C</span><span class="c12">V </span><span class="c0">] when </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) = </span><span class="c4">t </span><span class="c0">to 0</span><span class="c4">.</span><span class="c0">975, and plot the graphs for </span><span class="c4">t </span><span class="c0">= 0</span><span class="c4">.</span><span class="c0">05, </span><span class="c4">t </span><span class="c0">= 0</span><span class="c4">.</span><span class="c0">1, and </span><span class="c4">t </span><span class="c0">= 0</span><span class="c4">.</span><span class="c0">2 in Figure 1, with the parameters that minimize the running times of the algorithms. </span></p><p class="c2"><span class="c0">Note that Min-Hash generates fewer false positive candi- date pairs than ATLAS does when the similarity is close to </span><span class="c4">t</span><span class="c0">, but many more false positives when the similarity is very low. This is because the </span><span class="c4">S </span><span class="c0">in the optimal set of parameters for ATLAS is always higher than the </span><span class="c4">R </span><span class="c0">for Min-Hash. While ATLAS&rsquo;s candidate generation algorithm tends to generate more vector pairs with similarity near </span><span class="c4">t</span><span class="c0">, the number of such vector pairs is, in general, much smaller than the number of vector pairs with similarity significantly less than </span><span class="c4">t</span><span class="c0">. From the graphs, we can also see why the speed-up of ATLAS relative to Min-Hash decreases as </span><span class="c4">t </span><span class="c0">increases. </span><span class="c23">3.2 Candidate Filtering </span></p><p class="c8"><span class="c0">Given the candidate vector pairs from the output of the candidate generation algorithm, we are interested in find- ing a fast and reliable way to discard vector pairs with low similarity values. This is part of the second stage in a typi- cal similarity search algorithm: verifying if the similarity of each candidate pair is indeed above the threshold. </span></p><p class="c8"><span class="c0">To solve this problem, we consider all </span><span class="c4">n</span><span class="c0">(</span><span class="c4">n&minus;</span><span class="c0">1)</span><span class="c4">/</span><span class="c0">2 potential vector pairs as a whole. We can often divide these vector pairs into two sets, </span><span class="c4">A </span><span class="c0">and </span><span class="c4">B</span><span class="c0">, such that the similarity be- tween any vector pairs in </span><span class="c4">A </span><span class="c0">is at least </span><span class="c4">t</span><span class="c12">&lowast; </span><span class="c0">= </span><span class="c4">&gamma;t </span><span class="c0">(0 </span><span class="c4">&lt;&gamma;&lt; </span><span class="c0">1) and </span><span class="c4">B </span><span class="c0">contains the remaining vector pairs. Intuitively, </span><span class="c4">t</span><span class="c12">&lowast; </span><span class="c0">is a threshold between noise and non-noise; thus, we expect </span><span class="c4">|B| </span><span class="c0">to be much greater than </span><span class="c4">|A|</span><span class="c0">. We could greatly reduce the running time of a similarity search algorithm by reli- ably discarding vector pairs in </span><span class="c4">B</span><span class="c0">. We introduce a concept, </span><span class="c4">Threshold Sensitive Projection (TSP)</span><span class="c0">, to capture this intu- ition.</span><span class="c1">Definition 1. </span><span class="c22">A TSP family </span><span class="c1">F </span><span class="c22">is defined for a vector space </span><span class="c4">V</span><span class="c0">, a similarity function </span><span class="c4">sim</span><span class="c0">, two similarity thresholds </span><span class="c4">t &gt; t</span><span class="c12">&lowast; </span><span class="c4">&gt; </span><span class="c0">0, a probability of error </span><span class="c4">&epsilon; &gt; </span><span class="c0">0, an approximation factor </span><span class="c4">c &gt; </span><span class="c0">0, and a threshold </span><span class="c4">p</span><span class="c12">&lowast; </span><span class="c4">&isin; </span><span class="c76">R</span><span class="c0">. Here, </span><span class="c4">F </span><span class="c0">is a family of functions </span><span class="c4">h </span><span class="c0">: (</span><span class="c4">V, V</span><span class="c0">) </span><span class="c4">&rarr; </span><span class="c76">R </span><span class="c0">such that for any two points </span><span class="c4">x, y &isin; V</span><span class="c0">, and a function </span><span class="c4">h </span><span class="c0">chosen uniformly at random from </span><span class="c4">F</span><span class="c0">: </span></p><p class="c2"><span class="c0">1. If </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&gt; t</span><span class="c0">, then </span><span class="c7">Pr </span><span class="c0">[</span><span class="c4">h</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&isin; </span><span class="c0">[</span><span class="c4">p</span><span class="c12">&lowast;</span><span class="c4">,&infin;</span><span class="c0">)] </span><span class="c4">&ge; </span><span class="c0">1 </span><span class="c4">&minus; &epsilon;</span><span class="c0">, </span></p><p class="c2"><span class="c0">ln(1 </span><span class="c4">&minus; t</span><span class="c0">) </span><span class="c21">J &minus; </span><span class="c9">ln </span><span class="c21">S </span></p><p class="c2"><span class="c0">2. If </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&lt; t</span><span class="c12">&lowast;</span><span class="c0">, then </span><span class="c7">Pr</span><span class="c0">[</span><span class="c4">h</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&isin; </span><span class="c0">[</span><span class="c4">p</span><span class="c12">&lowast;</span><span class="c4">,&infin;</span><span class="c0">)] </span><span class="c4">&le; c&epsilon;</span><span class="c0">. </span></p><p class="c8"><span class="c0">Essentially, at </span><span class="c4">p</span><span class="c12">&lowast; </span><span class="c0">a </span><span class="c4">phase transition </span><span class="c0">occurs; </span><span class="c4">{x, y} </span><span class="c0">goes from almost surely being noise (i.e., </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&lt; t</span><span class="c12">&lowast;</span><span class="c0">), to al- most surely being interesting (i.e., </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&gt; t</span><span class="c0">). </span></p><p class="c8"><span class="c0">The above definition is similar to the definition of LSH families, but a LSH family is defined for individual vectors, whereas a TSP family is defined for pairs of vectors. </span></p><p class="c2"><span class="c0">A TSP family is useful only if we can quickly evaluate </span><span class="c4">h</span><span class="c0">. We formalize this in the following definition: </span></p><p class="c8"><span class="c4">Definition 2. </span><span class="c0">A family of TSP functions </span><span class="c4">F </span><span class="c0">is </span><span class="c4">interesting </span><span class="c0">if for </span><span class="c4">n </span><span class="c0">vectors whose average length is </span><span class="c4">L</span><span class="c0">, the expected time of evaluating </span><span class="c4">h</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) for </span><span class="c4">m </span><span class="c0">pairs of vectors among them is </span><span class="c4">O</span><span class="c0">(</span><span class="c4">nL</span><span class="c0">) + </span><span class="c4">o</span><span class="c0">(</span><span class="c4">mL</span><span class="c0">). </span></p><p class="c8"><span class="c0">Essentially, this definition says that evaluating </span><span class="c4">h </span><span class="c0">for all vector pairs in the candidate set should be asymptotically faster than performing pair-wise comparisons. It is impor- tant to note that </span><span class="c4">m </span><span class="c0">is generally much larger than </span><span class="c4">n</span><span class="c0">. </span></p><p class="c8"><span class="c0">One TSP family can be trivially derived from locality sen- sitive hashing. In this TSP family, </span><span class="c4">h </span><span class="c0">is just the number of common elements between the signatures of two vectors formed out of </span><span class="c4">R </span><span class="c0">LSH tokens. When the threshold </span><span class="c4">t </span><span class="c0">is low, this approach either takes too much time or does not give us enough discriminative power. Another TSP family is only applicable to binary vectors, but offers better discriminative power. This is the TSP family that we use in our algorithm, and we discuss it in the following subsection. </span></p><p class="c2"><span class="c51">3.2.1 A TSP Family for Binary Vectors </span></p><p class="c8"><span class="c0">Consider two binary vectors, </span><span class="c4">x </span><span class="c0">and </span><span class="c4">y</span><span class="c0">. If </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&gt; t</span><span class="c0">, then we can use Equations (1) and (2) to lower bound their dot product, </span><span class="c4">dot</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">), by </span><span class="c4">o </span><span class="c0">= </span><span class="c4">pd </span><span class="c0">(0 </span><span class="c4">&le; p &le; </span><span class="c0">1); on the other hand, if </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&lt; t</span><span class="c12">&lowast;</span><span class="c0">, then we could upper bound </span><span class="c4">dot</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) by </span><span class="c4">&delta;o </span><span class="c0">= </span><span class="c4">&delta;pd </span><span class="c0">(0 </span><span class="c4">&le; &delta; &lt; </span><span class="c0">1). Here, </span><span class="c4">p </span><span class="c0">and </span><span class="c4">&delta; </span><span class="c0">are two parameters whose values are determined by the similarity function </span><span class="c4">sim </span><span class="c0">and the two similarity thresholds, </span><span class="c4">t </span><span class="c0">and </span><span class="c4">t</span><span class="c12">&lowast;</span><span class="c0">. </span></p><p class="c8"><span class="c0">After applying a random permutation </span><span class="c4">&pi;</span><span class="c0">, we check the first </span><span class="c4">&alpha;d </span><span class="c0">dimensions of </span><span class="c4">&pi;</span><span class="c0">(</span><span class="c4">x</span><span class="c0">) and </span><span class="c4">&pi;</span><span class="c0">(</span><span class="c4">y</span><span class="c0">), and we discard </span><span class="c4">{x, y} </span><span class="c0">if the number of common non-zero dimensions is less than </span><span class="c4">&beta;o </span><span class="c0">= </span><span class="c4">&beta;pd </span><span class="c0">(</span><span class="c4">&alpha;&gt;&beta;</span><span class="c0">). Note that we have </span></p><p class="c49"><span class="c7">Pr</span><span class="c0">[Keep </span><span class="c4">| sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&gt; t</span><span class="c0">] = 1 </span><span class="c4">&minus; H</span><span class="c3">L</span><span class="c0">(</span><span class="c4">pd, d, &alpha;d, &beta;pd</span><span class="c0">)</span><span class="c4">, </span><span class="c7">Pr</span><span class="c0">[Discard </span><span class="c4">| sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&lt; t</span><span class="c12">&lowast;</span><span class="c0">] = 1 </span><span class="c4">&minus; H</span><span class="c3">H</span><span class="c0">(</span><span class="c4">&delta;pd, d, &alpha;d, &beta;pd</span><span class="c0">)</span><span class="c4">. </span></p><p class="c8"><span class="c0">Here, we use </span><span class="c4">H</span><span class="c3">L</span><span class="c0">(</span><span class="c4">M,N,n,k</span><span class="c0">) and </span><span class="c4">H</span><span class="c3">H</span><span class="c0">(</span><span class="c4">M,N,n,k</span><span class="c0">) to denote </span><span class="c7">Pr</span><span class="c0">[</span><span class="c4">X &le; k</span><span class="c0">] and </span><span class="c7">Pr</span><span class="c0">[</span><span class="c4">X &ge; k</span><span class="c0">] for a random variable </span><span class="c4">X </span><span class="c0">taken from a hypergeometric distribution with parameters </span><span class="c4">N</span><span class="c0">, </span><span class="c4">M</span><span class="c0">, and </span><span class="c4">n</span><span class="c0">, where </span></p><p class="c2"><span class="c7">Pr</span><span class="c0">[</span><span class="c4">X </span><span class="c0">= </span><span class="c4">k</span><span class="c0">] = </span><span class="c4">h</span><span class="c0">(</span><span class="c4">M,N,n,k</span><span class="c0">) = </span></p><p class="c2"><span class="c36">(</span><span class="c1">Mk </span></p><p class="c2"><span class="c36">)(</span><span class="c1">N &minus; M </span></p><p class="c2"><span class="c4">n &minus; k </span></p><p class="c2"><span class="c36">)(</span><span class="c1">Nn</span><span class="c36">)</span><span class="c11">&minus;</span><span class="c15">1</span><span class="c1">, </span></p><p class="c2"><span class="c4">H</span><span class="c11">L</span><span class="c22">(</span><span class="c1">M,N,n,k</span><span class="c22">) = </span></p><p class="c2"><span class="c39">&sum;</span><span class="c3">k</span><span class="c11">i</span><span class="c15">=0 </span></p><p class="c2"><span class="c4">h</span><span class="c0">(</span><span class="c4">M,N,n,i</span><span class="c0">)</span><span class="c4">,</span><span class="c0">and </span></p><p class="c2"><span class="c4">H</span><span class="c3">H</span><span class="c0">(</span><span class="c4">M,N,n,k</span><span class="c0">) = </span></p><p class="c2"><span class="c39">&sum;</span><span class="c3">n</span><span class="c11">i</span><span class="c15">=</span><span class="c11">k </span></p><p class="c2"><span class="c4">h</span><span class="c0">(</span><span class="c4">M,N,n,i</span><span class="c0">)</span><span class="c4">. </span></p><p class="c2"><span class="c0">We require </span><span class="c4">&alpha;&gt;&beta; </span><span class="c0">and </span><span class="c4">&delta;&alpha; &lt; &beta; </span><span class="c0">for the bounds to be mean- ingful, and we will assume so in the remainder of the paper. </span></p><p class="c2"><span class="c0">Lemma 1. </span><span class="c4">(A modification of Chv&aacute;tal&rsquo;s bound on the tail probability of hypergeometric distributions) </span></p><p class="c2"><span class="c0">1001 </span></p><p class="c2"><span class="c0">1 </span><span class="c4">&minus; p &minus; t</span><span class="c18">)</span><span class="c14">1</span><span class="c3">&minus;p&minus;t</span><span class="c18">)</span><span class="c3">n</span><span class="c22">(7) </span></p><p class="c2"><span class="c4">&le; e</span><span class="c12">&minus;</span><span class="c6">2</span><span class="c12">t</span><span class="c58">2</span><span class="c3">n </span><span class="c22">(8) </span></p><p class="c2"><span class="c4">for p </span><span class="c0">= </span><span class="c4">M/N and t </span><span class="c0">= </span><span class="c4">k/n &minus; p (t &ge; </span><span class="c0">0</span><span class="c4">); </span></p><p class="c2"><span class="c4">H</span><span class="c3">L</span><span class="c0">(</span><span class="c4">M,N,n,k</span><span class="c0">) </span><span class="c4">&le; </span></p><p class="c2"><span class="c36">(</span><span class="c39">( </span><span class="c1">p </span></p><p class="c2"><span class="c18">)</span><span class="c14">1</span><span class="c3">&minus;p&minus;t</span><span class="c18">)</span><span class="c3">n</span><span class="c0">1 </span><span class="c4">&minus; p &minus; t</span><span class="c22">(9) </span></p><p class="c2"><span class="c4">&le; e</span><span class="c12">&minus;</span><span class="c6">2</span><span class="c12">t</span><span class="c58">2</span><span class="c3">n </span><span class="c22">(10) </span></p><p class="c2"><span class="c4">for p </span><span class="c0">= (</span><span class="c4">N &minus; M</span><span class="c0">)</span><span class="c4">/N and t </span><span class="c0">= (</span><span class="c4">n &minus; k</span><span class="c0">)</span><span class="c4">/n &minus; p (t &ge; </span><span class="c0">0</span><span class="c4">). </span></p><p class="c8"><span class="c0">Proof. The bound for </span><span class="c4">H</span><span class="c11">H</span><span class="c22">(</span><span class="c1">M,N,n,k</span><span class="c22">) directly follows </span><span class="c0">from [15]. Using the symmetric relation </span><span class="c4">h</span><span class="c0">(</span><span class="c4">M,N,n,k</span><span class="c0">) = </span><span class="c4">h</span><span class="c0">(</span><span class="c4">N &minus; M,N,n,n &minus; k</span><span class="c0">), we obtain the bound for the lower tail of hypergeometric distribution. </span></p><p class="c2"><span class="c0">We can now prove the main theorem using Lemma 1. </span></p><p class="c8"><span class="c0">Theorem 1. </span><span class="c4">Given pd, the lower bound on the number of non-zero dimensions for vectors with similarity t, and &delta;pd, the upper bound on the number of non-zero dimensions for vectors with similarity t</span><span class="c12">&lowast; </span><span class="c0">(</span><span class="c4">t</span><span class="c12">&lowast; </span><span class="c4">&lt; t</span><span class="c0">)</span><span class="c4">, one can decide whether the similarity between the two, sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">)</span><span class="c4">, is above t or is below t</span><span class="c12">&lowast; </span><span class="c4">by examining a fixed proportion of the d dimensions, &alpha; </span><span class="c0">= </span><span class="c4">O</span><span class="c0">( </span><span class="c3">p</span><span class="c58">2</span><span class="c14">(1</span><span class="c3">&minus;&delta;</span><span class="c14">)</span><span class="c12">&minus; </span><span class="c6">ln </span><span class="c12">&epsilon; </span></p><p class="c2"><span class="c58">2</span><span class="c3">d</span><span class="c9">) </span><span class="c21">with probability of success </span><span class="c9">1 </span><span class="c21">&minus; &epsilon;. </span></p><p class="c8"><span class="c0">Theorem 1 states that as the average length of vectors increases, </span><span class="c4">pd </span><span class="c0">also increases, so the absolute number of di- mensions that we need to examine </span><span class="c4">decreases</span><span class="c0">. </span></p><p class="c8"><span class="c0">We now show experimentally that the actual value we need for </span><span class="c4">&alpha; </span><span class="c0">is quite small. We set </span><span class="c4">d </span><span class="c0">to 1,000,000, the prob- ability of discarding </span><span class="c4">{x, y} </span><span class="c0">with similarity </span><span class="c4">t </span><span class="c0">to 0</span><span class="c4">.</span><span class="c0">01, and the probability of keeping </span><span class="c4">{x, y} </span><span class="c0">with similarity </span><span class="c4">t</span><span class="c12">&lowast; </span><span class="c0">to 0</span><span class="c4">.</span><span class="c0">05. In Figure 2, we first plot the values of </span><span class="c4">&alpha; </span><span class="c0">while varying the expected number of common dimensions, </span><span class="c4">pd</span><span class="c0">, then plot the values of </span><span class="c4">&alpha; </span><span class="c0">while varying </span><span class="c4">&delta;</span><span class="c0">. </span></p><p class="c8"><span class="c0">In practice, Chv&aacute;tal&rsquo;s stronger bounds ((7) and (9)) give us a good approximation of the true tail probability. It is best to choose multiple values of </span><span class="c4">&delta; </span><span class="c0">and compute the corre- sponding value of </span><span class="c4">&alpha; </span><span class="c0">and </span><span class="c4">&beta; </span><span class="c0">based on the data distribution. We show our algorithm for choosing parameters in Section 5.1. </span></p><p class="c2"><span class="c37">Min-Hash </span><span class="c16">1 </span><span class="c37">Min-Hash </span><span class="c16">1 </span><span class="c37">Min-Hash </span><span class="c67">Atlas </span></p><p class="c2"><span class="c16">0.1 </span><span class="c67">Atlas </span></p><p class="c2"><span class="c16">0.1 </span><span class="c67">Atlas </span></p><p class="c2"><span class="c43">y tilibabor</span><span class="c20">P</span><span class="c16">0.01 0.001 0.0001 1e-005 </span><span class="c37">1e-006 0 0.02 0.04 0.06 0.08 0.1 </span></p><p class="c2"><span class="c16">0.01 0.001 0.0001 1e-005 1e-006 </span><span class="c37">1e-007 0 0.04 0.08 0.12 0.16 0.2 </span><span class="c20">Jaccard Similarity </span></p><p class="c2"><span class="c7">Figure 1: Probability of </span><span class="c4">{x, y} &isin; C</span><span class="c12">V </span></p><p class="c2"><span class="c4">H</span><span class="c3">H</span><span class="c0">(</span><span class="c4">M,N,n,k</span><span class="c0">) </span><span class="c4">&le; </span></p><p class="c2"><span class="c43">y tilibabor</span><span class="c20">PJaccard Similarity </span></p><p class="c2"><span class="c43">y tilibabor</span><span class="c20">PJaccard Similarity </span></p><p class="c2"><span class="c36">(</span><span class="c39">( </span><span class="c1">p </span></p><p class="c2"><span class="c37">1e-007 </span><span class="c16">1e-006 1e-005 0.0001 0.001 0.01 0.1 1 </span><span class="c37">0 0.01 0.02 0.03 0.04 0.05 </span></p><p class="c2"><span class="c4">p </span><span class="c0">+ </span><span class="c4">t</span><span class="c18">)</span><span class="c3">p</span><span class="c14">+</span><span class="c3">t </span><span class="c18">( </span><span class="c0">1 </span><span class="c4">&minus; p </span></p><p class="c2"><span class="c4">p </span><span class="c0">+ </span><span class="c4">t</span><span class="c18">)</span><span class="c3">p</span><span class="c14">+</span><span class="c3">t </span><span class="c18">( </span><span class="c0">1 </span><span class="c4">&minus; p </span></p><p class="c2"><span class="c44">0 </span><span class="c31">0.09 0.08 0.07 0.06 0.05 </span><span class="c44">delta = 0.1 </span><span class="c62">delta delta = = 0.2 0.3 delta delta = = 0.4 0.5 </span><span class="c31">0.04 0.03 0.02 1 0.9 0.8 0.7 </span><span class="c44">pd = 1000 </span><span class="c62">pd pd = = 2000 3000 </span><span class="c45">a hpl</span><span class="c17">a</span><span class="c31">0.6 0.5 0.4 0.3 0.2 0.01 0.1 </span><span class="c44">1000 2000 3000 4000 5000 </span></p><p class="c2"><span class="c44">0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 </span><span class="c45">a hpl</span><span class="c17">a</span><span class="c62">pd pd = = 4000 5000 </span></p><p class="c2"><span class="c17">pd </span></p><p class="c2"><span class="c17">delta </span></p><p class="c2"><span class="c7">Figure 2: Relationships Between </span><span class="c4">pd</span><span class="c7">, </span><span class="c4">&alpha;</span><span class="c7">, and </span><span class="c4">&delta; </span></p><p class="c2"><span class="c23">4. VECTOR PRE-CLUSTERING </span></p><p class="c2"><span class="c0">Vectors in real datasets possess cluster-like structures. Text documents can be described using a set of topics. The ma- jority of photographs are taken near objects of interests, such as landmarks. In social networks and recommendation sys- tems, we can categorize the users into various communities. In these cases, two vectors are likely to be similar if and only if they share at least one common &lsquo;topic&rsquo;. To exploit this property, we need an algorithm that reports if cluster-like structures exist, and if so, assigns vectors to clusters such that we only need to search for similar vectors within each cluster. We define the following subproblem: </span></p><p class="c24"><span class="c0">Problem 3. </span><span class="c4">(Vector Pre-Clustering) Given V , sim, and t, such find that m (possibly </span><span class="c0">(1</span><span class="c4">&minus;&delta;</span><span class="c14">3</span><span class="c12">&lowast;</span><span class="c9">) </span><span class="c21">of </span><span class="c4">overlapping) </span><span class="c21">all vector pairs </span><span class="c4">subsets </span><span class="c21">{x, y} </span><span class="c4">of V </span><span class="c21">with </span><span class="c4">, V</span><span class="c14">1</span><span class="c4">,V</span><span class="c14">2</span><span class="c4">,...,V</span><span class="c3">m</span><span class="c4">, </span><span class="c21">sim</span><span class="c9">(</span><span class="c21">x, y</span><span class="c9">) </span><span class="c21">&gt; t </span><span class="c4">are contained in at least one of the m subsets. </span></p><p class="c2"><span class="c0">and present our solution in the following subsection. </span><span class="c23">4.1 A Pre-Clustering Algorithm </span></p><p class="c8"><span class="c0">Traditional clustering algorithms assume that the similar- ities between objects have been pre-calculated and are given to the clustering algorithms as an input. Without such infor- mation, we instead assume that the vectors can be divided into </span><span class="c4">m dense regions</span><span class="c0">. These dense regions are defined such that for </span><span class="c4">x </span><span class="c0">and </span><span class="c4">y </span><span class="c0">taken from two different regions, the prob- ability that </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&gt; t </span><span class="c0">is close to zero. When the size of the smallest dense region is sufficiently large, Guha et al.&rsquo;s result allows us to identify these regions by sampling: </span></p><p class="c2"><span class="c0">Theorem 2. </span><span class="c4">([22]) For a dense region u, if the sample size s satisfies </span></p><p class="c2"><span class="c4">s &ge; fn </span><span class="c0">+ </span><span class="c21">n </span></p><p class="c2"><span class="c4">&delta;</span><span class="c9">) </span></p><p class="c2"><span class="c4">then the probability that the sample contains fewer than f|u| points belonging to u is less than &delta;, </span><span class="c0">0 </span><span class="c4">&le; &delta; &le; </span><span class="c0">1</span><span class="c4">. </span></p><p class="c2"><span class="c0">1002 </span></p><p class="c2"><span class="c4">|u| </span><span class="c9">log (1 </span></p><p class="c2"><span class="c4">&delta;</span><span class="c9">) + </span><span class="c21">n </span></p><p class="c2"><span class="c4">|u|</span><span class="c18">&radic; </span></p><p class="c2"><span class="c0">(log (</span><span class="c9">1 </span></p><p class="c2"><span class="c4">&delta;</span><span class="c9">))</span><span class="c6">2 </span><span class="c0">+ 2</span><span class="c4">f|u| </span><span class="c0">log (</span><span class="c9">1 </span></p><p class="c2"><span class="c0">We can bound the probability of selecting less than </span><span class="c4">f|u| </span><span class="c0">points from any of the </span><span class="c4">m </span><span class="c0">regions by </span><span class="c4">m&delta;</span><span class="c0">. </span></p><p class="c8"><span class="c0">It remains then to find a succinct representation for the dense regions and to identify which points belong to which regions. Common clustering approaches like </span><span class="c4">k</span><span class="c0">-means tend to output spherical shaped clusters with similar radiuses. Real datasets seldom exhibit such properties; our experi- ments confirmed that such approaches perform poorly. In- stead, we note that the density of points in a region tends to decrease as the distance from the center increases. This motivates us to apply a clustering algorithm, Affinity Prop- agation [20], to find these regions. </span></p><p class="c8"><span class="c0">Having obtained the region centers, we assign each point to its top </span><span class="c4">T </span><span class="c0">closest regions (as measured by the distance to the region center). When </span><span class="c4">T </span><span class="c0">equals 1, our pre-clustering scheme is roughly the same as a Voronoi decomposition, since each resulting region corresponds to a high dimensional Voronoi cell. We need to increase </span><span class="c4">T </span><span class="c0">for the cases in which a pair of points are contained in two different Voronoi cells but are close to their common boundary. In our experiments, setting </span><span class="c4">T </span><span class="c0">to 3 or 4 usually gives excellent results. </span></p><p class="c8"><span class="c0">Finally, we need a way to verify if the resulting clusters are accurate enough for similarity search. We can estimate this by sampling a small number of the vector pairs with similar- ity above </span><span class="c4">t </span><span class="c0">and calculating the proportion among them that are covered by the clusters. We use a LSH-based algorithm to efficiently generate these samples. For Jaccard similarity, we use Min-Hash as specified by the equation (4). For Co- sine similarity, we use random projections-based LSH which is given by the equation </span></p><p class="c2"><span class="c7">Pr </span><span class="c0">[</span><span class="c4">sgn</span><span class="c0">(</span><span class="c4">x &middot; r</span><span class="c0">) = </span><span class="c4">sgn</span><span class="c0">(</span><span class="c4">y &middot; r</span><span class="c0">)] = 1 </span><span class="c4">&minus; </span><span class="c21">&theta;</span><span class="c9">(</span><span class="c21">x, y</span><span class="c9">) </span></p><p class="c8"><span class="c4">&pi; </span><span class="c82">&sim; </span><span class="c9">Cosine(</span><span class="c21">x, y</span><span class="c9">)</span><span class="c0">(11) where </span><span class="c4">r </span><span class="c0">is a random point on the unit hypersphere [21], </span><span class="c4">&theta;</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) is the angle between the two vectors, and </span><span class="c4">sgn</span><span class="c0">(</span><span class="c4">x</span><span class="c0">) takes 1 when </span><span class="c4">x &ge; </span><span class="c0">0 and </span><span class="c4">&minus;</span><span class="c0">1 otherwise. Since LSH-based al- gorithms are more prone to generate vector pairs with higher similarities, we need to adjust the weights placed on the samples. In practice, a sample size of 500 usually provides a good estimation. </span></p><p class="c8"><span class="c0">Algorithm 3 describes an outline of our method. The al- gorithm consists of three stages: (1) random sampling, (2) clustering, and (3) vector labeling and verification. Note that if our proposed clustering procedure fails to work, Al- gorithm 3 will simply return a single cluster containing all </span><span class="c4">n </span><span class="c0">points. </span></p><p class="c2"><span class="c23">5. INTEGRATION AND SCALABILITY </span></p><p class="c8"><span class="c0">We can integrate the three algorithms that we have pre- viously discussed in Sections 3 and 4 as shown in Algorithm 4. While we need to find suitable values of </span><span class="c4">&delta;</span><span class="c12">&lowast; </span><span class="c0">for each of the three subproblems in order to integrate them, we find that the algorithm is not extremely sensitive to the values used, and a relatively rough estimation tends to work well in practice. </span></p><p class="c8"><span class="c0">Note that we can use the vector pre-clustering result in two ways. The first way is the procedure outlined in Al- gorithm 4. This is generally efficient, but cannot be easily parallelized since it may be difficult to balance the work- load across machines. The second way is to check the vector pre-clustering result after a candidate pair is generated, and </span></p><p class="c2"><span class="c0">discard it if all clusters that the two vectors belong to are different. </span></p><p class="c2"><span class="c23">5.1 Parameter Estimation </span></p><p class="c2"><span class="c51">5.1.1 Candidate Generation </span></p><p class="c8"><span class="c0">In this section, we discuss how to find the optimal values of </span><span class="c4">S </span><span class="c0">and </span><span class="c4">K </span><span class="c0">that both minimize the running time of the algorithm and ensure an expected recall rate of 1 </span><span class="c4">&minus; &delta;</span><span class="c12">&lowast;</span><span class="c14">1</span><span class="c9">. Our </span><span class="c0">conclusion can be applied both to our algorithm, ATLAS, and to locality sensitive hashing-based algorithms, such as Min-Hash. </span></p><p class="c8"><span class="c0">We cannot obtain good values for </span><span class="c4">S </span><span class="c0">and </span><span class="c4">K </span><span class="c0">without hav- ing some prior knowledge about the data distribution. (3) only shows what </span><span class="c4">S </span><span class="c0">and </span><span class="c4">K</span><span class="c0">s are necessary for the expected recall rate for a single vector pair </span><span class="c4">{x, y} </span><span class="c0">to be 1 </span><span class="c4">&minus; &delta;</span><span class="c12">&lowast;</span><span class="c14">1 </span><span class="c9">when </span><span class="c0">Jaccard(</span><span class="c4">x, y</span><span class="c0">) is </span><span class="c4">t</span><span class="c0">. It is impossible to directly estimate the recall rate for Cosine similarity. </span></p><p class="c8"><span class="c0">In our experiments, we estimate </span><span class="c4">S </span><span class="c0">and </span><span class="c4">K </span><span class="c0">by running the algorithms on a small sample dataset. We find that the parameters on a sample set of size between 10,000 and 20,000 lead to similar recall rates when we apply them on the larger datasets consisting of millions of records. These parameters, </span></p><p class="c2"><span class="c0">1003 </span></p><p class="c2"><span class="c7">Algorithm 3 </span><span class="c0">ATLAS: Vector Pre-Clustering </span></p><p class="c24"><span class="c7">Input: </span><span class="c4">V </span><span class="c0">; </span><span class="c4">&delta;</span><span class="c12">&lowast;</span><span class="c14">3</span><span class="c9">; number of samples </span><span class="c21">r</span><span class="c9">; estimated upper bound </span><span class="c4">M</span><span class="c11">max </span><span class="c22">and lower bound </span><span class="c1">M</span><span class="c11">min </span><span class="c22">on the number of clusters </span><span class="c7">Output: </span><span class="c0">(</span><span class="c4">V</span><span class="c14">1</span><span class="c4">,V</span><span class="c14">2</span><span class="c4">,...,V</span><span class="c3">m</span><span class="c0">) </span></p><p class="c2"><span class="c0">1: </span><span class="c4">V &larr; </span><span class="c0">a random sample of size </span><span class="c4">r </span><span class="c0">chosen from </span><span class="c4">V </span><span class="c0">2: </span><span class="c7">repeat </span><span class="c0">3: adjust the parameters of the clustering algorithm 4: (</span><span class="c4">V </span><span class="c15">1</span><span class="c4">,V </span><span class="c15">2</span><span class="c4">,...,V </span><span class="c11">m</span><span class="c0">) </span><span class="c4">&larr; </span><span class="c0">run affinity propagation algorithm </span></p><p class="c2"><span class="c0">on </span><span class="c4">V </span><span class="c0">(see [20] for details). 5: </span><span class="c7">until </span><span class="c4">m </span><span class="c0">is between </span><span class="c4">M</span><span class="c11">min </span><span class="c22">and </span><span class="c1">M</span><span class="c11">max</span><span class="c22">. </span><span class="c0">6: </span><span class="c4">&forall; x &isin; V </span><span class="c0">, </span><span class="c4">&forall; </span><span class="c0">1 </span><span class="c4">&le; i &le; m</span><span class="c0">, calculate </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, center</span><span class="c0">(</span><span class="c4">V </span><span class="c11">i </span><span class="c0">)) 7: Find sample pairs with </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&gt; t </span><span class="c0">and their weights 8: </span><span class="c4">T &larr; </span><span class="c0">1 9: </span><span class="c7">repeat </span><span class="c0">10: assign each point in </span><span class="c4">V </span><span class="c0">to its closest </span><span class="c4">T </span><span class="c0">regions in order </span></p><p class="c2"><span class="c0">to obtain (</span><span class="c4">V</span><span class="c14">1</span><span class="c4">,V</span><span class="c14">2</span><span class="c4">,...,V</span><span class="c3">m</span><span class="c0">) 11: </span><span class="c7">if </span><span class="c0">the estimated recall rate is above 1 </span><span class="c4">&minus; &delta;</span><span class="c12">&lowast;</span><span class="c14">3 </span><span class="c41">then </span><span class="c0">12: </span><span class="c7">return </span><span class="c0">(</span><span class="c4">V</span><span class="c14">1</span><span class="c4">,V</span><span class="c14">2</span><span class="c4">,...,V</span><span class="c3">m</span><span class="c0">) 13: </span><span class="c7">end if </span><span class="c0">14: </span><span class="c4">T &larr; T </span><span class="c0">+ 1 15: </span><span class="c7">until </span><span class="c4">T </span><span class="c0">or the size of an element </span><span class="c4">V</span><span class="c3">i </span><span class="c4">&isin; </span><span class="c0">(</span><span class="c4">V</span><span class="c14">1</span><span class="c4">,V</span><span class="c14">2</span><span class="c4">,...,V</span><span class="c3">m</span><span class="c0">) </span></p><p class="c2"><span class="c0">is sufficiently large 16: </span><span class="c7">return </span><span class="c0">(</span><span class="c4">V </span><span class="c0">) </span></p><p class="c2"><span class="c7">Algorithm 4 </span><span class="c0">ATLAS: Framework </span></p><p class="c2"><span class="c0">1: Pre-cluster </span><span class="c4">V </span><span class="c0">into (</span><span class="c4">V</span><span class="c15">1</span><span class="c1">,V</span><span class="c15">2</span><span class="c1">,...,V</span><span class="c11">m</span><span class="c22">). </span><span class="c0">2: </span><span class="c4">O &larr; &empty; </span><span class="c0">3: </span><span class="c7">for </span><span class="c4">i </span><span class="c0">= 1 to </span><span class="c4">m </span><span class="c7">do </span><span class="c0">4: </span><span class="c4">C</span><span class="c12">V</span><span class="c33">i </span><span class="c1">&larr; </span><span class="c22">candidate vector pairs in </span><span class="c1">V</span><span class="c11">i </span><span class="c0">5: </span><span class="c7">for all </span><span class="c4">{x, y} &isin; C</span><span class="c12">V</span><span class="c33">i </span><span class="c32">do </span><span class="c0">6: </span><span class="c7">if </span><span class="c4">{x, y} </span><span class="c0">passes the filtering test and </span><span class="c4">sim</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&gt; t </span></p><p class="c2"><span class="c7">then </span><span class="c0">7: </span><span class="c4">O &larr; O &cup; {{x, y}} </span><span class="c0">8: </span><span class="c7">end if </span><span class="c0">9: </span><span class="c7">end for </span><span class="c0">10: </span><span class="c7">end for </span><span class="c0">11: </span><span class="c7">return </span><span class="c4">O </span></p><p class="c8"><span class="c0">however, may not minimize the algorithm&rsquo;s running times on the larger datasets. We might need to increase </span><span class="c4">S </span><span class="c0">slightly to reduce the number of candidates for a larger dataset. </span></p><p class="c2"><span class="c51">5.1.2 Candidate Filtering </span></p><p class="c8"><span class="c0">As noted in Section 3.2, it is usually best to select mul- tiple (</span><span class="c4">&alpha;, &beta;</span><span class="c0">) pairs for our candidate filtering algorithm. To generate these pairs, we assume that we have a list of </span><span class="c4">&delta;</span><span class="c0">s, </span><span class="c4">&delta;</span><span class="c15">1</span><span class="c1">,&delta;</span><span class="c15">2</span><span class="c1">,...,&delta;</span><span class="c11">t</span><span class="c22">, that can be used to separate </span><span class="c1">t </span><span class="c22">and different </span><span class="c1">t</span><span class="c3">&lowast;</span><span class="c22">s, </span><span class="c0">and we would like to select </span><span class="c4">P </span><span class="c0">among them for generating the (</span><span class="c4">&alpha;, &beta;</span><span class="c0">) pairs. When we know nothing about the data, we can simply use a list like </span><span class="c4">&delta;</span><span class="c14">1 </span><span class="c0">= 0</span><span class="c4">.</span><span class="c0">05</span><span class="c4">, &delta;</span><span class="c14">2 </span><span class="c0">= 0</span><span class="c4">.</span><span class="c0">10</span><span class="c4">,...,&delta;</span><span class="c14">19 </span><span class="c0">= 0</span><span class="c4">.</span><span class="c0">95. We use </span><span class="c4">P </span><span class="c0">= 3 for all our experiments. This value can be increased or decreased depending on the magnitude of </span><span class="c4">pd</span><span class="c0">. </span></p><p class="c8"><span class="c0">We give an outline of the procedure we used for calculating these parameters in Algorithm 5. We assume that the user will give us an upper bound on the probability of discarding vector pairs with similarity above </span><span class="c4">t</span><span class="c0">, </span><span class="c4">p</span><span class="c11">d</span><span class="c22">, and a lower bound on </span><span class="c0">the probability of keeping vector pairs with similarity below </span><span class="c4">t</span><span class="c12">&lowast;</span><span class="c0">, </span><span class="c4">p</span><span class="c11">k</span><span class="c22">. The last step in the algorithm selects (</span><span class="c1">&alpha;</span><span class="c11">i</span><span class="c1">,&beta;</span><span class="c11">i</span><span class="c22">)s with </span><span class="c0">an objective of avoiding applying this filtering procedure too frequently. Other heuristics can be used to replace this step for different datasets. </span></p><p class="c2"><span class="c7">Algorithm 5 </span><span class="c0">Selecting Parameters for ATLAS&rsquo;s Candidate Filtering Algorithm (Threshold Sensitive Projection) </span></p><p class="c2"><span class="c7">Input: </span><span class="c4">d</span><span class="c0">; </span><span class="c4">&delta;</span><span class="c15">1</span><span class="c1">,&delta;</span><span class="c15">2</span><span class="c1">,...,&delta;</span><span class="c11">t </span><span class="c22">(</span><span class="c1">&delta;</span><span class="c15">1 </span><span class="c1">&lt; &delta;</span><span class="c15">2 </span><span class="c1">&lt; ... &lt; &delta;</span><span class="c11">t</span><span class="c22">); </span><span class="c1">pd</span><span class="c22">; </span><span class="c1">p</span><span class="c11">d</span><span class="c22">; </span><span class="c1">p</span><span class="c11">k</span><span class="c22">; </span></p><p class="c2"><span class="c4">P </span><span class="c0">(</span><span class="c4">P &ge; t</span><span class="c0">). </span><span class="c7">Output: </span><span class="c0">(</span><span class="c4">&alpha;</span><span class="c14">1</span><span class="c4">,&beta;</span><span class="c14">1</span><span class="c0">)</span><span class="c4">,</span><span class="c0">(</span><span class="c4">&alpha;</span><span class="c14">2</span><span class="c4">,&beta;</span><span class="c14">2</span><span class="c0">)</span><span class="c4">,..., </span><span class="c0">(</span><span class="c4">&alpha;</span><span class="c3">P </span><span class="c4">,&beta;</span><span class="c3">P</span><span class="c0">) </span></p><p class="c2"><span class="c0">1: </span><span class="c7">for </span><span class="c4">i </span><span class="c0">= 1 to </span><span class="c4">P </span><span class="c7">do </span><span class="c0">2: </span><span class="c4">&alpha;</span><span class="c14">min </span><span class="c4">&larr; </span><span class="c0">0</span><span class="c4">,&alpha;</span><span class="c14">max </span><span class="c4">&larr; </span><span class="c0">1. 3: </span><span class="c7">while </span><span class="c4">&alpha;</span><span class="c15">min </span><span class="c22">+ </span><span class="c1">&epsilon;&lt;&alpha;</span><span class="c15">max </span><span class="c32">do </span><span class="c0">4: </span><span class="c4">&alpha;</span><span class="c14">cur </span><span class="c4">&larr; </span><span class="c0">(</span><span class="c4">&alpha;</span><span class="c14">min </span><span class="c0">+ </span><span class="c4">&alpha;</span><span class="c14">max</span><span class="c0">)</span><span class="c4">/</span><span class="c0">2 5: </span><span class="c4">&beta;</span><span class="c14">cur </span><span class="c4">&larr; </span><span class="c0">the maximum </span><span class="c4">&beta; &isin; </span><span class="c0">(</span><span class="c4">&alpha;</span><span class="c14">cur </span><span class="c4">&middot; &delta;</span><span class="c3">i</span><span class="c4">,&alpha;</span><span class="c14">cur</span><span class="c0">) that sat- </span></p><p class="c2"><span class="c0">isfies </span><span class="c7">Pr</span><span class="c0">[Discard </span><span class="c4">| dot</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&ge; pd</span><span class="c0">] </span><span class="c4">&le; p</span><span class="c11">d </span><span class="c22">for </span><span class="c1">&alpha;</span><span class="c15">cur</span><span class="c22">. </span><span class="c0">6: </span><span class="c7">if Pr</span><span class="c0">[Keep </span><span class="c4">| dot</span><span class="c0">(</span><span class="c4">x, y</span><span class="c0">) </span><span class="c4">&le; &delta;</span><span class="c3">i</span><span class="c4">pd, &alpha;</span><span class="c14">cur</span><span class="c4">,&beta;</span><span class="c14">cur</span><span class="c0">] </span><span class="c4">&le; p</span><span class="c11">k </span><span class="c32">then </span><span class="c0">7: </span><span class="c4">&alpha;</span><span class="c15">min </span><span class="c1">&larr; &alpha;</span><span class="c15">cur </span><span class="c0">8: </span><span class="c4">&alpha;</span><span class="c3">i </span><span class="c4">&larr; &alpha;</span><span class="c14">cur</span><span class="c4">,&beta;</span><span class="c3">i </span><span class="c4">&larr; &beta;</span><span class="c14">cur </span><span class="c0">9: </span><span class="c7">else </span><span class="c0">10: </span><span class="c4">&alpha;</span><span class="c15">max </span><span class="c1">&larr; &alpha;</span><span class="c15">cur </span><span class="c0">11: </span><span class="c7">end if </span><span class="c0">12: </span><span class="c7">end while </span><span class="c0">13: </span><span class="c7">end for </span><span class="c0">14: Select </span><span class="c4">P </span><span class="c0">that </span><span class="c15">1</span><span class="c11">&le;i&le;P&minus;</span><span class="c15">1 </span></p><p class="c2"><span class="c4">min </span><span class="c0">(</span><span class="c4">&alpha;</span><span class="c11">i</span><span class="c1">,&beta;</span><span class="c11">i</span><span class="c22">) pairs, (</span><span class="c1">&alpha;</span><span class="c11">s</span><span class="c40">1</span><span class="c4">,&beta;</span><span class="c11">s</span><span class="c40">1</span><span class="c0">)</span><span class="c4">,...,</span><span class="c0">(</span><span class="c4">&alpha;</span><span class="c11">s</span><span class="c35">P </span><span class="c4">,&beta;</span><span class="c11">s</span><span class="c35">P </span><span class="c0">), such </span></p><p class="c2"><span class="c36">(</span><span class="c1">&alpha;</span><span class="c11">s</span><span class="c35">i</span><span class="c40">+1 </span><span class="c4">&minus; &alpha;</span><span class="c3">s</span><span class="c35">i</span><span class="c36">) </span><span class="c22">is maximized </span></p><p class="c2"><span class="c23">5.2 Scalability </span></p><p class="c8"><span class="c0">For large datasets containing millions of vectors, a simi- larity search algorithm needs to address two issues: (1) how to avoid the memory bottleneck, and (2) how to efficiently use multiple cores on multiple machines. </span></p><p class="c8"><span class="c0">Most similarity search algorithms need to store all vec- tors in memory. If storing each non-zero dimension takes 4 bytes, and the average length of the vectors is 2,000, then we need 8 Gbytes to store 1,000,000 binary vectors. This does not account for the memory needed for the additional data structures. </span></p><p class="c8"><span class="c0">A parallel algorithm can partially address this issue by block-based processing. When we have </span><span class="c4">m </span><span class="c0">machines, we can divide </span><span class="c4">V </span><span class="c0">into </span><span class="c4">m </span><span class="c0">disjoint blocks of equal size, </span><span class="c4">V</span><span class="c14">1</span><span class="c4">,V</span><span class="c14">2</span><span class="c4">,...,V</span><span class="c3">m</span><span class="c0">. Then, we assign the </span><span class="c4">i</span><span class="c0">-th block and half of the remaining vec- </span></p><p class="c8"><span class="c0">London 903,833 999,935 5,909 ClueWeb 2,255,129 4,238,871 796 arXiv 286,488 800,102 761 DBLP 31,700,425 461,818 15 </span></p><p class="c2"><span class="c7">Table 2: Statistics of the Datasets </span></p><p class="c8"><span class="c0">tors, </span><span class="c4">V </span><span class="c12">C</span><span class="c3">i </span><span class="c9">, to machine </span><span class="c21">i</span><span class="c9">. Machine </span><span class="c21">i </span><span class="c9">first performs a similarity </span><span class="c0">search on </span><span class="c4">V</span><span class="c11">i</span><span class="c22">, then finds vector pairs such that one of them </span><span class="c0">is in </span><span class="c4">V</span><span class="c3">i </span><span class="c0">and the other is in </span><span class="c4">V </span><span class="c12">C</span><span class="c3">i </span><span class="c9">. We can select </span><span class="c21">V </span><span class="c12">C</span><span class="c3">i </span><span class="c9">such that </span><span class="c0">each vector pair is only processed once. This parallelization scheme does not perform any redundant computations, but requires the algorithm to be incremental. We use this par- allelization scheme for evaluating ATLAS and Min-Hash in Section 6.4. </span></p><p class="c8"><span class="c0">Note that a consequence of Equation (6) from Section 3.1.1 is that for low similarity thresholds, Min-Hash needs to use a significant amount of memory in order to support incremental queries. </span></p><p class="c2"><span class="c23">6. EVALUATION </span></p><p class="c8"><span class="c0">We evaluated our algorithm against previous exact and approximate algorithms on four datasets with different char- acteristics. We tuned the parameters of all approximate al- gorithms to have a 97.5% recall rate. To understand the per- formance and scalability of the algorithms, we first evaluated their single-core performance on the smaller datasets, then compared their parallel performance on the larger datasets. We performed the parallelized experiments on a cluster of 51 machines, one of which is used for task scheduling. Each ma- chine is equipped with two 2.66GHz Quad Core Intel Xeon Processors and 16 Gbytes of memory. We used a machine with identical hardware for single-core experiments. </span></p><p class="c2"><span class="c23">6.1 Datasets </span></p><p class="c8"><span class="c7">London. </span><span class="c0">This dataset consists of images collected from Flickr [1] by searching for the keyword &ldquo;London&rdquo;. We first extracted SIFT features from the images [26], then we used a vocabulary tree-based approach [27] to learn one million visual words. Each image is represented as a bag of visual words. </span></p><p class="c8"><span class="c7">ClueWeb. </span><span class="c0">This dataset consists of web pages from the TREC Category B dataset [2]. For each web page, we ap- plied Krovetz&rsquo;s stemming algorithm and filtered common stop words. We then discarded web pages with less than 500 words. </span></p><p class="c8"><span class="c7">arXiv. </span><span class="c0">This dataset contains a collection of arXiv [3] re- search articles from 1991 through early 2005. It was first used in by Sorikina et al. for plagiarism detection [30]. We did not apply stemming and stop word filtering on this dataset. </span></p><p class="c8"><span class="c7">DBLP. </span><span class="c0">This dataset contains a snapshot of the DBLP publication records [4], and has been previously used in the context of set-similarity join [33]; we used their procedure to increase the size of this dataset by a factor of 25 times [33].</span><span class="c22">Statistics about the datasets are included in Table 2. We </span><span class="c0">did not consider the DBLP dataset suitable for similarity search, as each record in it is simply the title and the list of authors of a paper. However, the data duplication procedure used by Vernica et al. introduces natural cluster-like struc- </span></p><p class="c2"><span class="c0">1004 </span></p><p class="c2"><span class="c0">Dataset </span><span class="c4">n d L </span></p><p class="c2"><span class="c57">100000 Min Hash (95%) </span><span class="c54">Min Min Hash Hash (97.5%) (99%) Atlas (95%) </span><span class="c52">10000 </span></p><p class="c2"><span class="c54">Atlas Atlas (97.5%) (99%) </span></p><p class="c2"><span class="c52">1000 100 </span><span class="c57">10 </span><span class="c54">0.2 0.3 0.4 0.5 </span><span class="c57">Min Hash (95%) </span><span class="c34">) s(e miTg ninnu</span><span class="c27">R</span><span class="c34">) </span></p><p class="c2"><span class="c34">s(e miTg ninnu</span><span class="c27">R</span><span class="c54">Min Min Hash Hash (97.5%) (99%) Atlas (95%) Atlas Atlas (97.5%) (99%) </span></p><p class="c2"><span class="c27">Jaccard Similarity </span></p><p class="c2"><span class="c27">Jaccard Similarity </span><span class="c46">(a) London100K (b) ClueWeb100K </span></p><p class="c2"><span class="c7">Figure 4: Recall Rate </span></p><p class="c8"><span class="c0">tures, making DBLP a good choice for testing our vector pre-clustering algorithm [33]. Since the speed of the exact algorithms are very slow, we created two additional datasets each consisting of 100,000 randomly sampled records, which are called ClueWeb100K and London100K. </span></p><p class="c2"><span class="c23">6.2 Permutation-Based Algorithms </span></p><p class="c8"><span class="c0">We compared ATLAS&rsquo;s candidate generation and candi- date filtering algorithms with All-Pairs [8], PPJoin [35], and Min-Hash [11]. We show the results in Figure 3 (a), (b), (e) and (f), and summarize the comparison in Table 3. For low similarity thresholds, ATLAS&rsquo;s speed-up over Min-Hash generally increases as </span><span class="c4">t </span><span class="c0">decreases. As the number of vector pairs with similarity above </span><span class="c4">t </span><span class="c0">increases, the relative speed- up of both approximate algorithms over exact algorithms decreases. </span></p><p class="c8"><span class="c0">We are also interested in the impact of recall rates on the approximate algorithms. As we have shown in Figure 4, recall rates between 95% and 99% have relatively little effects on the running time of the algorithms. </span></p><p class="c8"><span class="c0">Both All-Pairs and PPJoin work poorly for low similarity thresholds. All-Pairs maintains an inverted index dynami- cally to exploit </span><span class="c4">t</span><span class="c0">. It first sorts the vectors in increasing order of their size and the dimensions in decreasing order of their frequency. Then, it processes the vectors sequentially and only indexes the first (1</span><span class="c4">&minus;t</span><span class="c0">)</span><span class="c4">|x| </span><span class="c0">non-zero dimensions in a vec- tor </span><span class="c4">x</span><span class="c0">. PPJoin contains several improvements to All-Pairs. Its main difference from All-Pairs is that it sorts the dimen- sions in the increasing order of their frequency, and applies a heuristic, </span><span class="c4">positional filtering</span><span class="c0">, that further reduces the num- ber of candidates. When </span><span class="c4">t </span><span class="c0">is low, these algorithms are slow as they need to index almost all the non-zero dimensions. </span></p><p class="c8"><span class="c0">We also experimented with PPJoin+, an improved version of PPJoin [35]. Our results show that PPJoin+ is consis- tently slower than PPJoin when </span><span class="c4">L </span><span class="c0">is large, because PPJoin+ employs an additional suffix filtering algorithm, which uses an estimation of the Hamming distance between two set of tokens for pruning. Such a procedure is costly and works well only when it can give a reasonably accurate estimation at a small cost; however, this happens only if </span><span class="c4">L </span><span class="c0">is small. </span></p><p class="c2"><span class="c0">We have already described a LSH-based algorithm, Min- Hash, in Section 3.1.1. Min-Hash does not work well for low </span></p><p class="c2"><span class="c0">ClueWeb100K </span><span class="c9">Jaccard </span></p><p class="c2"><span class="c0">4.8x&ndash;91.7x 1.2x&ndash;2.7x London100K </span><span class="c9">Jaccard </span></p><p class="c2"><span class="c0">1.1x&ndash;95.3x 6.4x&ndash;80.9x </span></p><p class="c2"><span class="c7">Table 3: Relative Speed-Up </span></p><p class="c2"><span class="c57">1e+007 1e+006 </span></p><p class="c2"><span class="c57">100000 </span></p><p class="c2"><span class="c52">10000 </span></p><p class="c2"><span class="c52">1000 </span><span class="c57">100 </span><span class="c54">0.025 0.05 0.075 0.1 </span></p><p class="c2"><span class="c0">Dataset </span></p><p class="c2"><span class="c0">29.1x&ndash;67.2x 0.6x&ndash;4.5x Cosine </span></p><p class="c2"><span class="c0">15.9x&ndash;210.2x 44.6x&ndash;79.3x Cosine </span></p><p class="c2"><span class="c0">Exact Approximate </span></p><p class="c49"><span class="c57">1e+007 Atlas 100000 Atlas </span><span class="c54">Min Baseline Hash </span><span class="c52">10000 </span></p><p class="c2"><span class="c54">Min Baseline Hash </span><span class="c52">1000 100 10 </span><span class="c57">1 </span><span class="c54">0.025 0.05 0.075 0.1 </span></p><p class="c2"><span class="c54">0.2 0.3 0.4 0.5 </span></p><p class="c2"><span class="c46">(a) London100K (b) ClueWeb100K </span></p><p class="c2"><span class="c7">Figure 5: Candidate Size </span></p><p class="c8"><span class="c0">similarity thresholds. Consider the case when we use the Jaccard similarity and </span><span class="c4">t </span><span class="c0">= 0</span><span class="c4">.</span><span class="c0">025. For </span><span class="c4">R </span><span class="c0">= 1, we only need </span><span class="c4">J </span><span class="c0">to be 146 to ensure that each vector pair with similarity </span><span class="c4">t </span><span class="c0">has at least 97</span><span class="c4">.</span><span class="c0">5% probability of being considered; however, this set of parameters would result in many false positive candidates. For </span><span class="c4">R </span><span class="c0">= 2, we need to choose </span><span class="c4">J </span><span class="c0">= 4700 to have the same recall rate. In this case, the algorithm will spend a long time building signatures and thus candidate generation will be very slow. Thus, it is not possible to find a good tradeoff between the time needed to build signatures and the time needed to filter candidates. The parameters chosen by our algorithm, in contrast, are </span><span class="c4">S </span><span class="c0">= 8 and </span><span class="c4">K </span><span class="c0">= 130. </span></p><p class="c8"><span class="c0">An alternative LSH-based algorithm, which we have omit- ted from our comparisons, uses equation (11). Note that while Min-Hash uses the minimum non-zero dimension of a vector as a token, this algorithm uses the bit returned by the </span><span class="c4">sgn</span><span class="c0">(</span><span class="c4">x</span><span class="c0">) function instead. For binary vectors, this al- gorithm is significantly slower than Min-Hash, even though Min-Hash is intended to be used for Jaccard similarity. This is because for a signature of length </span><span class="c4">R</span><span class="c0">, (11) gives us only 2</span><span class="c12">R </span><span class="c0">hash buckets, whereas (4) gives us a maximum of </span><span class="c4">d</span><span class="c12">R </span><span class="c0">hash buckets. Thus, the number of false hash collisions in Min-Hash is much less than that in the other LSH-based algorithm. </span></p><p class="c8"><span class="c0">Finally, to illustrate the filtering power of our candidate generation and candidate filtering algorithms, we consider the total number of times that we need to calculate simi- larities between vector pairs. Figure 5 shows the ratio of the number of candidates generated by the algorithm to the number of vector pairs with similarity above </span><span class="c4">t</span><span class="c0">. The exact al- gorithms are not included in the comparison since they will have scanned most of the non-zero indices in each vector by the time they finish generating candidates. </span></p><p class="c8"><span class="c0">Also, note that combining ATLAS&rsquo;s candidate filtering algorithm with Min-Hash </span><span class="c4">directly </span><span class="c0">will not significantly im- prove Min-Hash&rsquo;s performance. Min-Hash is very likely to generate vector pairs with higher similarities multiple times. Since there are more vector pairs with higher similarities in Min-Hash&rsquo;s candidate set, the effectiveness of ATLAS&rsquo;s can- didate filtering algorithm is greatly reduced. </span></p><p class="c2"><span class="c23">6.3 Vector Pre-Clustering </span></p><p class="c24"><span class="c0">We evaluated ATLAS&rsquo;s vector pre-clustering algorithm on all four datasets. In all four experiments, we manually set the number of samples to 3,000 and </span><span class="c4">T </span><span class="c0">to 4. We show the the number 1)</span><span class="c4">/</span><span class="c0">2, ratio the of of running old the vector number time, pairs, and of </span><span class="c18">&sum;</span><span class="c0">possible the </span><span class="c3">i </span><span class="c21">|V</span><span class="c12">i</span><span class="c21">|&middot;</span><span class="c9">(</span><span class="c21">|V</span><span class="c12">i</span><span class="c21">|&minus;</span><span class="c9">1)</span><span class="c21">/</span><span class="c9">2 </span><span class="c0">number new of vector clusters </span><span class="c9">to </span><span class="c0">pairs </span><span class="c21">|V </span><span class="c0">found </span><span class="c21">|&middot;</span><span class="c9">(</span><span class="c21">|V </span><span class="c0">to the </span><span class="c21">|&minus; </span><span class="c0">in Table 5. We show the recall rates in Table 4. </span></p><p class="c2"><span class="c0">We find that ATLAS&rsquo;s vector pre-clustering algorithm gives </span></p><p class="c2"><span class="c0">1005 </span></p><p class="c2"><span class="c34">e ziSt luseRo to ita</span><span class="c27">R</span><span class="c57">1e+006 100000 </span></p><p class="c2"><span class="c27">Jaccard Similarity </span><span class="c52">10000 1000 100 10 </span><span class="c34">e ziSt luseRo to ita</span><span class="c27">R</span><span class="c57">1 </span><span class="c27">Jaccard Similarity </span></p><p class="c2"><span class="c0">94.68% 98.54% 99.85% </span></p><p class="c2"><span class="c7">Table 4: Pre-Clustering: Recall Rates </span></p><p class="c8"><span class="c0">good results when </span><span class="c4">t </span><span class="c0">is </span><span class="c4">relatively </span><span class="c0">high. Intuitively, when </span><span class="c4">t </span><span class="c0">be- comes sufficiently low, vector pairs with similarity above </span><span class="c4">t </span><span class="c0">are distributed more randomly and it becomes harder to cap- ture these vector pairs with clusters. This can be seen from the recall rates for ClueWeb (Cosine, 0</span><span class="c4">.</span><span class="c0">2), London (Cosine, 0</span><span class="c4">.</span><span class="c0">025), and DBLP (Cosine, 0</span><span class="c4">.</span><span class="c0">7). The effectiveness of the algorithm increases as the size of the dataset increases. </span></p><p class="c2"><span class="c0">We also integrated ATLAS&rsquo;s vector pre-clustering algo- rithm with Min-Hash and evaluated it on the arXiv dataset. We used similarity thresholds between 0.5 and 0.9. The results are shown in Figure 6 (a) and 6 (b). We excluded the time needed for vector pre-clustering (around 3 minutes) from the running times as we could use a single clustering result for all five similarity thresholds. Min-Hash offers a significant speed-up (between 10</span><span class="c4">.</span><span class="c0">3x and 132</span><span class="c4">.</span><span class="c0">5x) relative to both PPJoin and All-Pairs. After combining Min-Hash with vector pre-clustering, we gain a further speed-up between 0</span><span class="c4">.</span><span class="c0">18x and 0</span><span class="c4">.</span><span class="c0">81x relative to Min-Hash. </span><span class="c23">6.4 Scalability </span></p><p class="c2"><span class="c0">We evaluated the running time of the parallelized algo- rithms on three datasets: London, ClueWeb and DBLP. The </span></p><p class="c2"><span class="c25">All-Pairs </span><span class="c19">10000 </span><span class="c5">PPJoin Min Hash Atlas </span></p><p class="c24"><span class="c19">1000 </span><span class="c25">100 0.2 0.3 0.4 0.5 100000 All-Pairs Min Hash Min Hash </span><span class="c5">PPJoin Atlas Atlas Min Hash Atlas </span></p><p class="c2"><span class="c19">10000 10000 </span></p><p class="c2"><span class="c19">1000 1000 </span><span class="c25">100 100 </span><span class="c5">0.2 0.3 0.4 0.5 </span></p><p class="c2"><span class="c46">(a) ClueWeb100K, single core (b) ClueWeb100K, single core (c) ClueWeb, 50 machines (d) ClueWeb, 50 machines </span></p><p class="c2"><span class="c19">10000 1000 </span><span class="c25">100 100000 </span><span class="c29">) s(e miTg ninnu</span><span class="c10">R</span><span class="c5">0.2 0.3 0.4 0.5 </span></p><p class="c2"><span class="c10">Jaccard Similarity </span></p><p class="c2"><span class="c29">) s(e miTg ninnu</span><span class="c10">RCosine Similarity </span></p><p class="c2"><span class="c29">) s(e miTg ninnu</span><span class="c10">RJaccard Similarity </span></p><p class="c2"><span class="c29">) s(e miTg ninnu</span><span class="c10">RCosine Similarity </span></p><p class="c24"><span class="c25">1e+006 All-Pairs All-Pairs 100000 Min Hash 100000 Min Hash </span><span class="c5">PPJoin PPJoin Atlas Atlas Min Hash Min Hash Atlas Atlas </span></p><p class="c2"><span class="c19">10000 </span></p><p class="c2"><span class="c19">10000 </span></p><p class="c49"><span class="c19">10000 1000 1000 1000 </span><span class="c25">100 100 100 </span><span class="c5">0.025 0.05 0.075 0.1 </span></p><p class="c2"><span class="c5">0.025 0.05 0.075 0.1 </span></p><p class="c2"><span class="c5">0.025 0.05 0.075 0.1 </span></p><p class="c2"><span class="c46">(e) London100K, single core (f) London100K, single core (g) London, 50 machines (h) London, 50 machines </span></p><p class="c2"><span class="c7">Figure 3: Running Time, Low Similarity Thresholds </span></p><p class="c2"><span class="c0">ClueWeb100K </span></p><p class="c2"><span class="c25">1e+006 </span><span class="c29">) s(e miTg ninnu</span><span class="c10">R</span><span class="c5">0.025 0.05 0.075 0.1 </span></p><p class="c2"><span class="c10">Jaccard Similarity </span><span class="c25">100000 </span></p><p class="c2"><span class="c29">) s(e miTg ninnu</span><span class="c10">R</span><span class="c29">) s(e miTg ninnu</span><span class="c10">RCosine Similarity </span><span class="c25">100000 </span></p><p class="c2"><span class="c29">) s(e miTg ninnu</span><span class="c10">RJaccard Similarity </span></p><p class="c2"><span class="c10">Cosine Similarity </span></p><p class="c2"><span class="c0">73.78% 98.97% 99.87% 99.87% </span></p><p class="c2"><span class="c0">London100K </span></p><p class="c2"><span class="c0">Jaccard </span></p><p class="c2"><span class="c0">Jaccard </span></p><p class="c2"><span class="c0">Jaccard </span></p><p class="c2"><span class="c0">Jaccard </span></p><p class="c2"><span class="c25">100000 </span><span class="c5">0.2 0.3 0.4 0.5 </span></p><p class="c2"><span class="c0">99.61% 99.73% 99.73% 99.85% Cosine </span></p><p class="c2"><span class="c0">99.19% 99.79% 99.89% 99.91% Cosine </span></p><p class="c2"><span class="c0">99.81% 99.97% 100.00% 100.00% Cosine </span></p><p class="c2"><span class="c0">99.10% 99.79% 99.98% Cosine </span></p><p class="c2"><span class="c0">99.65% 99.77% 99.96% 100.00% </span></p><p class="c2"><span class="c0">DBLP </span></p><p class="c2"><span class="c25">10 </span><span class="c19">100 1000 10000 </span></p><p class="c2"><span class="c0">93.94% 99.26% 99.64% 99.67% </span></p><p class="c2"><span class="c0">arXiv </span></p><p class="c2"><span class="c0">0.025 0.050 0.075 0.100 </span></p><p class="c2"><span class="c0">0.2 0.3 0.4 0.5 </span></p><p class="c2"><span class="c0">0.6 0.7 0.8 0.9 </span></p><p class="c2"><span class="c0">0.7 0.8 0.9 </span></p><p class="c2"><span class="c0">9.03% 1514.2s 225 </span></p><p class="c2"><span class="c7">Table 5: Pre-clustering: other statistics </span></p><p class="c8"><span class="c0">algorithms that we tested include ATLAS, Min-Hash, and Map-Reduce based PPJoin [33]. For ATLAS, we used its candidate generation and candidate filtering algorithms on London and ClueWeb, and its vector pre-clustering algo- rithm with Min-Hash on DBLP. We did not evaluate All- Pairs and PPJoin as they cannot easily take advantage of parallelism. Instead, we evaluated the Map-Reduce based approach described in [33]. Our parallel implementations of Min-Hash and ATLAS are based on the block-based pro- cessing approach that we outlined in Section 5.2. </span></p><p class="c8"><span class="c0">The Map-Reduce based solution suffers from the memory bottleneck issue [33]. It uses the prefix-filtering idea in [8, 35] to build a </span><span class="c4">virtual </span><span class="c0">inverted index. Then, it routes the vectors in each inverted list to a worker for further processing. As we have previously discussed, prefix-filtering is ineffective when the size of the index becomes large. When we allow each mapper/reducer task to use 8 Gbytes of memory, it can only process around 50,000 London records or 400,000 ClueWeb records in one pass. We have to divide the dataset into </span><span class="c4">k </span><span class="c0">blocks, then perform a similarity search on all </span><span class="c4">k</span><span class="c0">(</span><span class="c4">k &minus; </span><span class="c0">1)</span><span class="c4">/</span><span class="c0">2 pairs of blocks. Even so, within a 48 hours time limit, this algorithm did not terminate even on ClueWeb (</span><span class="c4">t </span><span class="c0">= 0</span><span class="c4">.</span><span class="c0">5) or on London (</span><span class="c4">t </span><span class="c0">= 0</span><span class="c4">.</span><span class="c0">1). In contrast, ATLAS finished processing these datasets in 3 minutes for these similarity thresholds. </span></p><p class="c8"><span class="c0">The results are shown in Figure 3 (c), (d), (g), and (h), and in Figure 6 (c) and (d). Note that the parallelization overhead makes the difference between the speeds of differ- ent algorithms appear smaller for higher thresholds. We do </span></p><p class="c2"><span class="c0">1006 </span></p><p class="c2"><span class="c0">London100K </span><span class="c9">Jaccard </span></p><p class="c2"><span class="c0">67.80% 342.4s 57 ClueWeb100K </span><span class="c9">Jaccard </span></p><p class="c2"><span class="c0">28.74% 163.5s 171 arXiv </span><span class="c9">Jaccard </span></p><p class="c2"><span class="c0">33.35% 177.2s 63 DBLP </span><span class="c9">Jaccard </span></p><p class="c2"><span class="c0">Dataset </span></p><p class="c2"><span class="c0">45.34% 418.5s 85 Cosine </span></p><p class="c2"><span class="c0">16.61% 188.1s 204 Cosine </span></p><p class="c2"><span class="c0">25.20% 207.2s 81 Cosine </span></p><p class="c2"><span class="c0">7.95% 1643.7s 246 Cosine </span></p><p class="c2"><span class="c0">Pairs (%) Time Clusters </span></p><p class="c2"><span class="c25">1e+006 </span><span class="c5">0.5 0.6 0.7 0.8 0.9 </span></p><p class="c2"><span class="c25">1e+006 </span><span class="c19">10000 </span><span class="c25">All-Pairs </span><span class="c5">PPJoin Min Hash Atlas+Min Hash </span></p><p class="c2"><span class="c19">1000 </span><span class="c25">100 </span><span class="c5">0.5 0.6 0.7 0.8 0.9 </span></p><p class="c2"><span class="c25">0.7 0.8 0.9 100000 All-Pairs MapReduce(PPJoin+) MapReduce(PPJoin+) </span><span class="c5">PPJoin Min Hash </span><span class="c29">) s(e miTg ninnu</span><span class="c10">R</span><span class="c25">100000 </span></p><p class="c2"><span class="c5">Min Hash Atlas+Min Hash </span></p><p class="c2"><span class="c29">) s(e miTg ninnu</span><span class="c10">R</span><span class="c25">100000 </span></p><p class="c2"><span class="c5">Min Hash Atlas+Min Hash Atlas+Min Hash </span></p><p class="c2"><span class="c19">10000 </span></p><p class="c2"><span class="c19">10000 </span></p><p class="c2"><span class="c19">1000 1000 100 </span><span class="c25">10 100 </span><span class="c5">0.7 0.8 0.9 </span><span class="c10">Jaccard Similarity </span></p><p class="c49"><span class="c10">Cosine Similarity </span><span class="c46">(a) arXiv, single core (b) arXiv, single core (c) DBLP, 10 machines (d) DBLP, 10 machines </span></p><p class="c2"><span class="c7">Figure 6: Running Time, High Similarity Thresholds. </span></p><p class="c8"><span class="c0">not include the running times of ATLAS and Min-Hash on the DBLP data with Cosine similarity and a threshold of 0</span><span class="c4">.</span><span class="c0">7, because we used a single clustering result for all experi- ments which discarded more than 2</span><span class="c4">.</span><span class="c0">5% of the true positives in this case (see Table 4). </span></p><p class="c2"><span class="c23">7. DISCUSSION </span></p><p class="c2"><span class="c0">Previous similarity search algorithms either do not ad- dress or unsatisfactorily solve the following four problems: </span></p><p class="c8"><span class="c7">1. Low Similarity Thresholds. </span><span class="c0">We have previously shown in Section 1 that many real-world applications need to use similarity search with low similarity thresholds (0</span><span class="c4">.</span><span class="c0">025 to 0</span><span class="c4">.</span><span class="c0">2 in our examples) and why this makes sense mathe- matically. Intuitively, we can also see that whenever one is interested in the fine-grained similarities and differences be- tween complex objects (such as in 3D reconstruction [5] and in plagiarism detection [30]), it is hard to find a representa- tion (and a corresponding similarity function) such that the similarity between every interesting pair of objects is high. </span></p><p class="c8"><span class="c7">2. High Dimensional Descriptors. </span><span class="c0">For feature-rich objects, such as images and videos, the average length of vectors, </span><span class="c4">L</span><span class="c0">, can be very large. The </span><span class="c4">L </span><span class="c0">in our London dataset, which we described in Section 6, is 5,909. Finding an effi- cient candidate filtering algorithm for these long vectors can make a similarity search algorithm much faster. </span></p><p class="c8"><span class="c7">3. Incremental Queries. </span><span class="c0">Many real-world applications need to support incremental queries. Examples of such ap- plications include similar music and video retrieval [19, 32], 3D reconstruction [5], and plagiarism detection [30]. In these applications, the user may add a new object to the applica- tion while it is running. The application then needs to apply a similarity search algorithm in order to quickly determine all objects that the new object is similar to. </span></p><p class="c8"><span class="c0">One possible way to deal with newly added objects is to run an all-pairs similarity search algorithm periodically. However, as we have shown in the experiments, recalculat- ing the similarities between all pairs of objects can be very expensive even with only 1 million objects. Similarity search algorithms that support incremental queries can greatly re- duce the computational cost. </span></p><p class="c8"><span class="c0">When new vectors are added, All-Pairs and PPJoin need to resort all vectors and reorder the dimensions, then restart the algorithm from scratch, and thus are not incremental. LSH-based algorithms can support incremental queries, but may need to store a large number of hash tables in mem- ory for low similarity thresholds. Our candidate generation algorithm not only supports incremental queries, but also generally uses less memory than LSH-based methods. </span></p><p class="c2"><span class="c7">4. Scalability. </span><span class="c0">We have previously discussed the scal- </span></p><p class="c2"><span class="c19">10000 1000 100 </span><span class="c29">) </span></p><p class="c2"><span class="c29">s(e miTg ninnu</span><span class="c10">RJaccard Similarity </span></p><p class="c2"><span class="c29">) s(e miTg ninnu</span><span class="c10">R</span><span class="c25">10 </span><span class="c10">Cosine Similarity </span></p><p class="c8"><span class="c0">ability issues in Section 5.2. It is unclear whether exact algorithms can be executed efficiently on a large cluster. As we have shown in Section 6.4, Vernica et al.&rsquo;s Map-Reduce based solution is ineffective for low similarity thresholds due to its use of prefix-filtering [33]. While LSH-based approx- imate algorithms can be parallelized using the scheme in Section 5.2, their hash indexes may use a huge amount of memory for low similarity thresholds. </span></p><p class="c2"><span class="c23">8. RELATED WORK </span></p><p class="c24"><span class="c7">Exact Algorithms. </span><span class="c0">Many exact algorithms reduce the search space by maintaining an inverted index dynamically. They also use various filtering techniques, such as prefix filtering, positional filtering, suffix filtering, and size filter- ing [29, 8, 35]. Among those, suffix filtering [35] provides an alternative solution to our candidate filtering problem, but it is effective only when the average length of the vectors is small. These algorithms are highly efficient on smaller datasets. Their drawbacks are mainly (a) the need to pre- process the data, (b) poor performance for low similarity thresholds, and (c) poor scalability as the average length of the vectors increases. Another class of algorithms generates </span><span class="c4">signatures </span><span class="c0">for each vector using the pigeonhole principle [7]. </span><span class="c7">Approximate Algorithms. </span><span class="c0">Locality sensitive hashing (LSH) based algorithms hash vectors such that the more similar the two vectors are, the more likely they are to be hashed into the same buckets [25]. In principle, LSH works for a similarity function if and only if the function admits a locality sensitive hash function family. Random projec- tions can be used for Cosine similarity [21, 12], and min- wise independent permutations [11] for Jaccard similarity. In practice, any hash function family that approximately preserves the property of a family of LSH functions can be used to achieve good retrieval performance. Previous work has also used the minimum </span><span class="c4">K </span><span class="c0">values under a hash function for approximate query processing [16, 9], but both their mo- tivation and their usage are quite different from ours. </span></p><p class="c8"><span class="c7">Dimensionality Reduction. </span><span class="c0">Another approach to pro- cess high dimensional vectors is the usage of dimensionality reduction techniques. Popular approaches include principal component analysis (PCA), latent semantic indexing (LSI), singular value decomposition (SVD), and applications of the Johnson-Lindenstrauss Lemma [18]. However, the effects of dimensionality reduction techniques on datasets are highly application-dependent [6]. </span></p><p class="c8"><span class="c7">Partitioning and Clustering. </span><span class="c0">Previous work proposed various partitioning and clustering techniques, but they usu- ally degenerate to linear scans in high dimensions [34]. Clus- tering methods have also been applied to variants of the </span></p><p class="c2"><span class="c0">1007 </span></p><p class="c2 c83"><span class="c0">similarity search problem, such as the nearest neighbor prob- lem [13]. </span></p><p class="c92"><span class="c23">9. CONCLUSIONS </span></p><p class="c79"><span class="c0">We have presented ATLAS, a new probabilistic algorithm for high dimensional similarity search with low similarity thresholds. Our analysis and experimental evaluation sug- gest that ATLAS is an efficient solution for a wide class of applications. </span></p><p class="c59"><span class="c7">Future Work. </span><span class="c0">Further improvements to ATLAS&rsquo;s can- didate generation procedure are possible. First, we might occasionally experience a low recall rate caused by a bad random seed. A possible remedy is to resort to a </span><span class="c4">union- intersection-union </span><span class="c0">procedure like LSH. Second, if the vari- ance in the lengths of the vectors is large, we may need to use a very large </span><span class="c4">K </span><span class="c0">to guarantee a high recall rate. One possible solution is to first run the algorithm to find vec- tor pairs whose differences in lengths are small, then re-run the algorithm with different parameters to find vector pairs whose differences in lengths are large. Third, we still need a large amount of main memory to store the inverted indexes for very low thresholds (in our experiments for the Cosine similarity and a similarity threshold below 0</span><span class="c4">.</span><span class="c0">05). </span></p><p class="c85"><span class="c7">Acknowledgments. </span><span class="c0">We thank the anonymous review- ers for their valuable comments. This research has been supported by the NSF under Grants IIS-0627680 and IIS- 1012593, by the New York State Foundation for Science, Technology, and Innovation under Agreement C050061, and by a Humboldt Research Award from the Alexander von Humboldt Foundation. Any opinions, findings, conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsors. </span></p><p class="c74"><span class="c23">10. REFERENCES </span></p><p class="c53"><span class="c0">[1] http://www.flickr.com/ [2] http://boston.lti.cs.cmu.edu/Data/clueweb09/ [3] http://arxiv.org/ [4] http://dblp.uni-trier.de/xml/dblp.xml.gz [5] S. Agarwal, N. Snavely, I. Simon, S. M. Seitz, and </span></p><p class="c56"><span class="c0">R. Szeliski. Building Rome in a day. In </span><span class="c4">ICCV</span><span class="c0">, 2009. [6] C. C. Aggarwal. On the effects of dimensionality </span></p><p class="c81"><span class="c0">reduction on high dimensional similarity search. In </span><span class="c4">PODS</span><span class="c0">, 2001. [7] A. Arasu, V. Ganti, and R. Kaushik. Efficient exact </span></p><p class="c93"><span class="c0">set-similarity joins. In </span><span class="c4">VLDB</span><span class="c0">, 2006. [8] R. J. Bayardo, Y. Ma, and R. Srikant. Scaling up all </span></p><p class="c48"><span class="c0">pairs similarity search. In </span><span class="c4">WWW</span><span class="c0">, 2007. [9] K. Beyer, R. Gemulla, P. J. Haas, B. Reinwald, and Y. Sismanis. Distinct-value synopses for multiset operations. </span><span class="c4">Commun. ACM</span><span class="c0">, 52(10), 2009. [10] K. S. Beyer, J. Goldstein, R. Ramakrishnan, and </span></p><p class="c77"><span class="c0">U. Shaft. When is &rdquo;nearest neighbor&rdquo; meaningful? In </span><span class="c4">ICDT</span><span class="c0">, 1999. [11] A. Z. Broder, M. Charikar, A. M. Frieze, and M. Mitzenmacher. Min-wise independent permutations. In </span><span class="c4">STOC</span><span class="c0">, 1998. [12] M. S. Charikar. Similarity estimation techniques from </span></p><p class="c84"><span class="c0">rounding algorithms. In </span><span class="c4">STOC</span><span class="c0">, 2002. [13] F. Chierichetti, A. Panconesi, P. Raghavan, M. Sozio, </span></p><p class="c61"><span class="c0">A. Tiberi, and E. Upfal. Finding near neighbors through cluster pruning. In </span><span class="c4">PODS</span><span class="c0">, 2007. </span></p><p class="c2 c86"><span class="c0">[14] O. Chum and J. Matas. Large-scale discovery of </span></p><p class="c90"><span class="c0">spatially related images. </span><span class="c4">TPAMI</span><span class="c0">, 32(2), 2010. [15] V. Chv&aacute;tal. The tail of the hypergeometric </span></p><p class="c71"><span class="c0">distribution. </span><span class="c4">Discrete Mathematics</span><span class="c0">, 25(3), 1979. [16] C. Cohen and H. Kaplan. Summarizing data using </span></p><p class="c70"><span class="c0">bottom-k sketches. In </span><span class="c4">PODS</span><span class="c0">, 2007. [17] A. S. Das, M. Datar, A. Garg, and S. Rajaram. </span></p><p class="c63"><span class="c0">Google news personalization: scalable online collaborative filtering. In </span><span class="c4">WWW</span><span class="c0">, 2007. [18] S. Dasgupta and A. Gupta. An elementary proof of </span></p><p class="c72"><span class="c0">the Johnson-Lindenstrauss lemma. Technical Report TR-99-006, ICSI, 1999. [19] S. L. Feng, R. Manmatha, and V. Lavrenko. Multiple </span></p><p class="c80"><span class="c0">bernoulli relevance models for image and video annotation. In </span><span class="c4">CVPR</span><span class="c0">, 2004. [20] B. J. Frey and D. Dueck. Clustering by passing </span></p><p class="c38"><span class="c0">messages between data points. </span><span class="c4">Science</span><span class="c0">, 315(5814), 2007. [21] M. X. Goemans and D. P. Williamson. Improved </span></p><p class="c60"><span class="c0">approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. </span><span class="c4">J. ACM</span><span class="c0">, 42(6), 1995. [22] S. Guha, R. Rastogi, and K. Shim. CURE: A </span></p><p class="c73"><span class="c0">clustering algorithm for large databases. Technical report, Bell Laboratories, 1997. [23] T. H. Haveliwala, A. Gionis, and P. Indyk. Scalable techniques for clustering the web. In </span><span class="c4">WebDB</span><span class="c0">, 2000. [24] M. R. Henzinger. Finding near-duplicate web pages: a </span></p><p class="c94"><span class="c0">large-scale evaluation of algorithms. In </span><span class="c4">SIGIR</span><span class="c0">, 2006. [25] P. Indyk and R. Motwani. Approximate nearest </span></p><p class="c88"><span class="c0">neighbors: towards removing the curse of dimensionality. In </span><span class="c4">STOC</span><span class="c0">, 1998. [26] D. G. Lowe. Distinctive image features from scale-invariant keypoints. </span><span class="c4">IJCV</span><span class="c0">, 60(2), 2004. [27] D. Nist&eacute;r and H. Stew&eacute;nius. Scalable recognition with </span></p><p class="c69"><span class="c0">a vocabulary tree. In </span><span class="c4">CVPR</span><span class="c0">, 2006. [28] M. Sahami and T. D. Heilman. A web-based kernel </span></p><p class="c26"><span class="c0">function for measuring the similarity of short text snippets. In </span><span class="c4">WWW</span><span class="c0">, 2006. [29] S. Sarawagi and A. Kirpal. Efficient set joins on </span></p><p class="c87"><span class="c0">similarity predicates. In </span><span class="c4">SIGMOD</span><span class="c0">, 2004. [30] D. Sorokina, J. Gehrke, S. Warner, and P. Ginsparg. </span></p><p class="c91"><span class="c0">Plagiarism detection in arXiv. In </span><span class="c4">ICDM</span><span class="c0">, 2006. [31] E. Spertus, M. Sahami, and O. Buyukkokten. </span></p><p class="c72"><span class="c0">Evaluating similarity measures: a large-scale study in the Orkut social network. In </span><span class="c4">KDD</span><span class="c0">, 2005. [32] D. Turnbull, L. Barrington, D. Torres, and </span></p><p class="c50"><span class="c0">G. Lanckriet. Towards musical query-by-semantic-description using the CAL500 data set. In </span><span class="c4">SIGIR</span><span class="c0">, 2007. [33] R. Vernica, M. J. Carey, and C. Li. Efficient parallel </span></p><p class="c30"><span class="c0">set-similarity joins using MapReduce. In </span><span class="c4">SIGMOD</span><span class="c0">, 2010. [34] R. Weber, H.-J. Schek, and S. Blott. A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces. In </span><span class="c4">VLDB</span><span class="c0">, 1998. [35] C. Xiao, W. Wang, X. Lin, and J. X. Yu. Efficient </span></p><p class="c42"><span class="c0">similarity joins for near duplicate detection. In </span><span class="c4">WWW</span><span class="c0">, 2008. </span></p><p class="c75"><span class="c0">1008 </span></p></body></html>