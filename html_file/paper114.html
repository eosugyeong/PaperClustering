<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c66{margin-left:-16.9pt;padding-top:1pt;text-indent:29.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.2pt}.c52{margin-left:-17.1pt;padding-top:2.2pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.4pt}.c76{margin-left:-16.9pt;padding-top:1pt;text-indent:32.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.2pt}.c174{margin-left:-18.7pt;padding-top:8.4pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c154{margin-left:-19pt;padding-top:17.8pt;text-indent:29.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.1pt}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c172{margin-left:-18.7pt;padding-top:9.4pt;text-indent:19pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.4pt}.c83{margin-left:-19pt;padding-top:1.9pt;text-indent:28.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.4pt}.c43{margin-left:-19.4pt;padding-top:16.6pt;text-indent:29.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c133{margin-left:-18.7pt;padding-top:7.2pt;text-indent:19pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c74{margin-left:-16.9pt;padding-top:11.8pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.7pt}.c169{margin-left:-13.5pt;padding-top:1pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19pt}.c7{margin-left:-18.5pt;padding-top:2.2pt;text-indent:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.2pt}.c56{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c64{margin-left:-19pt;padding-top:11pt;text-indent:19.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.8pt}.c111{margin-left:-19pt;padding-top:32.2pt;text-indent:28.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c68{margin-left:-18.7pt;padding-top:23.5pt;text-indent:19pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.2pt}.c36{margin-left:-16.9pt;padding-top:1pt;text-indent:32.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18pt}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Arial";font-style:normal}.c25{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.5pt;font-family:"Arial";font-style:normal}.c94{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Courier New";font-style:normal}.c54{margin-left:-19pt;padding-top:10.8pt;text-indent:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.2pt}.c58{margin-left:-17.8pt;padding-top:1.9pt;text-indent:27.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.2pt}.c84{margin-left:-18.7pt;padding-top:8.9pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c37{margin-left:-18.7pt;padding-top:2.2pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c123{margin-left:-19pt;padding-top:10.8pt;text-indent:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c126{margin-left:-13.5pt;padding-top:1.2pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.2pt}.c61{margin-left:-13.5pt;padding-top:1pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.8pt}.c6{margin-left:237.1pt;padding-top:1.9pt;text-indent:-227pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-273.2pt}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:9.1pt;font-family:"Arial";font-style:normal}.c186{margin-left:-16.9pt;padding-top:1pt;text-indent:32.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.8pt}.c10{margin-left:-19pt;padding-top:2.2pt;text-indent:29.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.4pt}.c80{margin-left:-19pt;padding-top:10.8pt;text-indent:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c27{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.9pt;font-family:"Arial";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.1pt;font-family:"Arial";font-style:normal}.c119{margin-left:-18.5pt;padding-top:2.2pt;text-indent:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c100{margin-left:-17.1pt;padding-top:35pt;text-indent:17.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18.2pt}.c87{margin-left:-18.7pt;padding-top:8.2pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.8pt}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.1pt;font-family:"Arial";font-style:normal}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c182{margin-left:-17.1pt;padding-top:10.6pt;text-indent:17.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.4pt}.c69{margin-left:-3.1pt;padding-top:13.4pt;text-indent:3.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:3.5pt}.c118{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c159{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c130{margin-left:-18.5pt;padding-top:1.9pt;text-indent:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c21{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c155{margin-left:-19pt;padding-top:7.7pt;text-indent:28.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.8pt}.c124{margin-left:-17.1pt;padding-top:11.3pt;text-indent:17.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19pt}.c165{margin-left:-18.7pt;padding-top:7.9pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c178{margin-left:-17.1pt;padding-top:7.7pt;text-indent:17.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18pt}.c8{margin-left:-16.9pt;padding-top:1pt;text-indent:32.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18.7pt}.c136{margin-left:-18.7pt;padding-top:10.3pt;text-indent:19pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:27.5pt}.c104{margin-left:-17.1pt;padding-top:11.8pt;text-indent:17.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.4pt}.c96{margin-left:-18.5pt;padding-top:8.6pt;text-indent:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.3pt;font-family:"Arial";font-style:normal}.c50{margin-left:-16.9pt;padding-top:2.2pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.4pt}.c138{margin-left:-11.4pt;padding-top:2.4pt;text-indent:72.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.8pt}.c157{margin-left:-17.1pt;padding-top:1.9pt;text-indent:27.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.4pt}.c129{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c45{margin-left:-19pt;padding-top:8.9pt;text-indent:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c132{margin-left:-17.1pt;padding-top:1.9pt;text-indent:27.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.2pt}.c99{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:17.2pt;font-family:"Arial";font-style:normal}.c4{margin-left:-18.5pt;padding-top:17.5pt;text-indent:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c86{margin-left:-18.7pt;padding-top:2.2pt;text-indent:28.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c168{margin-left:-17.1pt;padding-top:11.8pt;text-indent:17.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18pt}.c30{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.6pt;font-family:"Arial";font-style:normal}.c137{margin-left:-18.5pt;padding-top:15.8pt;text-indent:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c18{margin-left:-18.7pt;padding-top:2.2pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.1pt}.c57{margin-left:100pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:46.3pt}.c185{margin-left:-17.8pt;padding-top:0.7pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-19pt}.c55{margin-left:-19pt;padding-top:27.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c49{margin-left:218.6pt;padding-top:51.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c28{margin-left:-13.5pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-18pt}.c163{margin-left:-11.4pt;padding-top:224.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12.7pt}.c176{margin-left:-19pt;padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-17.1pt}.c42{margin-left:47.4pt;padding-top:45.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:34.8pt}.c103{margin-left:68.6pt;padding-top:13.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:55.7pt}.c47{margin-left:67.1pt;padding-top:8.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:98.4pt}.c139{margin-left:4.5pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:10.8pt}.c53{margin-left:-131.8pt;padding-top:104.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:245pt}.c125{margin-left:44.8pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:142.3pt}.c162{margin-left:218.6pt;padding-top:50.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c141{margin-left:18.2pt;padding-top:158.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:20.8pt}.c171{margin-left:-19pt;padding-top:10.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c91{margin-left:-16.1pt;padding-top:5.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:71.7pt}.c39{margin-left:27.8pt;padding-top:13.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:26.6pt}.c95{margin-left:-16.9pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-18.7pt}.c179{margin-left:60.9pt;padding-top:92.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:60.2pt}.c67{margin-left:24.9pt;padding-top:13.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:23.5pt}.c160{margin-left:63.8pt;padding-top:85.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:62.9pt}.c153{margin-left:-17.1pt;padding-top:21.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.4pt}.c75{margin-left:-23.8pt;padding-top:737.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24pt}.c51{margin-left:40pt;padding-top:14.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:38.6pt}.c85{margin-left:218.6pt;padding-top:313.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:218.6pt}.c29{margin-left:36.5pt;padding-top:14.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:39pt}.c110{margin-left:-16.9pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-19.2pt}.c121{margin-left:-18.5pt;padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.9pt}.c34{margin-left:22.7pt;padding-top:8.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:139.2pt}.c117{margin-left:45.8pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:32.9pt}.c77{margin-left:-1.3pt;padding-top:23.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-11.3pt}.c167{margin-left:-16.9pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19pt}.c142{margin-left:218.6pt;padding-top:51.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c31{margin-left:-13.9pt;padding-top:15.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:54.9pt}.c20{margin-left:-1.2pt;padding-top:155.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:1.4pt}.c175{margin-left:-16.9pt;padding-top:7.7pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.2pt}.c164{margin-left:-19pt;padding-top:17.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c73{margin-left:42.4pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:58.1pt}.c35{margin-left:-18.5pt;padding-top:10.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c122{margin-left:-11.4pt;padding-top:16.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-5.5pt}.c114{margin-left:2.9pt;padding-top:147.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:5.2pt}.c82{margin-left:218.6pt;padding-top:50.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c48{margin-left:56.8pt;padding-top:14.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:88.1pt}.c177{margin-left:-18.5pt;padding-top:21.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c93{margin-left:218.6pt;padding-top:53.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c148{margin-left:71.2pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:58.3pt}.c102{margin-left:-19pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.8pt}.c127{margin-left:-16.9pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-19pt}.c70{margin-left:-16.9pt;padding-top:10.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:144.5pt}.c131{margin-left:26.1pt;padding-top:10.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:143.3pt}.c135{margin-left:-18.5pt;padding-top:10.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.8pt}.c107{margin-left:-13.5pt;padding-top:3.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18.5pt}.c89{margin-left:-18.5pt;padding-top:4.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c115{margin-left:-18.5pt;padding-top:10.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.2pt}.c22{margin-left:148.5pt;padding-top:3.1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:9.8pt}.c17{margin-left:-16.9pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.2pt}.c44{margin-left:-17.1pt;padding-top:19.7pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-19.4pt}.c98{margin-left:-19pt;padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.8pt}.c149{margin-left:24.6pt;padding-top:14.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:23.5pt}.c101{margin-left:148.5pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:9.8pt}.c90{margin-left:-18.5pt;padding-top:11.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c12{margin-left:65.4pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:97.4pt}.c143{margin-left:-4.3pt;padding-top:13.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-1.8pt}.c13{margin-left:-18.5pt;padding-top:10.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.6pt}.c79{margin-left:-16.9pt;padding-top:2.2pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-19pt}.c152{margin-left:-5.3pt;padding-top:332.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3pt}.c60{margin-left:-5pt;padding-top:14.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-21.2pt}.c41{margin-left:48.5pt;padding-top:13.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:50.8pt}.c113{margin-left:218.6pt;padding-top:70.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c116{margin-left:-17.1pt;padding-top:8.2pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-19.4pt}.c183{margin-left:-1.3pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:115.2pt}.c158{margin-left:167pt;padding-top:198.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:167.8pt}.c88{margin-left:218.6pt;padding-top:51.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.4pt}.c62{margin-left:-18.5pt;padding-top:13.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c109{margin-left:14.1pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:24.7pt}.c72{margin-left:-7.7pt;padding-top:22.6pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:105pt}.c112{padding-top:1.9pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c120{padding-top:15.4pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c81{padding-top:0.7pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c151{padding-top:18pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c140{padding-top:2.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c19{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c184{padding-top:10.1pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c144{padding-top:10.3pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c145{padding-top:142.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c150{padding-top:22.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c32{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c147{padding-top:11.3pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c106{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c23{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c38{margin-left:237.1pt;text-indent:-227pt;margin-right:-19.4pt}.c46{margin-left:-17.8pt;text-indent:28.2pt;margin-right:-19.4pt}.c166{margin-left:-19pt;text-indent:19.2pt;margin-right:-17.6pt}.c108{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c187{margin-left:-17.1pt;text-indent:27.2pt;margin-right:-19.4pt}.c71{margin-left:-18.5pt;text-indent:28.3pt;margin-right:-22.4pt}.c173{margin-left:-18.5pt;text-indent:28.3pt;margin-right:-17.6pt}.c24{margin-left:-17.1pt;text-indent:17.4pt;margin-right:-18.7pt}.c97{margin-left:307.9pt;margin-right:-201pt}.c170{margin-left:22.5pt;margin-right:21.1pt}.c156{margin-left:-2.2pt;margin-right:-3.4pt}.c180{margin-left:-16.9pt;margin-right:-18.2pt}.c146{margin-left:-131.8pt;margin-right:245pt}.c63{margin-left:-16.9pt;margin-right:-18pt}.c92{margin-left:-134.7pt;margin-right:242.4pt}.c161{margin-left:-19pt;margin-right:-16.4pt}.c65{margin-left:5.3pt;margin-right:118pt}.c105{margin-left:107.4pt;margin-right:73pt}.c59{margin-left:87.3pt;margin-right:99.8pt}.c78{margin-left:-16.9pt;margin-right:-19.4pt}.c181{margin-left:3.8pt;margin-right:29.5pt}.c134{margin-left:-16.9pt;margin-right:-18.7pt}.c33{margin-left:1.9pt;margin-right:2.6pt}.c128{text-indent:27pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c108"><p class="c75"><span class="c21">Series ISSN: 2367-2005 301 </span><span class="c118">10.5441/002/edbt.2018.27 </span></p><p class="c23 c33"><span class="c99">Scalable and Dynamic Regeneration of Big Data Volumes </span></p><p class="c60"><span class="c159">Anupam Sanghi, Raghav Sood, Jayant Haritsa </span><span class="c129">Indian Institute of Science Bangalore, India {anupam,raghav,haritsa}@dsl.cds.iisc.ac.in </span></p><p class="c73"><span class="c159">Srikanta Tirthapura </span><span class="c129">Iowa State University Ames, USA snt@iastate.edu </span></p><p class="c45"><span class="c26">ABSTRACT </span><span class="c3">A core requirement of database engine testing is the ability to create synthetic versions of the customer&rsquo;s data warehouse at the vendor site. A rich body of work exists on synthetic data- base regeneration, but suffers critical limitations with regard to: (a) maintaining statistical fidelity to the client&rsquo;s query process- ing, and/or (b) scaling to large data volumes. In this paper, we present HYDRA, a workload-dependent database regenerator that leverages a declarative approach to data regeneration to assure volumetric similarity, a crucial aspect of statistical fidelity, and materially improves on the prior art by adding scale, dy- namism and functionality. Specifically, Hydra uses an optimized linear programming (LP) formulation based on a novel region- partitioning approach. This spatial strategy drastically reduces the LP complexity, enabling it to handle query workloads on which contemporary techniques fail. Second, Hydra incorporates deterministic post-LP processing algorithms that provide high efficiency and improved accuracy. Third, Hydra introduces the concept of dynamic regeneration by constructing a minuscule database summary that can on-the-fly regenerate databases of arbitrary size during query execution, while obeying volumet- ric specifications derived from the query workload. A detailed experimental evaluation on standard OLAP benchmarks demon- strates that Hydra can efficiently and dynamically regenerate large warehouses that accurately mimic the desired statistical characteristics. </span></p><p class="c177"><span class="c26">1 INTRODUCTION </span><span class="c3">In industrial practice, a common requirement for database ven- dors is to adequately test their database engines with represen- tative data and workloads that accurately mimic the data pro- cessing environments at customer deployments. This need can arise either in the analysis of problems currently being faced by clients, or in proactively assessing the performance impacts of planned engine upgrades on client applications. While, in princi- ple, clients could transfer their original data and workloads to the vendor for the intended evaluation purposes, this is often infeasible due to privacy and liability concerns. Moreover, even if a client is willing to share the data, transferring and storing the data at the vendor&rsquo;s site may prove to have impractical space and time overheads, especially in the anticipated Big Data era. For instance, if a customer faces a problem on exabyte (10</span><span class="c15">18</span><span class="c3">) sized relational tables, transferring and storing such data is likely to be infeasible even on the best of systems. Therefore, an important requirement, looking into the future, is to be able to dynamically regenerate representative databases, at query execution time that accurately mimic the behavior of the client&rsquo;s data processing environment. </span></p><p class="c62"><span class="c94">&copy; </span><span class="c14">2018 Copyright held by the owner/author(s). Published in Proceedings of the 21st International Conference on Extending Database Technology (EDBT), March 26-29, 2018, ISBN 978-3-89318-078-3 on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0. </span></p><p class="c19 c46"><span class="c3">A rich body of literature exists on data regeneration, beginning with workload-independent techniques (e.g [12, 15]), which pro- vide scalable and efficient solutions, but fail to retain complex sta- tistical characteristics such as the sizes of intermediate relations created during execution of a query plan. To address this problem, a particularly potent approach of workload-dependent database regeneration was introduced in QAGen [11], and has served as the foundation for many of the practicable systems proposed over the last decade [6, 18]. Workload-dependent techniques aim to generate synthetic data whose behavior is volumetrically similar to the client database on the pre-specified query workload. That is, assuming a common choice of query execution plans at the client and vendor sites (ensured through &ldquo;plan forcing&rdquo; [3] or &ldquo;metadata matching&rdquo; [8]), the output row cardinalities of indi- vidual operators in these plans are very similar in the original and synthetic databases. This similarity helps to preserve the multi-dimensional layout and flow of the data, a pre-requisite for achieving similar performance on the client&rsquo;s workload. As a case in point, the DataSynth [6, 7] tool from Microsoft expresses such volumetric constraints as a Linear Program (LP) whose solution is used to construct the synthetic database. </span></p><p class="c58"><span class="c3">A common limitation of contemporary techniques (reviewed in detail in Section 8), is that they run into issues of scale and efficiency at one stage or the other in the regeneration pipeline. This is partly due to their focus on materialized static solutions, making them impractical at large volumes. Further, the ability to scale to large query workloads and data volumes has not been clearly established, and validations have been typically restricted to relatively simple and small benchmarks such as TPC-H [2]. These limitations become especially problematic from a futuristic &ldquo;Big Data&quot; perspective, where we have to contend with enormous data volumes and complex query workloads. </span></p><p class="c157"><span class="c3">To materially address this challenge, we present HYDRA, a data regeneration tool, which ensures that scale and efficiency are addressed through the entire regeneration pipeline. As a con- crete example, Hydra was able to accurately regenerate the data processing environment of a 100 GB TPC-DS client database with a workload of 131 distinct representative queries, by generating a database summary in less than 2 minutes on a vanilla machine. This summary can be used to statically generate a materialized database, or more potently, to dynamically regenerate the de- sired database during query execution. When the former option is chosen, the static database was successfully created in less than 11 minutes. It is important to note here that the summary construction time is independent of the data scale &ndash; therefore, even the exabyte-sized data scenario alluded to earlier could be modeled in just a few minutes using Hydra! </span></p><p class="c78 c112"><span class="c3">The key contributions of Hydra are the following: Extended Workload Coverage: Hydra incorporates a novel LP formulation technique, region-partitioning, that can en- code volumetric constraints with an LP of low complexity. When compared with the grid-partitioning approach used in DataSynth, region-partitioning reduces the LP complex- ity by many orders of magnitude. For instance, an LP with </span></p><p class="c135"><span class="c3">more than a billion variables in DataSynth is reduced to an LP with a few thousand variables in Hydra&ndash; in fact, in this case, the LP solver crashes on the DataSynth formula- tion, but runs to completion in less than a minute on the Hydra formulation. The beneficial outcome of the low LP complexity is that it facilitates the efficient handling of much richer query workloads. Apart from enhancing the workload scale, Hydra also ex- pands the database scope to include relational schemas that have DAG-structured dependency graphs, and the query scope to include DNF filter predicates. Database Summary and Dynamic Regeneration: A unique feature of our data regeneration approach is that it delivers a database summary as the output, rather than the static data itself. This summary is of negligible size, depending only on the query workload and not on the database scale. It can be used for dynamically generating data during query execution, or for materializing static relations if so desired. This summary-based approach eliminates the enormous time and space overheads incurred by prior techniques in generating and storing data before initiating analysis. Accuracy with Efficiency: Hydra replaces the sampling-based approach to data regeneration in DataSynth by a determin- istic alignment strategy. The alignment operates directly on the database summary, and is therefore extremely ef- ficient. Further, it does not suffer the probabilistic errors that affect the sampling approach, and therefore delivers better fidelity with regard to volumetric similarity. Enhanced Evaluation: We evaluate Hydra on a diverse work- load of 100-plus queries constructed from the complex TPC-DS benchmark, and the results show that it can effi- ciently regenerate databases for such workloads at various data scales. Further, our evaluation is more comprehensive than prior techniques, which have largely been evaluated on simpler and small-sized query workloads operating on modest databases. For instance, DataSynth has been evaluated on simple TPC-H database environments that resulted, with their formulation, in LPs with only a few thousand variables. Integration with CODD: CODD [8] is a graphical tool through which database environments with desired meta-data char- acteristics can be efficiently simulated without persistently generating and/or storing their contents &ndash; i.e. a &ldquo;dataless&rdquo; approach. We have integrated Hydra with CODD, thus providing an end-to-end system that fully replicates the client data processing environment at the vendor&rsquo;s site, and is compliant with the CODD&rsquo;s &ldquo;dataless&rdquo; philosophy. </span></p><p class="c84"><span class="c3">Organization. The remainder of this paper is organized as follows: A brief background on the key underlying concepts is outlined in Section 2. The Hydra architecture is presented in Section 3, and our new region-based LP formulation in Section 4. The database summary generator and the tuple generator are described in Sections 5 and 6, respectively. Our experimental results are analyzed in Section 7. Related work is reviewed in Section 8, and our conclusions are summarized in Section 9. </span></p><p class="c90"><span class="c26">2 PRELIMINARIES </span><span class="c3">In this section, we provide background information on the key foundations &ndash; Annotated Query Plans [11] and Cardinality Con- straints [6] &ndash; that lie under this data regeneration framework. </span></p><p class="c23 c63"><span class="c26">2.1 Annotated Query Plans </span><span class="c3">Consider a toy scenario (for ease of presentation) where the client has the database schema shown in Figure 1a, where pk and fk refer to primary-key and foreign-key attributes, respectively. </span></p><p class="c52"><span class="c3">A sample client query on this schema is shown in Figure 1b, with the corresponding query execution plan in Figure 1c. Note that this execution plan has the output edge of each operator annotated with the associated row cardinality (as evaluated dur- ing the client&rsquo;s execution) &ndash; for instance, there are 50000 rows resulting from the join of R and (filtered) S. Such a plan is referred to as an &ldquo;Annotated Query Plan&rdquo; (AQP) in [11]. The goal now is to generate synthetic data at the vendor site such that when the above query is executed on this data, we obtain an identical, or very similar, AQP. </span></p><p class="c77"><span class="c3">R (R_pk, S_fk, T_fk) S (S_pk, A, B) T (T_pk, C) </span></p><p class="c103"><span class="c9">(a) Database Schema </span></p><p class="c139"><span class="c9">select * from R, S, T where R.S_fk = S.S_pk and R.T_fk = T.T_pk and S.A &gt;= 20 and S.A &lt; 60 and T.C &gt;= 2 and T.C &lt; 3 </span></p><p class="c148"><span class="c9">(b) Example Query </span></p><p class="c48"><span class="c1">30000 </span><span class="c3">&#9667;&#9657; </span><span class="c9">R.T_fk = T.T_pk </span></p><p class="c125"><span class="c1">900 </span></p><p class="c23 c105"><span class="c1">50000 </span></p><p class="c23 c105"><span class="c1">50000 </span></p><p class="c131"><span class="c3">&sigma;</span><span class="c0">C &isin;[2,3) </span></p><p class="c57"><span class="c3">&#9667;&#9657; </span><span class="c9">R.S_fk = S.S_pk </span></p><p class="c57"><span class="c3">&#9667;&#9657; </span><span class="c9">R.S_fk = S.S_pk </span></p><p class="c34"><span class="c3">T </span><span class="c9">size = 1500 </span></p><p class="c12"><span class="c3">&sigma;</span><span class="c0">A&isin;[20,60) </span></p><p class="c12"><span class="c3">&sigma;</span><span class="c0">A&isin;[20,60) </span></p><p class="c42"><span class="c9">(c) Annotated Query Plan (AQP) </span></p><p class="c109"><span class="c9">|R| = 80000 |S | = 700 |T | = 1500 |&sigma;</span><span class="c16">S .A&isin;[20,60)</span><span class="c9">(S)| = 400 |&sigma;</span><span class="c16">T .C&isin;[2,3)</span><span class="c9">(T )| = 900 |&sigma;</span><span class="c16">S .A&isin;[20,60)</span><span class="c9">(R &#9667;&#9657; S)| = 50000 |&sigma;</span><span class="c16">S .A&isin;[20,60)&and;T .C&isin;[2,3)</span><span class="c9">(R &#9667;&#9657; S &#9667;&#9657; T )| = 30000 </span></p><p class="c117"><span class="c9">(d) Cardinality Constraints (CCs) </span></p><p class="c67"><span class="c3">Figure 1: Example Database Scenario </span></p><p class="c100"><span class="c26">2.2 Cardinality Constraints </span><span class="c3">A unified and declarative mechanism for representing AQP data characteristics, called cardinality constraints (CCs), was proposed in [6]. For instance, the CCs expressing the AQP of Figure 1c are shown in Figure 1d. The data regeneration technique takes the schematic information and the set of CCs from the client site and produces synthetic data that closely meets these CCs. To make the problem tractable, it is assumed that CCs consist of filters on only non-key attributes, and that all joins are between primary keys and foreign keys, typically the case in data warehouses. </span></p><p class="c88"><span class="c21">302 </span></p><p class="c23 c59"><span class="c1">400 </span></p><p class="c23 c59"><span class="c1">400 </span></p><p class="c22"><span class="c3">R </span><span class="c9">size = 80000 </span></p><p class="c101"><span class="c3">R </span><span class="c9">size = 80000 </span></p><p class="c101"><span class="c3">R </span><span class="c9">size = 80000 </span></p><p class="c47"><span class="c3">S </span><span class="c9">size = 700 </span></p><p class="c158"><span class="c3">Figure 2: Hydra Architecture </span></p><p class="c164"><span class="c26">3 THE HYDRA ARCHITECTURE </span><span class="c3">In this section, we present an overview of Hydra&rsquo;s architecture, along with a summary of its various components and their inter- actions with the database engine. A pictorial view of the archi- tecture is presented in Figure 2 &ndash; in this picture, the green boxes represent the new components designed specifically for Hydra. Among these, the primary components are the LP Formulator, the Summary Generator, and the Tuple Generator, all shown with thick borders. The other modules have been sourced from the lit- erature, including the preprocessor (orange) from DataSynth [7], the CODD metadata processor (yellow) [8], and the Z3 solver (blue) [14]. (Refer [21] for complete details.) </span></p><p class="c144 c166"><span class="c26">3.1 Client Site </span><span class="c3">The information flow from the client to the vendor is as fol- lows: At the client site, Hydra fetches the schema information (S), and the query workload (q</span><span class="c0">1</span><span class="c11">,q</span><span class="c0">2</span><span class="c11">,q</span><span class="c0">3</span><span class="c11">, ...,q</span><span class="c0">n</span><span class="c11">) with its corresponding </span><span class="c3">AQPs (p</span><span class="c0">1</span><span class="c11">,p</span><span class="c0">2</span><span class="c11">,p</span><span class="c0">3</span><span class="c11">, ...,p</span><span class="c0">n</span><span class="c11">) obtained from the database engine. The </span><span class="c3">AQPs are converted to equivalent cardinality constraints (CCs) using a Parser. The metadata (M) from the database catalogs is captured with the help of CODD. In order to address client secu- rity concerns, all this information (schema, metadata, queries and CCs) is passed through an Anonymizer that suitably masks the information before shipping it to the vendor. Also in this process, non-numeric constants appearing in the queries and plans are mapped to numbers to facilitate LP formulation at the vendor site. Due to this mapping, the final database summary generated at the vendor site also consists of only numeric datatypes. It is possible to reverse this mapping to get back the original datatypes, but is not a relevant consideration with regard to satisfying CCs. </span></p><p class="c136"><span class="c26">3.2 Vendor Site </span><span class="c3">The main modules at the vendor site are as follows: </span></p><p class="c155"><span class="c3">Preprocessor [7]: In this module, sourced from DataSynth, the schema information and CCs obtained from the client are pro- cessed to create the input for the LP Formulator. Each relation is solved independently, and this process is initiated by first creat- ing a view comprised of its own non-key attributes, augmented with the non-key attributes of the relations on which it depends through referential constraints (both directly or transitively). This transformation results in replacing the join-expression present in </span></p><p class="c19 c63"><span class="c3">a CC with a view that covers all the attributes (non-key) featured in the relations participating in the join-expression. As a case in point, following views are generated for the example in Figure 1: </span></p><p class="c79"><span class="c3">R_view (A, B, C) S_view (A, B) T_view (C) Further, the last two constraints in Figure 1d can be rewritten as: </span></p><p class="c116"><span class="c3">|&sigma;</span><span class="c0">A&isin;[20,60) </span><span class="c3">(R_view)| = 50000 |&sigma;</span><span class="c0">A&isin;[20,60)&and;C &isin;[2,3) </span><span class="c3">(R_view)| = 30000 An LP is independently formulated for each view created by the above process. Since the LP complexity is adversely affected by the number of attributes in the view, the view is first decom- posed into a set of sub-views to reduce the effective complexity. This is achieved as follows: Construct a &ldquo;view-graph&rdquo; by first creating a node for each attribute, and then inserting an edge between a pair of nodes if the corresponding attributes appear together in one or more CCs. Further, additional edges are added (if required) to make the view-graph to be chordal, a property re- quired to ensure acylicity in the subsequent processing. Now, the sub-views are identified as the maximal cliques in the view-graph. </span></p><p class="c128 c167"><span class="c3">LP Formulator and Solver: For each view, the LP Formulator takes as input the corresponding set of subviews and applicable CCs, and then constructs the LP. The domain corresponding to each sub-view is partitioned into regions using a novel region- partitioning algorithm that takes as input the different cardinality constraints. There is one variable for each region, corresponding to the number of tuples chosen from the region. Each cardinality constraint is encoded as an LP constraint on these variables, and the solution of the LP is used in deciding which tuples to include in the sub-view. The complete details of this algorithm are enumerated in Section 4. </span></p><p class="c50"><span class="c3">Our region-partitioning strategy is in marked contrast to the grid-partitioning strategy used in DataSynth. Grid-Partitioning first intervalizes the domain of each attribute based on the con- stants appearing in the CCs, and divides the domain into a grid aligned with the interval boundaries for each attribute. If a sub- view has n attributes, and each attribute gets divided into l in- tervals, then the domain of the sub-view is partitioned into a grid of l</span><span class="c15">n </span><span class="c3">cells. For each cell in the grid, a variable is created that represents the number of data rows present in that cell. In contrast, our region-partitioning strategy divides the domain into only the number of regions required to precisely write out each </span></p><p class="c93"><span class="c21">303 </span></p><p class="c176"><span class="c3">cardinality constraint, and assigns one variable to each region &ndash; this typically leads to far fewer variables than grid-partitioning. To make the above concrete, consider a single view &ldquo;Person&quot; with the following three selection CCs: </span></p><p class="c89"><span class="c3">|age &lt; 40 &and; salary &lt; 40K (Person)| = 1000 |20 &le; age &lt; 60 &and; 20K &le; salary &lt; 60K (Person)| = 2000 |Person| = 8000 Grid-partitioning divides the domain of the view as shown in Figure 3a. With a variable assigned to each grid cell, there is a total of 16 variables. In contrast, the region-partitioning strategy partitions the space into 4 regions as shown in Figure 3b, resulting in a tally of only 4 variables. </span></p><p class="c19 c187"><span class="c3">However, we have chosen not to take this approach since the computational overheads incurred are enormous, and the sam- pling process introduces errors in volumetric fidelity. Instead, we have designed and implemented an alternative data-scale free, deterministic alignment algorithm (details in Section 5), which produces an intermediate database summary in the out- put. This component is also responsible for ensuring that the generated summary obeys referential integrity. Finally, summa- rized relations from corresponding view summary are obtained. An example database summary finally obtained from the AQP shown in Figure 1c, along with additional two AQPs, is shown in Figure 5. Here, entries of the type a - b in the primary key columns (e.g. 101-250 for S_pk in table S), mean that the relation hasb&minus;a+1 tuples with values (a,a+1, a+2,...,b) for that column, keeping the other columns unchanged. </span></p><p class="c53"><span class="c9">(b) Region-Partitioning </span></p><p class="c23 c65"><span class="c9">(a) Grid-Partitioning </span></p><p class="c23 c170"><span class="c3">Figure 5: Example Database Summary </span></p><p class="c23 c170"><span class="c3">Figure 5: Example Database Summary </span></p><p class="c143"><span class="c3">Figure 3: Grid-Partitioning vs Region-Partitioning </span></p><p class="c4"><span class="c3">The CCs of Person, expressed in terms of LP constraints, are shown below in Figure 4a and 4b for grid-partitioning and region- partitioning, respectively. </span></p><p class="c19 c128 c134"><span class="c3">Tuple Generator: The Tuple Generator resides in the database engine. It ensures that whenever a query is fired, data is not fetched from the disk but instead gets generated on-demand, using the database summary. The details of this component and its implementation in PostgreSQL are presented in Section 6. </span></p><p class="c72"><span class="c3">x</span><span class="c0">9 </span><span class="c11">+ x</span><span class="c0">10 </span><span class="c11">+ x</span><span class="c0">13 </span><span class="c11">+ x</span><span class="c0">14 </span><span class="c11">= 1000 </span><span class="c3">x</span><span class="c0">6 </span><span class="c11">+ x</span><span class="c0">7 </span><span class="c11">+ x</span><span class="c0">10 </span><span class="c11">+ x</span><span class="c0">11 </span><span class="c11">= 2000 </span><span class="c3">x</span><span class="c0">1 </span><span class="c11">+ x</span><span class="c0">2 </span><span class="c11">+ ... + x</span><span class="c0">16 </span><span class="c11">= 8000 </span></p><p class="c32 c92"><span class="c3">y</span><span class="c0">1 </span><span class="c11">+y</span><span class="c0">2 </span><span class="c11">= 1000 </span><span class="c3">y</span><span class="c0">2 </span><span class="c11">+y</span><span class="c0">3 </span><span class="c11">= 2000 </span><span class="c3">y</span><span class="c0">1 </span><span class="c11">+y</span><span class="c0">2 </span><span class="c11">+y</span><span class="c0">3 </span><span class="c11">+y</span><span class="c0">4 </span><span class="c11">= 8000 </span></p><p class="c19 c78 c128"><span class="c3">We note in closing that in order to ensure the execution plan chosen at the vendor site is the same as that in the client site, metadata matching is implemented in Hydra using CODD&rsquo;s meta- data transfer feature. </span></p><p class="c19 c78 c128"><span class="c3">We note in closing that in order to ensure the execution plan chosen at the vendor site is the same as that in the client site, metadata matching is implemented in Hydra using CODD&rsquo;s meta- data transfer feature. </span></p><p class="c24 c144"><span class="c26">4 LP FORMULATION </span><span class="c3">An LP for a view V is constructed as follows: For each sub-view s in V, every CC that is within its scope is formulated as an LP constraint. Since sub-views may share common attributes, additional consistency constraints are added to the LP to ensure that the marginal distributions along the common set of attributes are identical in the solutions for the sub-views. </span></p><p class="c23 c24"><span class="c26">4 LP FORMULATION </span><span class="c3">An LP for a view V is constructed as follows: For each sub-view s in V, every CC that is within its scope is formulated as an LP constraint. Since sub-views may share common attributes, additional consistency constraints are added to the LP to ensure that the marginal distributions along the common set of attributes are identical in the solutions for the sub-views. </span></p><p class="c23 c65"><span class="c9">(a) Grid-Partitioning </span></p><p class="c23 c146"><span class="c9">(b) Region-Partitioning </span></p><p class="c41"><span class="c3">Figure 4: LP Constraints </span></p><p class="c4"><span class="c3">The LPs are passed on to the solver, which provides one of the feasible solutions as the output &ndash; we have used Z3 [14], a popular SMT solver, to implement this functionality. With region- partitioning, the LP is usually much simpler due to the smaller number of variables. Further, as the cardinality constraints get more complex, the differences in complexity of the LPs produced by region-partitioning and grid-partitioning become more pro- nounced. This effect is quantified in Section 7. </span></p><p class="c174"><span class="c3">Summary Generator: This module generates the database sum- mary from the LP solutions obtained on the views. Since parti- tioning is carried out at a sub-view level, the LP solution, which is expressed in terms of sub-view variables, needs to be mapped to equivalents in the original view space. A sampling-based ap- proach was proposed in [6] for this purpose &ndash; for example, say a view (A,B,C) is split into a pair of sub-views (A,B) and(B,C), the algorithm computes the distributions Prob(A,B) and Prob(C|B). Then, each tuple is generated by first sampling a point from the former distribution, and then sampling a point from the latter conditioned on this outcome. </span></p><p class="c23 c24"><span class="c26">4 LP FORMULATION </span><span class="c3">An LP for a view V is constructed as follows: For each sub-view s in V, every CC that is within its scope is formulated as an LP constraint. Since sub-views may share common attributes, additional consistency constraints are added to the LP to ensure that the marginal distributions along the common set of attributes are identical in the solutions for the sub-views. </span></p><p class="c23 c24"><span class="c26">4 LP FORMULATION </span><span class="c3">An LP for a view V is constructed as follows: For each sub-view s in V, every CC that is within its scope is formulated as an LP constraint. Since sub-views may share common attributes, additional consistency constraints are added to the LP to ensure that the marginal distributions along the common set of attributes are identical in the solutions for the sub-views. </span></p><p class="c50"><span class="c3">In this section, we first present the mathematical basis under- lying our formulation of LP constraints for a set of CCs applicable on a sub-view. We then present an algorithm that partitions the domain into the minimum number of regions required to capture each CC precisely, resulting in an LP with the optimal number of variables. Finally, we discuss the formulation of additional consistency constraints to ensure consistency across multiple sub-views belonging to V. </span></p><p class="c134 c184"><span class="c26">4.1 Mathematical Basis for LP Formulation </span><span class="c3">Let n denote the number of attributes in the given sub-view s, D</span><span class="c0">i </span><span class="c11">the domain of the ith attribute, and D the data universe </span><span class="c3">D</span><span class="c1">1 </span><span class="c3">&times; D</span><span class="c1">2 </span><span class="c3">&times;&middot;&middot;&middot;D</span><span class="c1">n</span><span class="c3">. </span></p><p class="c63 c81"><span class="c3">We are given a set of m CCs that are applicable on s. For 1 &le; j &le; m, each constraint C</span><span class="c0">j </span><span class="c11">is a pair &#10216;&sigma;</span><span class="c0">j</span><span class="c11">,k</span><span class="c0">j</span><span class="c11">&#10217; where &sigma;</span><span class="c0">j </span><span class="c11">is a </span><span class="c3">selection predicate and k</span><span class="c1">j </span><span class="c3">is a non-negative integer equal to the number of rows satisfying predicate &sigma;</span><span class="c0">j</span><span class="c11">. We assume that each </span><span class="c3">predicate is in disjunctive normal form (DNF). </span></p><p class="c142"><span class="c21">304 </span></p><p class="c19"><span class="c3">Simple LP Formulation. Let us first consider a simple way of formulating an LP that encodes all CCs. For each tuple t &isin; D, assign a variable x</span><span class="c1">t </span><span class="c3">that denotes the number of copies of t in the sub-view s. Then, the LP formulation shown in Figure 6 ensures that a feasible solution satisfies all CCs, including a constraint on the total size of s. </span></p><p class="c19"><span class="c3">The problem with this formulation is that the number of vari- ables in the resulting LP is as large as the size of the universe D. Hence, it is infeasible to work directly with this formulation. </span></p><p class="c23"><span class="c3">(1) For each t &isin; D,x</span><span class="c1">t </span><span class="c3">&ge; 0 </span></p><p class="c23"><span class="c3">(2) </span></p><p class="c23"><span class="c3">[ </span><span class="c0">t &isin;D </span></p><p class="c23"><span class="c3">x</span><span class="c1">t</span><span class="c27">] </span></p><p class="c23"><span class="c3">= k </span></p><p class="c23"><span class="c3">(3) For each j,1 &le; j &le; m, </span></p><p class="c23"><span class="c3">&#63726;</span><span class="c11">&#63727;&#63727;&#63727;&#63727;&#63728; </span><span class="c1">t:&sigma;</span><span class="c2">j</span><span class="c0">(t)=true </span></p><p class="c23"><span class="c3">x</span><span class="c0">t</span><span class="c3">&#63737;</span><span class="c11">&#63738;&#63738;&#63738;&#63738;&#63739; </span></p><p class="c23"><span class="c3">= k</span><span class="c0">j </span></p><p class="c23"><span class="c3">Figure 6: Simple LP formulation </span></p><p class="c32"><span class="c3">Reduced LP Formulation. We can derive an LP with far fewer variables as follows: We first note that in the simple formulation, variables corresponding to a pair of points t</span><span class="c0">1</span><span class="c11">,t</span><span class="c0">2 </span><span class="c11">&isin; D that behave </span><span class="c3">identically with respect to a constraintC</span><span class="c0">j </span><span class="c11">(i.e.&sigma;</span><span class="c0">j</span><span class="c11">(t</span><span class="c0">1</span><span class="c11">) = &sigma;</span><span class="c0">j</span><span class="c11">(t</span><span class="c0">2</span><span class="c11">)) can </span><span class="c3">be constraint combined C</span><span class="c0">j</span><span class="c11">. </span><span class="c3">together </span><span class="c11">If this is </span><span class="c3">as </span><span class="c11">true </span><span class="c3">(x</span><span class="c1">t</span><span class="c2">1 </span><span class="c11">that </span><span class="c3">+x</span><span class="c1">t</span><span class="c11">with </span><span class="c2">2</span><span class="c3">), for </span><span class="c11">respect </span><span class="c3">the purposes </span><span class="c11">to every </span><span class="c3">of </span><span class="c11">constraint </span><span class="c3">satisfying </span></p><p class="c32"><span class="c3">C</span><span class="c0">j </span><span class="c11">for j = 1 ...m, &sigma;</span><span class="c0">j</span><span class="c11">(t</span><span class="c0">1</span><span class="c11">) = &sigma;</span><span class="c0">j</span><span class="c11">(t</span><span class="c0">2</span><span class="c11">), then there is no need to treat </span><span class="c3">t</span><span class="c1">1 </span><span class="c3">and t</span><span class="c1">2 </span><span class="c3">separately &ndash; instead, they can be combined into a single variable region, in the LP. and (xBy </span><span class="c0">t</span><span class="c2">1 </span><span class="c3">the + repeating xvariables </span><span class="c0">t</span><span class="c2">2</span><span class="c3">) in every this x</span><span class="c0">t</span><span class="c2">1 </span><span class="c3">variable equation, and x</span><span class="c0">t</span><span class="c2">2 </span><span class="c3">merging can leading be merged process to fewer into recursively variables a single </span></p><p class="c23"><span class="c3">until it is no further possible, we arrive at a vastly reduced LP. </span></p><p class="c19"><span class="c3">We hasten to add that the above LP construction process based on merging variables is only for illustrating the concept &ndash; the actual algorithm employed in our system directly derives the regions, as described in Section 4.2. </span></p><p class="c23"><span class="c3">For constraint C and t &isin; D, let C(t) be an indicator variable: </span></p><p class="c23"><span class="c3">C(t) = </span></p><p class="c106"><span class="c3">{ </span><span class="c11">true if t satisfies C </span><span class="c3">false otherwise </span></p><p class="c23"><span class="c3">Definition 4.1. For a pair of points p,q &isin; D and a set of con- straints C, we say pR</span><span class="c15">C</span><span class="c3">q if for each C &isin; C, C(p) = C(q). </span></p><p class="c23"><span class="c3">Observation 1. R</span><span class="c15">C </span><span class="c3">is an equivalence relation on D. </span></p><p class="c19"><span class="c3">Proof. It can be easily seen thatR</span><span class="c15">C </span><span class="c3">is reflexive and symmetric. For transitivity, suppose that for p,q,r &isin; D, pR</span><span class="c15">C</span><span class="c3">q and qR</span><span class="c15">C</span><span class="c3">r. Note that for each C &isin; C, it must be true that C(p) = C(q) and C(q) = C(r). Therefore, it must be true that C(p) = C(r) for each C &isin; C, showing that the relation is transitive. </span><span class="c56">D </span></p><p class="c19"><span class="c3">A partition of D is a set of subsets of D such that every element x &isin; D is in exactly one of these subsets. The individual sets in a partition are called blocks. </span></p><p class="c19"><span class="c3">Definition 4.2. A set of points b is said to be valid with respect to a set of constraints C if for any two points p,q &isin; b, pR</span><span class="c15">C</span><span class="c3">q. Given a set of constraints C, a partition P of D is said to be a valid partition if for each blockb &isin; P, b is valid with respect to C. </span></p><p class="c23"><span class="c3">In a valid partition of D with respect to C, any pair of points within the same block satisfy the same set of CCs. Once we </span></p><p class="c19"><span class="c3">obtain a valid partition P, the LP can be re-formulated as shown in Figure 7. Instead of a variable for each point t &isin; D, there is now a single variable x</span><span class="c0">b </span><span class="c3">for each block b &isin; P representing the number of tuples of the sub-view that are contained in this block. Note that the tuples in a sub-view need not be unique, therefore x</span><span class="c0">b </span><span class="c3">may include duplicates in its count. </span></p><p class="c23"><span class="c3">(1) For each b &isin; P,x</span><span class="c0">b </span><span class="c3">&ge; 0 </span></p><p class="c23"><span class="c3">(2) </span></p><p class="c23"><span class="c3">[ </span><span class="c0">b&isin;P</span><span class="c3">x</span><span class="c0">b</span><span class="c3">] </span></p><p class="c23"><span class="c3">= k </span></p><p class="c23"><span class="c3">(3) For each j, 1 &le; j &le; m, </span></p><p class="c23"><span class="c3">&#63726;</span><span class="c11">&#63727;&#63727;&#63727;&#63727;&#63728; </span><span class="c1">b:&sigma;</span><span class="c2">j</span><span class="c0">(b)=true </span></p><p class="c23"><span class="c3">x</span><span class="c0">b</span><span class="c3">&#63737;</span><span class="c11">&#63738;&#63738;&#63738;&#63738;&#63739; </span></p><p class="c23"><span class="c3">= k</span><span class="c0">j </span></p><p class="c23"><span class="c3">Figure 7: Reduced LP formulation </span></p><p class="c19"><span class="c3">The total number of variables in the reduced LP shown in Figure 7 is equal to the number of blocks in the partition P and is potentially much smaller than the number of variables in the original LP, shown in Figure 6. Since we desire an LP with the smallest number of variables, we look for a valid partition of D with the minimum number of blocks. A valid partition with respect to C is an optimal partition if it has the smallest number of blocks from among all valid partitions of D with respect to C. </span></p><p class="c23"><span class="c3">Lemma 4.3. The quotient set of D by R</span><span class="c15">C </span><span class="c3">is the (unique) optimal partition of D with respect to C. </span></p><p class="c32"><span class="c3">Proof. Let P</span><span class="c0">1 </span><span class="c11">denote the quotient set</span><span class="c1">1 </span><span class="c11">of D by R</span><span class="c1">C</span><span class="c11">. By the </span><span class="c3">definition of an equivalence relation, for any block b points in b are related to each other by R</span><span class="c15">C</span><span class="c3">, and hence &isin; P</span><span class="c0">1</span><span class="c11">, all </span><span class="c3">P</span><span class="c0">1 </span><span class="c11">is a </span><span class="c3">valid partition. </span></p><p class="c32"><span class="c3">Suppose that P</span><span class="c1">1 </span><span class="c3">is not the unique optimal partition. Then, there must exist another valid partition P</span><span class="c0">2 </span><span class="c11">such that P</span><span class="c0">2 </span><span class="c11">P</span><span class="c0">1 </span><span class="c11">and </span><span class="c3">|P</span><span class="c0">2</span><span class="c11">| &le; |P</span><span class="c0">1</span><span class="c11">|. This implies that there exist two points p,q &isin; D </span><span class="c3">such that p and q are in different blocks in P</span><span class="c1">1</span><span class="c3">, but in the same block in P</span><span class="c0">2</span><span class="c11">. Since p </span><span class="c3">be true that p andq </span><span class="c11">andq </span><span class="c3">are </span><span class="c11">belong to different </span><span class="c3">not related by R</span><span class="c15">C</span><span class="c3">. </span><span class="c11">blocks in P</span><span class="c0">1</span><span class="c11">, it must </span><span class="c3">But, in P</span><span class="c0">2 </span><span class="c11">points p and </span><span class="c3">q belong to the same block, which implies that P</span><span class="c1">2 </span><span class="c3">cannot be a valid partition, a contradiction. </span><span class="c56">D </span></p><p class="c23"><span class="c26">4.2 Deriving the Optimal Partition </span><span class="c3">We now present an algorithm to derive the optimal partition of D with respect to C. Each constraint C &isin; C is in DNF, and is ex- pressed as the union of many smaller &ldquo;sub-constraints&quot;. Each sub- constraint is the conjunction of many per-attribute constraints, and each per-attribute constraint is a constraint on the values that the attribute is permitted to take. For example, the following constraint on attributes A</span><span class="c0">1 </span><span class="c11">and A</span><span class="c0">2</span><span class="c11">: </span></p><p class="c23"><span class="c3">((A</span><span class="c0">1 </span><span class="c11">&le; 20)&and;(A</span><span class="c0">2 </span><span class="c11">&gt; 30)) &or; (A</span><span class="c0">1 </span><span class="c11">&gt; 50) </span><span class="c3">is divided into the basic sub-constraints: </span></p><p class="c106"><span class="c3">(A</span><span class="c0">1 </span><span class="c11">&le; 20)&and;(A</span><span class="c0">2 </span><span class="c11">&gt; 30) and (A</span><span class="c0">1 </span><span class="c11">&gt; 50) </span><span class="c3">Algorithm 1 (Optimal Partition) takes a set of DNF constraints as input, and returns a partition with the smallest number of regions with respect to this set. Internally, it invokes Algorithm 2 (Valid Partition) that takes a set of sub-constraints as input and returns a valid partition of the domain with respect to this set. </span></p><p class="c23"><span class="c25">1</span><span class="c30">The quotient set is the set of equivalence classes resulting from </span><span class="c0">R</span><span class="c25">C </span><span class="c30">on </span><span class="c0">D</span><span class="c30">. </span></p><p class="c23"><span class="c21">305 </span></p><p class="c23"><span class="c3">Algorithm 1: Optimal Partition(D, C) </span></p><p class="c23"><span class="c3">Input: Universe D, set of DNF constraints C Output: An optimal partition P</span><span class="c15">&lowast; </span><span class="c3">of D subject to C </span><span class="c5">1 </span><span class="c3">Generate the set of sub-constraints C</span><span class="c15">&prime; </span><span class="c3">resulting from the </span></p><p class="c23"><span class="c3">constraints in C; </span><span class="c5">2 </span><span class="c3">Construct a valid partition P</span><span class="c15">&prime; </span><span class="c3">of D subject to C</span><span class="c15">&prime; </span><span class="c3">using </span></p><p class="c23"><span class="c3">Valid-Partition(D, C</span><span class="c15">&prime;</span><span class="c3">) (Algorithm 2); </span><span class="c5">3 </span><span class="c3">For each block b &isin; P</span><span class="c15">&prime;</span><span class="c3">, compute the label l(b), equal to the </span></p><p class="c23"><span class="c3">set of all constraints in C that b satisfies. Let L denote the set of all distinct labels from {l(b)|b &isin; P</span><span class="c15">&prime;</span><span class="c3">}; </span><span class="c5">4 </span><span class="c3">Coarsen partition P</span><span class="c15">&prime; </span><span class="c3">into P</span><span class="c15">&lowast; </span><span class="c3">as follows: For each label l &isin; L, merge all blocks in P</span><span class="c15">&prime; </span><span class="c3">whose labels equal l into a single block; </span><span class="c5">5 </span><span class="c3">Return P</span><span class="c15">&lowast;</span><span class="c3">; </span></p><p class="c23"><span class="c3">Lemma 4.4. Given a set of DNF constraints C, Algorithm 1 re- turns an optimal partition of D with respect to C. </span></p><p class="c19"><span class="c3">Proof. As in the algorithm, let C</span><span class="c15">&prime; </span><span class="c3">denote the set of sub- constraints resulting from constraints in C. From Lemma 4.7, we know that P</span><span class="c15">&prime; </span><span class="c3">is a valid partition with respect to C</span><span class="c15">&prime;</span><span class="c3">. Consider any block b &isin; P</span><span class="c15">&prime;</span><span class="c3">. Since b is valid with respect to C</span><span class="c15">&prime;</span><span class="c3">, and each constraint in C</span><span class="c15">&prime; </span><span class="c3">is stricter than a corresponding constraint in C, b is valid with respect to C. Hence, P</span><span class="c15">&prime; </span><span class="c3">is a valid partition with respect to C. </span></p><p class="c19"><span class="c3">Next, consider that each block b</span><span class="c15">&lowast; </span><span class="c3">in P</span><span class="c15">&lowast; </span><span class="c3">was obtained by merg- ing blocks in P</span><span class="c15">&prime; </span><span class="c3">that have the same label. For any pair of points p,q in b</span><span class="c15">&lowast;</span><span class="c3">, it is true they satisfy the same set of constraints in C, showing that P</span><span class="c15">&lowast; </span><span class="c3">is a valid partition wrt C. Also, any two blocks in P</span><span class="c15">&lowast; </span><span class="c3">have distinct labels (if they had the same label, they would have been merged). Therefore, we conclude using arguments similar to Lemma 4.3 that P</span><span class="c15">&lowast; </span><span class="c3">is an optimal partition of D with respect to C. </span><span class="c56">D </span></p><p class="c19"><span class="c3">Deriving a Valid Partition for a Set of Sub-Constraints: We now present an algorithm for deriving a valid partition with a small number of blocks, for a set of sub-constraints C. </span></p><p class="c19"><span class="c3">Definition 4.5. For a sub-constraint C and dimension i, letC</span><span class="c15">i </span><span class="c3">denote the restriction (projection) ofC to dimension i. Further, let C</span><span class="c15">i</span><span class="c1">1 </span><span class="c27">= </span><span class="c1">k=1...i </span><span class="c27">C</span><span class="c15">k </span><span class="c3">denote the restriction of C to dimensions 1, 2,...,i. For instance, if C = (A</span><span class="c0">1 </span><span class="c11">&ge; 1)&and;(A</span><span class="c0">2 </span><span class="c11">&ge; 4)&and;(A</span><span class="c0">2 </span><span class="c11">&le; </span><span class="c3">5)&and;(A</span><span class="c1">3 </span><span class="c3">&gt; 6), then C</span><span class="c15">2 </span><span class="c3">= (A</span><span class="c1">2 </span><span class="c3">&ge; 4)&and;(A</span><span class="c1">2 </span><span class="c3">&le; 5), and C</span><span class="c15">2</span><span class="c1">1 </span><span class="c27">= (A</span><span class="c15">1 </span><span class="c27">&ge; </span><span class="c3">1)&and;(A</span><span class="c0">2 </span><span class="c11">&ge; 4)&and;(A</span><span class="c0">2 </span><span class="c11">&le; 5). For convenience, if C does not have a </span><span class="c3">constraint along dimension i, then C</span><span class="c15">i </span><span class="c3">is defined to be &ldquo;true&rdquo;. </span></p><p class="c19"><span class="c3">Our algorithm, described in Algorithm 2, proceeds iteratively, one dimension at a time. Before processing dimension i, it has a partition of D that is a valid partition subject to constraints along dimensions 1 till (i&minus;1). In processing dimensioni, it refines the current partition as follows: For each block b in the current partition, it appropriately divides the block along dimension i if there is a constraint C &isin; C such that there are some points in b that satisfy constraint C</span><span class="c15">i</span><span class="c3">, and some that do not. </span></p><p class="c19"><span class="c3">Definition 4.6. A constraint C is said to split a block b &sube; D if there exist a pair of points p</span><span class="c0">1</span><span class="c11">,p</span><span class="c0">2 </span><span class="c11">&isin; b such that C(p</span><span class="c0">1</span><span class="c11">) = true </span><span class="c3">and C(p</span><span class="c1">2</span><span class="c3">) = false. If C splits b, then refining b by C partitions b into two subsets b</span><span class="c15">+</span><span class="c3">(C) = {x &isin; b|C(x) = true} and b</span><span class="c15">&minus;</span><span class="c3">(C) = {x &isin; b|C(x) = false}. </span></p><p class="c23"><span class="c3">Lemma 4.7. Given a set of sub-constraintsC, Algorithm 2 returns a valid partition of D with respect to C. </span></p><p class="c23"><span class="c3">Algorithm 2: Valid-Partition(D, C) </span></p><p class="c23"><span class="c3">Input: Universe D, set of sub-constraints C Output: A valid partition P of D subject to set of </span></p><p class="c23"><span class="c3">sub-constraints C </span><span class="c5">1 </span><span class="c3">P</span><span class="c15">0 </span><span class="c3">= {D} </span><span class="c56">// A partition with one set, </span><span class="c3">D</span><span class="c56">. </span><span class="c5">2 </span><span class="c3">for i from 1 to n do </span><span class="c5">3 </span><span class="c3">M &larr; P</span><span class="c15">i&minus;1</span><span class="c3">; </span><span class="c5">4 </span><span class="c3">foreach C &isin; C do </span><span class="c5">5 </span><span class="c3">M</span><span class="c15">&prime; </span><span class="c3">&larr; &empty;; </span><span class="c5">6 </span><span class="c3">foreach block b &isin; M do </span><span class="c5">7 </span><span class="c3">if C</span><span class="c15">i </span><span class="c3">splits b then </span><span class="c5">8 </span><span class="c3">Let b</span><span class="c15">+ </span><span class="c3">and b</span><span class="c15">&minus; </span><span class="c3">result from refining b withC</span><span class="c15">i</span><span class="c3">; </span><span class="c5">9 </span><span class="c3">Add b</span><span class="c15">+ </span><span class="c3">and b</span><span class="c15">&minus; </span><span class="c3">to M</span><span class="c15">&prime;</span><span class="c3">; </span><span class="c5">10 </span><span class="c3">else </span><span class="c5">11 </span><span class="c3">Add b to M</span><span class="c15">&prime;</span><span class="c3">; </span></p><p class="c23"><span class="c5">12 </span><span class="c3">M &larr; M</span><span class="c15">&prime;</span><span class="c3">; </span><span class="c5">13 </span><span class="c3">P</span><span class="c15">i </span><span class="c3">&larr; M; </span><span class="c5">14 </span><span class="c3">Return P</span><span class="c15">n</span><span class="c3">; </span></p><p class="c19"><span class="c3">Proof. For 1 &le; i &le; n, let C</span><span class="c15">i</span><span class="c1">1 </span><span class="c27">= {C</span><span class="c15">i</span><span class="c1">1</span><span class="c27">|C &isin; C}. We show by </span><span class="c3">induction on i that after the ith iteration of the outermost for loop in the algorithm, P</span><span class="c15">i </span><span class="c3">contains a valid partition of D with respect to C</span><span class="c15">i</span><span class="c1">1</span><span class="c27">. Since C</span><span class="c15">n</span><span class="c1">1 </span><span class="c27">= C, it follows that after n iterations, P</span><span class="c15">n </span><span class="c3">contains a valid partition of D with respect to C. We consider i = 0 as the base case, and the set C</span><span class="c15">0</span><span class="c1">1 </span><span class="c27">as a set of &ldquo;always true&quot; </span><span class="c3">constraints. Hence, P</span><span class="c15">0</span><span class="c3">, which consists of only one element, D, is a valid partition with respect to C</span><span class="c15">0</span><span class="c1">1</span><span class="c27">. </span></p><p class="c19"><span class="c3">For the inductive step, suppose that for i &gt; 0, P</span><span class="c15">i&minus;1 </span><span class="c3">is a valid partition of D with respect to C</span><span class="c15">i&minus;1 </span><span class="c1">1 </span><span class="c27">. For each blockb &isin; P</span><span class="c15">i&minus;1</span><span class="c3">, two cases are possible: (1) b is not split by C</span><span class="c15">i</span><span class="c3">, for any C &isin; C. Then b is valid with respect to C</span><span class="c15">i</span><span class="c1">1</span><span class="c27">, and will be retained in P</span><span class="c15">i</span><span class="c3">. (2) b is split by one more constraints C</span><span class="c15">i</span><span class="c3">. The algorithm iterates through all such constraints that split b, and partitions block b such that every resulting block is valid with respect to each C</span><span class="c15">i</span><span class="c3">, C &isin; C. </span></p><p class="c19"><span class="c3">We next note thatP</span><span class="c15">i </span><span class="c3">is indeed a partition of D (i.e. the union of all blocks equals D). To see this observe that each blockb &isin; P</span><span class="c15">i&minus;1 </span><span class="c3">is either present in P</span><span class="c15">i </span><span class="c3">or has been refined and all its constituent blocks (whose union equals b) are in P</span><span class="c15">i</span><span class="c3">. Thus, P</span><span class="c15">i </span><span class="c3">is a valid parti- tion with respect to C</span><span class="c15">i</span><span class="c1">1</span><span class="c27">. This proves the inductive step. </span><span class="c40">D </span></p><p class="c19"><span class="c3">Consistency Constraints. Since different sub-views can have common attribute(s), additional constraints need to be added to ensure that their distributions for the common attribute(s) are the same. In order to do so, we may need to further refine the partition generated from the above procedure. Specifically, consider a pair of sub-views s</span><span class="c0">1 </span><span class="c11">and s</span><span class="c0">2 </span><span class="c11">with attribute sets A</span><span class="c0">1 </span><span class="c11">and </span><span class="c3">A</span><span class="c1">2 </span><span class="c3">respectively, such that A</span><span class="c1">1 </span><span class="c3">&cap; A</span><span class="c1">2 </span><span class="c3">&empty;. Let D</span><span class="c15">1 </span><span class="c3">= </span><span class="c0">i &isin;A</span><span class="c2">1 </span><span class="c3">D</span><span class="c1">i</span><span class="c3">, and D</span><span class="c15">2 </span><span class="c3">= </span><span class="c0">j&isin;A</span><span class="c2">2 </span><span class="c3">D</span><span class="c0">j </span><span class="c11">be the corresponding domains for s</span><span class="c0">1 </span><span class="c11">and </span><span class="c3">s</span><span class="c0">2 </span><span class="c11">respectively, and D</span><span class="c1">1,2 </span><span class="c11">= </span><span class="c0">i &isin;A</span><span class="c2">1</span><span class="c0">&cap;A</span><span class="c2">2 </span><span class="c3">D</span><span class="c0">i</span><span class="c11">. Let the partitions </span><span class="c3">obtained on D</span><span class="c15">1 </span><span class="c3">and D</span><span class="c15">2 </span><span class="c3">be P</span><span class="c0">1 </span><span class="c11">and P</span><span class="c0">2</span><span class="c11">, respectively. In order to </span><span class="c3">keep P</span><span class="c0">1 </span><span class="c11">and P</span><span class="c0">2 </span><span class="c11">consistent with each other, we need to ensure </span><span class="c3">that their region boundaries are aligned with each other, and this is achieved by refining P</span><span class="c0">1 </span><span class="c11">and P</span><span class="c0">2 </span><span class="c11">so that they have common </span><span class="c3">boundaries along dimensions A</span><span class="c0">1 </span><span class="c11">&cap; A</span><span class="c0">2</span><span class="c11">. We consider the union </span><span class="c3">of the &ldquo;split points&quot; of P</span><span class="c1">1 </span><span class="c3">and P</span><span class="c1">2 </span><span class="c3">along dimensions A</span><span class="c1">1 </span><span class="c3">&cap; A</span><span class="c1">2 </span><span class="c3">and further for each block in P</span><span class="c0">1 </span><span class="c11">(and P</span><span class="c0">2</span><span class="c11">), we refine this block until it </span><span class="c3">no longer crosses such a split point. Finally, we add LP constraints that equate distributions of the common attributes in P</span><span class="c1">1 </span><span class="c3">and P</span><span class="c1">2</span><span class="c3">. </span></p><p class="c23"><span class="c21">306 </span></p><p class="c133"><span class="c26">5 DATABASE SUMMARY GENERATOR </span><span class="c3">This component takes the LP solution for each view as the input and generates the database summary, which as mentioned pre- viously, can be used for dynamically generating data for query execution, or can optionally be used to generate the materialized database. </span></p><p class="c37"><span class="c3">Recall that a variable in the LP (for a view) represents an un- derlying block in a sub-view&rsquo;s partition, and its assigned value is the number of rows present in that block &ndash; this value is here- after referred to generically as NumTuples. The collection of NumTuples values represent the sub-view solutions, and these solutions are integrated to obtain the solution for the complete view. However, since each view is solved independently, the refer- ential constraints that exist between the corresponding relations may be lost in these view solutions. Therefore, they may have to be modified to ensure global consistency. Finally, it is necessary to extract relations from the views in order to populate the data- base. Accordingly, the summary generator component in Hydra is responsible for the following sequence of tasks: (1) Constructing a solution for complete views (2) Instantiating view summaries (3) Making view summaries consistent wrt each other (4) Extracting relation summaries from view summaries </span></p><p class="c171"><span class="c26">5.1 Constructing Solution for the View </span><span class="c3">For integrating the sub-view solutions to obtain the collective solution for the complete view, we firstorder the sub-views. Then, we iteratively build the view-solution by aligning and merging the next sub-view solution in the given order. Let S denote the input list of sub-view solutions, and viewSol be the final view solution that we wish to compute. Algorithm 3 describes the high- level process for constructing viewSol from S, and its ordering, aligning and merging procedures are described in the remainder of this sub-section. </span></p><p class="c31"><span class="c3">Algorithm 3: View Solution Construction </span></p><p class="c91"><span class="c5">1 </span><span class="c3">S &larr; OrderSubViews(S); </span><span class="c5">2 </span><span class="c3">viewSol &larr; &empty;; </span><span class="c5">3 </span><span class="c3">foreach s &isin; S do </span><span class="c5">4 </span><span class="c3">viewSol,s &larr; Align(viewSol,s) ; </span><span class="c5">5 </span><span class="c3">viewSol &larr; Merge(viewSol,s); </span></p><p class="c111"><span class="c3">5.1.1 Sub-View Ordering. Ordering is implemented through a greedy iterative algorithm where we can start with any sub-view as the first choice. Subsequently, at iterationi, let the set of visited sub-views until now be S. A sub-view s from outside this set can be chosen to be the next in the ordering only if it satisfies the following condition: On removing the common vertices between s and S in the (chordal) view-graph, there should not exist any path between the remaining vertices of s and the remaining vertices of S. This algorithm is described in detail in [21]. </span></p><p class="c165"><span class="c3">5.1.2 Aligning. After obtaining the sub-view merge order as per above, in every iteration we merge the next sub-view solution (s) in the sequence to the current view-solution (viewSol), after a process of alignment. The alignment algorithm is a two step exercise, as shown in the example of Figure 8: Solution Sorting: First, the viewSol and s solutions are each sorted on their common set of attributes to facilitate direct </span></p><p class="c23 c97"><span class="c9">(c) Merged View Solution </span></p><p class="c39"><span class="c3">Figure 8: Align and Merge Example </span></p><p class="c78 c128 c150"><span class="c3">5.1.3 Merging. This is the last step in the construction of the view solution. Here we simply merge the two solutions obtained after alignment through a &ldquo;position&rdquo; based join, where the phys- ically corresponding rows in each solution are combined, with the common attributes being represented once. For example, the aligned solutions of Figure 8b are merge-joined using the po- sitions (or row identifiers) to deliver the final view solution of Figure 8c. </span></p><p class="c74"><span class="c3">As discussed earlier, DataSynth adopted a sampling algorithm for constructing the view solutions post LP solving. In marked contrast, Hydra deterministically generates the view solutions, facilitating us to operate purely in the summary space. There are </span></p><p class="c49"><span class="c21">307 </span></p><p class="c23 c78"><span class="c3">comparison of their matching ranges. For instance, the solutions A,B and A,C in Figure 8a are each sorted on the intervals enumerated in the common attribute A. Row Splitting: Our addition of consistency constraints during the LP formulation ensured that the distribution of tu- ples along the common set of attributes is the same in the various sub-views. Therefore it easy to see that the sum of NumTuples values in any interval of the common attributes is the same for the sub-view solutions under alignment. For example, in Figure 8a, the total number of tuples with A = [40, 60) is 30K in both the A,B and A,C solutions. Likewise, the other entries in column A also have matching total number of tuples across the solutions. The align step splits the rows in these solutions such that the corresponding rows in both solutions have the same number of tuples. The sub-view solutions of Figure 8a are shown in Figure 8b after undergoing the alignment pro- cess, with both solutions now having identicalNumTuples in the corresponding rows. </span></p><p class="c179"><span class="c9">(a) Sub-view Solution </span></p><p class="c160"><span class="c9">(b) View Alignment </span></p><p class="c35"><span class="c3">two tangible benefits of this deterministic strategy: (a) elimina- tion of the time and space overheads due to sampling, and (b) elimination of sampling-based errors in satisfying CCs. </span></p><p class="c123"><span class="c26">5.2 Instantiating View Summaries </span><span class="c3">As shown in Figure 8c, each row in the view solution is comprised of a series of intervals (across various attributes) and the number of tuples in the region represented by these intervals. We now need to decide as to how these tuples are distributed within the attribute intervals. Our current solution is very simple: Assign the entire cardinality to the left boundaries of the intervals. For example, the third row in Figure 8c would result in generation of 10000 tuples all having A = 40,B = 5,C = 2 values. </span></p><p class="c130"><span class="c3">Note that, in principle, we could have used a more sophisti- cated cardinality distribution within the intervals. However, our simple deterministic choice helps to reduce the subsequent addi- tive errors that are incurred while ensuring referential integrity across views (described in next subsection). This is so because choosing values deterministically within a bucket minimizes the likelihood of encountering an fk value that is not present in the corresponding pk column. </span></p><p class="c13"><span class="c26">5.3 Making View Summaries Consistent </span><span class="c3">Since the solution for each view is obtained independently, there could be inconsistencies across them. For example, referring back to the view schema shown in Section 3.2, R_view has attributes borrowed from S_view and T_view, and its solution may fea- ture values that are not present in the corresponding attributes of these two views. To address this problem, we first carry out a topological sort on the &ldquo;referential dependency graph&rdquo;</span><span class="c15">2 </span><span class="c3">and then iteratively make the current view consistent with its prede- cessors. Since a topological sort is employed, Hydra can handle dependency graphs that are DAGs unlike DataSynth which is restricted to tree traversals. </span></p><p class="c10"><span class="c3">To make a pair of views V</span><span class="c0">i </span><span class="c11">and V</span><span class="c0">j </span><span class="c11">consistent with each other, </span><span class="c3">whereV</span><span class="c1">i </span><span class="c3">is dependent onV</span><span class="c1">j</span><span class="c3">, we iterate over the rows in the view solution ofV</span><span class="c0">i </span><span class="c11">and look for the value combination that each row </span><span class="c3">has for the attributes borrowed fromV</span><span class="c0">j</span><span class="c11">. If that value combination </span><span class="c3">is not present in the solution of V</span><span class="c1">j</span><span class="c3">, we add a new row in its solution with the corresponding NumTuples attribute set to 1. This results in an additive error in the total number of tuples in the view as compared to the original AQP at the client. But we hasten to add that the error is a fixed number of rows, determined by the nature of the constraints and the LP solution, and not by the data scale. Therefore, at Big Data volumes, the discrepancy can be expected to be minuscule, and our experiments empirically confirm this expectation. </span></p><p class="c7"><span class="c3">The inter view consistency component is present in DataSynth as well, but since its view solutions are comprised of complete database instantiations, and not just summaries, the time and space overheads incurred for making the views consistent can be large. Moreover, the additive error in DataSynth is amplified due to its inherent sampling errors. Our experiments also capture this distinction between the errors incurred due to referential constraints in Hydra and DataSynth. </span></p><p class="c54"><span class="c26">5.4 Constructing Relation Summaries </span><span class="c3">After constructing consistent solutions across all the views, we next need to obtain the corresponding relation summaries. For </span></p><p class="c121"><span class="c25">2</span><span class="c30">A graph where each relation is represented by a node and an edge (</span><span class="c0">u, v</span><span class="c30">) is added </span><span class="c14">if relation </span><span class="c1">u </span><span class="c14">is dependent on relation </span><span class="c1">v </span><span class="c14">through a referential constraint. </span></p><p class="c17"><span class="c3">this, we create a summarized relation schema </span><span class="c27">&nbsp;&#771;</span><span class="c3">R</span><span class="c1">i </span><span class="c3">for each relation R</span><span class="c0">i</span><span class="c11">. This schema consists of all attributes in R</span><span class="c0">i </span><span class="c11">except the primary </span><span class="c3">key attribute, and additionally, the NumTuples value for each entry in </span><span class="c27">&nbsp;&#771;</span><span class="c3">R</span><span class="c1">i</span><span class="c3">, as sourced from the view solutions. </span></p><p class="c185"><span class="c3">For the common attributes between the summarized relation and the corresponding view solution, the value combinations and corresponding NumTuples value are directly borrowed from the solution. What remains are the foreign key attributes. For filling a foreign key attribute fk, we need to first consult the view corresponding to fk&rsquo;s s target relation, say V</span><span class="c1">j</span><span class="c3">. To fill the fk value in row r of </span><span class="c27">&nbsp;&#771;</span><span class="c3">R</span><span class="c0">i</span><span class="c11">, we extract the value combination in row r of view </span><span class="c3">solution ofV</span><span class="c1">i</span><span class="c3">, and then project the attributes corresponding toV</span><span class="c1">j </span><span class="c3">&ndash; let this be denoted by v. Now, we iterate over the solution set of V</span><span class="c0">j </span><span class="c11">and compute the cumulative sum of the cardinality entries </span><span class="c3">till v is reached. This sum provides the fk value corresponding to the rth row of </span><span class="c27">&nbsp;&#771;</span><span class="c3">R</span><span class="c1">i</span><span class="c3">, and we thus obtain </span><span class="c27">&nbsp;&#771;</span><span class="c3">R</span><span class="c1">i </span><span class="c3">for each relation R</span><span class="c1">i</span><span class="c3">. The set of relation summaries, computed as described above, provides the entire database summary &ndash; a sample such summary was previously shown in Figure 5 (for simplicity, the figure shows the PK columns instead of the number of tuples). </span></p><p class="c50"><span class="c3">Like before, DataSynth again iterates over the complete instan- tiated (consistent) views to construct the corresponding material- ized relations. Obviously, this leads to enormous time and space overheads in contrast to our data-scale independent summary based approach. </span></p><p class="c104"><span class="c26">6 TUPLE GENERATOR </span><span class="c3">The Tuple Generator component resides inside the database en- gine, and needs to be explicitly incorporated in the engine code- base by the vendor. As a proof of concept, we have implemented it for the PostgreSQL v9.3 engine by adding a new feature called datagen, which is included as a property for each relation in the database. Whenever this feature is enabled for a relation, the scan operator for that relation is replaced with the dynamic generation operator. As a result, during query execution, the executor does not fetch the data from the disk but is instead supplied by the Tuple Generator in an on-demand manner, using the available relation summary. </span></p><p class="c140 c128 c180"><span class="c3">Each row in the relation summary has a value combination and an associated NumTuples entry. We consider the pk values to be the row numbers of the relation. Therefore, to get the rth tuple of a relation R, the pk is chosen as r and the rest of the attributes come from the relation summary. We iterate over the rows of </span><span class="c27">&nbsp;&#771;</span><span class="c3">R and take the cumulative sum of the NumTuples entries until the sum exceeds r. Say the summation crosses the value r in jth row of </span><span class="c27">&nbsp;&#771;</span><span class="c3">R. Then the rest of the values of the rth tuple are assigned to be precisely the same as those present in the jth row of </span><span class="c27">&nbsp;&#771;</span><span class="c3">R. For example, the 120th row of relation S in Figure 5, would be &#10216;120, 20, 15&#10217;. </span></p><p class="c50"><span class="c3">Note that this form of tuple generation is expected to be ef- ficient since the attribute value assignments are deterministic and independent, and these expectations are confirmed in the experiments shown in the following section. </span></p><p class="c168"><span class="c26">7 EXPERIMENTS </span><span class="c3">We have implemented the Hydra design, described in the previous sections, in a Java tool running to over 15K lines of code. The popular Z3 [14] solver is leveraged to compute solutions for the LP formulations. In this section, we evaluate Hydra&rsquo;s empirical performance, using our implementation of DataSynth as the comparative yardstick in the analysis. </span></p><p class="c82"><span class="c21">308 </span></p><p class="c152"><span class="c3">Figure 9: Distribution of Cardinality in CCs (WL_c) </span></p><p class="c43"><span class="c3">The above constraints result in a large number of geometri- cally overlapping regions. Hydra, due to its region-partitioning approach, comfortably handles this scenario. In marked contrast, DataSynth, due to its grid-partitioning construction, generates a very large number of LP variables (in the several billion) from the constraints, overwhelming the solver&rsquo;s capabilities. We there- fore also created an alternative simplified query workload, called WL</span><span class="c1">s</span><span class="c3">, with 311 CCs, wherein the variables created by DataSynth were less than a million, and therefore well within the solver&rsquo;s processing power. </span></p><p class="c80"><span class="c26">7.1 Quality of Volumetric Similarity </span><span class="c3">We begin by investigating how closely the volumetric similarity, with regard to operator output cardinalities, is achieved between the client and vendor sites for theWL</span><span class="c0">s </span><span class="c11">workload by the Hydra </span><span class="c3">and DataSynth regenerators. This behavior is captured in Fig- ure 10, which plots the percentage of CCs that are within a given relative error of volumetric similarity. From the plot it is evident that Hydra satisfies around 90 percent of the CCs with virtually no error, and the remaining CCs are also satisfied within a rela- tive error of less than 10%. This is in contrast to DataSynth, which accurately satisfies around 80 percent of the CCs, but then incurs as much as 60% relative error to achieve complete coverage of the remaining CCs. </span></p><p class="c140 c173"><span class="c3">There are two reasons for the error-prone behavior of Data- Synth: (1) the probabilistic sampling technique, and (2) the main- tenance of referential integrity. While Hydra also is forced to </span></p><p class="c172"><span class="c25">3</span><span class="c30">Similar to DataSynth, the restriction to non-key-based filters is because the con- </span><span class="c14">version from relations to views lose the key attributes. Likewise, only PK-FK joins are supported since they are inherently present in the design of views. </span></p><p class="c23 c71"><span class="c3">Database Environment. The TPC-DS [1] decision-support bench- mark database, with a default size of 100GB, is used as the baseline in our experiments. The database is hosted on a PostgreSQL v9.3 engine [4] with the hardware platform being a vanilla HP work- station (3.2 GHz 16 core processor, 32 GB memory, 500 GB SSD hard drive) running Ubuntu Linux 16.04.3. </span></p><p class="c86"><span class="c3">A complex query workload,WL</span><span class="c1">c</span><span class="c3">, featuring 131 distinct queries (enumerated in [21]), was created by customizing the 99 queries of the benchmark such that only non-key filter predicates and PK-FK joins were retained, and all nested queries were separated into independent sub-queries</span><span class="c15">3</span><span class="c3">. The AQPs for these queries were generated on the PostgreSQL query processor, resulting in 351 cardinality constraints. The distribution of the cardinalities for these CCs are shown in Figure 9, with the cardinalities measured on a log-scale. The figure clearly indicates that a wide range of cardinalities are present in the constraints, going from a few tuples to almost a billion. </span></p><p class="c163"><span class="c3">Figure 11: Extra tuples for Referential Integrity (WL_c) </span></p><p class="c78 c120"><span class="c3">As a final observation, it is interesting to note that DataSynth has to contend with both negative (volumes less than desired) and positive (volume greater than desired) relative errors, due to its sampling strategy &ndash; in fact, about one-third of the CCs suf- fered negative relative errors. In contrast, Hydra only generates positive errors due to the inclusion of extra tuples for satisfying referential integrity. From a practical standpoint, it is perhaps preferable to have positive errors since they induce greater stress on the data processing elements in the engine. </span></p><p class="c182"><span class="c26">7.2 Scalability with Workload Complexity </span><span class="c3">We now turn our attention to evaluating the complexity of the underlying LP that is formulated by Hydra and DataSynth. Since LP complexity is essentially proportional to the number of vari- ables in the problem, we compare this number for the two tech- niques. Further, since LP complexity is, to the first degree of approximation, independent of the database size, we present the comparison only for the 100 GB instance. </span><span class="c15">4 </span><span class="c3">The number of LP variables for a representative set of TPC-DS relations, including the major fact and dimension tables (catalog_sales, store_sales, </span></p><p class="c178"><span class="c25">4</span><span class="c30">Of course, the database engine&rsquo;s choice of query plans may change to some extent </span><span class="c14">with database size, leading to a slightly different set of CCs. </span></p><p class="c162"><span class="c21">309 </span></p><p class="c23 c156"><span class="c3">Figure 10: Quality of Volumetric Similarity (WL_c) </span></p><p class="c44"><span class="c3">insert additional tuples to maintain referential integrity, the num- ber is substantially smaller than those injected by DataSynth. This is because the integrity errors are amplified by the impact of the sampling errors. This effect is quantified in Figure 11, where the number of extra tuples inserted is plotted on a log-scale for representative TPC-DS tables. We see here that Hydra is often an order-of-magnitude smaller with regard to the addition of these extra tuples as compared to DataSynth. Also, recall that integrity errors in Hydra are independent of the data scale and therefore are minuscule at Big Data volumes. We also show this in [21]. </span></p><p class="c98"><span class="c3">item) is captured, on a log-scale, in Figure 12 for theWL</span><span class="c0">c </span><span class="c11">com- </span><span class="c3">plex workload. We observe here that the LPs formulated using the region-partitioning strategy in Hydra generate several orders of magnitude fewer variables than the corresponding LPs derived from the grid-partitioning in DataSynth. As a case in point, con- sider the catalog_sales table &ndash; the number of variables created by DataSynth was almost 5.5 million, which is reduced to as low as 1620 by Hydra. Even more dramatic is the change for item table, where the number of variables is reduced from an enormous 10</span><span class="c15">11 </span><span class="c3">to around 3700. </span></p><p class="c114"><span class="c3">Figure 12: Number of variables in the LP (WL</span><span class="c1">c</span><span class="c3">) </span></p><p class="c137"><span class="c3">From an absolute perspective also, the large number of vari- ables created by DataSynth is a critical problem since, as men- tioned previously, the LP solver crashed in handling these cases. In marked contrast, the few thousands of LP variables generated by Hydra were easily solvable in less than a minute. Moreover, even when we switched to the simple workload, WL</span><span class="c1">s</span><span class="c3">, the LP solution time for DataSynth was almost an hour, whereas Hydra completed in a few seconds as shown in Figure 13. </span></p><p class="c69"><span class="c9">Complex Workload (W L</span><span class="c16">c</span><span class="c9">) Simple Workload (W L</span><span class="c16">s</span><span class="c9">) DataSynth Hydra DataSynth Hydra crash 58 sec 50 min 13 sec </span></p><p class="c29"><span class="c3">Figure 13: LP Processing Time </span></p><p class="c68"><span class="c26">7.3 Scalability with Materialized Data Size </span><span class="c3">This experiment compares the data instantiation times, post LP solution, of DataSynth and Hydra on theWL</span><span class="c1">s </span><span class="c3">workload. While Hydra, in principle, due to its summary-based approach, does not have to instantiate the data immediately, we assume in this experiment that the vendor requires complete materialization. </span></p><p class="c83"><span class="c3">The experimental results are shown in Figure 14, where we also present, for comparative purposes, the performance with 10 GB and 1000 GB databases, apart from the default 100 GB database. We see here that there is a huge reduction in the materialization time of Hydra at all scales. Further, even in absolute terms, Hydra is able to output a 100 GB database in around 11 minutes, whereas DataSynth takes 42 hours to complete the same task. </span></p><p class="c18"><span class="c3">The marked difference in the efficiency of the two techniques is attributed to the fact that DataSynth instantiates complete views through sampling, subsequently performs several passes on these instantiations to ensure referential integrity, and to derive relations from them. Hydra on the other hand, after LP-solving, constructs the database summary in just a few seconds, and then instantiates the materialized database directly from it. </span></p><p class="c23 c181"><span class="c9">Size (in GB) DataSynth Hydra 10 4 hours 2 min 100 42 hours 11 min 1000 &gt; 1 week 1.6 hours </span></p><p class="c149"><span class="c3">Figure 14: Data Materialization Time </span></p><p class="c153"><span class="c26">7.4 Scalability to Big Data Volumes </span><span class="c3">In our next experiment, we validated the ability of Hydra, thanks to its summary-based technique, to scale to Big Data volumes. To demonstrate this feature, we modeled an exabyte-sized (10</span><span class="c15">18 </span><span class="c3">bytes) data scenario as follows: We used CODD, which is ca- pable of modeling arbitrary metadata scenarios, to obtain the optimizer-chosen plans at the exabyte database scale for all the workload queries. To get AQPs for this database, we executed the obtained plans on the 100 GB instance and scaled the in- termediate row counts with the appropriate scale factor. Hydra was able to formulate and solve the LPs (one per relation), and generate the database summary in less than 2 minutes. Once the summary is generated, the database can begin to submit the workload queries since the data required for the execution can be produced on-the-fly by the Tuple Generator. </span></p><p class="c78 c147"><span class="c26">7.5 Dynamism in Data Generation </span><span class="c3">Our next experiment evaluates Hydra&rsquo;s ability, due to the Tu- ple Generator and Database Summary architecture, to produce tuples on-the-fly instead of first materializing them, and then reading from the disk. To verify whether dynamic generation can indeed produce data at rates that are practical for support- ing query execution, we compared the total time that Hydra&rsquo;s tuple generator took to construct and supply tuples to the execu- tor, while running simple aggregate queries, as compared to the standard sequential scan from the disk. </span></p><p class="c122"><span class="c9">Rel. Name Size Row count Scan time (secs) </span></p><p class="c138"><span class="c9">(in GB) (in millions) Disk Dynamic store_returns 3 29 16 8 web_sales 10 72 43 25 inventory 19 399 107 74 catalog_sales 20 144 46 48 store_sales 34 288 168 87 </span></p><p class="c51"><span class="c3">Figure 15: Data Supply Times </span></p><p class="c134 c128 c151"><span class="c3">The results of this experiment are shown in Figure 15 for the five biggest relations in the 100 GB database instance. We see here that the tuple generator is not only competitive with a materialized solution, but is in fact typically faster. Therefore, using dynamic generation can prove to be a good option since it can help to eliminate the large time and space overheads incurred in: (1) dumping generated data on the disk, and (2) loading the data on the engine under test. </span></p><p class="c124"><span class="c26">7.6 Performance on JOB Benchmark </span><span class="c3">A legitimate concern with regard to the above encouraging results for Hydra is that they may be an artifact of the TPC-DS database, and perhaps might under-perform on other datasets. To address this concern, we consider in our final experiment, a schematically highly different database, namely the JOB benchmark [17], which is based on the IMDB real-world dataset. Here, we created a </span></p><p class="c88"><span class="c21">310 </span></p><p class="c144 c161"><span class="c3">workload of 260 queries, resulting in 523 CCs, whose cardinality distribution is again highly varied as seen in Figure 16. </span></p><p class="c20"><span class="c3">Figure 16: Cardinality distribution of CCs in JOB </span></p><p class="c154"><span class="c3">We found that Hydra efficiently solved this workload as well, with the number of variables in each view being typically in the few thousands, and never exceeding a hundred thousand, as shown quantitatively in Figure 17. The overall database summary was quickly generated in around 20 seconds, and produced a database of high fidelity that satisfied all the constraints with no more than 2 percent relative error. </span></p><p class="c141"><span class="c3">Figure 17: Number of Variables for JOB </span></p><p class="c55"><span class="c26">8 RELATED WORK </span><span class="c3">Over the past few decades, a rich corpus of literature has devel- oped on synthetic database construction. There are two broad streams of research on the topic, one dealing with the ab ini- tio generation of new databases using standard mathematical distributions (e.g. [12, 15]), and the other with regeneration of an arbitrary existing database. In the latter category, there are two approaches, one of which uses only schematic and statis- tical information from the original database (e.g. [19, 22]). The other uses both the original database and the query workload to achieve statistical fidelity during evaluation (e.g [6, 11]) &ndash; our work on Hydra falls into this class. In this section, we briefly review recent literature on this spectrum of research categories. </span></p><p class="c96"><span class="c3">Ab Initio Generation. Descriptive languages for the definitions of data dependencies and column distributions were proposed in [12, 16, 20]. For example, [12] proposed a special purpose lan- guage called Data Generation Language (DGL) that is used by the tool to generate synthetic data distributions by utilizing the </span></p><p class="c19 c78"><span class="c3">concept of iterators. It supports a broad range of dependencies be- tween relations but the construction of dependent tables always requires access to the referenced table, creating a bottleneck on the data generation speed. </span></p><p class="c6"><span class="c3">In contrast to the above, MUDD [23] and PSDG [16] generate all related data at the same time. However, this approach can also be rendered inefficient if the referenced tables are large in size. MUDD proposes algorithms to parallelize the data generation process, and to efficiently generate dense-unique-pseudo-random sequences and derive nonuniform distributions. Both MUDD and PSDG decouple data generation details from data description, facilitating customization of the tool to suit user needs. </span></p><p class="c50"><span class="c3">In the distributed setting, a faster way of generating references is through recomputing since it eliminates the I/O costs incurred to satisfy referential constraints across relations that are present across different nodes. PDGF [20] was designed with this goal of achieving scalability and decoupling. In PDGF, the user specifies two XML configuration files, one for the data model and one for the formatting instructions. The generation strategy is based on the exploitation of determinism in pseudo-random number generators (PRNG), which enables regeneration of the same se- quences, hence eliminating the scan overheads. PDGF supports the generation of data with cyclic dependencies as well, but in- curs high computation costs for generating the associated keys. Finally, PDGF comes with a set of fixed generators for different datatypes and basic distribution functions. </span></p><p class="c78 c128 c145"><span class="c3">Database-dependent Regeneration. DBSynth[19] is an exten- sion to PDGF, which builds data models from an existing database by extracting schema information, and using sampling to con- struct histograms and dictionaries of text-valued data. Further, if the textual data contains multiple words, Markov chain genera- tors are used to analyze the word combination frequencies and probabilities. Finally, after the model construction is complete, PDGF is invoked to generate the corresponding data. </span></p><p class="c132"><span class="c3">Like DBSynth, RSGen [22] takes a metadata dump, including 1-D histograms, as the input, and generates database tables along with a loading script as the output. It uses a bucket based model at its core, which is able to generate trillions of records with minimum memory footage. However, the proposed technique works well only for queries with only a single range predicate. Further, due to the inaccurate statistical models in the query optimizer, the volumetric similarity is poor for queries involving predicates on correlated attributes. </span></p><p class="c50"><span class="c3">UpSizeR [24] is a graph-based tool that uses attribute correla- tions extracted from an existing database to generate an equiva- lent synthetic database. A derivative work, Rex [13] produces an extrapolated database given an integer scaling factor and the orig- inal database, while maintaining referential constraints and the distributions between the consecutive linked tables. Dscaler [26] </span></p><p class="c19 c38"><span class="c3">A similar generator is Myriad [5], which implements an effi- cient parallel execution strategy leveraged by extensive use of PRNGs with random access support. With these PRNGs, Myriad distributes the generation process across the compute nodes and ensures that they can run independently from each other, without imposing any restrictions on the data modeling language. </span></p><p class="c38 c140"><span class="c3">Finally, a rule-based probabilistic approach, based on an ex- tension of Datalog, has been recently proposed in [9], which is capable of generating data characterized by parametrized clas- sical discrete distributions &ndash; however, it is not always feasible to assign such distributions to real-world data, especially over multivariate spaces. </span></p><p class="c85"><span class="c21">311 </span></p><p class="c102"><span class="c3">addresses the problem of generating a non-uniformly</span><span class="c15">5 </span><span class="c3">scaled ver- sion of a database using fine-grained, per-tuple correlations for key attributes, but such information is typically hard to come by. Moreover, all these techniques only generate the key attributes, whereas the non-key values are sampled from the original data- base using these key values. Hence, the approach becomes imprac- tical in Big Data and security-conscious environments. Finally, Dscaler fails to retain accuracy for some common query classes. </span></p><p class="c87"><span class="c3">Query-dependent Regeneration. Apart from the above tech- niques, another line of work [6, 10, 11, 18] is based on workload dependence (as in the case of Hydra). Here the aim is to gener- ate a database given a workload of queries such that volumetric similarity is achieved on these queries. In particular, RQP [10] gets a query and a result as input, and returns a possible data- base instance that could have produced the result for that query. The idea of using cardinalities from a query plan tree was first introduced in QAGen [11]. They start by constructing a symbolic database</span><span class="c15">6</span><span class="c3">, and then translate the input AQPs to constraints over the symbols in the database. Subsequently, a constraint satisfac- tion program (CSP) is invoked to identify values for symbols that satisfy all the constraints. </span></p><p class="c7"><span class="c3">On the positive side, these generators are capable of handling complex operators as they use a general CSP, but the performance cost is huge since the number of CSP calls also increases with the database size. Further, it requires operating on a symbolic database of matching size to the original database, and processing of the entire database during the algorithm execution. This makes it impractical for Big Data environments. Finally, QAGen supports only one query plan in the input. This limitation was addressed in a follow-up tool called MyBenchmark [18], which creates a symbolic database on a per query basis and at the end tries to heuristically merge the various databases into a small number of databases. Clearly, generating a database on a per query basis has enormous time and space overheads, and further, a single database is not guaranteed in the output. </span></p><p class="c119"><span class="c3">DataSynth [6] identified the declarative property of cardinality constraints and its ability to specify data characteristics. Given a large number of cardinality constraints as input, the paper proposed algorithms based on the LP solver and graphical models to instantiate tables that satisfy those constraints. However, it suffers from high LP complexity, data scale dependencies, and inaccuracies with regard to volumetric similarity, as we have discussed in this paper. Hydra materially extends the DataSynth approach by adding dynamism, scale and functionality. </span></p><p class="c64"><span class="c26">9 CONCLUSIONS </span><span class="c3">The ability to synthetically regenerate data that accurately con- forms to the volumetric behavior on queries at client sites is of crucial importance to database vendors, and will become even more so with the advent of Big Data applications. In this paper, we have proposed Hydra, a data regeneration tool that takes a substantial step forward towards achieving this goal. Specif- ically, by reworking the basic LP problem formulation into a region-based variable assignment, Hydra improves on the state- of-the-art DataSynth&rsquo;s performance by orders of magnitude with regard to problem complexity, data materialization time, and scalability to large volumes. Secondly, by using a deterministic alignment technique for database consistency, it provides far </span></p><p class="c115"><span class="c25">5</span><span class="c30">In non-uniform scaling, individual tables are scaled by different factors. </span><span class="c2">6</span><span class="c30">A symbolic database is similar to a regular database, but its attribute values are </span><span class="c14">symbols (variables), not constants. </span></p><p class="c19 c63"><span class="c3">better accuracy in meeting volumetric constraints as compared to the probabilistic approach employed in DataSynth. Finally, its summary-based framework organically supports the dynamic regeneration of streaming data sources, an essential pre-requisite for efficiently testing contemporary deployments. </span></p><p class="c50"><span class="c3">In our future work, we plan to focus on covering a richer set of query operators, such as grouping functions, within the Hydra framework. Also, we would like to investigate how to leverage additional summary information (such as value-based correla- tions) that the client might be willing to provide for achieving stronger fidelity with the original database. </span></p><p class="c128 c175"><span class="c3">Acknowledgements. We thank the anonymous reviewers for their expert and constructive comments on the material presented here. We also thank Huawei Technologies India Pvt. Ltd. and the members of the Database Systems Lab at IISc for their valuable feedback and support in this work. </span></p><p class="c70"><span class="c26">REFERENCES </span></p><p class="c107"><span class="c14">[1] TPC-DS. http://www.tpc.org/tpcds/. [2] TPC-H. http://www.tpc.org/tpch/. [3] USE PLAN SQL Server. https://technet.microsoft.com/en-us/library/ </span></p><p class="c61"><span class="c14">ms186954(v=sql.105).aspx. [4] PostgreSQL. http://www.postgresql.org/docs/9.3/static/release.html. [5] A. Alexandrov, K. Tzoumas and V. Markl. Myriad: Scalable and Expressive </span></p><p class="c169"><span class="c14">Data Generation. PVLDB, 5(12), 2012. [6] A. Arasu, R. Kaushik and J. Li. Data generation using declarative constraints. </span></p><p class="c28"><span class="c14">Proc. of the ACM SIGMOD Intl. Conf. on Management of Data, 2011. [7] A. Arasu, R. Kaushik and J. Li. DataSynth: Generating synthetic data using </span></p><p class="c126"><span class="c14">declarative constraints. PVLDB, 4(12), 2011. [8] S. Ashoke and J. R. Haritsa. CODD: a dataless approach to big data testing. </span></p><p class="c66"><span class="c14">PVLDB, 8(12), 2015. [9] V. Barany, B. Cate, B. Kimelfeld, D. Olteanu and Z. Vagena. Declarative Prob- abilistic Programming with Datalog. Proc. of the 19th Intl. Conf. on Database Theory, 2016. [10] C. Binnig, D. Kossmann and E. Lo. Reverse Query Processing. Proc. of the 23rd </span></p><p class="c76"><span class="c14">Intl. Conf. on Data Engineering, 2007. [11] C. Binnig, D. Kossmann, E. Lo and M. Tamer &Ouml;zsu. QAGen: generating query- aware test databases. Proc. of the ACM SIGMOD Intl. Conf. on Management of Data, 2007. [12] N. Bruno and S. Chaudhuri. Flexible database generators. Proc. of the 31st Intl. </span></p><p class="c8"><span class="c14">Conf. on Very Large Data Bases, 2005. [13] T. S. Buda, T. Cerqueus, J. Murphy and M. Kristiansen. ReX: Extrapolating Relational Data in a Representative Way. Proc. of the British Intl. Conf. on Databases, 2015. [14] L. De Moura and N. Bj&oslash;rner. Z3: An efficient SMT solver. Proc. of the Intl. Conf. on Tools and Algorithms for the Construction and Analysis of Systems, 2008. [15] J. Gray, P. Sundaresan, S. Englert, K. Baclawski and P. J. Weinberger. Quickly Generating Billion-record Synthetic Databases. Proc. of the ACM SIGMOD Intl. Conf. on Management of Data, 1994. [16] J. E. Hoag and C. W. Thompson. A parallel general-purpose synthetic data </span></p><p class="c36"><span class="c14">generator. ACM SIGMOD Record, 2007. [17] V. Leis, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper and T. Neumann. How </span></p><p class="c186"><span class="c14">Good Are Query Optimizers, Really? PVLDB, 9(3), 2015. [18] E. Lo, N. Cheng, W. W. K. Lin, W. Hon and B. Choi. MyBenchmark: generating </span></p><p class="c76"><span class="c14">databases for query workloads. The VLDB Journal, 23(6), 2014. [19] T. Rabl, M. Danisch, M. Frank, S. Schindler and H. Jacobsen. Just Can&rsquo;T Get Enough: Synthesizing Big Data. Proc. of the ACM SIGMOD Intl. Conf. on Management of Data, 2015. [20] T. Rabl, M. Frank, H. M. Sergieh and H. Kosch. A Data Generator for Cloud- </span></p><p class="c110"><span class="c14">scale Benchmarking. Proc. of the 2nd TPC Technology Conference on Performance Evaluation, Measurement and Characterization of Complex Systems, 2010. [21] A. Sanghi, R. Sood, J. R. Haritsa and S. Tirthapura. Scalable and Dynamic Workload Dependent Data Regeneration. Tech. Report TR-2017-01, DSL/CDS, IISc, 2017, dsl.cds.iisc.ac.in/publications/report/TR/TR-2017-01.pdf. [22] E. Shen and L. Antova. Reversing statistics for scalable test databases genera- </span></p><p class="c127"><span class="c14">tion. Proc. of the 6th Intl. Workshop on Testing Database Systems, 2013. [23] J. M. Stephens and M. Poess. MUDD: A Multi-dimensional Data Generator. </span></p><p class="c95"><span class="c14">Proc. of the 4th Intl. Workshop on Software and Performance, 2004. [24] Y. C. Tay, B. T. Dai, D. T. Wang, E. Y. Sun, Yong Lin and Yuting Lin. UpSizeR: </span></p><p class="c36"><span class="c14">Synthetically Scaling an Empirical Relational Database. Inf. Syst. 38(8), 2013. [25] R. S. Trivedi, I. Nilavalagan and J. R. Haritsa. Codd: Constructing dataless </span></p><p class="c36"><span class="c14">databases. Proc. of the 5th Intl. Workshop on Testing Database Systems, 2012. [26] J. W. Zhang and Y. C. Tay. Dscaler: Synthetically scaling a given relational </span></p><p class="c183"><span class="c14">database. PVLDB, 9(14), 2016. </span></p><p class="c113"><span class="c21">312 </span></p></body></html>