<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c99{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c19{margin-left:-20.9pt;padding-top:1pt;text-indent:34.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:1.1pt}.c140{margin-left:-25pt;padding-top:4.1pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c61{margin-left:-25pt;padding-top:9.1pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c91{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.4pt;font-family:"Arial";font-style:normal}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c9{margin-left:-16.2pt;padding-top:3.8pt;text-indent:25.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.9pt}.c24{margin-left:-25pt;padding-top:3.8pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c142{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Courier New";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.5pt;font-family:"Arial";font-style:normal}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c116{margin-left:-25pt;padding-top:60.2pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c39{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Arial";font-style:normal}.c34{color:#bfbfbf;font-weight:400;text-decoration:none;vertical-align:super;font-size:2.5pt;font-family:"Courier New";font-style:normal}.c67{margin-left:-25pt;padding-top:5.5pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.8pt}.c59{margin-left:-25pt;padding-top:1.4pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c87{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.2pt;font-family:"Arial";font-style:normal}.c36{margin-left:-16.2pt;padding-top:1.7pt;text-indent:25.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.9pt}.c35{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.9pt;font-family:"Arial";font-style:normal}.c49{margin-left:-25pt;padding-top:1.4pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c56{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.1pt;font-family:"Courier New";font-style:normal}.c25{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.4pt;font-family:"Arial";font-style:normal}.c82{color:#ffffff;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.1pt;font-family:"Arial";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:2.9pt;font-family:"Times New Roman";font-style:normal}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c92{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:5.9pt;font-family:"Courier New";font-style:normal}.c80{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.9pt;font-family:"Arial";font-style:normal}.c38{margin-left:-16.2pt;padding-top:1.7pt;text-indent:25.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.9pt}.c72{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c96{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.4pt;font-family:"Courier New";font-style:normal}.c6{color:#ffffff;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.9pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:13pt;font-family:"Arial";font-style:normal}.c101{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.5pt;font-family:"Courier New";font-style:normal}.c147{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.6pt;font-family:"Arial";font-style:normal}.c32{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.9pt;font-family:"Arial";font-style:normal}.c124{color:#bfbfbf;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:1.5pt;font-family:"Courier New";font-style:normal}.c113{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c43{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.8pt;font-family:"Arial";font-style:normal}.c107{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.7pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:4.8pt;font-family:"Times New Roman";font-style:normal}.c33{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.8pt;font-family:"Arial";font-style:normal}.c29{margin-left:-7.8pt;padding-top:1.2pt;text-indent:191.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-6pt}.c131{margin-left:-25pt;padding-top:4.1pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.6pt}.c129{margin-left:-25pt;padding-top:8.9pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c144{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.4pt;font-family:"Arial";font-style:normal}.c109{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:4.8pt;font-family:"Times New Roman";font-style:normal}.c93{margin-left:-25pt;padding-top:4.1pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c30{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.1pt;font-family:"Arial";font-style:normal}.c57{margin-left:-25pt;padding-top:1.4pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c133{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.9pt;font-family:"Arial";font-style:normal}.c60{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.3pt;font-family:"Arial";font-style:normal}.c55{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.9pt;font-family:"Courier New";font-style:normal}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:normal}.c48{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c110{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.5pt;font-family:"Courier New";font-style:normal}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:normal}.c45{margin-left:-25pt;padding-top:1.7pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c118{margin-left:-25pt;padding-top:1.7pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-20.2pt}.c132{margin-left:-18.7pt;padding-top:6.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:30.9pt}.c73{margin-left:-25pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c139{margin-left:-20.9pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-10.6pt}.c117{margin-left:-25pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c112{margin-left:-16.2pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-19.2pt}.c121{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.5pt}.c122{margin-left:-25pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c137{margin-left:-25pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c120{margin-left:1.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:9.8pt}.c114{margin-left:-11.8pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18.7pt}.c86{margin-left:-25pt;padding-top:14.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:106pt}.c97{margin-left:-25pt;padding-top:100.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c134{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-10.6pt}.c77{margin-left:-16.2pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-42pt}.c54{margin-left:-25pt;padding-top:36.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:157.8pt}.c145{margin-left:-25pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:83pt}.c66{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-12pt}.c27{margin-left:-16.2pt;padding-top:13.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.9pt}.c88{margin-left:-23.8pt;padding-top:68.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24pt}.c90{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-20.9pt}.c64{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.9pt}.c46{margin-left:-3.6pt;padding-top:8.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:4.5pt}.c95{margin-left:218.2pt;padding-top:80.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c115{margin-left:15.1pt;padding-top:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:14.4pt}.c71{margin-left:-16.2pt;padding-top:13.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-8.6pt}.c37{margin-left:46.1pt;padding-top:23pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:59.9pt}.c47{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.6pt}.c41{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:1pt}.c119{margin-left:218.2pt;padding-top:643.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:217.9pt}.c89{margin-left:-13pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-22.8pt}.c76{margin-left:-20.9pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-11.1pt}.c62{margin-left:-20.9pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c84{margin-left:-16.2pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.7pt}.c42{margin-left:-13.4pt;padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:4.2pt}.c81{margin-left:-25pt;padding-top:16.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c58{margin-left:-25pt;padding-top:11pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:4pt}.c51{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-23pt}.c123{margin-left:38.2pt;padding-top:29.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:37.4pt}.c138{margin-left:-25pt;padding-top:13pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:40.2pt}.c65{margin-left:-279pt;padding-top:15.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:237.1pt}.c98{margin-left:-16.1pt;padding-top:7.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c106{margin-left:-25pt;padding-top:12.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:76pt}.c74{margin-left:-16.2pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18.7pt}.c79{margin-left:-25pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-25.8pt}.c22{margin-left:-11.8pt;padding-top:13.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:22.5pt}.c21{margin-left:218.2pt;padding-top:66.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c146{margin-left:-25pt;padding-top:9.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:6.4pt}.c126{margin-left:-25pt;padding-top:11pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:113.7pt}.c70{margin-left:39.4pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:47.4pt}.c102{margin-left:-169.5pt;padding-top:110.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:301.2pt}.c8{margin-left:-25pt;padding-top:7.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c103{padding-top:8.2pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c125{padding-top:12.5pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c108{padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c104{padding-top:6.7pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c141{padding-top:29.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c78{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c63{padding-top:3.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c136{padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c85{padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c68{padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c128{padding-top:7pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c83{padding-top:15.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c52{padding-top:3.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c100{padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c149{margin-left:-25pt;text-indent:33.8pt;margin-right:-16.9pt}.c11{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c148{margin-left:33.1pt;text-indent:-30.7pt;margin-right:34.2pt}.c50{margin-left:-16.2pt;text-indent:25.3pt;margin-right:-25.9pt}.c127{margin-left:-16.2pt;text-indent:25.3pt;margin-right:-34.1pt}.c130{margin-left:-4.1pt;margin-right:-8.2pt}.c111{margin-left:-7.4pt;margin-right:8.8pt}.c44{margin-left:47pt;margin-right:126.2pt}.c135{margin-left:-11.8pt;margin-right:-13pt}.c143{margin-left:-7.8pt;margin-right:-19.7pt}.c105{margin-left:-16.2pt;margin-right:-7pt}.c69{margin-left:248.6pt;margin-right:-15.1pt}.c31{margin-left:-11.8pt;margin-right:2.4pt}.c53{margin-left:261.1pt;margin-right:-255.9pt}.c75{margin-left:-16.2pt;margin-right:-25.9pt}.c94{margin-left:-16.2pt;margin-right:9.4pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c11"><p class="c115"><span class="c113">EXstream: Explaining Anomalies in Event Stream Monitoring </span></p><p class="c123"><span class="c15">Haopeng Zhang, Yanlei Diao, Alexandra Meliou </span><span class="c144">College of Information and Computer Sciences, University of Massachusetts Amherst {haopeng, yanlei, ameli}@cs.umass.edu </span></p><p class="c54"><span class="c15">ABSTRACT </span></p><p class="c67"><span class="c3">In this paper, we present the EXstream system that pro- vides high-quality explanations for anomalous behaviors that users annotate on CEP-based monitoring results. Given the new requirements for explanations, namely, conciseness, consistency with human interpretation, and prediction power, most existing techniques cannot produce explanations that satisfy all three of them. The key technical contributions of this work include a formal definition of optimally explaining anomalies in CEP monitoring, and three key techniques for generating sufficient feature space, characterizing the con- tribution of each feature to the explanation, and selecting a small subset of features as the optimal explanation, respec- tively. Evaluation using two real-world use cases shows that EXstream can outperform existing techniques significantly in conciseness and consistency while achieving comparable high prediction power and retaining a highly efficient imple- mentation of a data stream system. </span></p><p class="c86"><span class="c15">1. INTRODUCTION </span></p><p class="c24"><span class="c3">Complex Event Processing (CEP) extracts useful infor- mation from large-volume event streams in real-time. Users define interesting patterns in a CEP query language (e.g,. [3, 4]). With expressive query languages and high perfor- mance processing power, CEP technology is now at the core of real-time monitoring in a variety of areas, including the Internet of Things [16], financial market analysis [16], and cluster monitoring [26]. </span></p><p class="c118"><span class="c3">However, today&rsquo;s CEP technology supports only passive monitoring by requesting the monitoring application (or user) to explicitly define patterns of interest. There is a recent re- alization that many real-world applications demand a new service beyond passive monitoring, that is, the ability of the monitoring system to identify interesting patterns (includ- ing anomalous behaviors), produce a concrete explanation from the raw data, and based on the explanation enable a user action to prevent or remedy the effect of an anomaly. We broadly refer to this new service as proactive monitoring. </span></p><p class="c116"><span class="c2">c 2017, Copyright is with the authors. Published in Proc. 20th Inter- national Conference on Extending Database Technology (EDBT), March 21-24, 2017 - Venice, Italy: ISBN 978-3-89318-073-8, on OpenProceed- ings.org. Distribution of this paper is permitted under the terms of the Cre- ative Commons license CC-by-nc-nd 4.0 </span></p><p class="c1 c94"><span class="c3">We present two motivating applications as follows. </span><span class="c15">1.1 Motivating Applications </span></p><p class="c9"><span class="c3">Production Cluster Monitoring. Cluster monitoring is crucial to many enterprise businesses. For a concrete ex- ample, consider a production Hadoop cluster that executes a mix of Extract-Transform-Load (ETL) workloads, SQL queries, and data stream tasks. The programming model of Hadoop is MapReduce, where a MapReduce job is com- posed of a </span><span class="c48">map </span><span class="c3">function that performs data transformation and filtering, and a </span><span class="c48">reduce </span><span class="c3">function that performs aggre- gation or more complex analytics for all the data sharing the same key. During job execution, the map tasks (called mappers) read raw data and generate intermediate results, and the reduce tasks (reducers) read the output of mappers and generate final output. Many of the Hadoop jobs have deadlines because any delay in these jobs will affect the en- tire daily operations of the enterprise business. As a result, monitoring of the progress of these Hadoop jobs has become a crucial component of the business operations. </span></p><p class="c50 c68"><span class="c3">However, the Hadoop system does not provide sufficient monitoring functionality by itself. CEP technology has been shown to be efficient and effective for monitoring a variety of measures [26]. By utilizing the event logs generated by Hadoop and system metrics collected by Ganglia[12], CEP queries can be used to monitor Hadoop job progress; to find tasks that cause cluster imbalance; to find data pull strag- glers; and to compute the statistics of lifetime of mappers and reducers. Consider a concrete example below, where the CEP query monitors the size of intermediate results that have been queued between mappers and reducers. </span></p><p class="c104 c75"><span class="c3">Example 1.1 (Data Queuing Monitoring). Collect all the events capturing intermediate data generation/consumption for each Hadoop job. Return the accumulative intermediate data size calculated from those events (Q1). </span></p><p class="c104 c50"><span class="c3">Figure 1(a) shows the data queuing size of a monitored Hadoop job. The X-axis stands for the time elapsed since the beginning of the job, while the Y-axis represents the size of queued data. In this case, the job progress turns out to be normal: the intermediate results output by the mappers start to queue at the beginning and reach a peak after a short period of time. This is because a number of mappers have completed in this period while the reducers have not been scheduled to consume the map output. Afterwards, the queued data size decreases and then stabilizes for a long period of time, meaning that the mappers and reducers are producing and consuming data at constant rates, until the queued data reduces to zero at the end of the job. </span></p><p class="c88"><span class="c72">Series ISSN: 2367-2005 156 </span><span class="c107">10.5441/002/edbt.2017.15 </span></p><p class="c1"><span class="c60">Visualization </span></p><p class="c1"><span class="c60">Explanation </span></p><p class="c1"><span class="c33">(c) </span><span class="c43">Architecture of EXstream </span></p><p class="c1"><span class="c3">Figure 1: Hadoop cluster monitoring: examples and system architecture. </span></p><p class="c7"><span class="c3">Suppose that a Hadoop user sees a different progress plot, as shown Figure 1(b), for the same job on another day: there is a long initial period where the data queuing size increases gradually but continually, and this phase causes the job com- pletion time to be delayed by more than 500 seconds. When the user sees the job with an odd shape in Figure 1(b), he may start considering the following questions: </span></p><p class="c1"><span class="c48">&#9654; </span><span class="c3">What is happening with the submitted job? </span><span class="c48">&#9654; </span><span class="c3">Should I wait for the job to complete or re-submit it? </span><span class="c48">&#9654; </span><span class="c3">Is the phenomenon caused by the bugs in the code or </span></p><p class="c1"><span class="c3">some system anomalies? </span><span class="c48">&#9654; </span><span class="c3">What should I do to bring the job progress back to </span></p><p class="c1"><span class="c3">normal? Today&rsquo;s CEP technology, unfortunately, does not provide any additional information that helps answer the above ques- tions. The best practice is manual exploration by the Hadoop user: he can dig into the complex Hadoop logs and manually correlate the Hadoop events with the system metrics such as CPU and memory usage returned by a cluster monitoring tool like Ganglia [12]. If he is lucky to get help from the cluster administrator, he may collect additional information such as the number of jobs executed concurrently with his job and the resources consumed by those jobs. </span></p><p class="c7"><span class="c3">For our example query, the odd shape in Figure 1(b) is due to high memory usage of other programs in the Hadoop cluster. However, this fact is not obvious from the visualiza- tion of the user&rsquo;s monitoring query, Q1. It requires analyzing additional data beyond what is used to compute Q1 (which used data relevant only to the user&rsquo;s Hadoop job, but not all the jobs in the system). Furthermore, the discovery of the fact requires new tools that can automatically generate explanations for the anomalies in monitoring results such that these explanations can be understood by the human and lead to corrective / preventive actions in the future. </span></p><p class="c7"><span class="c3">Supply Chain Management. The second use case is derived from an aerospace company with a global supply chain. By talking with the experts in supply chain manage- ment, we identified an initial set of issues in the company&rsquo;s complex production process which may lead to imperfect or faulty products. For instance, in the manufacturing pro- cess of a certain product the environmental features must to be strictly controlled because they affect the quality of pro- duction. For example, the temperature and humidity need to be controlled in a certain range, and they are recorded by the sensors deployed in the environment. However, if some sensors stop working, the environmental features may not be controlled properly and hence the products manufac- </span></p><p class="c1"><span class="c60">An Explanation Engine </span></p><p class="c1"><span class="c33">(a) </span><span class="c43">Data queuing size of a normal Hadoop job </span><span class="c147">(b) </span><span class="c10">Data queuing size of an abnor- </span></p><p class="c1"><span class="c43">mal Hadoop job </span></p><p class="c7"><span class="c3">tured during that period can have quality issues. When such anomalies arise, it is a huge amount of work to investigate the claims from customers given the complexity of manufac- turing process and to analyze a large set of historical data to find explanations that are meaningful and actionable. </span></p><p class="c1"><span class="c15">1.2 Problem Statement and Contributions </span></p><p class="c7"><span class="c3">The overall goal of EXstream is to provide good explana- tions for anomalous behaviors that users annotate on CEP monitoring results. We assume that an enterprise informa- tion system has CEP monitoring functionality: a CEP moni- toring system offers a dashboard to illustrate high-level met- rics computed by a CEP query, such as job progress, network traffic, and data queuing. When a user observes an abnor- mal value in the monitoring results, he annotates the value in the dashboard and requests EXstream to search for an explanation from the archived raw data streams. EXstream generates an optimal explanation(formalized in Section 2.2) by quickly replaying a fraction of the archived data streams. Then the explanation can be encoded into the system for proactive monitoring for similar anomalies in the future. </span></p><p class="c7"><span class="c3">Challenges. The challenges in the design of XStream arise from the requirements for such explanations. Informed by the two real-world applications mentioned above, we con- sider three requirements in this work: (a) Conciseness: The system should favor smaller explanations, which are easier for humans to understand. (b) Consistency: The system should produce explanations that are consistent with hu- man interpretation. In practice, this means that explana- tions should match the true reasons for an anomaly (ground truth). (c) Prediction power: We prefer explanations that have predictive value for future anomalies. </span></p><p class="c7"><span class="c3">It is difficult for existing techniques to meet all three re- quirements. In particular, prediction techniques such as logistic regression and decision trees [2] suffer severely in conciseness or consistency as shown in our evaluation re- sults. This is because these techniques were designed for prediction, but not for explanations with conciseness and consistency requirements. Recent database research [25, 20] seeks to explain outliners in SQL query answers. This line of work assumes that explanations can be found by search- ing through various subsets of the tuples that were used to compute the query answers. This assumption does not suit real-world stream systems for two reasons: As shown for our example, Q1, the explanation of memory usage contention among different jobs cannot be generated from only those events that produced the monitoring results of Q1. Fur- thermore, the stream execution model does not allow us to </span></p><p class="c1"><span class="c72">157 </span></p><p class="c1"><span class="c60">A CEP-based Monitoring System </span></p><p class="c1"><span class="c60">Data source </span></p><p class="c1"><span class="c60">Data </span></p><p class="c1"><span class="c60">Archive </span></p><p class="c1"><span class="c60">CEP </span></p><p class="c1"><span class="c60">Results </span></p><p class="c1"><span class="c60">Explanation </span></p><p class="c1"><span class="c60">Annotation </span></p><p class="c1"><span class="c2">Event type Meaning Schema JobStart Recording a Hadoop job starts (timestamp, eventType, eventId, jobId, clusterNodeNumber) JobEnd Recording a Hadoop job finishes (timestamp, eventType, eventId, jobId, clusterNodeNumber) DataIO Recording the activities of generation (positive values) / consumption (nega- tive values) of intermediate data </span></p><p class="c1"><span class="c2">(timestamp, eventType, eventId, jobId, taskId, attemptId, clusterN- odeNumber, dataSize) </span></p><p class="c1"><span class="c2">CPUUsage Recording the CPU usage for a node </span></p><p class="c1"><span class="c2">in the cluster </span></p><p class="c1"><span class="c2">(timestamp, eventType, eventId, clusterNodeNumber, CPUUsage) </span></p><p class="c1"><span class="c2">MemUsage Recording the memory usage for a </span></p><p class="c1"><span class="c2">node in the cluster </span></p><p class="c1"><span class="c2">(timestamp, eventType, eventId, clusterNodeNumber, memUsage) </span></p><p class="c1"><span class="c3">Figure 2: </span><span class="c2">Example event types in Hadoop cluster monitoring. Event types can be specific to the Hadoop job (e.g., JobStart, DataIO, JobEnd), or they may report system metrics (e.g., CPUUsage, FreeMemory). </span></p><p class="c1"><span class="c3">Q Pattern seq(Component</span><span class="c0">1</span><span class="c4">, Component</span><span class="c0">2 </span><span class="c4">, ...) </span></p><p class="c1"><span class="c3">Where [partitionAttribute] &and; Pred</span><span class="c17">1 </span><span class="c3">&and; Pred</span><span class="c17">2 </span><span class="c3">&and; ... Return (timestamp, partitionAttribute, derivedA</span><span class="c0">1</span><span class="c4">, </span></p><p class="c1"><span class="c3">derivedA</span><span class="c17">2</span><span class="c3">, ...)[] </span></p><p class="c1"><span class="c3">Q</span><span class="c0">1 </span><span class="c4">Pattern seq(JobStart a, DataIO+ b[], JobEnd c) </span></p><p class="c78"><span class="c3">Where [jobId] Return (b[i].timestamp, a.jobId, sum(b[1&middot;&middot;&middot;i].dataSize))[] Figure 3: </span><span class="c2">Syntax of a query in SASE (on the left), and an example query for monitoring data activity (on the right). </span></p><p class="c1"><span class="c3">repeat query execution over different subsets of events or perform any precomputation in a given database [20]. </span></p><p class="c1"><span class="c3">Contributions. In this work, we take an important step towards discovering high-quality explanations for anomalies observed in monitoring results. Toward this goal, we make the following contributions: 1) Formalizing explanations (Section 2): We provide a formal definition of optimally explaining anomalies in CEP monitoring as a problem that maximizes the information reward provided by the explanation. 2) Sufficient feature space (Section 3): A key insight in our work is that discovering explanations first requires a sufficient feature space that includes all necessary features for explaining observed anomalies. EXstream includes a new module that automatically transforms raw data streams into a richer feature space, F, to enable explanations. 3) Entropy-based, single-feature reward (Section 4): As a basis for building the information reward of an expla- nation, we model the reward that each feature, f &isin; F, may contribute using a new entropy-based distance function. 4) Optimal explanations via submodular optimiza- tion (Section 5): We next model the problem of finding an optimal explanation from the feature space, F, as a submod- ular maximization problem. Since submodular optimization is NP-hard, we design a heuristic algorithm that ranks and filters features efficiently and effectively. 5) Evaluation (Section 6): We have implemented EXstream on top of the SASE stream engine [3, 26]. Experiments us- ing two real-world use cases show promising results: (1) Our entropy distance function outperforms state-of-the-art dis- tance functions on time series by reducing the features con- sidered by 94.6%. (2) EXstream significantly outperforms logistic regression [2], decision tree [2], majority voting [15] and data fusion [19] in consistency and conciseness of expla- nations while achieving comparable, high predication accu- racy. Specifically, it outperforms others by improving con- sistency from 10.7% to 87.5% on average, and reduces 90.5% of features on average to ensure conciseness. (3) Our imple- mentation is also efficient: with 2000 concurrent monitoring queries, the triggered explanation analysis returns explana- tions within half a minute and affects the performance only </span></p><p class="c1"><span class="c3">slightly, delaying events processing by 0.4 second on average. </span></p><p class="c1"><span class="c15">2. EXPLAINING CEP ANOMALIES </span></p><p class="c7"><span class="c3">The goal of EXstream is to provide good explanations for anomalous behaviors that users annotate on CEP-based monitoring results. We first describe the system setup, and give examples of monitoring queries and anomalous obser- vations that a user may annotate. We then discuss the re- quirements for providing explanations for such anomalies, and examine whether some existing approaches can derive explanations that fit these requirements. Finally, we define the problem of optimally explaining anomalies in our set- ting. </span></p><p class="c1"><span class="c15">2.1 CEP Monitoring System and Queries </span></p><p class="c1"><span class="c3">In this section, we describe the system setup for our prob- lem setting. The overall architecture of EXstream is shown in Figure 1(c). Within the top dashed rectangle in Fig- ure 1(c) is a CEP-based monitoring system. We consider a data source S, generating events of n types, E = {E</span><span class="c0">1</span><span class="c4">,E</span><span class="c0">2</span><span class="c4">,...,E</span><span class="c0">n</span><span class="c4">}. </span><span class="c3">Events of these types are received by the CEP-based moni- toring system continuously. Each event type follows a schema, comprised of a set of attributes; all event schemas share a common timestamp attribute. The timestamp attribute records the occurrence time of each event. Figure 2 shows some example event types in the Hadoop cluster monitoring use case [26]. </span></p><p class="c1"><span class="c3">We consider a CEP engine that monitors these events using user-defined queries. For the purposes of this pa- per, monitoring queries are defined in the SASE query lan- guage [3], but this is not a restriction of our framework, and our results extend to other CEP query languages. Figure 3 shows the general syntax of CEP queries in SASE, and an example query, Q</span><span class="c17">1</span><span class="c3">, from the Hadoop cluster monitoring use case. Q</span><span class="c0">1 </span><span class="c4">collects all data-relevant events during the lifetime </span><span class="c3">of a Hadoop job. We now explain the main components of a SASE query. Sequence. A query Q may specify a sequence using the </span><span class="c48">SEQ </span><span class="c3">operator, which requires components in the sequence to occur in the specified order. One component is either a single event or the Kleene closure of events. For example, Q</span><span class="c0">1 </span><span class="c4">specifies three components: the first component is a sin- </span></p><p class="c1"><span class="c72">158 </span></p><p class="c117"><span class="c3">gle event of the type JobStart; the second component is a Kleene closure of a set of events of the type DataIO; and the third component is a single event of type JobEnd. Predicates. Q can also specify a set of predicates in its </span><span class="c48">Where </span><span class="c3">clause. One special predicate among these is the bracketed partitionAttribute. The brackets apply an equiv- alence test on the attribute inside, which requires all se- lected events to have the same value for this attribute. The partitionAttribute tells the CEP engine which attribute to partition by. In Q</span><span class="c0">1</span><span class="c4">, jobId is the partition attribute. </span><span class="c3">Return matches. Q specifies the matches to return in the </span><span class="c48">Return </span><span class="c3">clause. Matches comprise a series of events with raw or derived attributes; we assume timestamp and the partitionAttribute are included in the returned events. We denote with m a match on one partition and with M</span><span class="c0">Q </span><span class="c4">the </span><span class="c3">set of all matches. Q</span><span class="c0">1 </span><span class="c4">returns a series of events based on </span><span class="c3">selected DataIO events, and the returned attributes include timestamp, jobId, and a derived attribute&mdash; the total size for all selected DataIO events. In order to visualize results in real time, matches will be sent to the visualization module as events are collected. </span></p><p class="c79"><span class="c99">Visualizations and feedback. </span><span class="c3">Our system visualizes matches from monitoring queries on a dashboard that users can inter- act with. The visualizations typically display the (relative) occurrence time on the X-axis. The Y-axis represents one of the derived attributes in returned events. Users can specify simple filters to focus on particular partitions. All returned events data of M</span><span class="c17">Q </span><span class="c3">are to be visualized stored in a relational table for a particular partition T</span><span class="c17">M</span><span class="c3">is </span><span class="c20">Q</span><span class="c3">, specified and the </span></p><p class="c122"><span class="c3">as &pi;</span><span class="c0">t,attr i</span><span class="c4">(&sigma;</span><span class="c0">partitionAttribute=v</span><span class="c4">(M)). Figure 1(a) shows the </span><span class="c3">visualization of a partition, which corresponds to a Hadoop job for this query. In this visualization, the X-axis displays the time elapsed since the job started, and the Y-axis shows the derived sum over the &ldquo;DataSize&rdquo; attribute. </span></p><p class="c45"><span class="c3">Users can interact with the visualizations by annotating anomalies. For example, the visualization of Figure 1(b) demonstrates an unexpected behavior, with the queueing data size growing slowly. A user can drag and draw rect- angles on the visualization, to annotate the abnormal com- ponent, as well as reference intervals that demonstrate nor- mal behavior. We show an example of these annotations in Figure 4. A user may also annotate an entire period as abnormal, and choose a reference interval in a different partition. The annotations will be sent to the explanation engine of EXstream, which is shown in the bottom dashed rectangle of Figure 1(c). The explanation engine will be introduced in detail in following sections. We use I</span><span class="c0">A </span><span class="c4">to </span><span class="c3">denote the annotated abnormal interval in a partition P</span><span class="c17">A</span><span class="c3">: I</span><span class="c0">A </span><span class="c4">= (Q,[lower, upper],P</span><span class="c0">A</span><span class="c4">). We use I</span><span class="c0">R </span><span class="c4">to denote the refer- </span><span class="c3">ence interval, which can be explicitly annotated by the user, or inferred by EXstream as the non-annotated parts of the partition. We write I</span><span class="c17">R </span><span class="c3">= (Q,[lower, upper],P</span><span class="c17">R</span><span class="c3">), where P</span><span class="c17">R </span><span class="c3">and P</span><span class="c0">A </span><span class="c4">might be the same or different partitions. </span></p><p class="c145"><span class="c15">2.2 Explaining Anomalies </span></p><p class="c24"><span class="c3">Monitoring visualizations allow users to observe the evolu- tion of various performance metrics in the system. While the visualizations help indicate that something may be unusual (when an anomaly is observed), they do not offer clues that point to the reasons for the unexpected behavior. In our example from Figure 4, there are two underlying reasons for </span></p><p class="c89"><span class="c142">I</span><span class="c96">A </span><span class="c55">I</span><span class="c96">R </span><span class="c3">Figure 4: </span><span class="c2">Abnormal (I</span><span class="c0">A</span><span class="c2">) and reference (I</span><span class="c0">R</span><span class="c2">) intervals. </span></p><p class="c27"><span class="c3">the abnormal behavior: (1) the free memory is lower than normal, and (2) the free swap space is lower than normal. However, these reasons are not obvious from the visualiza- tion; rather, a Hadoop expert had to manually check a large volume of logs to derive this explanation. Our goal is to au- tomate this process, by designing a system that seamlessly integrates with CEP monitoring visualizations, and which can produce explanations for surprising observations. </span></p><p class="c38"><span class="c3">We define three desirable criteria for producing explana- tions in EXstream: 1. Conciseness: The system should favor smaller, and thus simpler explanations. Conciseness follows the Occam&rsquo;s razor principle, and produces explanations that are easier for humans to understand. </span></p><p class="c75 c128"><span class="c3">2. Consistency: The system should produce explanations that are consistent with human interpretation. In prac- tice, this means that explanations should match the true reasons for an anomaly (ground truth). </span></p><p class="c75 c104"><span class="c3">3. Prediction power: We prefer explanations that have predictive value for future anomalies. Such explanations can be used to perform proactive monitoring. </span></p><p class="c75 c125"><span class="c99">Explanations through predictive models. </span><span class="c3">The first step of our study explored the viability of existing prediction techniques for the task of producing explanations for CEP monitoring anomalies. Prediction techniques typically learn a model from training data; by using the anomaly and refer- ence annotations as the training data, the produced model can be perceived as an explanation. For now, we will assume that a sufficient set of features is provided for training (we discuss how to construct the feature space in Section 3), and evaluate the explanations produced by two standard predic- tion techniques for the example of Figure 4. Logistic regression [2] produces models as weights over a set of features. The algorithm processes events from the two annotated intervals as training data, and the trained predic- tion model&mdash;a classifier between abnormal and reference classes &mdash; can be considered an explanation to the anomaly. The resulting logistic regression model for this example is shown in Figure 5. While the model has good predictive power, it is too complex, and cannot facilitate human un- derstanding of the reported anomaly. The model assigns non-zero weights to 30 out of 345 input features, and while the two ground truth explanations identified by the human expert are among these features (23 and 24), their weights in this model are low. This model is too noisy to be of use, and it is not helpful as an explanation. </span></p><p class="c21"><span class="c72">159 </span></p><p class="c132"><span class="c2">No. Feature Weight 1 DataIOFrequency -0.01376 2 CPUIdleMean 0.0089 3 PullFinishFrequency -0.00708 4 ProcTotalMean 0.00085 ... ... ... 23 SwapFreeMean -4.79E-07 24 MemFreeMean -3.28E-07 ... ... ... 30 BoottimeMean 2.61E-10 </span></p><p class="c137"><span class="c3">Figure 5: </span><span class="c2">Model generated by logistic regression for the annotated anomaly of Figure 4. </span></p><p class="c37"><span class="c13">MapFinishNodeNumberMean </span></p><p class="c70"><span class="c13">&lt;4.7 </span><span class="c101">&ge;</span><span class="c13">4.7 </span></p><p class="c22"><span class="c13">PullFinishNodeNumberMean MemFreeMean </span></p><p class="c85 c130"><span class="c13">&lt;4.5 </span><span class="c101">&ge;</span><span class="c13">4.5 &lt;1684942 </span><span class="c101">&ge;</span><span class="c13">1684942 </span></p><p class="c42"><span class="c13">Abnormal Normal Abnormal Normal </span></p><p class="c46"><span class="c3">Figure 6: </span><span class="c2">Model Generated by Decision Tree </span></p><p class="c73"><span class="c3">Decision tree [2] builds a tree for prediction. Each non-leaf node of the tree is a predicate while leaf nodes are predic- tion decisions. Figure 6 shows the resulting tree for our example. The decision tree algorithm selects three features for the non-leaf nodes, and only one of them is part of the ground truth determined by our expert. The other two fea- tures happen to be coincidentally correlated with the two intervals, as revealed in our profiling. This model is more concise than the result of logistic regression, but it is not consistent with the ground truth. </span></p><p class="c59"><span class="c3">The above analyses showed that prediction techniques are not suitable for producing explanations in our setting. While the produced models have good predictive power (as this is what the techniques are designed for), they make poor explanations, as they suffer in consistency and conciseness. Our goal is to design a method for deriving explanations that satisfies all three criteria (Figure 7). </span><span class="c15">2.3 Formalizing Explanations </span></p><p class="c93"><span class="c3">Explanations need to be understandable to human users, and thus need to have a simple format. EXstream builds explanations as a conjunction of predicates. In their general format, explanations are defined as follows. </span></p><p class="c8"><span class="c3">Definition 2.1 (Explanation). An explanation is a boolean expression in Conjunctive Normal Form (CNF). It contains a conjunction of clauses, each clause is a disjunction of pred- icates, and each predicate is of the form {voc}, where v is a variable value, c is a constant, and o is one of five operators: o &isin; {&gt;,&ge;,=,&le;,&lt;}. </span></p><p class="c8"><span class="c3">Example 2.1. The formal form of the true explanations for the anomaly annotated in Figure 4 is (MemFreeMean &lt; 1978482 &and; SwapFreeMean &lt; 361462), which is a conjunc- tion of two predicates. It means that the average available memory is less than 1.9GB and free swap space is less than 360MB. The two predicates indicate that the memory usage is high in the system (due to resource contention), thus the job runs slower than normal. </span></p><p class="c98"><span class="c3">Arriving at the explanation of Example 2.1 requires two </span></p><p class="c1 c143"><span class="c39">Algorithm Conciseness Consistency Prediction </span></p><p class="c29"><span class="c39">quality Logistic regression Bad Bad Good Decision tree Ok Bad Good Goal Good Good Good </span></p><p class="c75 c103"><span class="c3">Figure 7: </span><span class="c2">Performance of prediction methods on our three criteria for explanations. </span></p><p class="c75 c83"><span class="c3">non-trivial components. First, we need to identify important features for the annotated intervals (e.g., MemFreeMean, SwapFreeMean); these features will be the basis of form- ing meaningful predicates for the explanations. Second, we have to derive the best explanation given a metric of op- timality. For example, the explanation (MemFreeMean &lt; 1978482) is worse than (MemFreeMean &lt; 1978482 &and; SwapFreeMean &lt; 361462), because, while it is smaller, it does not cover all issues that contribute to the anomaly, and is thus less consistent with the ground truth. </span></p><p class="c36"><span class="c3">Ultimately, explanations need to balance two somewhat conflicting goals: simplicity, which pushes explanations to smaller sizes, and informativeness, which pushes explana- tions to larger sizes to increase the information content. We model these goals through a reward function that models the information that an explanation carries, and we define the problem of deriving optimal explanations as the problem of maximizing this reward function. </span></p><p class="c75 c136"><span class="c3">Definition 2.2 (Optimal Explanation). Given an archive of data streams D for CEP, a user-annotated abnormal inter- val I</span><span class="c17">A </span><span class="c3">and a user-annotated reference interval I</span><span class="c17">R</span><span class="c3">, an optimal explanation e is one that maximizes a non-monotone, sub- modular information reward R over the annotated intervals: argmax</span><span class="c0">e </span><span class="c3">R</span><span class="c0">I</span><span class="c20">A</span><span class="c17">,I</span><span class="c20">R</span><span class="c3">(e) </span></p><p class="c50 c85"><span class="c3">The reward function in Definition 2.2 is governed by an important property: rewards are not additive, but submod- ular. This means that the sum of the reward of two expla- nations is greater than or equal to the reward of their union: R</span><span class="c17">I</span><span class="c20">A</span><span class="c17">,I</span><span class="c20">R</span><span class="c3">(e</span><span class="c17">1</span><span class="c3">) + R</span><span class="c17">I</span><span class="c20">A</span><span class="c17">,I</span><span class="c20">R</span><span class="c3">(e</span><span class="c17">2</span><span class="c3">) &ge; R</span><span class="c17">I</span><span class="c20">A</span><span class="c17">,I</span><span class="c20">R</span><span class="c3">(e</span><span class="c17">1 </span><span class="c3">&cup; e</span><span class="c17">2</span><span class="c3">). The intuition for the submodularity property is based on the observation that adding predicates to a conjunctive explanation offers diminishing returns: the more features an explanation al- ready has, the lower the reward of adding a new predicate tends to be. Moreover, R is non-monotone. This means that adding predicates to an explanation could decrease the reward. This is due to the conciseness requirement that penalizes big explanations. The optimal explanation prob- lem (Definition 2.2) is therefore a submodular maximization problem, which is known to be NP-hard [11]. </span><span class="c15">2.4 Existing Approximation Methods </span></p><p class="c63 c127"><span class="c3">Submodular optimization problems are commonly addressed with greedy approximation techniques. We next investigate the viability of these methods for our problem setting. </span></p><p class="c36"><span class="c3">For this analysis, we assume a reward function for ex- planations based on mutual information. Mutual informa- tion is a measure of mutual dependence between features. This is important in our problem setting, as features are often correlated. For example, PullStartFrequency and P ullF inishF requency are highly correlated, because they always appear together for every pull operation. For this precise reason, Definition 2.2 demands a submodular reward function. Mutual information satisfies the submodularity property. Greedy algorithms are often used in mutual infor- </span></p><p class="c21"><span class="c72">160 </span></p><p class="c97"><span class="c3">Figure 8: </span><span class="c2">Accumulative mutual information gain under greedy and random strategies. </span></p><p class="c81"><span class="c3">mation maximization problems. The way they would work in this setting is the following: given an explanation e, which is initially empty, at each greedy step, we select the feature f that maximizes the mutual information of e &cup; f. </span></p><p class="c45"><span class="c3">Figure 8 shows the performance of the greedy algorithm for maximization of mutual information, with a strawman alternative. The random algorithm selects a random feature at each step. The greedy strategy clearly outperforms the al- ternative by reaching higher mutual information gains with fewer features, but it still selects a large number of features (around 20-30 features before it levels off). This means that this method produces explanations that are too large, and unlikely to be useful for human understanding. </span></p><p class="c146"><span class="c15">2.5 Overview of the EXstream Approach </span></p><p class="c63 c149"><span class="c3">Since standard approaches for solving the optimal expla- nation problem are insufficient for our problem setting, we develop a new heuristic method based on good intuitions to address the problem. We next provide a high-level overview of our approach in building EXstream. 1. Sufficient feature space (Section 3): A key insight in our work is that discovering optimal explanations first re- quires a sufficient feature space that includes all necessary features for explaining observed anomalies. Our work differs fundamentally from existing work on discovering explana- tions from databases [25, 20]: First, EXstream operates on raw data streams, as opposed to the data carefully curated and stored in a relational database. Second, EXstream does not assume that the raw data streams carry all necessary fea- tures for explaining anomalous behaviors. In our above ex- ample, the feature, SwapFreeMean, captures average free swap space and it does not exist in Hadoop event logs or Ganglia output. Our system includes a module that auto- matically transforms raw data streams into a richer feature space, F, to enable the discovery of optimal explanations. 2. Entropy-based, single-feature reward (Section 4): As a basis for building the information reward defined in Definition 2.2, we consider the reward that each feature, f &isin; F, may contribute. To capture the reward in such a base case, we propose a new, entropy-based distance function that is defined on a single feature across the abnormal interval, I</span><span class="c17">A</span><span class="c3">, and the reference interval, I</span><span class="c17">R</span><span class="c3">. The larger the distance, the more differentiating power over the two intervals that the feature contributes, and hence more reward produced. 3. Optimal explanations via submodular optimiza- tion (Section 5): The next task is to find an optimal ex- planation from the feature space, F, that maximizes the in- formation reward provided by the explanation. The reward function in Definition 2.2 is non-monotone and submodu- lar, resulting in a submodular maximization problem. Since </span></p><p class="c1 c53"><span class="c2">timestamp node usagePercent 4 2 35 5 5 49 6 8 99 7 1 86 8 2 61 9 6 43 </span></p><p class="c75 c141"><span class="c3">submodular optimization is NP-hard, our goal is to design a heuristic to solve this problem. Our heuristic algorithm first uses the entropy-based, single-feature reward to rank features, subsequently identifies a cut-off to reject features with low reward, and finally uses correlation-based filtering to eliminate features with information overlap (emulating the submodularity property). Our evaluation shows that our heuristic method is extremely effective in practice. </span></p><p class="c71"><span class="c15">3. DISCOVERING USEFUL FEATURES </span></p><p class="c9"><span class="c3">Explanations comprise of predicates on measurable prop- erties of the CEP system. We call such properties features. Some features for our running example are DiskFreeMean, MemFreeMean, DataIOFrequency, etc. In most existing work on explanations, features are typically determined by the query or the schema of the data (e.g., the query predi- cates in Scorpion [25]). In CEP monitoring, using as features the query predicates or data attributes is not sufficient, be- cause many factors that impact the observed performance are due to other events and changes in the system. This poses an additional challenge in our problem setting, as the set of relevant features is non-trivial. In this section, we dis- cuss how EXstream derives the space of features as a first step to producing explanations. </span></p><p class="c68 c50"><span class="c3">In an explanation problem, we are given an anomaly inter- val I</span><span class="c17">A </span><span class="c3">and a reference interval I</span><span class="c17">R</span><span class="c3">; the relevant features for this explanation problem are built from events that occurred during these two intervals. To support the functionality of providing explanations, the CEP system has to maintain an archive of the streaming data. The system has the ability to purge archived data after the relevant monitoring queries terminate, but maintaining the data for longer can be useful, as the reference interval can be specified on any past data. </span></p><p class="c36"><span class="c3">Formally, the events arriving in a CEP system in input streams and the generated matches compose the input to the feature space construction problem. We assume that the CEP system maintains a table for each event type, such as the one depicted in Figure 9. That is, for each event type E</span><span class="c17">i</span><span class="c3">, logically there is a relational table R(E</span><span class="c17">i</span><span class="c3">) to store all events of this type in temporal order. There is also a table R(M) to archive all match events, denoted as type M. Let D denote the database for EXstream, which is composed of those tables. So, D is defined as D = {R(E</span><span class="c17">i</span><span class="c3">)|1 &le; i &le; n} &cup; R(M). </span></p><p class="c36"><span class="c3">Each attribute in event type E</span><span class="c0">i</span><span class="c4">, except the timestamp, </span><span class="c3">forms a time series in a given interval (which can be an anomaly interval I</span><span class="c0">A </span><span class="c4">or a reference interval I</span><span class="c0">R</span><span class="c4">). Such time </span><span class="c3">series as features are called raw features. </span></p><p class="c77"><span class="c3">Example 3.1. The table of Figure 9 records events of type CPUUsage in a given time interval [4, 9], and forms two raw features, from two time series. The first one is CPUUsage.Node, and its values are ((4, 2), (5,5), (6,8), (7,1), (8,2), (9,6)); the other is CPUUsage.UsagePercent with values ((4,35), </span></p><p class="c1 c69"><span class="c3">Figure 9: </span><span class="c2">Sample events in the type of CPUUsage. </span></p><p class="c119"><span class="c72">161 </span></p><p class="c102"><span class="c6">Abnormal Mixed </span></p><p class="c65"><span class="c3">Figure 10: </span><span class="c2">Visualization of the separating power of four features: (1) free memory size, (2) idle CPU percentage, (3) CPU percentage used by IO, and (4) system load. This visualization is not part of EXstream, but we show it here for exposition purposes. </span></p><p class="c1 c44"><span class="c6">Normal </span></p><p class="c7 c50"><span class="c3">Intuitively, the first two features in Figure 10 are better explanations for the anomaly, and thus have higher reward. The first feature means when the anomalies occur, the free memory size is relatively low, while during the reference in- terval the free memory size is relatively high. The second feature means that during the abnormal interval, idle CPU percentage is low while it is high during the reference in- terval. The unclear separation of the other two features, in particular the blue segments, indicate randomness between the two intervals, making them less suitable to explain the annotated anomalies. </span></p><p class="c7 c50"><span class="c3">Intuitively, the first two features in Figure 10 are better explanations for the anomaly, and thus have higher reward. The first feature means when the anomalies occur, the free memory size is relatively low, while during the reference in- terval the free memory size is relatively high. The second feature means that during the abnormal interval, idle CPU percentage is low while it is high during the reference in- terval. The unclear separation of the other two features, in particular the blue segments, indicate randomness between the two intervals, making them less suitable to explain the annotated anomalies. </span></p><p class="c106"><span class="c3">(5,49), (6,99), (7,86), (8,61), (9,43)). </span></p><p class="c61"><span class="c3">We found that the raw feature space is not good for deriv- ing explanations due to noise. Instead, we need higher-level features, which we construct by applying aggregation func- tions to features at different granularities. We apply sliding windows over the time series features and over each win- dow, aggregate functions including count and avg to gener- ate new time serious features. The EXstream system has an open architecture that allows any window size and any new aggregate functions to be used in the feature generation pro- cess. Features produced this way are&ldquo;smoothed&rdquo;time series; they demonstrate more general trends than raw features, and outliers are smoothed. Example high-level features that we produce by applying aggregations over windows on the raw features are DataIOFrequency and MemFreeMean. </span></p><p class="c138"><span class="c15">4. SINGLE-FEATURE REWARD </span></p><p class="c140"><span class="c3">In this section, we present the core of our technique: an entropy-based distance function that models the reward of a single feature. We first discuss the intuition and require- ments for this function, we then discuss existing, state-of- the-art distance functions and explain why they are not effective in this setting, and, finally, we present our new entropy-based distance metric. </span><span class="c15">4.1 Motivation and Insights </span></p><p class="c24"><span class="c3">In seeking explanations for CEP monitoring anomalies, users contrast an anomaly interval with a reference interval. An intuitive way to think about the different behaviors in the two intervals is to consider the differences in the events that occur within each interval. We can measure this dif- ference per feature: how different is each feature between the reference and the anomaly. Each feature is a vector of values, a time series, and our goal is to measure the distance between the time series of a feature during the abnormal interval and the time series of the same feature during the normal interval. </span></p><p class="c45"><span class="c3">To explain one of the desirable properties of the distance function, we visualize a feature as follows: We order the values of a feature in increasing order and assign a color to each value; red for values that appear in the abnormal interval only, yellow for values that appear in the normal interval only, and blue for values that appear in both normal and abnormal intervals. Figure 10 shows this visualization </span></p><p class="c1 c148"><span class="c124">0</span><span class="c34">70 60</span><span class="c82">K NARD ETRO</span><span class="c6">S</span><span class="c34">5040302010</span><span class="c124">0 1 2 3 4 5 </span><span class="c6">FEATURE ID </span></p><p class="c7 c75"><span class="c3">for 4 different features. In this figure, we note that the first 2 features show a clear separation of values between the normal and abnormal periods. The third feature has less clear separation, but still shows the trend that lower values are more likely to be abnormal. Finally, the fourth feature is mixed for a significant portion of values. </span></p><p class="c36"><span class="c3">Intuitively, the first two features in Figure 10 are better explanations for the anomaly, and thus have higher reward. The first feature means when the anomalies occur, the free memory size is relatively low, while during the reference in- terval the free memory size is relatively high. The second feature means that during the abnormal interval, idle CPU percentage is low while it is high during the reference in- terval. The unclear separation of the other two features, in particular the blue segments, indicate randomness between the two intervals, making them less suitable to explain the annotated anomalies. </span></p><p class="c50 c108"><span class="c3">This example provides insights on the properties that we need from the distance function: it should favor clear separa- tion of normal and abnormal values, and it should penalize features with mixed segments (values that appear in both normal and abnormal periods). Therefore, the reward of a feature is high if the feature has good separating power, and it is lower with more segmentation in its values. </span><span class="c15">4.2 Existing State of the Art </span></p><p class="c50 c63"><span class="c3">Distance functions measuring similarities of time series have been well studied [24], and there is over a dozen dis- tance functions in the literature. However, these metrics were designed with different goals in mind, and they do not fit our explanation problem well. We discuss this issue for the two major categories of distance functions [24]. Lock-step measure. In the comparison of two time se- ries, lock-step measures compare the ith point in one time series to exactly the ith point in another. Such measures include the Manhattan distance (L</span><span class="c17">1</span><span class="c3">), Euclidean distance (L</span><span class="c0">2</span><span class="c4">) [9], other L</span><span class="c0">p</span><span class="c4">-norms distances and approximation based </span><span class="c3">DISSIM distance. Those distance functions treat each pair of points independently, but in our case, we need to compare the time series holistically. For example, assume four simple time series: TS</span><span class="c17">1 </span><span class="c3">= (1,1,1), TS</span><span class="c17">2 </span><span class="c3">= (0,0,0), TS</span><span class="c17">3 </span><span class="c3">= (1,0,1) and TS</span><span class="c0">4 </span><span class="c4">= (0,1,0). Based on our separating power crite- </span><span class="c3">rion, D(TS</span><span class="c0">1</span><span class="c4">,TS</span><span class="c0">2</span><span class="c4">) should be larger than D(TS</span><span class="c0">3</span><span class="c4">,TS</span><span class="c0">4</span><span class="c4">) be- </span><span class="c3">cause there is a clear separation between the values of TS</span><span class="c17">1 </span><span class="c3">and TS</span><span class="c0">2</span><span class="c4">, while the values of TS</span><span class="c0">3 </span><span class="c4">and TS</span><span class="c0">4 </span><span class="c4">are conflicting. </span><span class="c3">However, applying any of the lock-step measures produces D(TS</span><span class="c0">1</span><span class="c4">,TS</span><span class="c0">2</span><span class="c4">) = D(TS</span><span class="c0">3</span><span class="c4">,TS</span><span class="c0">4</span><span class="c4">). </span><span class="c3">Elastic measure. Elastic measures allow comparison of one-to-many points to find the minimum difference between two time series. These measures try to compare time series on overall patterns. For example, Dynamic Time Warping (DTW) tries to stretch or compress one time series to better match another time series; while Longest Common SubSe- quence(LCSS) is based on the longest common subsequence model. Although these measures also take value difference into account, the additional emphasis on pattern matching makes them ill-suited for our problem. </span></p><p class="c50 c52"><span class="c3">Both lock-step and elastic measures fall in the category of sequence-based metrics. This means that they consider the order of values. Lock-step functions perform strict step- by-step, or event-by-event comparisons; such rigid measures cannot find similarities in the flexible event series of our problem setting. Elastic measures allow more flexibility, but </span></p><p class="c21"><span class="c72">162 </span></p><p class="c1"><span class="c3">the emphasis on matching the microstructure of sequences introduces too much randomness in the metric. </span></p><p class="c1"><span class="c3">In our case, temporal ordering is not important, because we assume the sample points in time series are independent. This makes set-based functions a better fit (as opposed to sequence-based). Set-based functions measure the macro trend while smoothing low-level details. </span><span class="c15">4.3 Entropy-Based Single-Feature Reward </span></p><p class="c7"><span class="c3">Since existing distance functions are not suitable to model single-feature rewards, we design a new distance function that emphasizes the separation of feature values between normal and abnormal intervals (Section 4.1). Our distance function is inspired by an entropy-based discretization tech- nique [10], which cuts continuous values into value intervals by minimizing the class information entropy. The segmen- tation visualized in Figure 10, shows an intuitive connection with entropy: The more mixed the color segments are, the higher the entropy (i.e., more bits are needed to describe the distribution). We continue with some background defi- nitions, and then define our entropy-based distance function, which we will use to model single-feature rewards. </span></p><p class="c1"><span class="c3">Definition 4.1 (Class Entropy). Class entropy is the infor- mation needed to describe the class distributions between two time series. Given a pair of time series, TS</span><span class="c0">A </span><span class="c4">and TS</span><span class="c0">R</span><span class="c4">, </span><span class="c3">belonging to the abnormal and reference classes, respec- tively. Let |TS</span><span class="c0">A</span><span class="c4">| and |TS</span><span class="c0">R</span><span class="c4">| denote the number of points </span><span class="c3">in the two </span><span class="c17">|TS</span><span class="c20">R</span><span class="c17">| |TS</span><span class="c20">A</span><span class="c17">|+|TS</span><span class="c3">time series, let p</span><span class="c0">A </span><span class="c4">= </span><span class="c17">|TS</span><span class="c20">A</span><span class="c17">| </span></p><p class="c78"><span class="c17">|TS</span><span class="c20">A</span><span class="c17">|+|TS</span><span class="c20">R</span><span class="c17">|</span><span class="c25">, and let p</span><span class="c17">R </span><span class="c3">= </span><span class="c20">R</span><span class="c17">|</span><span class="c25">. Then, the entropy of the class distribution is: </span><span class="c3">H</span><span class="c0">Class</span><span class="c4">(f) = p</span><span class="c0">A </span><span class="c4">&lowast; log( p</span><span class="c0">A </span><span class="c3">1) + p</span><span class="c0">R </span><span class="c4">&lowast; log( p</span><span class="c0">R </span><span class="c3">1) (1) </span></p><p class="c7"><span class="c3">Definition 4.2 (Segmentation Entropy). Segmentation en- tropy is the information needed to describe how merged points are segmented by class labels. If there are n segmen- tations, and p</span><span class="c0">i </span><span class="c4">represents the ratio of data points included </span><span class="c3">in the ith segmentation, the segmentation entropy is: </span></p><p class="c1"><span class="c3">H</span><span class="c17">Segmentation </span><span class="c3">= </span></p><p class="c1"><span class="c18">&sum;</span><span class="c17">n</span><span class="c0">i=1 </span></p><p class="c1"><span class="c3">p</span><span class="c17">i </span><span class="c3">&lowast; log( p</span><span class="c25">1</span><span class="c0">i </span><span class="c3">) (2) </span></p><p class="c7"><span class="c3">Complicated segmentations in a feature result in more en- tropy. When there is a clear separation of the two classes, as in the first two features of Figure 10, the segmentation entropy is the same as the class entropy. Otherwise, the segmentation entropy is more than the class entropy. </span></p><p class="c7"><span class="c3">Penalizing for mixed segments. Segmentation en- tropy captures the segmentation of the normal and abnormal classes, but does not penalize mixed segments with values that appear in both classes (blue segments in the visualiza- tion). Take an extreme case, where all values appear in both classes (single mixed segment). This is the scenario with the worst separation power, but its segmentation entropy is 0, because it is treated as a single segment. This indicates that we need special treatment for mixed (blue) segments. </span></p><p class="c7"><span class="c3">We assume the worst case distribution of normal and ab- normal data points within the segment. This is the uniform distribution, which leads to most segmentation and highest entropy. For example, if a mixed segment c consists of 5 data points, 3 contributed from the normal class (N) and 2 contributed from the abnormal class (A), distributing them uniformly leads to 5 segments: (N,A,N,A,N). We denote this </span></p><p class="c7"><span class="c3">worst-case ordering of segment c as c</span><span class="c14">&lowast;</span><span class="c3">. We assign a penalty term for each segment c, which is equal to the segmentation entropy of its worst-case ordering, c</span><span class="c14">&lowast;</span><span class="c3">: H</span><span class="c0">Segmentation</span><span class="c4">(c</span><span class="c17">&lowast;</span><span class="c4">). </span><span class="c3">We thus define the regularized segmentation entropy: </span></p><p class="c1"><span class="c3">H</span><span class="c17">Segmentation </span><span class="c14">+</span><span class="c25">= H</span><span class="c14">Segmentation</span><span class="c25">+ </span></p><p class="c1"><span class="c18">&sum;</span><span class="c17">m</span><span class="c0">j=1 </span></p><p class="c1"><span class="c3">H</span><span class="c17">Segmentation</span><span class="c3">(c</span><span class="c14">&lowast;</span><span class="c17">j</span><span class="c25">) (3) </span></p><p class="c7"><span class="c3">The first term in this formula is the segmentation entropy of the feature, and the second term sums the regularization penalties of all mixed segments (m). </span></p><p class="c7"><span class="c3">Accounting for feature size. Features may be of differ- ent sizes, as different event types may occur more frequently than others. The segmentation entropy is only comparable between two features f</span><span class="c0">1</span><span class="c4">, f</span><span class="c0">2</span><span class="c4">, if |f</span><span class="c0">1</span><span class="c4">.T S</span><span class="c0">A</span><span class="c4">| = |f</span><span class="c0">2</span><span class="c4">.T S</span><span class="c0">A</span><span class="c4">| and </span><span class="c3">|f</span><span class="c17">1</span><span class="c3">.T S</span><span class="c17">R</span><span class="c3">| = |f</span><span class="c17">2</span><span class="c3">.T S</span><span class="c17">R</span><span class="c3">|. However this does not hold for most features. To make these metrics comparable, we normalize segmentation entropy using class entropy and get the follow- ing definition for our entropy-based feature distance: </span></p><p class="c1"><span class="c3">D(f) = H</span><span class="c17">Segmentation</span><span class="c14">+</span><span class="c25">H</span><span class="c14">Class</span><span class="c25">(f) </span></p><p class="c78"><span class="c25">(f) (4) </span><span class="c3">We use this distance function as a measure of single-feature reward. Features with perfect separation, such as the first two features of Figure 10, have reward equal to 1. Features with more complex segmentation have lower rewards. For the 4 features displayed in Figure 10, the rewards are 1, 1, 0.31, and 0.18, respectively. </span></p><p class="c1"><span class="c15">5. CONSTRUCTING EXPLANATIONS </span></p><p class="c1"><span class="c3">The entropy-based single-feature reward identifies the fea- tures that best distinguish the normal and abnormal peri- ods. However, ranking the features based on this distance metric is not sufficient to generate explanations. We need to address three additional challenges. First, it is not clear how to select a set of features from the ranked list. There is no specific constant k for selecting a set of top-k features, and moreover, such a set would likely not be meaningful as a top-k set is likely to contain highly-correlated features with redundant information. Second, there are cases where large distances are coincidental, and not associated with anoma- lies. Third, the rewards are computed for each feature indi- vidually, and due to submodularity, they are not additive. Determining how to combine features into an explanation re- quires eliminating redundancies due to feature correlations. We proceed to describe the EXstream approach to con- structing explanations by addressing these challenges in three steps. Each step filters the feature set to eliminate features based on intuitive criteria, until we are left with a high- quality explanation. </span><span class="c15">5.1 Step 1: reward leap filtering </span></p><p class="c1"><span class="c3">The single-feature distance function produces a ranking of all features based on their individual rewards. Sharp changes in the reward between successive features in the ranking in- dicate a semantic change: Features that rank below a sharp drop in the reward are unlikely to contribute to an explana- tion. Therefore, features whose distance is low, relatively to other features, can be safely discarded. </span><span class="c15">5.2 Step 2: false positive filtering </span></p><p class="c1"><span class="c3">It is possible for features to have high rewards due to rea- sons unrelated to the investigated anomaly. For example, a </span></p><p class="c1"><span class="c72">163 </span></p><p class="c1"><span class="c56">I</span><span class="c4">(a) </span><span class="c23">Temporal </span><span class="c92">A </span><span class="c110">I</span><span class="c92">R </span><span class="c23">alignment </span></p><p class="c1"><span class="c4">(b) </span><span class="c56">I</span><span class="c92">A </span><span class="c23">Point-based </span><span class="c110">I</span><span class="c92">R </span><span class="c23">alignment </span><span class="c3">Figure 11: Two ways of alignment </span></p><p class="c1"><span class="c3">feature that measures system uptime can have strong sepa- rating power between the annotated anomaly and reference regions (e.g., the anomaly is before the reference), but this is simply due to the nature of the particular feature, and it is not related to the anomaly. We call these features false positives. Our method for identifying and purging such false positives leverages other partitions (e.g., other Hadoop jobs in our running example). The intuition is that if a feature is a false positive, the feature will demonstrate similar behav- ior in other partitions without an indication of anomaly. Identifying related partitions. We search the archived streams to identify similar partitions. Intuitively, such par- titions should be results generated by the same query, mon- itoring the same Hadoop program, on the same dataset. EXstream maintains a record of partitions in a partition table to facilitate fast retrieval. The partition table con- tains dimension attributes that record categorical informa- tion (e.g., CEP &minus; QueryID, HadoopJobName, Dataset) about the partition, and measure attributes that record par- tition statistics (e.g., monitoring duration, number of points). The system identifies related partitions, as those that match the dimension attributes. Partition alignment. Once it discovers related partitions, EXstream needs to map the annotated regions to each re- lated partition. This alignment can be temporal-based or point-based. In temporal-based alignment, an annotation is mapped to a partition based on its temporal length. For example, in Figure 4, the abnormal period occupies 31% of temporal length; this annotation will align with the the first 31% of the temporal length in a related partition (Fig- ure 11(a)). In point-based alignment an annotation is mapped to a partition based on the ratio of data points that it occu- pies in the monitoring graph. For example, the annotated high-memory usage partition of Figure 4 includes 113,070 points, with 2116 points falling in the abnormal annotation; this annotation will align with the first equal fraction of points in a related partition (Figure 11(b)). EXstream se- lects the alignment for which the two partitions have the smallest relative difference. For example, if a related parti- tion has 10% more points, but is 50% longer in time com- pared to the annotated partition, point-based alignment is preferred. Interval labeling. Alignment maps the annotations to all related partitions. Now, these new annotations need to be labeled as normal or abnormal. EXstream assigns labels through hierarchical clustering: a period that is placed in the same cluster as the annotated anomaly is labeled as abnor- mal. The clustering uses two distance functions: entropy- based, and normalized difference of frequencies. Periods whose cluster is far from the anomaly cluster are labeled as normal (reference). Finally, periods that cannot be as- signed with certainty are discarded and not used later for </span></p><p class="c1"><span class="c133">Feature Reward (annotated) Reward (all) Free memory size 1 0.77 Hadoop DataIO size 1 0.64 Num. of processes 1 0.64 Free swap size 1 1 Cached memory size 0.81 0.77 Buffer memory size 0.65 0.72 </span></p><p class="c1"><span class="c3">Figure 12: </span><span class="c2">The six validated features after the removal of false positives. </span></p><p class="c1"><span class="c3">validation. </span></p><p class="c1"><span class="c3">In Figure 11(b), both intervals are assigned a &ldquo;Reference&rdquo; label. The left one is &ldquo;Reference&rdquo; because its frequency is significantly different from the annotated one (3.7 vs. 50.1); while the right one is &ldquo;Reference&rdquo; because both its frequency and value difference are quite small, meaning it is similar to the annotated &ldquo;Reference&rdquo; interval. Feature validation. The process of partition discovering and automatic labeling generates a lot more labeled data that helps EXstream filter out false positives, and improve the current set of features. Features that have high entropy reward on the annotated partition will be reevaluated on the large dataset. If the high reward is validated in the larger dataset as well, the feature is maintained; otherwise, it is discarded. In our running example, after the validation step, only 6 out of 670 features remain. Figure 12 shows the reward for each of these 6 features for the annotated partition and the augmented partition set. </span></p><p class="c1"><span class="c15">5.3 Step 3: filtering by correlation clustering </span></p><p class="c78"><span class="c3">After the validation step, we are usually left with a small set of features, which have high individual rewards, and the high rewards are likely related to the investigated anomaly. However, it is still possible that several of these features have information overlap. For example, two identical fea- tures, are good individually, but putting them together in an explanation does not increase the information content. We identify and remove correlated features using clustering. We use pairwise correlation to identify similar features. We represent a feature as a node; two nodes are connected, if the pairwise correlation of the two features exceeds a thresh- old. We treat each connected component in this graph as a cluster, and select only one representative feature from each cluster. In our running example, the final six features are clustered into two clusters, one cluster with a single node, and another cluster with five nodes. Based on this result, the final explanation has two features. </span></p><p class="c1"><span class="c15">5.4 Building final explanations </span></p><p class="c7"><span class="c3">Once we make the final selection of features, the construc- tion of an explanation is straightforward. For each selected feature, we can build a partial explanation in the format defined in Section 2.3. The feature name becomes the vari- able name. The value boundaries for the abnormal intervals become the constants. If a feature offers perfect separation during segmentation (Section 4), there is one boundary and only one predicate is built: e.g., the abnormal value range of feature f</span><span class="c0">1 </span><span class="c4">is (&minus;&infin;,10], then the predicate is f</span><span class="c0">1 </span><span class="c4">&le; 10. If a </span><span class="c3">feature has more than one abnormal intervals, then multiple predicates are built to compose the explanation: e.g., the ab- normal value ranges of feature f</span><span class="c17">2 </span><span class="c3">are (&minus;&infin;,20],[30,50], and then the explanations are f</span><span class="c0">2 </span><span class="c4">&le; 20&or;(f</span><span class="c0">2 </span><span class="c4">&ge; 30&and;f</span><span class="c0">2 </span><span class="c4">&le; 50). Then </span></p><p class="c1"><span class="c72">164 </span></p><p class="c78"><span class="c2">No. Anomaly Hadoop workload 1 High memory WC-frequent users 2 High memory WC-sessions 3 Busy Disk WC-frequent users 4 High High CPU WC-frequent users 5 High High CPU WC-sessions 6 Busy High CPU Twitter trigram 7 High Busy Network WC-sessions 8 High Busy Network Twitter trigram </span><span class="c3">Figure 13: </span><span class="c2">Workloads for evaluating the explanations returned by EXstream. </span></p><p class="c7"><span class="c3">we simply connect the partial explanations constructed from different features using conjunction and write the final for- mula into the conjunctive normal form. </span></p><p class="c1"><span class="c15">6. EVALUATION </span></p><p class="c1"><span class="c3">We have implemented EXstream on top of the SASE stream engine [3, 26]. Due to the space constraints, the implemen- tation details are left to our technical report [27]. In this section, we evaluate EXstream on the conciseness, consis- tency, and prediction power of its returned explanations, and compare its performance with a range of alternative tech- niques in the literature. We further evaluate the efficiency of EXstream when the explanation module is run concur- rently with monitoring queries in an event stream system. </span><span class="c15">6.1 Experimental Setup </span></p><p class="c7"><span class="c3">In our first use case, we monitored a Hadoop cluster of 30 nodes which was used intensively for experiments at the Uni- versity of Massachusetts Amherst. To evaluate EXstream for explaining anomalous observations, we used three Hadoop jobs: (A) Twitter Trigram: count trigrams in a twitter stream; (B) WC-Frequent users: find frequent users in a click stream; (C) WC-session: sessionization over a click stream. </span></p><p class="c7"><span class="c3">The running example throughout this paper, which starts to show in Figure 1(a) and 1(b), is a real use case. A Hadoop expert found out the root causes by manually checking a large volume of logs. The expert also confirmed that the results generated by EXstream match the ground truth per- fectly. </span></p><p class="c7"><span class="c3">To enable the ground truth for evaluation further, we manually created four types of anomalies by running ad- ditional programs to interfere with resource consumption: (1) High memory usage: the additional programs use up memory. (2) High CPU: the additional programs keep CPU busy. (3) Busy disk: the programs keep writing to disk. (4) Busy network: the programs keep transmitting data be- tween nodes. By combining the anomaly types and Hadoop jobs, we create 8 workloads listed in Figure 13. The ground truth features are verified by a Hadoop expert. </span></p><p class="c7"><span class="c3">Our second use case is supply chain management of an aerospace company. Due to confidentiality issues we were unable to get real data. Instead, we consulted an expert and built a simulator to generate manufacturing data and anomalies such as faulty sensors and subpar material. Since both use cases generate similar results, we report results using the first use case and refer the reader to [27] for results of the second use case. </span></p><p class="c7"><span class="c3">All of our experiments were run on a server with two Intel Xeon 2.67GHz, 6-core CPUs and 16GB memory. EXstream is implemented in Java and runs on Java HotSpot 64-bit server VM 1.7 with the maximum heap size set to 8GB. </span></p><p class="c1"><span class="c3">Figure 14: </span><span class="c2">Consistency comparison </span></p><p class="c1"><span class="c15">6.2 Effectiveness of Explanations by EXstream </span></p><p class="c7"><span class="c3">We compare EXstream with a range of alternative tech- niques. We use decision trees to build explanations based on the latest version of weka, and logistic regression based on a popular R package. We consider two additional tech- niques, majority voting [15] and data fusion [19]. Both techniques make full use of every feature, and make predic- tion based on all features. Majority voting treats features equally and uses the label which counts the most as the prediction result. The fusion method fuses the prediction result from each feature based on their precision, recall and correlations. We compare these techniques on three mea- sures: (1) consistency: selected features as compared against ground truth; (2) conciseness: the number of selected fea- tures; (3) prediction accuracy when the explanation is used as a prediction model on new test data. </span></p><p class="c7"><span class="c3">Consistency. First we compare the selected features of each algorithm with the ground truth features. The results are shown in Figure 14. X-axis represents different work- loads (1 - 8), while Y-axis is the F-measure, namely, the harmonic mean of precision and recall regarding the inclu- sion of ground truth features in the returned explanations. EXstream represents our results before applying clustering on selected features, while EXstream-cluster represents re- sults clustered by correlations (Section 5). We can see that EXstream-cluster works better than EXstream without clus- tering for most of workloads, and EXstream-cluster provides much better quality than the alternative techniques. Ma- jority voting and fusion do not select features, and hence their F-measures are low. Logistic regression and decision tree generate models with selected features, with sightly increased F-measures but still significantly below those of EXstream-cluster. </span></p><p class="c7"><span class="c3">Conciseness. Figure 15 shows the sizes of explanations from each solution. Here the Y-axis (in logarithmic scale) is the number of features selected by each solution, where the total number of available features is 345. &ldquo;Ground truth&rdquo; represents the number of features in ground truth, while &ldquo;Ground truth cluster&rdquo; represents the number of clusters af- ter we apply clustering on the contained features. Again, majority voting and fusion do not select features, so the size is the same as the size of feature space. The models of logis- tic regression includes 20 - 30 features, which is roughly 10 times of the ground truth. Decision trees are more concise with less than 10 features selected. Overall, EXstream out- performs other algorithms, and is quite close to the number of features in ground truth cluster. </span></p><p class="c1"><span class="c72">165 </span></p><p class="c1"><span class="c30">Fusion </span><span class="c35">XStreamacluster Majority</span><span class="c109">&#65533;</span><span class="c35">voting </span></p><p class="c1"><span class="c30">XStream Logistic</span><span class="c16">&#65533;</span><span class="c30">regression Decision</span><span class="c16">&#65533;</span><span class="c30">tree </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c12">0.8</span><span class="c5">&#65533;</span><span class="c32">1 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c12">0.6 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c12">0.4 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c12">0.2 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c12">0 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c12">1 </span><span class="c16">&#65533;</span><span class="c12">2 </span><span class="c16">&#65533;</span><span class="c12">3 </span><span class="c16">&#65533;</span><span class="c12">4 </span><span class="c16">&#65533;</span><span class="c12">5 </span><span class="c16">&#65533;</span><span class="c12">6 </span><span class="c16">&#65533;</span><span class="c12">7 </span><span class="c16">&#65533;</span><span class="c12">8 </span></p><p class="c1"><span class="c80">Workloads </span></p><p class="c1"><span class="c3">Figure 16: </span><span class="c2">Prediction power comparison </span></p><p class="c7"><span class="c3">Predication accuracy. In Figure 16 we compare the prediction accuracy of each method. The Y-axis represents F-measure for prediction over new test data. The F-measures of EXstream, logistic regression and decision tree are quite stable, most of time above 0.95. Data fusion and major- ity voting fluctuate more. Overall, our method can provide consistent high-quality prediction power. </span></p><p class="c1"><span class="c3">Effectiveness of the distance function. We finally demonstrate the effectiveness of our entropy-based distance function by comparing it with a set of existing distance func- tions [24] for time series: (1) Manhattan distance, (2) Eu- clidean distance, (3) DTW, (4) EDR, (5) ERP and (6) LCSS. The results are shown in Figure 17. In each method, all available features are sorted by the distance function of choice in decreasing order. We measure the number of fea- tures retrieved from each sorted list in decreasing order in order to cover all the features in the ground truth, shown as the Y-axis. We see that our entropy distance is always the one using the minimum number of features to cover the ground truth. LCSS works well in the first two workloads, but it works poorly for workloads 3, 4, 5, and 6. This is be- cause the ground truth features for the first two workloads have perfect separating power based on LCSS distance, while in other workloads they contain some noisy signals. So LCSS is not as robust as our distance function. Other distance functions always use large number of features. Summary. Our explanation algorithm outperforms other techniques in consistency and conciseness while achieving comparable, high predication accuracy. Specifically, EXstream improves consistency to other methods from 10.7% to 87.5% on average, and up to 100% in some cases. EXstream is also more concise, reducing the number of features in an explana- tion 90.5% on average, up to 99.5% in some cases. EXstream </span></p><p class="c1"><span class="c30">XStream </span><span class="c35">XStreamacluster Majority</span><span class="c109">&#65533;</span><span class="c35">voting </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c12">350 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c12">100 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c12">10 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c12">1 </span><span class="c16">&#65533;</span><span class="c12">2 </span><span class="c16">&#65533;</span><span class="c12">3 </span><span class="c16">&#65533;</span><span class="c12">4 </span><span class="c16">&#65533;</span><span class="c12">5 </span><span class="c16">&#65533;</span><span class="c12">6 </span><span class="c16">&#65533;</span><span class="c12">7 </span><span class="c16">&#65533;</span><span class="c12">8 </span></p><p class="c1"><span class="c80">Workloads </span></p><p class="c1"><span class="c28">XStream </span><span class="c87">XStreamacluster Majority</span><span class="c109">&#65533;</span><span class="c87">voting </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c40">0.8</span><span class="c5">&#65533;</span><span class="c26">1 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c40">0.6 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c40">0.4 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c40">0.2 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c40">1 </span><span class="c16">&#65533;</span><span class="c40">2 </span><span class="c16">&#65533;</span><span class="c40">3 </span><span class="c16">&#65533;</span><span class="c40">4 </span><span class="c16">&#65533;</span><span class="c40">5 </span><span class="c16">&#65533;</span><span class="c40">6 </span><span class="c16">&#65533;</span><span class="c40">7 </span><span class="c16">&#65533;</span><span class="c40">8 </span></p><p class="c1"><span class="c91">Workloads </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c12">1</span><span class="c5">&#65533;</span><span class="c32">2</span><span class="c5">&#65533;</span><span class="c32">3</span><span class="c5">&#65533;</span><span class="c32">4</span><span class="c5">&#65533;</span><span class="c32">5 </span></p><p class="c1"><span class="c3">Figure 15: </span><span class="c2">Conciseness comparison </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c40">0 </span></p><p class="c1"><span class="c30">Fusion </span></p><p class="c1"><span class="c28">Logistic</span><span class="c16">&#65533;</span><span class="c28">regression </span></p><p class="c1"><span class="c30">Ground</span><span class="c16">&#65533;</span><span class="c30">truth</span><span class="c16">&#65533;</span><span class="c30">cluster </span></p><p class="c1"><span class="c30">Logistic</span><span class="c16">&#65533;</span><span class="c30">regression </span></p><p class="c1"><span class="c28">Decision</span><span class="c16">&#65533;</span><span class="c28">tree </span></p><p class="c1"><span class="c30">Decision</span><span class="c16">&#65533;</span><span class="c30">tree Ground</span><span class="c16">&#65533;</span><span class="c30">truth </span></p><p class="c1"><span class="c28">Fusion </span></p><p class="c1"><span class="c3">Figure 17: </span><span class="c2">Distance function comparison </span></p><p class="c7"><span class="c3">is as good as other techniques on prediction quality: its F- measure on prediction is only slightly worse than logistic regression by 0.4%, while it is 3.3% higher than majority voting, 6.1% percent higher than fusion, and 1.9% higher than decision tree. </span></p><p class="c1"><span class="c3">Our entropy distance function works better than existing distance functions on time series. It reduces the size of ex- planations by 94.6% on average, up to 97.2%, compared to other functions. </span><span class="c15">6.3 Efficiency of EXstream </span></p><p class="c7"><span class="c3">We further evaluate the efficiency of EXstream. Our main result shows that our implementation is highly efficient: with 2000 concurrent monitoring queries, triggered explanation analysis returns explanations within half a minute and af- fects the performance only slightly, delaying events process- ing by only 0.4 second on average. Additional details are available at [27]. </span></p><p class="c1"><span class="c15">7. RELATED WORK </span></p><p class="c7"><span class="c3">In the previous section, we compared our entropy distance with a set of state-of-the-art distance functions [24] and com- pared our techniques with prediction techniques including decision trees and logistic regression [2]. In this section we survey broadly related work. </span></p><p class="c7"><span class="c3">CEP systems. There are a number of CEP systems in the research community [8, 17, 1, 21, 23]. These systems focus on passive monitoring using CEP queries by providing either more powerful query languages or better evaluation performance. Existing CEP techniques do not produce ex- planations for anomalous observations. </span></p><p class="c7"><span class="c3">Explaining outliers in SQL query results. Scor- pion [25] explains outliers in group-by aggregate queries. Users annotate outliers on the results of group-by queries, and then scorpion searches for predicates that remove these outliers while minimally affect the normal answers. It does not suit our problem because it works only for group-by ag- gregation queries and it searches through various subsets of the tuples that were used to compute the query answers. As shown for our example, Q1, the explanation of memory usage contention among different jobs cannot be generated from only those events that produced the monitoring results of Q1. Recent work [20] extends Scropion by supporting richer and insightful explanations by pre-computation and thus enables interactive explanation discovery. This work assumes a set of explanation templates given by the user and requires precomputation in a given database. Neither of the assumptions fits our problem setting. </span></p><p class="c1"><span class="c72">166 </span></p><p class="c1"><span class="c28">EDR Ground</span><span class="c16">&#65533;</span><span class="c28">truth </span><span class="c87">Manhanttan </span></p><p class="c7"><span class="c28">Entropy Euclidean LCSS DTW </span></p><p class="c1"><span class="c28">ERP </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c40">350 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c40">100 </span></p><p class="c1"><span class="c16">&#65533;</span><span class="c40">10 </span></p><p class="c1"><span class="c5">&#65533;</span><span class="c26">5 </span><span class="c5">&#65533;</span><span class="c26">4</span><span class="c5">&#65533;</span><span class="c26">3</span><span class="c5">&#65533;</span><span class="c26">2</span><span class="c16">&#65533;</span><span class="c40">1</span><span class="c16">&#65533;</span><span class="c40">1 </span><span class="c16">&#65533;</span><span class="c40">2 </span><span class="c16">&#65533;</span><span class="c40">3 </span><span class="c16">&#65533;</span><span class="c40">4 </span><span class="c16">&#65533;</span><span class="c40">5 </span><span class="c16">&#65533;</span><span class="c40">6 </span><span class="c16">&#65533;</span><span class="c40">7 </span><span class="c16">&#65533;</span><span class="c40">8 </span></p><p class="c1"><span class="c91">Workloads </span></p><p class="c129"><span class="c3">Explaining outputs in iterative analytics. Recent work [7] focuses on tracking, maintaining, and querying lin- eage and &ldquo;how&rdquo; provenance in the context of arbitrary itera- tive data flows. It aims to create a set of recursively defined rules that determine which records in a data-parallel com- putation inputs, intermediate records, and outputs require explanation. It allows one to identify when (i.e., the points in the computation) and how a data collection changes, and provides explanations for only these few changes. </span></p><p class="c57"><span class="c3">Set-based distance function for time series. Besides the lock-step and elastic distance functions we compared with, time series are also transformed into sets [18] for mea- surement. However, the goal of the set-based function is to speed up the computation of existing elastic distance, so it is different from our entropy based distance function. </span></p><p class="c45"><span class="c3">Anomaly detection. Common anomaly detection tech- niques [5, 6, 14, 13, 22] do not fit our problem setting. There are two main approaches. One is using a prediction model, which is learned on labeled or unlabeled data. Then incom- ing data is compared against with expected value by the model. If the difference is significant, the point or time se- ries will be reported as outlier. The other approach is using distance functions, and outliers are those points or time se- ries far from normal values. Both approaches report only outliers, but not the reasons (explanations) why they occur. </span></p><p class="c126"><span class="c15">8. CONCLUSIONS </span></p><p class="c131"><span class="c3">In this paper, we present EXstream, a system that pro- vides high-quality explanations for anomalous behaviors that users annotate on CEP-based monitoring results. Formu- lated as a submodular optimization problem, which is hard to solve, we provide a new approach that integrates a new entropy-based distance function and effective feature rank- ing and filtering methods. Evaluation results show that EXstream outperforms existing techniques significantly in conciseness and consistency, while achieving comparable high prediction power and retaining a highly efficient implemen- tation of a data stream system. </span></p><p class="c49"><span class="c3">To enable proactive monitoring in CEP systems, our fu- ture work will address temporal correlation in discovering explanations, automatic recognition and explanation of anoma- lous behaviors, and exploration of richer feature space to enable complex explanations. Acknowledgements. This work was supported in part by the National Science Foundation under grants IIS-1421322, IIS-1453543, and IIS-1218524. </span></p><p class="c58"><span class="c15">9. </span><span class="c23">[1] D. </span><span class="c15">REFERENCES </span><span class="c23">J. Abadi, D. Carney, U. &nbsp;&#807;Cetintemel, M. Cherniack, </span></p><p class="c139"><span class="c2">C. Convey, S. Lee, M. Stonebraker, N. Tatbul, and S. Zdonik. Aurora: a new model and architecture for data stream management. The VLDB Journal, 12(2):120&ndash;139, 2003. [2] C. C. Aggarwal. Data Mining: The Textbook. Springer </span></p><p class="c19"><span class="c2">Publishing Company, Incorporated, 2015. [3] J. Agrawal, Y. Diao, D. Gyllstrom, and N. Immerman. </span></p><p class="c62"><span class="c2">Efficient pattern matching over event streams. In SIGMOD, pages 147&ndash;160, New York, NY, USA, 2008. ACM. [4] R. S. Barga, J. Goldstein, M. Ali, and M. Hong. Consistent </span></p><p class="c76"><span class="c2">streaming through time: A vision for event stream processing. arXiv preprint cs/0612115, 2006. [5] L. Cao, J. Wang, and E. A. Rundensteiner. Sharing-aware </span></p><p class="c100 c111"><span class="c2">outlier analytics over high-volume data streams. In Proceedings of the 2016 International Conference on Management of Data, pages 527&ndash;540. ACM, 2016. </span></p><p class="c1 c31"><span class="c2">[6] V. Chandola, A. Banerjee, and V. Kumar. Anomaly </span></p><p class="c100 c135"><span class="c2">detection: A survey. ACM Computing Surveys (CSUR), 41(3):15, 2009. [7] Z. Chothia, J. Liagouris, F. McSherry, and T. Roscoe. </span></p><p class="c114"><span class="c2">Explaining outputs in modern data analytics. Proceedings of the VLDB Endowment, 9(4), 2015. [8] A. J. Demers, J. Gehrke, B. Panda, M. Riedewald, </span></p><p class="c51"><span class="c2">V. Sharma, and W. M. White. Cayuga: A general purpose event monitoring system. In CIDR, pages 412&ndash;422, 2007. [9] C. Faloutsos, M. Ranganathan, and Y. Manolopoulos. Fast subsequence matching in time-series databases, volume 23. ACM, 1994. [10] U. Fayyad and K. B. Irani. Multi-interval discretization of </span></p><p class="c100 c105"><span class="c2">continuous-valued attributes for classification learning. 1993. [11] U. Feige, V. S. Mirrokni, and J. Vondrak. Maximizing </span></p><p class="c75 c100"><span class="c2">non-monotone submodular functions. SIAM Journal on Computing, 40(4):1133&ndash;1153, 2011. [12] Ganglia monitoring system. http://ganglia.sourceforge.net/. [13] M. Gupta, J. Gao, C. Aggarwal, and J. Han. Outlier </span></p><p class="c112"><span class="c2">detection for temporal data. Synthesis Lectures on Data Mining and Knowledge Discovery, 5(1):1&ndash;129, 2014. [14] H. Huang and S. P. Kasiviswanathan. Streaming anomaly detection using randomized matrix sketching. Proceedings of the VLDB Endowment, 9(3):192&ndash;203, 2015. [15] L. Lam and S. Suen. Application of majority voting to </span></p><p class="c90"><span class="c2">pattern recognition: an analysis of its behavior and performance. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, 27(5):553&ndash;568, 1997. [16] D. Luckham. Event Processing for Business: Organizing </span></p><p class="c41"><span class="c2">the Real-Time Enterprise. Wiley, 2011. [17] Y. Mei and S. Madden. Zstream: a cost-based query </span></p><p class="c74"><span class="c2">processor for adaptively detecting composite events. In SIGMOD Conference, pages 193&ndash;206, 2009. [18] J. Peng, H. Wang, J. Li, and H. Gao. Set-based similarity </span></p><p class="c64"><span class="c2">search for time series. In Proceedings of the 2016 International Conference on Management of Data, pages 2039&ndash;2052. ACM, 2016. [19] R. Pochampally, A. Das Sarma, X. L. Dong, A. Meliou, and D. Srivastava. Fusing data with correlations. In Proceedings of the 2014 ACM SIGMOD international conference on Management of data, pages 433&ndash;444. ACM, 2014. [20] S. Roy, L. Orr, and D. Suciu. Explaining query answers </span></p><p class="c134"><span class="c2">with explanation-ready databases. Proceedings of the VLDB Endowment, 9(4):348&ndash;359, 2015. [21] StreamSQL Team. StreamSQL: a data stream language </span></p><p class="c66"><span class="c2">extending SQL. http://blogs.streamsql.org/. [22] L. Tran, L. Fan, and C. Shahabi. Distance based outlier </span></p><p class="c47"><span class="c2">detection for data streams. Proceedings of the VLDB Endowment, 9(4):1089&ndash;1100, 2015. [23] D. Wang, E. A. Rundensteiner, and R. T. Ellison. Active </span></p><p class="c100 c105"><span class="c2">complex event processing over event streams. PVLDB, 4(10):634&ndash;645, 2011. [24] X. Wang, A. Mueen, H. Ding, G. Trajcevski, </span></p><p class="c84"><span class="c2">P. Scheuermann, and E. Keogh. Experimental comparison of representation methods and distance measures for time series data. Data Mining and Knowledge Discovery, 26(2):275&ndash;309, 2013. [25] E. Wu and S. Madden. Scorpion: explaining away outliers </span></p><p class="c100 c75"><span class="c2">in aggregate queries. Proceedings of the VLDB Endowment, 6(8):553&ndash;564, 2013. [26] H. Zhang, Y. Diao, and N. Immerman. On complexity and </span></p><p class="c121"><span class="c2">optimization of expensive queries in complex event processing. In Proceedings of the 2014 ACM SIGMOD international conference on Management of data, pages 217&ndash;228. ACM, 2014. [27] H. Zhang, Y. Diao, and A. Meliou. Exstream: Explaining </span></p><p class="c120"><span class="c2">anomalies in event stream monitoring tech report. https://cs.umass.edu/%7Ehaopeng/tr.pdf). </span></p><p class="c95"><span class="c72">167 </span></p></body></html>