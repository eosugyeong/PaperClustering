<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c123{margin-left:-26.2pt;padding-top:1.7pt;text-indent:35.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c21{color:#0d0887;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:3.5pt;font-family:"Courier New";font-style:normal}.c81{margin-left:-17.1pt;padding-top:1.9pt;text-indent:28.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.6pt}.c79{margin-left:-16.4pt;padding-top:1.7pt;text-indent:25.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-27.4pt}.c98{margin-left:-23.5pt;padding-top:21.6pt;text-indent:24pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12.6pt}.c106{color:#0d0887;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:3.8pt;font-family:"Courier New";font-style:normal}.c113{color:#0d0887;font-weight:400;text-decoration:none;vertical-align:super;font-size:6.3pt;font-family:"Courier New";font-style:normal}.c101{margin-left:-26.2pt;padding-top:1.4pt;text-indent:35.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c67{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Courier New";font-style:normal}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:italic}.c135{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c30{margin-left:-26.2pt;padding-top:1.7pt;text-indent:35.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.4pt}.c71{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:italic}.c117{margin-left:-26.9pt;padding-top:21.1pt;text-indent:32.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9.2pt}.c15{color:#000000;font-weight:700;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:normal}.c91{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.8pt;font-family:"Arial";font-style:normal}.c88{margin-left:-16.4pt;padding-top:11.8pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-26.2pt}.c121{margin-left:-25.7pt;padding-top:1.7pt;text-indent:34.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.1pt}.c64{margin-left:-21.4pt;padding-top:1.7pt;text-indent:21.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.1pt}.c89{margin-left:-17.4pt;padding-top:1.7pt;text-indent:26.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-26.6pt}.c92{margin-left:-3.4pt;padding-top:1.2pt;text-indent:12.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:33.6pt}.c159{margin-left:-13.5pt;padding-top:34.3pt;text-indent:59.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:0.5pt}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Times New Roman";font-style:italic}.c47{margin-left:-26.4pt;padding-top:4.1pt;text-indent:35.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.4pt}.c26{margin-left:-26.2pt;padding-top:3.8pt;text-indent:35.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c130{margin-left:-26.2pt;padding-top:6.5pt;text-indent:35.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.4pt}.c110{margin-left:-17.1pt;padding-top:1.7pt;text-indent:17.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.5pt}.c23{color:#1a1a1a;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.3pt;font-family:"Arial";font-style:normal}.c58{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.5pt;font-family:"Arial";font-style:normal}.c22{margin-left:-17.4pt;padding-top:1.4pt;text-indent:26.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-26.6pt}.c99{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c63{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:italic}.c95{margin-left:-21.4pt;padding-top:1.7pt;text-indent:22.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-14pt}.c69{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.5pt;font-family:"Arial";font-style:normal}.c116{color:#0d0887;font-weight:400;text-decoration:none;vertical-align:sub;font-size:6.3pt;font-family:"Courier New";font-style:normal}.c37{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c56{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c96{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.8pt;font-family:"Arial";font-style:normal}.c32{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c44{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.6pt;font-family:"Arial";font-style:normal}.c155{margin-left:-16.2pt;padding-top:1.4pt;text-indent:25.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-27.1pt}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:italic}.c9{color:#7f7f7f;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.3pt;font-family:"Arial";font-style:normal}.c136{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Times New Roman";font-style:normal}.c119{margin-left:-17.1pt;padding-top:1.7pt;text-indent:26.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-26.4pt}.c151{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.7pt;font-family:"Arial";font-style:normal}.c152{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:10.7pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c74{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.9pt;font-family:"Arial";font-style:normal}.c128{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:italic}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c43{color:#7f7f7f;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.4pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c100{color:#7f7f7f;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.5pt;font-family:"Arial";font-style:normal}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:italic}.c122{color:#7f7f7f;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.7pt;font-family:"Arial";font-style:normal}.c145{margin-left:-25pt;padding-top:1.7pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.6pt}.c132{margin-left:-26.6pt;padding-top:1.7pt;text-indent:35.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c40{color:#7f7f7f;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c49{color:#7f7f7f;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.6pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c46{margin-left:-25pt;padding-top:3.8pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.6pt}.c54{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.3pt;font-family:"Arial";font-style:normal}.c115{margin-left:-25.4pt;padding-top:3.8pt;text-indent:34.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.8pt}.c61{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.6pt;font-family:"Arial";font-style:normal}.c45{margin-left:-16.4pt;padding-top:1.7pt;text-indent:25.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-27.6pt}.c148{margin-left:-16.4pt;padding-top:25pt;text-indent:16.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-27.6pt}.c51{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c72{margin-left:-25.4pt;padding-top:1.4pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.3pt}.c48{margin-left:-17.1pt;padding-top:1.4pt;text-indent:17.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.4pt}.c73{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c85{margin-left:-25.2pt;padding-top:1.7pt;text-indent:34.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.6pt}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c142{margin-left:-25pt;padding-top:1.4pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.1pt}.c39{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.7pt;font-family:"Arial";font-style:normal}.c28{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:10.5pt;font-family:"Arial";font-style:normal}.c24{margin-left:-16.4pt;padding-top:4.1pt;text-indent:25.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-27.4pt}.c129{margin-left:-26.9pt;padding-top:21.6pt;text-indent:27.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-13.5pt}.c161{margin-left:-16.4pt;padding-top:1.7pt;text-indent:25.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-27.8pt}.c55{color:#7f7f7f;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.6pt;font-family:"Arial";font-style:normal}.c84{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.4pt;font-family:"Arial";font-style:normal}.c41{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.4pt;font-family:"Arial";font-style:normal}.c75{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.3pt;font-family:"Arial";font-style:normal}.c139{margin-left:-16.4pt;padding-top:1.4pt;text-indent:25.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-27.4pt}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:normal}.c158{margin-left:-26.4pt;padding-top:1.7pt;text-indent:35.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.6pt}.c93{margin-left:-26.2pt;padding-top:1.4pt;text-indent:35.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.8pt}.c97{margin-left:-21.4pt;padding-top:1.4pt;text-indent:35.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-10.2pt}.c25{color:#0d0887;font-weight:400;text-decoration:none;vertical-align:super;font-size:5.8pt;font-family:"Courier New";font-style:normal}.c144{color:#0d0887;font-weight:400;text-decoration:none;vertical-align:sub;font-size:5.8pt;font-family:"Courier New";font-style:normal}.c80{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.7pt;font-family:"Arial";font-style:normal}.c124{margin-left:-25pt;padding-top:1.9pt;text-indent:37pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:italic}.c59{margin-left:-21.4pt;padding-top:1.7pt;text-indent:21.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-10.2pt}.c143{margin-left:-26.2pt;padding-top:4.1pt;text-indent:35.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.9pt}.c102{margin-left:-16.2pt;padding-top:1.4pt;text-indent:25.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-27.4pt}.c134{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c70{margin-left:-16.4pt;padding-top:1.7pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-27.4pt}.c42{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c114{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.4pt;font-family:"Arial";font-style:normal}.c53{margin-left:-26.2pt;padding-top:4.1pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-17.6pt}.c112{margin-left:-26.2pt;padding-top:10.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:70.7pt}.c78{margin-left:-26.4pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-17.6pt}.c104{margin-left:-3.4pt;padding-top:11.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:124.1pt}.c149{margin-left:-26.2pt;padding-top:10.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:121.4pt}.c140{margin-left:40.1pt;padding-top:27.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:117.3pt}.c33{margin-left:-12.7pt;padding-top:7.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-22.6pt}.c65{margin-left:244.1pt;padding-top:78.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-273.7pt}.c162{margin-left:-26.2pt;padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:18.6pt}.c94{margin-left:1.4pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.9pt}.c133{margin-left:-22.3pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:96.6pt}.c147{margin-left:-12.7pt;padding-top:7.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.9pt}.c68{margin-left:-17.1pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.6pt}.c86{margin-left:-25.4pt;padding-top:28.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.6pt}.c27{margin-left:-25.2pt;padding-top:30.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.1pt}.c154{margin-left:11pt;padding-top:22.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:21.8pt}.c90{margin-left:-17.1pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.7pt}.c103{margin-left:-22.3pt;padding-top:10.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:76.5pt}.c10{margin-left:-21.4pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:8.8pt}.c107{margin-left:-23.3pt;padding-top:21.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:94.7pt}.c11{margin-left:223.2pt;padding-top:66.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-31pt}.c108{margin-left:-17pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.4pt}.c156{margin-left:-17.8pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-26.6pt}.c77{margin-left:-17.1pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.4pt}.c120{margin-left:-26.2pt;padding-top:10.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:116.1pt}.c118{margin-left:-17.1pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.2pt}.c157{margin-left:-12.7pt;padding-top:6.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.6pt}.c50{margin-left:-18.8pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:24pt}.c125{margin-left:-21.4pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-14.5pt}.c60{margin-left:-17.1pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-26.4pt}.c38{margin-left:220.6pt;padding-top:69.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-33.4pt}.c31{margin-left:-7.7pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.4pt}.c150{margin-left:-25pt;padding-top:14.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:56.1pt}.c82{margin-left:-17.1pt;padding-top:13.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-27.1pt}.c111{margin-left:-17.1pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-24.7pt}.c153{margin-left:-22.8pt;padding-top:57.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12.6pt}.c52{margin-left:-26.2pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.9pt}.c105{margin-left:-17.1pt;padding-top:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:46.8pt}.c146{margin-left:-17.1pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.6pt}.c35{margin-left:-26.2pt;padding-top:10.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:91.8pt}.c127{margin-left:32.1pt;padding-top:30.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:130.6pt}.c87{margin-left:-16.2pt;padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:82.3pt}.c57{margin-left:-16.2pt;padding-top:28.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:167.3pt}.c138{padding-top:1.7pt;text-indent:26.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c20{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c29{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c126{padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c19{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c137{margin-left:-21.4pt;text-indent:21.8pt;margin-right:-2.2pt}.c66{margin-left:-17.8pt;text-indent:27.2pt;margin-right:-13.9pt}.c62{margin-left:-4.6pt;text-indent:36.3pt;margin-right:-14.4pt}.c160{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c83{margin-left:-18.8pt;margin-right:89.5pt}.c131{margin-left:-17.1pt;margin-right:-25.2pt}.c36{margin-left:157.4pt;margin-right:17.5pt}.c76{margin-left:-17.1pt;margin-right:-25pt}.c141{margin-left:-130.9pt;margin-right:306pt}.c109{margin-left:-12.3pt;margin-right:-3.1pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c160"><p class="c20"><span class="c134">Parallel Array-Based Single- and Multi-Source Breadth First Searches on Large Dense Graphs </span></p><p class="c3"><span class="c42">Moritz Kaufmann </span><span class="c99">Technical </span><span class="c56">kaufmanm@in.tum.de </span></p><p class="c3"><span class="c99">University of Munich </span><span class="c42">Manuel Then </span><span class="c99">Technical </span><span class="c56">then@in.tum.de </span></p><p class="c3"><span class="c99">University of Munich </span><span class="c42">Alfons Kemper </span><span class="c99">Technical </span><span class="c56">kemper@in.tum.de </span><span class="c99">University of Munich </span><span class="c42">Thomas Neumann </span><span class="c99">Technical </span><span class="c56">neumann@in.tum.de </span><span class="c99">University of Munich </span><span class="c34">ABSTRACT </span><span class="c7">One of the fundamental algorithms in analytical graph data- bases is breadth-first search (BFS). It is the basis of reach- ability queries, centrality computations, neighborhood enu- meration, and many other commonly-used algorithms. </span></p><p class="c19"><span class="c7">We take the idea of purely array-based BFSs introduced in the </span><span class="c12">sequential </span><span class="c7">multi-source MS-BFS algorithm and extend this approach to </span><span class="c12">multi-threaded single- </span><span class="c7">and </span><span class="c12">multi-source </span><span class="c7">BFSs. Replacing the typically used queues with fixed-sized arrays, we eliminate major points of contention which other BFS algorithms experience. To ensure equal work distribution between threads, we co-optimize work stealing paralleliza- tion with a novel vertex labeling. Our BFS algorithms have excellent scaling behavior and take advantage of multi-core NUMA architectures. </span></p><p class="c19"><span class="c7">We evaluate our proposed algorithms using real-world and synthetic graphs with up to 68 billion edges. Our evaluation shows that the proposed multi-threaded single- and multi- source algorithms scale well and provide significantly better performance than other state-of-the-art BFS algorithms. </span></p><p class="c3"><span class="c34">1. INTRODUCTION </span></p><p class="c19"><span class="c7">Graphs are a natural abstraction for various common con- cepts like communication, interactions as well as friendships. Thus, graphs are a good way of representing social networks, web graphs, and communication networks. To extract struc- tural information and business insights, a plethora of graph algorithms have been developed in multiple research commu- nities. </span></p><p class="c19"><span class="c7">At the core of many analytical graph algorithms are breadth first searches (BFSs). During a BFS, the vertices of a graph are traversed in order of their distance&mdash;measured in hops&mdash; from a source vertex. This traversal pattern can for example be used to do shortest path computations, pattern match- ings, neighborhood enumerations, and centrality calculations. While all these algorithms are BFS-based, many different </span></p><p class="c19"><span class="c136">c 2017, Copyright is with the authors. Published in Proc. 20th International Conference on Extending Database Technology (EDBT), March 21-24, 2017 - Venice, Italy: ISBN 978-3-89318-073-8, on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Com- mons license CC-by-nc-nd 4.0 </span></p><p class="c3"><span class="c4">single-source multi-source </span><span class="c75">laitneuqe</span><span class="c4">s</span><span class="c75">l ellara</span><span class="c4">pBeamer [5] Yasui [20] </span></p><p class="c3"><span class="c4">MS-BFS(Then) [18] Our contributions SMS-PBFS MS-PBFS </span></p><p class="c20"><span class="c4">Hong [12] Agarwal [2] Chhugani [8] </span></p><p class="c3"><span class="c4">iBFS(Liu) [14] </span></p><p class="c3"><span class="c5">Figure 1: State-of-the-art single-server breadth-first- search publications </span></p><p class="c19"><span class="c7">BFS variants have been published. Important aspects that differentiate BFS variants are their </span><span class="c12">degree of parallelism</span><span class="c7">, the </span><span class="c12">number of sources </span><span class="c7">they consider, and the </span><span class="c12">type of graph </span><span class="c7">they are suited for. </span></p><p class="c19"><span class="c7">Possible degrees of parallelism include single-threaded and multi-threaded execution as well as distributed processing. Many emerging systems, e.g., Pregel [16], Spark [21], and GraphLab [15], focus heavily on distributed processing, but often neglect to optimize for the single-machine use case. However, especially for graph analytics, distributed process- ing is a very hard problem. The main reason for this is the high communication cost between compute nodes which is directly influenced by the inherent complexity of graph parti- tioning [4]. While there are cases in which distribution cannot be avoided, we argue that in graph analytics it is often done unnecessarily, leading to diminished performance. Actually, most&mdash;even large-scale&mdash;real-world graphs easily fit into the main memory of modern server-class machines [11]. Thus, we only consider the single-node scenario but differentiate between single and multi-threaded processing. </span></p><p class="c3"><span class="c7">In Figure 1 we give an overview of the multi-threaded single-node state-of-the-art BFS algorithms. </span></p><p class="c19"><span class="c7">The figure also includes the second important aspect of a BFS variant: its number of sources. Traditionally, the BFS problem is stated as traversing the graph from a single source vertex. While this single-source model can be applied to any BFS-based algorithm, it hampers inter-BFS optimizations. Specialized multi-source BFS algorithms like MS-BFS [18] and the GPU based iBFS [14] concurrently traverse the graph from multiple source vertices and try to share common work between the BFSs. This is, for example beneficial when the all pairs shortest path (APSP) problem needs to be solved as it is the case for the closeness centrality metric. </span></p><p class="c3"><span class="c6">Series ISSN: 2367-2005 1 </span><span class="c135">10.5441/002/edbt.2017.02 </span></p><p class="c78"><span class="c7">For this computation, a full BFS is necessary from every vertex in the graph. Considering that small-world networks often consist of a single large connected component, a single- source BFS would visit every vertex in each traversal while a multi-source-optimized BFS batches visits where possible. One central limitation of current multi-source BFS algo- rithms is their limited ability to analyze large graphs ef- ficiently. The GPU-based iBFS is limited to the memory available on GPU cards which is over an order of magnitude less than what is available in modern servers. The CPU-based MS-BFS on the other hand is sequential; utilizing all cores would require an separate BFS instance for each CPU core. It can only speed up analysis when a huge number of sources is analyzed and it requires much more memory due to the separate BFS states. </span></p><p class="c132"><span class="c7">In this paper we propose two breadth-first search algo- rithms that are optimized for modern massively-parallel multi-socket machines: SMS-PBFS and MS-PBFS. </span><span class="c12">SMS- PBFS </span><span class="c7">is a parallelized single-source BFS, while </span><span class="c12">MS-PBFS </span><span class="c7">is a parallelized multi-source BFS. Both algorithms are based on the approaches introduced by the sequential MS-BFS. By adding scalable parallelization we enable massive speedups especially when working with a limited number of sources. Our approach also significantly reduces memory requirements for parallelized multi-source BFSs. </span></p><p class="c123"><span class="c7">Our evaluation using real-world graphs as well as arti- ficial graphs, including the industry-standard benchmark Graph500 [1], shows that SMS-PBFS and MS-PBFS greatly outperform the existing state-of-the-art BFS algorithms. Be- cause the overhead for parallelization is negligible, our paral- lelized algorithms can be efficiently used for sequential BFS traversals without modifications. </span></p><p class="c108"><span class="c7">Specifically, the contributions of this paper are as follows: </span></p><p class="c157"><span class="c7">&bull; We present the </span><span class="c12">MS-PBFS </span><span class="c7">algorithm, a multi-core NUMA- aware multi-source BFS that ensures full machine uti- lization even for a limited number of sources. We also in- troduce </span><span class="c12">SMS-PBFS</span><span class="c7">, a multi-core NUMA-aware single- source BFS based on MS-PBFS that shows better per- formance than existing single-source BFS algorithms. </span></p><p class="c147"><span class="c7">&bull; We introduce a new vertex labeling scheme that is both cache-friendly as well as skew-avoiding. </span></p><p class="c33"><span class="c7">&bull; We propose a parallel low-overhead work stealing schedul- ing scheme that preserves NUMA locality in BFS work- loads. </span></p><p class="c130"><span class="c7">The latter two contributions can also boost the perfor- mance of existing BFS algorithms as well as other graph algorithms. </span></p><p class="c158"><span class="c7">The paper is structured as follows. In Section 2 we describe the state-of-the-art BFS algorithms for the sequential and parallel single-source case as well as for the sequential multi- source case and summarize their limitations. Afterward, in Section 3 we present our novel algorithms MS-PBFS and SMS-PBFS. In Section 4 we describe our optimized schedul- ing algorithm, vertex labeling scheme, and memory-layout for modern NUMA architectures. Section 5 contains the evalua- tion of our algorithms. We give an overview over the related work in Section 6. Section 7 summarizes our findings. </span></p><p class="c120"><span class="c34">2. BACKGROUND </span></p><p class="c143"><span class="c7">In this section we describe the current state-of-the-art BFS algorithm variants. We focus on algorithms that operate on </span></p><p class="c19 c131"><span class="c7">undirected, unweighted graphs. Such a graph is represented by a tuple </span><span class="c14">G </span><span class="c7">= {</span><span class="c14">V,E</span><span class="c7">}, where </span><span class="c14">V </span><span class="c7">is the set of vertices and </span><span class="c14">E </span><span class="c7">= {</span><span class="c12">neighbors</span><span class="c1">v</span><span class="c7">|</span><span class="c14">v </span><span class="c7">&isin; </span><span class="c14">V </span><span class="c7">} where </span><span class="c12">neighbors</span><span class="c1">v </span><span class="c7">is the set of neighbors of </span><span class="c14">v</span><span class="c7">. Additionally, we assume that the graphs of interest are </span><span class="c12">small-world networks </span><span class="c7">[3], i.e., that they are strongly connected and their number of neighbors per vertex follows a power law distribution. This is the case for most real-world graphs; examples include social networks, communication graphs and web graphs. </span></p><p class="c22"><span class="c7">Given a graph </span><span class="c14">G </span><span class="c7">and a source vertex </span><span class="c14">s</span><span class="c7">, a BFS traverses the graph from </span><span class="c14">s </span><span class="c7">until all reachable vertices have been visited. During this process, vertices with a one-hop distance from </span><span class="c14">s </span><span class="c7">are visited first, then all vertices with distance two and so on. Each distance corresponds to one iteration. While executing an iteration the neighbors of vertices that were newly discovered in the previous iteration are checked to see if they have not yet been discovered. If so, they are marked as newly seen and enqueued for the next iteration. Consequently, the basic data structures during execution are a queue of vertices that were discovered in the previous iteration and must be processed in the current iteration, called </span><span class="c12">frontier</span><span class="c7">, a mapping </span><span class="c12">seen </span><span class="c7">that allows checking if a vertex has already been visited, and a queue </span><span class="c12">next </span><span class="c7">of vertices that were newly discovered in the current iteration. The latter queue is used as input for the next iteration. The number of BFS iterations corresponds to the maximum distance of any vertex from the source. It is bound by the diameter of the graph, i.e., the greatest shortest distance between any two vertices. </span></p><p class="c119"><span class="c7">Our novel MS-PBFS and SMS-PBFS algorithms build on multiple existing techniques which we introduce in the following. We categorize the presented algorithms as either parallel or sequential, and as either single-source or multi- source as shown in Figure 1. To the best of our knowledge, each of the presented algorithms in this chapter is the current single-server state-of-the art in its category. </span></p><p class="c156"><span class="c34">2.1 Sequential and Parallel Single-Source BFS </span><span class="c7">The fastest sequential single source BFS algorithm for dense graphs was presented by Beamer et al. [5]. It breaks up the algorithmic structure of the traditional BFS to especially reduce the amount of work required to analyze small-world networks. In such graphs most vertices are reached within few iterations [18]. This has the effect that at the end of this &ldquo;hot phase&rdquo; the frontier for the next iteration contains many more vertices than there are unseen vertices in the graph, as most were already discovered. At this point the classical </span><span class="c12">top-down </span><span class="c7">approach &mdash; trying to find unseen vertices by processing the </span><span class="c12">frontier </span><span class="c7">&mdash; becomes inefficient. Most vertices&rsquo; neighbors will already have been seen but would still need to be checked. The ratio of vertices discovered per traversed edge becomes very low. For these cases Beamer et al. propose to use a </span><span class="c12">bottom-up </span><span class="c7">approach and iterate over the vertices that were not yet seen in order to try to find an already seen vertex in their neighbor lists. Even though the result of the BFS is not changed, this approach significantly reduces the number of neighbors that have to be checked. This translates into better traversal performance, thus, this approach is often used in more specialized BFS algorithms [2, 18, 20]. </span></p><p class="c131 c138"><span class="c7">Those algorithmic changes also have implications on the BFS data structures that can be used. Typically, the queues in a BFS are implemented using either a dense bitset or a sparse vector. The bottom-up phase, though, requires efficient lookups of vertices in the queue, thus, it can not be used </span></p><p class="c11"><span class="c6">2 </span></p><p class="c19"><span class="c7">efficiently with a sparse vector. The original authors solve this by converting the data structures from bitset to sparse vector when switching from top-down to bottom-up or vice versa. </span></p><p class="c3"><span class="c7">For </span><span class="c12">parallelization</span><span class="c7">, this approach can be combined with existing work on scalable queues for BFSs [2, 12, 8, 5, 20] and on scalability on multi-socket NUMA architectures [19, 8] through static partitioning of vertices and data across NUMA nodes. Many of these techniques are combined in the BFS algorithm proposed by Yasui et al. [20, 19] which is the fastest multi-threaded single-source BFS. </span><span class="c34">2.2 Sequential Multi-Source BFS </span></p><p class="c19"><span class="c7">The MS-BFS algorithm [18] is targeted towards multi- source traversal and further reduces the total number of neighbor lookups across all sources compared to Beamer et al. It is based on two important observations about BFS algorithms. Firstly, regardless the data structure used, for sufficiently large graphs it is expensive to check whether a vertex is already contained in </span><span class="c12">seen </span><span class="c7">as CPU cache hit rates decrease. For this very frequent operation even arrays with their containment check bound of </span><span class="c14">O</span><span class="c7">(1) are bound by memory latency. This problem is further exacerbated on non-uniform memory access (NUMA) architectures that are common in modern server-class machines. Secondly, when multiple BFSs are run in the same connected component, every vertex of this component is visited </span><span class="c12">separately </span><span class="c7">in each BFS. This leads to redundant computations, because whenever two or more BFS traversals in the same component find a vertex </span><span class="c14">v </span><span class="c7">in the same distance </span><span class="c14">d </span><span class="c7">from their respective source vertices, the remainder of those BFS traversals from </span><span class="c14">v </span><span class="c7">will likely be very similar, i.e., visit most remaining vertices in the same distance. </span></p><p class="c19"><span class="c7">MS-BFS alleviates some of these issues by optimizing for the case of executing multiple independent BFSs from different sources in the same graph. It uses three k-wide bitsets to encode the state of each vertex during </span><span class="c14">k </span><span class="c7">concurrent BFSs: </span></p><p class="c3"><span class="c7">1. </span><span class="c12">seen</span><span class="c7">[</span><span class="c14">v</span><span class="c7">], where the bit at position </span><span class="c14">i </span><span class="c7">indicates whether </span><span class="c14">v </span></p><p class="c3"><span class="c7">was already seen during the BFS </span><span class="c14">i</span><span class="c7">, 2. </span><span class="c12">frontier</span><span class="c7">[</span><span class="c14">v</span><span class="c7">], determining if </span><span class="c14">v </span><span class="c7">must be visited in the </span></p><p class="c3"><span class="c7">current iteration for the BFSs, and 3. </span><span class="c12">next</span><span class="c7">[</span><span class="c14">v</span><span class="c7">], with each set bit marking that the vertex </span><span class="c14">v </span><span class="c7">needs to be visited in the following iteration for the respective BFS. </span></p><p class="c19"><span class="c7">For example given </span><span class="c14">k </span><span class="c7">= 4 concurrent BFSs, the bitset </span><span class="c12">seen</span><span class="c7">[</span><span class="c14">v</span><span class="c7">] = (1</span><span class="c14">,</span><span class="c7">0</span><span class="c14">,</span><span class="c7">0</span><span class="c14">,</span><span class="c7">1) indicates that vertex </span><span class="c14">v </span><span class="c7">is already discov- ered in BFSs 0 and 3 but not in BFSs 1 and 2. Using this information, a BFS step to determine </span><span class="c12">seen </span><span class="c7">and </span><span class="c12">next </span><span class="c7">for all neighbors </span><span class="c14">n </span><span class="c7">of </span><span class="c14">v </span><span class="c7">can be executed using the bitwise operations </span><span class="c12">and </span><span class="c7">( &amp; ), </span><span class="c12">or </span><span class="c7">( | ), and </span><span class="c12">negation </span><span class="c7">(&sim;): </span></p><p class="c3"><span class="c8">for each </span><span class="c2">n </span><span class="c4">&isin; </span><span class="c2">neighbors</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] </span></p><p class="c3"><span class="c2">next</span><span class="c4">[</span><span class="c2">n</span><span class="c4">] &larr; </span><span class="c2">next</span><span class="c4">[</span><span class="c2">n</span><span class="c4">] | (</span><span class="c2">frontier</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] &amp; &sim;</span><span class="c2">seen</span><span class="c4">[</span><span class="c2">n</span><span class="c4">]) </span><span class="c2">seen</span><span class="c4">[</span><span class="c2">n</span><span class="c4">] &larr; </span><span class="c2">seen</span><span class="c4">[</span><span class="c2">n</span><span class="c4">] | </span><span class="c2">frontier</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] </span></p><p class="c19"><span class="c7">Here, if </span><span class="c14">n </span><span class="c7">is not yet marked seen for a BFS and this BFS&rsquo;s respective bit is set in </span><span class="c12">frontier</span><span class="c7">[</span><span class="c14">v</span><span class="c7">], then the vertex </span><span class="c14">n </span><span class="c7">is marked as seen and must be visited in the next iteration. The bitwise operations calculate </span><span class="c12">seen </span><span class="c7">and </span><span class="c12">next </span><span class="c7">for </span><span class="c14">k </span><span class="c7">BFSs at the same time and can be computed efficiently by leveraging the wide registers of modern CPUs. A full MS-BFS iteration consists of executing these operations for all vertices </span><span class="c14">v </span><span class="c7">in the </span></p><p class="c3"><span class="c84">C</span><span class="c74">) %(n oitazilituU P</span><span class="c40">100 </span><span class="c55">75 5025</span><span class="c40">0</span><span class="c32">Algorithm </span></p><p class="c3"><span class="c37">MS&minus;BFS </span></p><p class="c3"><span class="c37">MS&minus;PBFS </span></p><p class="c3"><span class="c40">0 1000 2000 3000 4000 </span></p><p class="c3"><span class="c84">Number of BFS sources </span></p><p class="c3"><span class="c5">Figure 2: CPU utilization of MS-BFS and MS-PBFS as the number of sources increases. </span></p><p class="c29"><span class="c74">d aehrevoe vitale</span><span class="c84">R</span><span class="c55">9 63</span><span class="c40">00 </span><span class="c32">Algorithm </span></p><p class="c3"><span class="c37">MS&minus;BFS </span></p><p class="c3"><span class="c37">MS&minus;PBFS </span></p><p class="c3"><span class="c40">20 40 60 </span><span class="c84">Number of threads </span></p><p class="c3"><span class="c5">Figure 3: Relative memory overhead compared to graph size as number of threads increases. </span></p><p class="c19"><span class="c7">graph. Note that all </span><span class="c14">k </span><span class="c7">BFSs are run concurrently on a single CPU core with their traversals implicitly merged whenever possible. </span></p><p class="c19"><span class="c7">MS-BFS works for any number of concurrent BFSs using bitset sizes chosen accordingly. However, it is especially ef- ficient when the vertex bitsets have a width for which the target machine natively supports bit operations. Modern 64-bit x86 CPUs do not only have registers and instructions that support 64 bit wide values, but also ones for 128 bit and 256 bit using the SSE and AVX-2 extensions, respectively. The original publication elaborates on the trade-offs of var- ious bitset widths and how they influence the algorithm&rsquo;s performance. </span></p><p class="c3"><span class="c7">In Section 3 we show how MS-BFS can be efficiently par- allelized and present an optimized variant that is highly efficient for single source traversals. </span><span class="c34">2.3 Limitations of Existing Algorithms </span></p><p class="c19"><span class="c7">The MS-BFS algorithm is limited to sequential execution. The only way to saturate a multi-core system is to run a separate MS-BFS instance on each core. However, if the number of BFS sources is limited, e.g., to only 64 as in the Graph500 benchmark, MS-BFS can only run single-threaded or, at best, on few cores. In such cases, the capabilities of a multi-core system cannot be fully utilized. Figure 2 analyzes this problem using a 60-core machine and 64 concurrent BFSs per MS-BFS. Every 64 sources one more thread can be used. Hence, only with 3840 or more sources all cores are utilized. Also, by running multiple sequential instances simultaneously, the memory requirements rise drastically to the point that the dynamic state of the BFSs require much more memory than the graph itself. This is demonstrated in Figure 3. It compares the memory required for the MS-BFS and our proposed MS-PBFS data structures to the size of the analyzed graph. We calculated the memory requirement based on 16 edges per vertex like the Kronecker graphs in the Graph500 benchmark. While traditional BFSs only require a fraction of the graph memory for their working set, MS-BFS </span></p><p class="c3"><span class="c6">3 </span></p><p class="c129"><span class="c5">Listing 1: Top-down MS-BFS algorithm from [18]. </span><span class="c4">1 </span><span class="c8">for each </span><span class="c2">v </span><span class="c4">&isin; </span><span class="c2">V </span><span class="c4">2 </span><span class="c8">if </span><span class="c2">frontier</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] = 0, </span><span class="c8">skip </span><span class="c4">3 </span><span class="c8">for each </span><span class="c2">n </span><span class="c4">&isin; </span><span class="c2">neighbors</span><span class="c1">v </span><span class="c0">4 </span><span class="c18">next</span><span class="c0">[</span><span class="c18">n</span><span class="c0">] &larr; </span><span class="c18">next</span><span class="c0">[</span><span class="c18">n</span><span class="c0">] | </span><span class="c18">frontier</span><span class="c0">[</span><span class="c18">v</span><span class="c0">] </span><span class="c4">5</span><span class="c0">6 </span><span class="c15">for each </span><span class="c18">v </span><span class="c0">&isin; </span><span class="c18">V </span><span class="c4">7 </span><span class="c8">if </span><span class="c2">next</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] = 0, </span><span class="c8">skip </span><span class="c4">8 </span><span class="c2">next</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] &larr; </span><span class="c2">next</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] &amp; &sim;</span><span class="c2">seen</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] 9 </span><span class="c2">seen</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] &larr; </span><span class="c2">seen</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] | </span><span class="c2">next</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] 10 </span><span class="c8">if </span><span class="c2">next</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] = B</span><span class="c67">&empty; </span><span class="c0">11 </span><span class="c18">v </span><span class="c0">is found by BFSs in </span><span class="c18">next</span><span class="c0">[</span><span class="c18">v</span><span class="c0">] </span></p><p class="c86"><span class="c7">already requires more memory than the graph using only 6 threads. With 60 threads it requires over 10 times more memory! Hence, more than one terabyte of main memory would be needed to analyze a 100GB graph using all cores. An alternative could be to use smaller batch sizes, thus, requiring fewer sources and memory to take advantage of all cores. However, that would decrease the traversal performance as less work can be shared between the BFSs. In contrast, the parallel multi-source algorithm MS-PBFS proposed in this paper can use all cores at 64 BFSs and only consumes as much memory as a single MS-BFS. </span></p><p class="c145"><span class="c7">State-of-the-art parallel single-source algorithms are lim- ited by either locality and scalability issues associated with the sparse queues. Even if partitioned at NUMA socket gran- ularity there can be a lot of contention and the trend of having more cores per CPU socket does not work in such approaches favor. </span></p><p class="c150"><span class="c34">3. PARALLEL SINGLE- AND </span></p><p class="c124"><span class="c34">MULTI-SOURCE BFS </span><span class="c7">In this section we present our </span><span class="c12">parallelized </span><span class="c7">multi-source BFS algorithm as well as a single-source BFS variant, both designed to avoid those problems. </span><span class="c34">3.1 MS-PBFS </span></p><p class="c46"><span class="c7">In the following we introduce MS-PBFS, a parallel multi- source BFS algorithm that ensures full machine utilization even for a single multi-source BFS. </span></p><p class="c85"><span class="c7">MS-PBFS is based on MS-BFS and parallelizes both its top-down (Section 3.1.1) and its bottom-up (Section 3.1.2) variant. Our basic strategy is to parallelize all loops over the vertices by partitioning them into disjunct subsets and pro- cessing those in parallel. State that is modified and accessed in parallel then has to be synchronized to ensure correctness. </span></p><p class="c133"><span class="c17">3.1.1 Top-down MS-PBFS </span></p><p class="c115"><span class="c7">MS-BFS uses a two-phase top-down variant, shown in Listing 1. As described in the Section 2, each value in </span><span class="c12">seen</span><span class="c7">, </span><span class="c12">frontier </span><span class="c7">and </span><span class="c12">next </span><span class="c7">is not a single boolean value but a bitset. The first phase, lines 1 through 4, aggregates information about which vertices are reachable in the current iteration. Af- ter it finishes, the second phase, the loop in lines 6 through 11, identifies which of the reachable vertices are newly discovered and processes them. </span></p><p class="c121"><span class="c7">Our strategy is to parallelize both of these loops and separate them using a barrier. During this parallel processing, the first loop accesses the fixed-size </span><span class="c12">frontier</span><span class="c7">, </span><span class="c12">neighbors </span><span class="c7">and </span><span class="c12">next </span><span class="c7">data structures. As the former two are constant during </span></p><p class="c3 c62"><span class="c7">First phase Second phase </span><span class="c12">frontier next next seen </span></p><p class="c127"><span class="c2">writes to neighbors </span></p><p class="c3 c36"><span class="c2">writes </span></p><p class="c3 c36"><span class="c2">writes </span></p><p class="c65"><span class="c5">Figure 4: Concurrent top-down memory accesses </span></p><p class="c148"><span class="c7">the loop accesses, they do not require any synchronization. The </span><span class="c12">next </span><span class="c7">data structure on the other hand is updated for each neighbor </span><span class="c14">n </span><span class="c7">by combining </span><span class="c14">n</span><span class="c7">&rsquo;s </span><span class="c12">next </span><span class="c7">bitset with the currently visited </span><span class="c14">v</span><span class="c7">&rsquo;s </span><span class="c12">frontier</span><span class="c7">. As vertices in general can be reached via multiple edges from different vertices, different threads might update </span><span class="c12">next </span><span class="c7">simultaneously for a vertex. To avoid losing information in this situation, we use an atomic compare and swap (CAS) instruction, replacing line 4 with the following: </span></p><p class="c104"><span class="c8">do</span><span class="c18">oldNext </span><span class="c0">&larr; </span><span class="c18">next</span><span class="c0">[</span><span class="c18">n</span><span class="c0">] </span></p><p class="c92"><span class="c2">newNext </span><span class="c4">&larr; </span><span class="c2">oldNext </span><span class="c4">| </span><span class="c2">frontier</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] </span><span class="c8">while </span><span class="c2">atomic</span><span class="c4">_</span><span class="c2">cas</span><span class="c4">(</span><span class="c2">next</span><span class="c4">[</span><span class="c2">n</span><span class="c4">]</span><span class="c2">,oldNext, newNext</span><span class="c4">) </span></p><p class="c88"><span class="c7">Bitsets wider than the CPU&rsquo;s largest atomic operation value type can be supported by implementing the update operation as a series of independent atomic CAS updates of each sub-part of the bitset. For example a 512-bit bitset could be updated using eight 64-bit CAS as described above. This retains the desired semantics as the operation can only add bits but never unset them. It is also not required to track which thread first added which bit as the updates of the newly discovered vertices is only done in the second phase. </span></p><p class="c155"><span class="c7">The second phase iterates over all vertices in the graph and updates </span><span class="c12">next </span><span class="c7">and </span><span class="c12">seen</span><span class="c7">. In contrast to the first phase, no two worker threads can access the same entry in the data structures. Regardless of how the vertex ranges are partitioned, there is a bijective mapping between a vertex, the accessed data entries, and the worker that processes it. Consequently, there cannot be any conflicts, thus, no synchronization is necessary. </span></p><p class="c70"><span class="c7">Figure 4 visualizes the memory access patterns for all writes in the first and second phase of the top-down MS- PBFS algorithm. The example shows a configuration with two parallel workers and a task size of two. Squares show the currently active vertices and arrows point to entries that are modified. The linestyle of the square shows the association to the different workers. We come back to this figure in Section 4.4 to further explain the linestyles. </span></p><p class="c161"><span class="c7">To reduce the time spent between iterations, we directly clear each </span><span class="c12">frontier </span><span class="c7">entry inside the second parallelized loop. This allows MS-PBFS to re-use the memory of the current </span><span class="c12">frontier </span><span class="c7">for </span><span class="c12">next </span><span class="c7">in the subsequent iteration without hav- ing to clear the memory separately. Thus, we reduce the algorithm&rsquo;s memory bandwidth consumption. </span></p><p class="c139"><span class="c7">Furthermore, we only update </span><span class="c12">next </span><span class="c7">entries if the computa- tion results in changes to the bitset. This avoids unnecessary writes and cache line invalidations on other CPUs [2]. </span></p><p class="c11"><span class="c6">4 </span></p><p class="c98"><span class="c5">Listing 2: Bottom-up MS-BFS traversal from [18]. </span><span class="c4">1 </span><span class="c8">for each </span><span class="c2">u </span><span class="c4">&isin; </span><span class="c2">V </span><span class="c4">2 </span><span class="c8">if </span><span class="c4">|</span><span class="c2">seen</span><span class="c4">[</span><span class="c2">u</span><span class="c4">]| = |</span><span class="c2">S</span><span class="c4">|, </span><span class="c8">skip </span><span class="c4">3 </span><span class="c0">4 </span><span class="c8">for each </span><span class="c18">next</span><span class="c0">[</span><span class="c18">u</span><span class="c0">] </span><span class="c2">v </span><span class="c4">&isin; </span><span class="c2">neighbors</span><span class="c0">&larr; </span><span class="c18">next</span><span class="c0">[</span><span class="c18">u</span><span class="c0">] | </span><span class="c1">u </span><span class="c18">frontier</span><span class="c0">[</span><span class="c18">v</span><span class="c0">] </span><span class="c4">5 </span><span class="c2">next</span><span class="c4">[</span><span class="c2">u</span><span class="c4">] &larr; </span><span class="c2">next</span><span class="c4">[</span><span class="c2">u</span><span class="c4">] &amp; &sim;</span><span class="c2">seen</span><span class="c4">[</span><span class="c2">u</span><span class="c4">] 6 </span><span class="c2">seen</span><span class="c4">[</span><span class="c2">u</span><span class="c4">] &larr; </span><span class="c2">seen</span><span class="c4">[</span><span class="c2">u</span><span class="c4">] | </span><span class="c2">next</span><span class="c4">[</span><span class="c2">u</span><span class="c4">] 7 </span><span class="c8">if </span><span class="c4">|</span><span class="c2">next</span><span class="c4">[</span><span class="c2">u</span><span class="c4">]| = 0 8 </span><span class="c2">u </span><span class="c4">is found by BFSs in </span><span class="c2">next</span><span class="c4">[</span><span class="c2">u</span><span class="c4">] </span></p><p class="c154"><span class="c12">frontier seen next </span></p><p class="c3 c76"><span class="c5">Listing 3: Single-source parallel top-down algorithm </span></p><p class="c50"><span class="c4">1 </span><span class="c8">parallel for each </span><span class="c2">v </span><span class="c4">&isin; </span><span class="c2">V </span><span class="c4">2 </span><span class="c8">if not</span><span class="c4">(</span><span class="c2">frontier</span><span class="c4">[</span><span class="c2">v</span><span class="c4">]), </span><span class="c8">skip </span><span class="c4">3 </span><span class="c8">for each </span><span class="c2">n </span><span class="c4">&isin; </span><span class="c2">neighbors</span><span class="c1">v </span><span class="c0">4 </span><span class="c15">if not</span><span class="c0">(</span><span class="c18">next</span><span class="c0">[</span><span class="c18">n</span><span class="c0">]), </span><span class="c15">atomic</span><span class="c0">(</span><span class="c18">next</span><span class="c0">[</span><span class="c18">n</span><span class="c0">] &larr; </span><span class="c18">true</span><span class="c0">) </span><span class="c4">5 </span><span class="c2">frontier</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] &larr; </span><span class="c2">false </span><span class="c4">6</span><span class="c0">7 </span><span class="c15">parallel for each </span><span class="c18">v </span><span class="c0">&isin; </span><span class="c18">V </span><span class="c4">8 </span><span class="c8">if not</span><span class="c4">(</span><span class="c2">next</span><span class="c4">[</span><span class="c2">v</span><span class="c4">]), </span><span class="c8">skip </span><span class="c4">9 </span><span class="c2">next</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] &larr; </span><span class="c8">not</span><span class="c4">(</span><span class="c2">seen</span><span class="c4">[</span><span class="c2">v</span><span class="c4">]) 10 </span><span class="c8">if not</span><span class="c4">(</span><span class="c2">seen</span><span class="c4">[</span><span class="c2">v</span><span class="c4">]) 11 </span><span class="c2">seen</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] &larr; </span><span class="c2">true </span><span class="c4">12 </span><span class="c2">v </span><span class="c4">is found </span></p><p class="c140"><span class="c2">reads from neighbors </span></p><p class="c3 c141"><span class="c2">writes </span></p><p class="c146"><span class="c5">Listing 4: Single-source parallel bottom-up algo- rithm </span></p><p class="c153"><span class="c5">Figure 5: Concurrent bottom-up memory accesses </span></p><p class="c107"><span class="c17">3.1.2 Bottom-up MS-PBFS </span></p><p class="c53"><span class="c7">As explained in Section 2.1, a BFS&rsquo;s bottom-up variant linearly traverses the </span><span class="c12">seen </span><span class="c7">data structure to find vertices that have not been marked yet. For every vertex </span><span class="c14">v </span><span class="c7">that is not yet seen in all concurrent BFSs, MS-BFS&rsquo;s bottom-up variant checks whether any of its neighbors was already seen in the respective BFS. If so, </span><span class="c14">v </span><span class="c7">is marked as seen and is visited in the next iteration. We show the full bottom-up loop in Listing 2. MS-PBFS parallelizes this loop by splitting the graph into distinct vertex ranges which are then processed by worker threads. Inside the loop, the current iteration&rsquo;s </span><span class="c12">frontier </span><span class="c7">is only read. Both </span><span class="c12">seen </span><span class="c7">and </span><span class="c12">next </span><span class="c7">are read as well as updated. Similar to the second phase described in the previous sec- tion, there is a bijective mapping between each updated entry and the worker that processes the respective vertex. Consequently, there cannot be any read-write or write-write conflicts and, thus, no synchronization is required within the ranges. Figure 5 depicts the bottom-up variant&rsquo;s memory access pattern, again for two parallel workers. </span></p><p class="c101"><span class="c7">Once all active BFSs bits are set in </span><span class="c12">next </span><span class="c7">we stop checking further neighbors to avoid unnecessary read operations. This check is also used in the original bottom-up algorithm by Beamer et al. </span></p><p class="c162"><span class="c34">3.2 Parallel Single-Source: SMS-PBFS </span></p><p class="c47"><span class="c7">In order to also apply our novel algorithm to BFS that traverse the graph from only a single source, we derive a single-source variant: SMS-PBFS. SMS-PBFS contains two main changes: the values in each array are represented by boolean values instead of bitsets, and checks that are only required when multiple BFS are bundled can be replaced by constants. This allows us to simplify the atomic update in the top-down algorithm, as a single atomic write is sufficient, instead of a compare and swap loop. The SMS-PBFS top- down and bottom-up algorithms are shown in Listing 3 and 4, respectively. Parallel coordination is only required when scheduling the vertex-parallel loops and during the single </span></p><p class="c3 c83"><span class="c4">1 </span><span class="c8">parallel for each </span><span class="c2">u </span><span class="c4">&isin; </span><span class="c2">V </span><span class="c4">2 </span><span class="c8">if </span><span class="c2">seen</span><span class="c4">[</span><span class="c2">u</span><span class="c4">] 3 </span><span class="c2">next</span><span class="c4">[</span><span class="c2">u</span><span class="c4">] &larr; </span><span class="c2">false </span><span class="c4">4 </span><span class="c8">else </span><span class="c4">5 </span><span class="c8">for each </span><span class="c2">v </span><span class="c4">&isin; </span><span class="c2">neighbors</span><span class="c1">u </span><span class="c0">6 </span><span class="c15">if </span><span class="c18">frontier</span><span class="c0">[</span><span class="c18">v</span><span class="c0">] </span><span class="c4">7 </span><span class="c2">next</span><span class="c4">[</span><span class="c2">u</span><span class="c4">] &larr; </span><span class="c2">true </span><span class="c4">8 </span><span class="c8">break </span><span class="c4">9 </span><span class="c8">if </span><span class="c2">next</span><span class="c4">[</span><span class="c2">v</span><span class="c4">] 10 </span><span class="c2">seen</span><span class="c4">[</span><span class="c2">u</span><span class="c4">] &larr; </span><span class="c2">true </span><span class="c4">11 </span><span class="c2">u </span><span class="c4">is found </span></p><p class="c105"><span class="c7">atomic update in the first top-down loop. </span></p><p class="c89"><span class="c7">While MS-PBFS always has to use an array of bitsets to implement </span><span class="c12">next</span><span class="c7">, </span><span class="c12">frontier </span><span class="c7">and </span><span class="c12">seen</span><span class="c7">, there is more freedom when implementing SMS-PBFS. It is still restricted to using dense arrays, but each entry can either be a bit, a byte or a wider data type. In the parallel case, where the state of 512 vertices fits into one 64-byte CPU cache line using bit representation, the chance of concurrent modification is very high. Choosing a larger data type allows to balance cache efficiency and reduced contention between workers. We demonstrate these effects in our evaluation. To reduce the number of branches, we try to detect when a consecutive range of vertices is not active in the current iteration and skip it. Instead of checking each vertex individually we check ranges of size 8 bytes, which can efficiently be implemented on 64-bit CPUs. Using a bit representation, each such range contains the status of 64 vertices. If no bit is set, we directly jump to the next chunk and save a large number of individual bit checks. Otherwise, each vertex is processed individually. This is similar to the </span><span class="c12">Bitsets-and-summary </span><span class="c7">optimization [19] but does not require an explicit summary bit. </span></p><p class="c82"><span class="c34">4. SCHEDULING AND PARALLEL GRAPH </span></p><p class="c81"><span class="c34">DATA STRUCTURES </span><span class="c7">The parallelized algorithms&rsquo; descriptions in Section 3 focus on how to provide semantical correctness. It leaves out the implementation details of how to actually partition the work to workers and how to store the BFS data structures as well as the graph. As shown by existing work on parallel single- source BFSs, these implementation choices can have a huge influence on the performance of algorithms that are intended to run on multi-socket machines with a large number of cores. In this section we describe the data structures and memory </span></p><p class="c11"><span class="c6">5 </span></p><p class="c3"><span class="c23">Ordered Random </span></p><p class="c20"><span class="c44">s robhgiend etisi</span><span class="c58">V</span><span class="c9">1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 </span><span class="c58">Workers </span></p><p class="c3"><span class="c28">Workers </span></p><p class="c3"><span class="c54">1</span><span class="c69">2345678 </span></p><p class="c19"><span class="c5">Figure 6: Visited neighbors per worker during a BFS using static partitioning on a social network graph with different vertex labelings </span></p><p class="c3"><span class="c44">s etatsS FBd etadpU</span><span class="c58"># </span><span class="c23">1 2 3 4 5 </span><span class="c28">Workers </span></p><p class="c3"><span class="c23">6 </span></p><p class="c3"><span class="c54">1</span><span class="c69">2345678 </span></p><p class="c20"><span class="c9">12345678 12345678 12345678 12345678 12345678 12345678 </span><span class="c58">Workers </span></p><p class="c19"><span class="c5">Figure 7: Updated BFS vertex states per worker per iteration during a BFS using static partitioning on a social network graph with ordered vertex labeling </span></p><p class="c3"><span class="c7">organization to efficiently scale MS-PBFS and SMS-PBFS (together abbreviated as (S)MS-BFS) on such machines. </span></p><p class="c3"><span class="c34">4.1 Parallelization Strategies </span></p><p class="c19"><span class="c7">The initial version of our parallelized algorithms used pop- ular techniques from state-of-the-art parallel single-source implementations. Specifically, it used static partitioning of vertices to workers, and degree-ordered vertex labeling[19]. With this labeling scheme, we re-labeled the graph&rsquo;s ver- tices and assigned dense ids in the order of the vertices&rsquo; degrees, with the highest-degree vertex getting the smallest id. That way, the states of high degree vertices are located close together which improves cache hit rates. This first implementation showed very good performance for a small number of workers. However, at higher thread counts, the overall scalability was severely limited. </span></p><p class="c29"><span class="c7">Together with static partitioning, degree based labeling has the effect that due to the power-law distribution of vertex degrees in many real world graphs, the vertices in the first partitions have orders of magnitude more neighbors than those in later partitions. We visualize this effect in Figure 6. In that the vertices and so experiment, in the on. As the graph, amount the first the of worker second work processes per worker partition the the second first increases </span><span class="c13">1</span><span class="c61">8</span><span class="c73">th </span><span class="c13">1</span><span class="c61">8</span><span class="c73">th, of </span></p><p class="c19"><span class="c7">with the number of neighbors that need to be visited, the described skew directly affects the workers&rsquo; runtime, as it is one of the most costly operations besides updating the BFS vertex state. While it may be possible to create balanced static partitions such that each worker has to do the same amount of work across all BFS iterations, it is not enough to significantly increase utilization. The problem would then be that in different iterations different parts of the graph are active, thus, there would still be a large runtime skew </span></p><p class="c19"><span class="c7">in each iteration. The different workload for workers across iterations is shown in Figure 7 using the number of updated BFS vertex states as an indicator for the actual amount of work. </span></p><p class="c19"><span class="c7">Additionally, this figure gives an indication why dynamic work assignment does not ensure full utilization on its own. Intuitively, in a small-world network an average BFS traverses the graph starting from the source vertex to the vertices with the highest degrees, because these are well-connected, and from there to the remainder of the graph. For such a BFS, the high-degree vertices are typically discovered after two to three iterations as shown in Figure 7. There in iteration two only a tiny fraction of vertices is updated. On the other hand as these are the high-degree vertices a lot of undiscovered vertices are reachable from them, resulting in a huge number of updates in iteration three. The updates themselves, which are processed in the second phase of the top-down algorithm could be well distributed across workers. Identifying the newly reachable vertices, which is done in the first phase, by searching the neighbors of the high-degree vertices is more challenging to schedule because there are only few and due to the labeling they are all clustered together. In combination, this iteration is very expensive but a large part of the work is spent when processing very few high-degree vertices. To achieve even utilization, tiny task sizes would be required. Such tiny tasks mean, however, that the scheduling overhead would become so significant that the overall performance would not improve. </span></p><p class="c19"><span class="c7">Instead, our design relies on two strategies. We use fine- granular tasks together with work-stealing to significantly reduce the number of vertices that are assigned at once and enable load-balancing between the cores. We also use a novel vertex labeling scheme that is scheduling-aware and distributes high-degree vertices in such a way that they are both clustered but also spread across multiple tasks. This allows us to avoid the use of tiny task ranges. </span></p><p class="c3"><span class="c34">4.2 Parallel Vertex Processing </span></p><p class="c19"><span class="c7">In this section we focus on providing a parallelization scheme that minimizes synchronization between threads and balances work between nodes to achieve full utilization of all cores during the whole algorithm. </span></p><p class="c3"><span class="c7">Our concept allows load balancing through work stealing with negligible synchronization overhead. </span></p><p class="c3"><span class="c17">4.2.1 Task creation and assignment </span></p><p class="c19"><span class="c7">Efficient load balancing requires tasks to have two related properties: there need to be many tasks and their runtime needs to be relatively short compared to the overall runtime. If there were only two tasks on average per thread, a scenario is very probable where a slow thread only starts its last task when all other threads are already close to being finished with their work. This creates potential to have all other threads idling until this last thread is finished. Given a fixed overall runtime, the shorter the runtime of each work unit and the more tasks are available, the easier it becomes to get all threads to finish at approximately the same time. On the other hand, if the ranges are very small, the threads have to request tasks more often from the task pool. This can lead to contention and, thus, decrease efficiency because more processing time is spent in scheduling instead of doing actual work. </span></p><p class="c3"><span class="c7">Due to the fixed-size </span><span class="c12">frontier </span><span class="c7">and </span><span class="c12">next </span><span class="c7">arrays which span </span></p><p class="c3"><span class="c6">6 </span></p><p class="c117"><span class="c5">Listing 5: Task creation algorithm: create_tasks </span><span class="c4">1 </span><span class="c8">Input: </span><span class="c2">queueSize, splitSize,numThreads </span><span class="c4">2 </span><span class="c2">workerTasks </span><span class="c4">&larr; 0 3 </span><span class="c2">curWorker </span><span class="c4">&larr; 0 4 </span><span class="c8">for</span><span class="c4">(</span><span class="c2">offset </span><span class="c4">= 0; </span><span class="c2">offset &lt; queueSize</span><span class="c4">; </span><span class="c2">offset</span><span class="c4">+ = </span><span class="c2">splitSize</span><span class="c4">) 5 </span><span class="c2">wId </span><span class="c4">&larr; </span><span class="c2">curWorker </span><span class="c8">mod </span><span class="c2">numThreads </span><span class="c4">6 </span><span class="c2">range </span><span class="c4">&larr; {</span><span class="c2">offset, </span><span class="c8">min</span><span class="c4">(</span><span class="c2">offset </span><span class="c4">+ </span><span class="c2">splitSize,queueSize</span><span class="c4">)} 7 </span><span class="c2">workerTasks</span><span class="c4">[</span><span class="c2">wId</span><span class="c4">] &larr; </span><span class="c2">workerTasks</span><span class="c4">[</span><span class="c2">wId</span><span class="c4">] &cup; </span><span class="c2">range </span><span class="c4">8 </span><span class="c2">curWorker </span><span class="c4">&larr; </span><span class="c2">curWorker </span><span class="c4">+ 1 9 </span><span class="c2">taskQueues </span><span class="c4">&larr; 0 10 </span><span class="c8">for </span><span class="c2">i </span><span class="c4">= 1</span><span class="c2">, . . . , num</span><span class="c4">_</span><span class="c2">threads </span><span class="c4">11 </span><span class="c2">taskQueues</span><span class="c4">[</span><span class="c2">i</span><span class="c4">] &larr; {|</span><span class="c2">workerTasks</span><span class="c4">[</span><span class="c2">i</span><span class="c4">]|</span><span class="c2">, workerTasks</span><span class="c4">[</span><span class="c2">i</span><span class="c4">]} 12 </span><span class="c8">return </span><span class="c2">taskQueues </span></p><p class="c27"><span class="c7">all the vertices no matter how many of them are actually enqueued, all parallel loops of (S)MS-PBFS can follow the same pattern: a given operation has to be executed for all vertices in the graph. To create the tasks we divide the list of vertices into small ranges. In our experiments we found that task range sizes of 256 or more vertices do not have any significant scheduling overhead (below 1% of total runtime) for a graph with more than one million vertices. With about 3900 tasks in such a graph there are enough tasks to load balance even machines with hundreds of cores. </span></p><p class="c142"><span class="c7">For work assignment we do not use one central task queue, but similar to static partitioning give each worker its own queue. When a parallelized loop over the vertices of the graph is executed, the queues are initialized using the </span><span class="c12">create</span><span class="c7">_</span><span class="c12">tasks </span><span class="c7">function shown in Listing 5. Each task queue </span><span class="c12">taskQueues</span><span class="c7">[</span><span class="c14">i</span><span class="c7">] = {</span><span class="c12">curTaskIx</span><span class="c14">,</span><span class="c12">queuedTasks</span><span class="c7">} belonging to worker </span><span class="c14">i </span><span class="c7">consists of an index </span><span class="c12">curTaskIx </span><span class="c7">pointing to the next task and a list of tasks </span><span class="c12">queuedTasks</span><span class="c7">. The number of vertices per task is controlled by the parameter </span><span class="c12">splitSize</span><span class="c7">. We use a round-robin distribution scheme, so the difference in queue sizes can be at most one task. </span></p><p class="c103"><span class="c17">4.2.2 Work stealing scheduling </span></p><p class="c46"><span class="c7">The coordination of workers during task execution is han- dled by the lock-free function </span><span class="c12">fetch</span><span class="c7">_</span><span class="c12">task</span><span class="c7">, which is shown in Listing 6. The function can be kept simple due to the fact that during a phase of parallel processing no new tasks need to be added. Only after all tasks have been completed the next round of tasks is processed&mdash;e.g., a new iteration or the second phase of the top-down algorithm. </span></p><p class="c72"><span class="c7">Initially, each worker fetches tasks from its own, local queue which is identified using the </span><span class="c12">workerId </span><span class="c7">parameter. It atomi- cally fetches and increments the current value of </span><span class="c12">curTaskIx </span><span class="c7">as shown in line 5. Using modern CPU instructions, this can be done without explicit locking. If the task id is within the bounds of the current task queue (line 7), the corresponding task range is processed by the worker. Otherwise, it switches to the next worker&rsquo;s task queue by incrementing the task queue offset, and tries again to fetch a task. This is repeated until either a task is found or, alternatively, after all queues have been checked, an empty task range is returned to the worker to signal that no task is available anymore. Further optimizations like remembering the task queue index where the current task was found and resuming from that offset when the next task is fetched, guarantee that every worker skips each queue exactly once. Incrementing the </span><span class="c12">curTaskIx </span><span class="c7">only if the queue is not empty avoids atomic writes which could lead to cache misses when other workers are visiting </span></p><p class="c3 c66"><span class="c5">Listing 6: Task retrieval algorithm: fetch_task </span><span class="c4">1 </span><span class="c8">Input: </span><span class="c2">taskQueues, workerId </span><span class="c4">2 </span><span class="c2">offset </span><span class="c4">&larr; 0 3 </span><span class="c8">do </span><span class="c4">4 </span><span class="c2">i </span><span class="c4">&larr; (</span><span class="c2">threadId </span><span class="c4">+ </span><span class="c2">offset </span><span class="c4">)</span><span class="c8">mod </span><span class="c4">|</span><span class="c2">taskQueues</span><span class="c4">| 5 </span><span class="c2">taskId </span><span class="c4">&larr; </span><span class="c2">fetch</span><span class="c4">_</span><span class="c2">add</span><span class="c4">_</span><span class="c2">task</span><span class="c4">_</span><span class="c2">ix</span><span class="c4">(</span><span class="c2">taskQueues</span><span class="c4">[</span><span class="c2">i</span><span class="c4">]</span><span class="c2">,</span><span class="c4">1) 6 </span><span class="c8">if </span><span class="c2">taskId &lt; num</span><span class="c4">_</span><span class="c2">tasks</span><span class="c4">(</span><span class="c2">taskQueues</span><span class="c4">[</span><span class="c2">i</span><span class="c4">]) 7 </span><span class="c8">return </span><span class="c2">get</span><span class="c4">_</span><span class="c2">task</span><span class="c4">(</span><span class="c2">taskQueues</span><span class="c4">)[</span><span class="c2">i</span><span class="c4">] 8 </span><span class="c8">else </span><span class="c4">9 </span><span class="c2">offset </span><span class="c4">&larr; </span><span class="c2">offset </span><span class="c4">+ 1 10 </span><span class="c8">while </span><span class="c2">offset &lt; </span><span class="c4">|</span><span class="c2">taskQueues</span><span class="c4">| 11 </span><span class="c8">return </span><span class="c2">empty</span><span class="c4">_</span><span class="c2">range</span><span class="c4">() </span></p><p class="c159"><span class="c5">Listing 7: Parallelized </span><span class="c128">for </span><span class="c5">loop </span><span class="c4">1 </span><span class="c2">tasks </span><span class="c4">&larr; </span><span class="c2">create</span><span class="c4">_</span><span class="c2">tasks</span><span class="c4">(|</span><span class="c2">V </span><span class="c4">|</span><span class="c2">,splitSize, </span><span class="c4">|</span><span class="c2">workers</span><span class="c4">|) 2 </span><span class="c8">run on each </span><span class="c2">w </span><span class="c4">&isin; </span><span class="c2">workers </span><span class="c4">3 </span><span class="c2">workerId </span><span class="c4">&larr; </span><span class="c2">getWorkerId</span><span class="c4">() 4 </span><span class="c8">while</span><span class="c4">((</span><span class="c2">range </span><span class="c4">&larr; </span><span class="c2">fetch</span><span class="c4">_</span><span class="c2">task</span><span class="c4">(</span><span class="c2">tasks,workerId</span><span class="c4">)) = 0) 5 </span><span class="c8">for each </span><span class="c2">v </span><span class="c4">&isin; </span><span class="c2">range </span><span class="c4">6 {</span><span class="c2">Loop body</span><span class="c4">} 7 </span><span class="c2">wait</span><span class="c4">_</span><span class="c2">until</span><span class="c4">_</span><span class="c2">all</span><span class="c4">_</span><span class="c2">finished</span><span class="c4">(</span><span class="c2">workers</span><span class="c4">) </span></p><p class="c57"><span class="c7">that queue. </span></p><p class="c79"><span class="c7">Listing 7 shows how the task creation and fetching algo- rithms can be combined to implement the parallel </span><span class="c12">for </span><span class="c7">loop which is used to replace the original sequential loops in the top-down and bottom-up traversals. Here, </span><span class="c12">workers </span><span class="c7">is the set of parallel processors. In line 2 all workers are notified that new work is available and given the current </span><span class="c12">task </span><span class="c7">queues. Each worker fetches tasks and loops over the contained vertices until all tasks are finished. Once a worker can not retrieve further tasks it signals the main thread, which waits until all workers are finished. </span></p><p class="c45"><span class="c7">As long as a worker only fetches from its own queue, the task retrieval cost is minimal&mdash;mostly only an atomic in- crement which is barely more expensive than a non-atomic increment on the x86 architecture[17]. Even when reading from remote queues, our scheduling has only minimal cost that mostly results from reading one value if the respective queue is already finished, and one write when fetching a task. This is negligible compared to the normal BFS workload of at least one atomic write per vertex in the graph. The construc- tion cost of the initial queues in the </span><span class="c12">create</span><span class="c7">_</span><span class="c12">tasks </span><span class="c7">function is also barely measurable and could be easily parallelized if required. </span></p><p class="c87"><span class="c34">4.3 Striped vertex order </span></p><p class="c24"><span class="c7">In the introduction of Section 4.1, we discussed that the combination of multi-threading, degree ordered labeling and array-based BFS processing leads to large skew between worker runtimes. As (S)MS-PBFS&rsquo;s top-down algorithms is designed for multi-threading and requires efficient random- access to the frontier, neither the threading model, nor the backing data structure must be changed. Thus, though our single-threaded benchmarks confirmed that the increased cache locality achieved through degree-ordered labeling leads to significantly shorter runtimes compared to random vertex labeling, we cannot use degree-ordered labeling. </span></p><p class="c102"><span class="c7">Instead, we propose a cache-friendly semi-ordered ap- proach: We distribute degree-ordered vertices in a round robin fashion across the workers&rsquo; task ranges. The highest- </span></p><p class="c11"><span class="c6">7 </span></p><p class="c19"><span class="c7">degree vertex is labeled such that it comes at the start of the first task of worker one. The second-highest degree ver- tex is labeled such that it comes at the start of the first task of worker two, etc. This round robin distribution is continued until all the first tasks for the workers are filled. Then, all the second tasks, and so on, until all vertices are assigned to the workers. Using this approach we still cannot guarantee that all task ranges have the same cost, but we can control that the cost of the ranges in each task queue are approximately the same per worker. Also, because the highest degree vertices are assigned first, the most expensive tasks will be executed first. Having small task sizes at the end has the advantage of reducing wait times towards the end of processing when no tasks are available for stealing anymore. The pre-computation cost of this </span><span class="c12">striped vertex labeling </span><span class="c7">is similar to that of degree-ordered labeling. </span></p><p class="c3"><span class="c34">4.4 NUMA Optimizations </span></p><p class="c19"><span class="c7">Our (S)MS-PBFS algorithms, as described above, scale well on single-socket machines. When running on multi-socket machines, however, the performance does only improve in- significantly, even though the additional CPUs&rsquo; processing power is used. The main problems causing this are twofold. First, if all data is located in a single socket&rsquo;s local mem- ory, i.e., in a single NUMA region, reading it from other sockets can expose memory bandwidth limitations. Second, writing data in a remote NUMA region can be very expen- sive [8]. This leads to a situation where the scalability of seemingly perfectly parallelizable algorithms is limited to a single socket. In the following, we describe (S)MS-PBFS opti- mizations that substantially improve the algorithms&rsquo; scaling on NUMA architectures. </span></p><p class="c19"><span class="c7">In our (S)MS-PBFS algorithms it is very predictable which data is read, particularly within a task range. Consider a bottom-up iteration as described in Section 3.1.2. When processing a task, it updates only the information of vertices inside that task. We designed our algorithm with the goals of not only providing NUMA scalability but </span><span class="c12">also </span><span class="c7">of avoiding any overhead from providing this scalability. We deterministically place memory pages for all BFS data structures, e.g., for </span><span class="c12">seen</span><span class="c7">, in the NUMA region of the worker that is assigned to vertices contained in the memory page. Further, we pin each worker thread to a specific CPU core so that it is not migrated during traversals. The desired result of this pinning is visualized in Figures 4 and 5. In addition to the figures&rsquo; already discussed elements, we use the linestyle to encode the NUMA region of both the data and the workers. The memory pages backing the arrays are interleaved across the NUMA nodes at exactly the task range borders&mdash;for the example shown in the figures there are two vertices per task&mdash;and the workers only process vertices with data on the same NUMA node except for stolen tasks. </span></p><p class="c19"><span class="c7">We calculate the mapping of vertices to memory pages and the size of task ranges as follows. Consider that a 64 bit wide bitset is used per </span><span class="c12">seen </span><span class="c7">entry, and memory pages have a common size of 4096 bytes. In this example, the task range size should be a multiple of </span><span class="c63">pageSize </span></p><p class="c19"><span class="c71">bitsetSize/</span><span class="c61">8 </span><span class="c73">= 512 vertices. Given </span><span class="c7">a task range, it is straightforward to calculate the memory range of the data belonging to the associated vertices. </span></p><p class="c3"><span class="c7">Because we initialize large data structures like </span><span class="c12">seen</span><span class="c7">, </span><span class="c12">frontier</span><span class="c7">, and </span><span class="c12">next </span><span class="c7">only once at the beginning of the BFS and use them across iterations, we need to make sure that the data is placed deterministically, and that tasks accessing the same </span></p><p class="c19"><span class="c7">vertices are scheduled accordingly in all iterations. Thus, work stealing must not occur during the parallel initializa- tion of the data structures to ensure proper initial NUMA region assignments of the memory pages. </span></p><p class="c19"><span class="c7">When the BFS tasks only update memory regions that were initialized by themselves, we achieve NUMA locality. Note that even though our work stealing scheduling approach results in additional flexibility regarding task assignment, most tasks are still executed by their originally assigned workers when the total runtime for the tasks in each queue is balanced. </span></p><p class="c19"><span class="c7">While this goal is perfectly attainable for the bottom- up variant and the second loop of top-down processing, we cannot efficiently predict which vertex information is updated in the first top-down loop. Besides processing stolen tasks, this is the </span><span class="c12">only </span><span class="c7">part of our algorithm in which non-local writes can happen. </span></p><p class="c19"><span class="c7">In summary, given that each worker thread initializes the memory pages that correspond to the vertex ranges it is assigned to, nearly all write accesses, except for the first top-down loop and the work stolen from other threads, are NUMA-local. (S)MS-PBFS further guarantee that the share of memory located in each NUMA region is proportional to the share of threads that belong to that NUMA region. If, for example, 8 threads are located in NUMA region 0 and 2 threads are located in NUMA region 1, 80% of the memory required for the BFS data structures are located in region 0 and 20% will be in region 1. </span></p><p class="c19"><span class="c7">Similar to the NUMA optimizations of the BFS data struc- tures, also the graph storage can be optimized. We minimize cross-NUMA accesses by allocating the neighbor lists of the vertices processed in each task range on the same NUMA node as the worker which the task is assigned to. By using the same vertex range assignment while loading the neigh- bor lists and during BFS traversal, we ensure that all the data entries for each vertex are co-located. This principle is similar to the </span><span class="c14">G</span><span class="c63">B </span><span class="c7">partitioning described by Yasui et al. [19], which, however, uses static partitioning with one partition per NUMA node. </span></p><p class="c3"><span class="c34">5. EVALUATION </span></p><p class="c3"><span class="c7">In our evaluation we analyze four key aspects: </span></p><p class="c3"><span class="c7">&bull; What influence do the different labeling schemes have? </span></p><p class="c19"><span class="c7">&bull; How does the sequential performance of SMS-PBFS&rsquo;s algorithmic approach compare to Beamer&rsquo;s direction- optimizing BFS? </span></p><p class="c3"><span class="c7">&bull; How effectively does it scale both in terms of number of cores and dataset size? </span></p><p class="c3"><span class="c7">&bull; How does it compare to MS-BFS and the parallel single- source BFS by Yasui et al.? </span></p><p class="c19"><span class="c7">In addition to the MS-PBFS and SMS-PBFS algorithms described before, we test two more variants. </span><span class="c12">MS-PBFS (se- quential) </span><span class="c7">is our novel MS-PBFS algorithm run the same way as MS-BFS: a single instance per core requiring multiple batches to be evaluated in parallel. This tests the impact of the early exit optimization in the bottom-up phase, as well as our optimized data-structures. Another variant, </span><span class="c12">MS-PBFS (one per socket) </span><span class="c7">runs one parallel multi-source BFS per CPU socket using MS-PBFS. We use the performance of this vari- ant to determine the cost of parallelization across all NUMA </span></p><p class="c3"><span class="c6">8 </span></p><p class="c19"><span class="c7">nodes when using MS-PBFS. Furthermore, SMS-PBFS is run in two variants: </span><span class="c12">SMS-PBFS (byte) </span><span class="c7">uses an array of bytes for </span><span class="c12">seen</span><span class="c7">, </span><span class="c12">frontier</span><span class="c7">, and </span><span class="c12">next</span><span class="c7">, and </span><span class="c12">SMS-PBFS (bit) </span><span class="c7">uses an array of bits. </span></p><p class="c19"><span class="c7">Our test machine is a 4-socket NUMA system with 4x Intel Xeon E7-4870 v2 CPUs @ 2.3 GHz with one terabyte of main memory. Across all four CPUs, the system has 60 hardware threads. In the experiments we also used all Hyper-Threads. </span></p><p class="c19"><span class="c7">We used both synthetic as well as real-world graphs. The synthetic graphs are Kronecker graphs [13] with the same parameters that are used in the Graph500 benchmark. They exhibit a structure similar to many large social networks. Additionally, for validation we also use artificial graphs gener- ated by the LDBC data generator [9]. The generated LDBC graphs are designed to match the characteristics of real social networks very closely. Our used real-world graphs are chosen to cover different domains with various characteristic: the twitter follower graph, the uk-2005 web crawl graph and the hollywood-2011 graph describing who acted together in movies. The uk-2005 and hollywood-2011 graph were provided by the WebGraph project [7]. Table 1 lists the properties of all graphs used in our experiments. For the Kronecker graph we omit some of the in-between sizes as they always grow by a factor of 2. The vertex counts only consider vertices that have at least one neighbor. KG0 is a special variation of the Kronecker graph that was used in the evaluation of [14]; it was generated using an average out-degree of 1024. </span></p><p class="c19"><span class="c7">The listed memory size is based on using 32-bit vertex identifiers and requiring 2 &lowast; </span><span class="c12">vertex</span><span class="c7">_</span><span class="c12">size </span><span class="c7">= 8 bytes per edge. To measure the performance of MS-BFS we use the source code published on github</span><span class="c13">1</span><span class="c7">. For comparison with Beamer, we used their implementation provided as part of the GAP Benchmark Suite (GAPBS) [6]. We did not have access to an implementation of Yasui et al. or iBFS; instead, we compare to published numbers on a similar machine using the same graph. </span></p><p class="c3"><span class="c7">Our basic metric for comparison is the edge traversal rate (GTEPS). The Graph500 specification defines the number of traversed edges per source as number of input edges con- tained in the connected component which the source belongs to. Compared to the runtime which increases linearly with the graph size, this metric it is more suitable to compare performance across different graphs. In the MS-BFS paper, the number of edges was calculated by summing up the degrees of all vertices in the connected component. In the official benchmark, however, each undirected edge is only counted once. We use this method in our new measurements. To compare the numbers in this paper with the number of the MS-BFS paper, the other numbers have to be divided by two. In order to give an intuition about the effort required to analyze a specific graph we also show the time MS-PBFS requires for processing 64 sources in Table 1. </span><span class="c34">5.1 Labeling Comparison </span></p><p class="c19"><span class="c7">To evaluate the different labeling approaches, we ran both MS-PBFS and SMS-PBFS using 120 threads on a scale 27 Kronecker graph with work stealing scheduling. The three tested variants are random vertex labeling (random), degree- ordered labeling (ordered), and our proposed striped vertex labeling (striped). The average runtime per BFS iteration for each scheme and algorithm is shown in Figure 8. Our re- </span></p><p class="c3"><span class="c61">1</span><span class="c51">https://github.com/mtodat/ms-bfs </span></p><p class="c3"><span class="c23">MS&minus;PBFS </span><span class="c9">150 </span></p><p class="c3"><span class="c23">SMS&minus;PBFS </span></p><p class="c3"><span class="c44">s mn ie mitnu</span><span class="c58">R</span><span class="c100">50 </span></p><p class="c3"><span class="c9">00 </span></p><p class="c3"><span class="c28">Labeling </span></p><p class="c3"><span class="c9">100 </span></p><p class="c3"><span class="c9">600 </span></p><p class="c3"><span class="c54">Ordered </span></p><p class="c3"><span class="c9">400 </span></p><p class="c3"><span class="c54">Random </span></p><p class="c3"><span class="c54">Striped </span></p><p class="c3"><span class="c9">200 </span></p><p class="c20"><span class="c9">1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 </span><span class="c58">Iteration </span></p><p class="c19"><span class="c5">Figure 8: Runtime of BFS iterations under different vertex labeling strategies using SMS-PBFS and MS- PBFS. </span></p><p class="c3"><span class="c44">t setrohso tt segnolo ita</span><span class="c58">R</span><span class="c44">e mitnurr ekro</span><span class="c58">w</span><span class="c100">15 105</span><span class="c9">0</span><span class="c23">MS&minus;PBFS </span><span class="c9">1 2 3 4 5 6 7 8 </span><span class="c23">SMS&minus;PBFS </span></p><p class="c3"><span class="c28">Labeling </span></p><p class="c3"><span class="c54">Ordered </span></p><p class="c3"><span class="c54">Random </span></p><p class="c3"><span class="c54">Striped </span></p><p class="c3"><span class="c9">1 2 3 4 5 6 7 8 </span><span class="c58">Iteration </span></p><p class="c19"><span class="c5">Figure 9: Skew in worker runtimes per iteration when running MS-PBFS and SMS-PBFS with dif- ferent vertex labelings. </span></p><p class="c19"><span class="c7">sults show that degree-ordered labeling exhibits significantly better runtimes that random labeling for the MS-PBFS al- gorithm. Especially in the most expensive third iteration, the difference between the approaches is close to a factor of two. This supports the results of existing work about graph re-labeling[19]. </span></p><p class="c19"><span class="c7">In contrast, for our array-based parallel single-source SMS- PBFS, random ordering exhibits better runtimes. Here, the skew, described in Section 4, and its related problems show their full impact. We evaluated this further in Figure 9 which shows the runtime difference between the longest to the shortest worker per iteration for all three labeling approaches. We see that skew is a much larger problem for SMS-PBFS than for MS-PBFS. Especially in the costly third iteration, there is a significant difference&mdash;more than factor 15 for degree-ordered&mdash;between </span><span class="c12">worker </span><span class="c7">runtimes per iteration. In MS-PBFS skew is a smaller problem as a much larger number of vertices is active in each iteration as there are so many BFSs active at once. Our novel striped vertex ordering shows the best overall runtimes and also balances the workload well. It combines the benefits of degree-based and random ordering in SMS-PBFS and MS-PBFS, while avoiding the other labelings&rsquo; disadvantages. Similar to random labeling, striped vertex ordering provides good skew resilience, and like degree-ordering, it achieves very good cache locality. Using SMS-BFS the overall runtimes per BFS were: 42ms (striped), 86ms (ordered), 68ms (random). </span></p><p class="c3"><span class="c34">5.2 Sequential Comparison </span></p><p class="c19"><span class="c7">In this section, we analyze SMS-PBFS in a sequential setting and compare it against Beamer et al.&rsquo;s state-of-the- art in sequential single-source BFSs. In addition to Beamer&rsquo;s GAPBS implementation, we also implemented two variants of their BFS that use the same graph, data structure and </span></p><p class="c3"><span class="c6">9 </span></p><p class="c3"><span class="c4">Name Nodes Edges Memory size MS-PBFS MS-PBFS MS-BFS MS-BFS 64 SMS-PBFS </span></p><p class="c3"><span class="c4">(x10</span><span class="c13">6</span><span class="c4">) (x10</span><span class="c13">6</span><span class="c4">) (GB) (runtime per 64) (GTEPS) (GTEPS) (GTEPS) (GTEPS) Kronecker 20 2</span><span class="c13">20 </span><span class="c4">15.7 0.119 3.27 ms </span><span class="c8">307 </span><span class="c4">160 4.44 56.2 (bit) Kronecker 26 2</span><span class="c13">26 </span><span class="c4">1,050 7.96 246 ms </span><span class="c8">274 </span><span class="c4">65.8 2.23 58.9 (bit) Kronecker 32 2</span><span class="c13">32 </span><span class="c4">68,300 5q5 39,700 ms </span><span class="c8">110 </span><span class="c2">failed (OOM) </span><span class="c4">0.845 76.7 (bit) KG0 0.982 364 2.72 12.5ms </span><span class="c8">1860 </span><span class="c4">241 11.2 110 (bit) LDBC 100 1.61 102 0.764 24.4 ms </span><span class="c8">267 </span><span class="c4">76.6 3.01 39.2 (byte) LDBC 1000 12.4 1010 7.61 551 ms </span><span class="c8">118 </span><span class="c4">45.5 1.30 83.2 (byte) Hollywood-2011 1.99 114 0.860 49.8 ms </span><span class="c8">147 </span><span class="c4">59.6 2.19 26.5 (byte) UK-2005 39.5 783 5.98 2220 ms </span><span class="c8">22.6 </span><span class="c4">13.2 0.773 4.96 (bit) Twitter 41.7 1,200 9.11 934 ms </span><span class="c8">82.4 </span><span class="c4">32.5 1.13 21.0 (bit) </span></p><p class="c3"><span class="c5">Table 1: Graphs description and algorithm performance in GTEPS using 60 threads. </span></p><p class="c3"><span class="c91">S PETGn it uphguorh</span><span class="c80">T</span><span class="c21">&bull; </span></p><p class="c3"><span class="c152">Algorithm </span></p><p class="c3"><span class="c21">&bull; </span><span class="c151">Beamer </span><span class="c41">(dense) Beamer </span><span class="c39">(GAPBS) </span><span class="c41">Beamer </span><span class="c39">(sparse) </span><span class="c41">SMS&minus;PBFS </span><span class="c39">(bit) </span><span class="c41">SMS&minus;PBFS </span><span class="c39">(byte) </span></p><p class="c3"><span class="c5">Figure 10: Performance of single-threaded BFS runs over varying graph sizes </span></p><p class="c19"><span class="c7">chunk skipping optimizations which we use for SMS-PBFS (bit). In the first variant, the queues in the top-down phase are backed by a sparse vector, and in the second variant we used a dense bit array. Both variants use the same bottom-up implementation. </span></p><p class="c3"><span class="c7">Figure 10 shows the single-source BFSs throughput on a range of Kronecker graphs. The measurements show that for graphs with as few as 2</span><span class="c13">20 </span><span class="c7">vertices, our SMS-PBFS is already faster than Beamer et al.&rsquo;s BFS. As the graph size increases, the probability that the data associated with a vertex is in the CPU cache decreases. There, our top-down approach benefits from having fewer non-sequential data accesses. On the other hand, our BFS has to iterate over all vertices twice. At small graph sizes this overhead can not be recouped as the reduction of random writes does not pay off when the write locations are in the CPU cache. For larger graph sizes, the improvement of SMS-PBFS over our Beamer implementation is limited, as the algorithms only differ in the top-down algorithms but a majority of the runtime in each BFS is spent in the bottom-up phase. </span><span class="c34">5.3 Parallel Comparison </span></p><p class="c19"><span class="c7">In this section we compare our (S)MS-PBFS algorithms against the MS-BFS algorithm in a multi-threaded scenario. Inspired by the Graph500 benchmark, we fix the size of a batch for all algorithms to at most 64 sources. The MS- BFS algorithm is sequential and can only utilize all cores by running one algorithm instance per core. Thus, it requires at least </span><span class="c12">batch</span><span class="c7">_</span><span class="c12">size </span><span class="c7">&lowast; </span><span class="c12">num</span><span class="c7">_</span><span class="c12">threads </span><span class="c7">= 7</span><span class="c14">,</span><span class="c7">680 sources to fully utilize the machine. To minimize the influence of straggling threads when executing MS-BFS we used three times as many sources for all measurements. All algorithms have to analyze the same set of source vertices that were randomly selected from the graph. For MS-BFS, the sources are processed one 64-vertex batch at a time per CPU core. MS-PBFS can saturate all compute resources with a single 64-vertex batch; </span></p><p class="c3"><span class="c114">Algorithm </span></p><p class="c3"><span class="c21">&bull; </span><span class="c39">MS&minus;BFS </span></p><p class="c3"><span class="c41">MS&minus;PBFS </span></p><p class="c3"><span class="c41">MS&minus;PBFS </span><span class="c39">(one per socket)) </span><span class="c41">MS&minus;PBFS </span><span class="c39">(sequential) </span><span class="c41">SMS&minus;PBFS </span><span class="c39">(byte) </span></p><p class="c3"><span class="c5">Figure 11: Relative speedup as number of threads increase in a </span><span class="c7">2</span><span class="c13">26 </span><span class="c5">vertices Kronecker graph </span></p><p class="c3"><span class="c7">it, thus, analyzes one batch at a time. SMS-PBFS analyzes all sources one single source at a time, utilizing all cores. </span></p><p class="c3"><span class="c17">5.3.1 Thread Count Scaling </span></p><p class="c29"><span class="c7">To ensure that the amount of work is constant in the CPU scaling experiments, we kept the number of sources fixed even when running with fewer cores. The first 15 cores are located on the first CPU socket, 16&ndash;30 on the second socket, 31&ndash;45 on the third and 46&ndash;60 on the fourth. Figure 11 shows that MS-PBFS scales better than MS-BFS even though the latter has no synchronization between the threads. MS- PBFS (sequential) which uses the same optimizations as MS-PBFS but is executed like MS-BFS with one BFS batch per core exhibits the same limited scaling behavior for large graphs. This contradicts the MS-BFS paper&rsquo;s hypothesis that multiple sequential instances always beat a parallel algorithm as no synchronization is required. The explanation for this can be found when analyzing the cache hit rates. With our (S)MS-PBFS algorithms, the different CPU cores share large portions of their working set, and, thus, can take advantage of the sockets&rsquo; shared last level caches. In contrast, each sequential MS-BFS mostly uses local data structures; only the graph is shared. This diminishes CPU caches&rsquo; efficiency. The scalability of around factor 45 for MS-PBFS and factor 35 for SMS-PBFS using 60 threads is comparable to the results reported by Yasui et al. [20] for their parallel single-source BFS. This is a very good result especially as our multi-source algorithm operates at a much higher throughput of 274 GTEPS compared to their best reported result of around 60 GTEPS on a similar machine in the Graph500 benchmark. The close results between the MS-PBFS (one per socket) variant, where all data except for the graph is completely local, and MS-PBFS show that our algorithm is mostly resilient to NUMA effects and is not limited by contention. </span></p><p class="c3"><span class="c7">When analyzing the performance gains achieved by the </span></p><p class="c3"><span class="c6">10 </span></p><p class="c3"><span class="c122">60 </span><span class="c43">2.0 </span></p><p class="c3"><span class="c21">&bull; </span><span class="c25">&bull; &bull; </span><span class="c21">&bull; </span></p><p class="c3"><span class="c21">&bull; </span></p><p class="c3"><span class="c21">&bull; </span></p><p class="c3"><span class="c21">&bull; </span><span class="c43">1.5 </span></p><p class="c3"><span class="c21">&bull; </span><span class="c144">&bull; </span></p><p class="c3"><span class="c43">1.0 </span></p><p class="c3"><span class="c91">p udeepse vitale</span><span class="c80">R</span><span class="c122">453015</span><span class="c25">&bull; &bull; </span></p><p class="c3"><span class="c21">&bull; </span><span class="c25">&bull; </span></p><p class="c3"><span class="c21">&bull; </span></p><p class="c3"><span class="c21">&bull; </span></p><p class="c3"><span class="c21">&bull; </span><span class="c43">0.5 </span></p><p class="c3"><span class="c21">&bull; </span><span class="c25">&bull; &bull; </span></p><p class="c3"><span class="c21">&bull; </span></p><p class="c3"><span class="c43">0.0 </span></p><p class="c3"><span class="c43">1</span><span class="c21">&bull; </span></p><p class="c3"><span class="c21">&bull; </span></p><p class="c3"><span class="c43">16 18 20 22 24 26 </span><span class="c96">Number of vertices as power of 2 </span></p><p class="c3"><span class="c21">&bull; </span></p><p class="c3"><span class="c43">1 15 30 45 60 120 </span></p><p class="c3"><span class="c80">Number of threads </span></p><p class="c3"><span class="c7">In </span><span class="c74">S PETGn it uphguorh</span><span class="c84">T</span><span class="c40">600 400 </span></p><p class="c3"><span class="c40">200 </span></p><p class="c3"><span class="c106">&bull; </span></p><p class="c3"><span class="c7">Table 1 we summarize the algorithms&rsquo; performance for real world datasets. Additionally, we show the performance when MS-BFS is only limited to processing 64 sources at a time (MS-BFS 64) like MS-PBFS. The results show that in this kind of use case the performance of MS-BFS is very low as it can only utilize one CPU. Overall, our measurements show that even if MS-BFS is given enough sources to utilize all cores, MS-PBFS performs significantly better on large graphs. </span><span class="c40">16 18 20 22 24 26 28 30 32 </span><span class="c84">Number of vertices as power of 2 </span></p><p class="c19"><span class="c7">We also wanted to compare to the currently fastest parallel single-source BFS by Yasui and fastest parallel multi-source iBFS but did not have access to their implementations. As we could evaluate our algorithms on the same synthetic graphs, we instead compare to their published numbers. For Yasui et al. we compare our SMS-PBFS to their most recent numbers published on the Graph500 ranking. Their results in the Graph500 ranking are based on a CPU that is about 20% faster than ours but from the same CPU generation so they should be comparable. Their result places them 67st overall, 1st single-machine (CPU-only), on the June 2016 ranking and they achieve a throughput of 59.9 GTEPS compared to the 76.7 GTEPS demonstrated by our single-source SMS-PBFS on the same scale 32 graph. For iBFS we use the numbers from their paper, they do not use the default kronecker graph generator settings but test on graphs with larger degrees. We compare MS-PBFS against their algorithm on the KG0 graph where they report their best performance. Using 64 threads, the iBFS CPU implementation reaches 397 GTEPS, and their GPU implementation 729 GTEPS. MS-PBFS reaches 1860 GTEPS on 120 threads showing a significant improvements even when accounting for number of threads. </span></p><p class="c3"><span class="c34">6. RELATED WORK </span></p><p class="c19"><span class="c7">The closest work in the area of multi-source BFS algo- rithms are MS-BFS [18] and the iBFS[14] which is designed for GPUs. Compared to the first algorithm, our parallelized approach using striped labeling significantly improves the performance, and reduces the memory requirements. Fur- thermore, MS-PBFS enables the use of multi-source BFS in a wider setting by providing full performance also with a limited number of sources. iBFS describes a parallelized approach for GPUs which uses a sparse joint frontier queue (JFQ) containing the active vertices for a iteration. By us- ing special GPU voting instructions, it manages to avoid queue contention on the GPU. However, those instructions don&rsquo;t have equivalents on mainstream CPU architectures. Consequently, the CPU adaption of their algorithm exhibits significantly lower performance than ours. </span></p><p class="c19"><span class="c7">The work on parallel single-source algorithms primarily fo- cuses on how to reduce the cost of insertion into the </span><span class="c12">frontier </span><span class="c7">and </span><span class="c12">next </span><span class="c7">queues. Existing approaches span from using multi- socket-optimized queues like FastForward [10], to batch inser- tions and deletions [2], as well as to having a single queue per NUMA node as it is used by the parallel Yasui BFS [20]. Yet, all of these approaches have in common that they share a single insertion point either at the global level or per NUMA node. Even if organized at NUMA socket granularity there is potentially a lot of contention, and the trend of having more cores per CPU does not work in such approaches&rsquo; favor. The work of Chhugani et al. [8] which also focuses on dynamic load balancing has similar limitations as it only focuses on distributing work inside each NUMA socket. Our analysis shows that while this may be sufficient for sparse queue-based </span></p><p class="c3"><span class="c6">11 </span></p><p class="c3"><span class="c113">&bull; </span><span class="c37">MS&minus;BFS </span></p><p class="c3"><span class="c37">MS&minus;PBFS </span></p><p class="c3"><span class="c37">MS&minus;PBFS </span><span class="c16">(one per socket)) </span><span class="c37">MS&minus;PBFS </span><span class="c49">0 </span></p><p class="c3"><span class="c106">&bull; &bull; </span><span class="c116">&bull; &bull; &bull; &bull; &bull; &bull; &bull; &bull; &bull; &bull; &bull; </span><span class="c16">(sequential) </span><span class="c37">SMS&minus;PBFS </span><span class="c16">(bit) </span><span class="c37">SMS&minus;PBFS </span><span class="c16">(byte) </span></p><p class="c3"><span class="c5">Figure 12: Throughput using 60 cores as graph size increases </span></p><p class="c19"><span class="c7">additional 60 Hyper-Threads, the difference between multi- source and single-source processing is clearly visible. SMS- PBFS is memory latency-bound and does not saturate the memory bandwidth; thus, it can gain additional performance by having more threads. The multi-source algorithms on the other hand are already mostly memory-bound, and, thus, they do not benefit from the additional threads. </span></p><p class="c3"><span class="c17">5.3.2 Graph Size Scaling </span></p><p class="c29"><span class="c7">Orthogonal to the thread count scaling experiment, we also measured how the algorithms behave for various graph sizes using Kronecker graphs. We use graph sizes from ap- proximately 2</span><span class="c13">16 </span><span class="c7">to 2</span><span class="c13">32 </span><span class="c7">vertices and 1 million to 68 billion edges, respectively. As the traversal speed should be indepen- dent of the graph size, an ideal result would have constant throughput. Our measured results are shown in Figure 12. MS-BFS as well as the sequential MS-PBFS variant show a continuous decline in performance as the graph size increases. This can be explained with memory bottlenecks, as for larger graph sizes a smaller faction of the graph resides in the CPU cache, and more data has to be fetched from main memory. In contrast, the parallel BFSs struggle at small graph sizes. Their two biggest problems are contention and that there is only very little work per iteration (on average less than 1 ms runtime). The reason for the contention in very small graphs is the high probability that in the top-down phase multiple threads will try to update the same entry. Furthermore, for small graphs, the constant overheads for task creation, memory allocation, etc., have a relatively high impact on the overall result. </span></p><p class="c19"><span class="c7">Parallelization is much more important for large graph sizes. Starting at 2</span><span class="c13">20 </span><span class="c7">(around 1 million) vertices, MS-PBFS manages to beat the MS-PBFS (sequential) implementation. At this size, MS-PBFS requires 3.27ms for one batch of 64 sources. At larger sizes a decline in performance can be mea- sured again, caused by a reduction in cache hit rates resulting in memory bandwidth bottlenecks. SMS-PBFS maintains its peak performance for a larger range of graph sizes, though at a lower level. As it only operates on a single BFS, it is more bound by memory latency in case of a cache miss than by memory bandwidth. Other BFS approaches [2, 20] also exhibit a similar drop in performance at larger scales. The measurement for MS-BFS and MS-PBFS (sequential) only include graphs up to scale 29, as at larger graph sizes the available one terabyte of memory did not suffice to run 120 instances of the algorithms, demonstrating the severe limitations of MS-BFS in a multi-threaded scenario. </span></p><p class="c52"><span class="c7">algorithms, it would not provide scalability in array-based algorithms. </span></p><p class="c149"><span class="c34">7. CONCLUSION </span></p><p class="c26"><span class="c7">In our work we presented the MS-PBFS and SMS-PBFS al- gorithms that improve on the state-of-the art BFS algorithms in several dimensions. </span></p><p class="c30"><span class="c12">MS-PBFS </span><span class="c7">is a parallel multi-source breadth-first search that builds on MS-BFS&rsquo;s principles of sharing redundant traversals in concurrent BFSs in the same graph. In con- trast to MS-BFS, our novel algorithm provides CPU scal- ability even for a limited number of source vertices, fully utilizing large NUMA systems with many CPU cores, while consuming significantly less memory, and providing better single-threaded performance. Our parallelization and NUMA optimizations come at minimal runtime costs so that no separate algorithms are necessary for sequential and parallel processing, neither for NUMA and non-NUMA systems. </span></p><p class="c93"><span class="c12">SMS-PBFS </span><span class="c7">is a parallel single-source BFS that builds on the ideas of MS-PBFS. Compared to existing state-of-the- art single-source BFSs, our proposed SMS-PBFS algorithm provides comparable scalability at much higher absolute performance. Unlike other BFS algorithms, SMS-PBFS has a simple algorithmic structure, requiring very few atomic instructions and no complex lock or queue implementations. Our novel striped vertex labeling allows more coarse-grained task sizes while limiting the skew between task runtimes. Striped vertex labeling can also be used to improve the performance of other BFS algorithms. </span></p><p class="c112"><span class="c34">8. ACKNOWLEDGMENTS </span></p><p class="c143"><span class="c7">Manuel Then is a recipient of the Oracle External Research Fellowship. </span></p><p class="c35"><span class="c34">9. REFERENCES </span><span class="c51">[1] The Graph 500 Benchmark. </span></p><p class="c59"><span class="c7">http://www.graph500.org/specifications. Accessed: 2016-09-09. [2] V. Agarwal, F. Petrini, D. Pasetto, and D. A. Bader. </span></p><p class="c125"><span class="c7">Scalable graph exploration on multicore processors. In </span><span class="c12">Proc. of the 22nd IEEE and ACM Supercomputing Conference (SC10)</span><span class="c7">, SC &rsquo;10, pages 1&ndash;11. IEEE, 2010. [3] L. A. N. Amaral, A. Scala, M. Barth&eacute;l&eacute;my, and H. E. </span></p><p class="c10"><span class="c7">Stanley. Classes of small-world networks. </span><span class="c12">PNAS</span><span class="c7">, 97(21), 2000. [4] D. A. Bader, H. Meyerhenke, P. Sanders, and </span></p><p class="c95"><span class="c7">D. Wagner, editors. </span><span class="c12">Graph Partitioning and Graph Clustering</span><span class="c7">, volume 588 of </span><span class="c12">Contemporary Mathematics</span><span class="c7">. American Mathematical Society, 2013. [5] S. Beamer, K. Asanovi&#263;, and D. Patterson. </span></p><p class="c126 c137"><span class="c7">Direction-optimizing breadth-first search. </span><span class="c12">Scientific Programming</span><span class="c7">, 21(3-4):137&ndash;148, 2013. [6] S. Beamer, K. Asanovic, and D. A. Patterson. The </span></p><p class="c97"><span class="c7">GAP benchmark suite. </span><span class="c12">CoRR</span><span class="c7">, abs/1508.03619, 2015. [7] P. Boldi and S. Vigna. The WebGraph framework I: </span></p><p class="c64"><span class="c7">Compression techniques. In </span><span class="c12">WWW &rsquo;04</span><span class="c7">, pages 595&ndash;601, 2004. [8] J. Chhugani, N. Satish, C. Kim, J. Sewall, and </span></p><p class="c31"><span class="c7">P. Dubey. Fast and efficient graph traversal algorithm for cpus: Maximizing single-node efficiency. In </span><span class="c12">Parallel Distributed Processing Symposium (IPDPS), 2012 IEEE 26th International</span><span class="c7">, pages 378&ndash;389, May 2012. </span></p><p class="c3 c109"><span class="c7">[9] O. Erling, A. Averbuch, J. Larriba-Pey, H. Chafi, </span></p><p class="c68"><span class="c7">A. Gubichev, A. Prat, M.-D. Pham, and P. Boncz. The LDBC social network benchmark: Interactive workload. In </span><span class="c12">Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data</span><span class="c7">, SIGMOD &rsquo;15, pages 619&ndash;630. ACM, 2015. [10] J. Giacomoni, T. Moseley, and M. Vachharajani. </span></p><p class="c90"><span class="c7">Fastforward for efficient pipeline parallelism: A cache-optimized concurrent lock-free queue. In </span><span class="c12">Proceedings of the 13th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</span><span class="c7">, PPoPP &rsquo;08, pages 43&ndash;52. ACM, 2008. [11] P. Gupta, A. Goel, J. Lin, A. Sharma, D. Wang, and </span></p><p class="c68"><span class="c7">R. Zadeh. WTF: The Who to Follow Service at Twitter. In </span><span class="c12">WWW &rsquo;13</span><span class="c7">, pages 505&ndash;514, 2013. [12] S. Hong, T. Oguntebi, and K. Olukotun. Efficient </span></p><p class="c118"><span class="c7">parallel graph exploration on multi-core cpu and gpu. In </span><span class="c12">Parallel Architectures and Compilation Techniques (PACT)</span><span class="c7">, pages 78&ndash;88, Oct 2011. [13] J. Leskovec, D. Chakrabarti, J. Kleinberg, C. Faloutsos, and Z. Ghahramani. Kronecker graphs: An approach to modeling networks. </span><span class="c12">J. Mach. Learn. Res.</span><span class="c7">, 11:985&ndash;1042, Mar. 2010. [14] H. Liu, H. H. Huang, and Y. Hu. iBFS: Concurrent </span></p><p class="c110"><span class="c7">breadth-first search on gpus. In </span><span class="c12">Proceedings of the 2016 International Conference on Management of Data</span><span class="c7">, SIGMOD &rsquo;16, pages 403&ndash;416. ACM, 2016. [15] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, </span></p><p class="c76 c126"><span class="c7">A. Kyrola, and J. M. Hellerstein. Distributed GraphLab: A framework for machine learning and data mining in the cloud. </span><span class="c12">Proc. VLDB Endow.</span><span class="c7">, 5(8):716&ndash;727, Apr. 2012. [16] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, </span></p><p class="c77"><span class="c7">I. Horn, N. Leiser, and G. Czajkowski. Pregel: A system for large-scale graph processing. In </span><span class="c12">Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data</span><span class="c7">, SIGMOD &rsquo;10, pages 135&ndash;146. ACM, 2010. [17] H. Schweizer, M. Besta, and T. Hoefler. Evaluating the </span></p><p class="c60"><span class="c7">cost of atomic operations on modern architectures. In </span><span class="c12">2015 International Conference on Parallel Architecture and Compilation (PACT)</span><span class="c7">, pages 445&ndash;456, Oct 2015. [18] M. Then, M. Kaufmann, F. Chirigati, T.-A. Hoang-Vu, K. Pham, A. Kemper, T. Neumann, and H. T. Vo. The more the merrier: Efficient multi-source graph traversal. </span><span class="c12">Proceedings of the VLDB Endowment</span><span class="c7">, 8(4), 2014. [19] Y. Yasui and K. Fujisawa. Fast and scalable </span></p><p class="c48"><span class="c7">NUMA-based thread parallel breadth-first search. In </span><span class="c12">High Performance Computing &amp; Simulation (HPCS)</span><span class="c7">, pages 377&ndash;385. IEEE, 2015. [20] Y. Yasui, K. Fujisawa, and Y. Sato. Fast and </span></p><p class="c111"><span class="c7">energy-efficient breadth-first search on a single NUMA system. In </span><span class="c12">Proceedings of the 29th International Conference on Supercomputing</span><span class="c7">, ISC 2014, pages 365&ndash;381. Springer-Verlag New York, Inc., 2014. [21] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, </span></p><p class="c94"><span class="c7">M. McCauley, M. J. Franklin, S. Shenker, and I. Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. In </span><span class="c12">Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation</span><span class="c7">, NSDI&rsquo;12, pages 2&ndash;2. USENIX Association, 2012. </span></p><p class="c38"><span class="c6">12 </span></p></body></html>