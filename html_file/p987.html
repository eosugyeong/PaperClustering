<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c113{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:normal}.c66{margin-left:-13.7pt;padding-top:1.7pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Courier New";font-style:normal}.c22{margin-left:-18.2pt;padding-top:4.1pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c108{margin-left:-9.2pt;padding-top:1.4pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.1pt}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Courier New";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c146{margin-left:13.1pt;padding-top:1.7pt;text-indent:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:2.9pt}.c94{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c32{margin-left:-9.2pt;padding-top:1.7pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.1pt}.c163{margin-left:4.3pt;padding-top:1.4pt;text-indent:75.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:54.4pt}.c158{margin-left:-18.2pt;padding-top:1.4pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9.9pt}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c42{margin-left:-9.2pt;padding-top:3.8pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.1pt}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Courier New";font-style:normal}.c117{margin-left:-9.2pt;padding-top:1.7pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.4pt}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c35{margin-left:-18.2pt;padding-top:3.8pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c30{margin-left:-9.2pt;padding-top:1.4pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.1pt}.c141{margin-left:-18.2pt;padding-top:1.7pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9.2pt}.c83{margin-left:-9.2pt;padding-top:9.1pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.1pt}.c75{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.6pt;font-family:"Courier New";font-style:normal}.c121{margin-left:-18.2pt;padding-top:3.8pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c29{margin-left:3.5pt;padding-top:1.7pt;text-indent:68.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3.1pt}.c109{margin-left:-13.7pt;padding-top:1.7pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3pt}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Times New Roman";font-style:normal}.c73{margin-left:-13.7pt;padding-top:1.4pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-6.1pt}.c68{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c82{margin-left:-9.2pt;padding-top:1.7pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-6.5pt}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c72{margin-left:-18.2pt;padding-top:1.7pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c17{margin-left:-9.2pt;padding-top:4.1pt;text-indent:18.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.1pt}.c24{margin-left:-18.2pt;padding-top:1.7pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7.8pt}.c96{margin-left:4.3pt;padding-top:1.7pt;text-indent:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:21.5pt}.c143{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c3{margin-left:-18.2pt;padding-top:1.4pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c51{margin-left:13.1pt;padding-top:1.4pt;text-indent:-3.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:40.6pt}.c60{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.3pt;font-family:"Courier New";font-style:normal}.c165{margin-left:-9.2pt;padding-top:1.7pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:4.3pt}.c84{margin-left:-18.2pt;padding-top:8.9pt;text-indent:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.3pt;font-family:"Arial";font-style:normal}.c90{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c27{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Courier New";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Courier New";font-style:normal}.c38{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Courier New";font-style:normal}.c155{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c140{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-1pt}.c65{margin-left:227.3pt;padding-top:34.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c61{margin-left:4.3pt;padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:63.8pt}.c58{margin-left:-9.2pt;padding-top:1.4pt;text-indent:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c144{margin-left:-9.2pt;padding-top:5.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.1pt}.c63{margin-left:227.3pt;padding-top:35.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c33{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12.7pt}.c88{margin-left:12.4pt;padding-top:13.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:5.5pt}.c86{margin-left:-18.2pt;padding-top:20.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c138{margin-left:-9.2pt;padding-top:6.7pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.1pt}.c97{margin-left:22.7pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:101.5pt}.c74{margin-left:-18.2pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c79{margin-left:-18.2pt;padding-top:47.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c115{margin-left:4.3pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:107.4pt}.c122{margin-left:-18.2pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c64{margin-left:3.5pt;padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3.4pt}.c137{margin-left:13.1pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:12.5pt}.c25{margin-left:7pt;padding-top:10.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:15.1pt}.c101{margin-left:13.1pt;padding-top:7.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:96.2pt}.c14{margin-left:88.7pt;padding-top:6.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.8pt}.c105{margin-left:-7pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c48{margin-left:-4.8pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c114{margin-left:-18.2pt;padding-top:23.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c50{margin-left:2.1pt;padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3.4pt}.c127{margin-left:12.5pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:8.3pt}.c124{margin-left:-18.2pt;padding-top:36.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c132{margin-left:71.9pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:50.2pt}.c39{margin-left:-18.2pt;padding-top:12pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:85.6pt}.c37{margin-left:-18.2pt;padding-top:12.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:57.8pt}.c123{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-6pt}.c136{margin-left:-9.2pt;padding-top:9.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.1pt}.c54{margin-left:-9.2pt;padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.1pt}.c21{margin-left:-9.8pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:219.8pt}.c85{margin-left:-17.3pt;padding-top:6.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:114.6pt}.c40{margin-left:-9.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:3.1pt}.c78{margin-left:4.2pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-15.8pt}.c104{margin-left:-9.2pt;padding-top:16.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:21.8pt}.c110{margin-left:227.3pt;padding-top:181.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c98{margin-left:22.7pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:45.4pt}.c52{margin-left:-9.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-13.7pt}.c89{margin-left:37.2pt;padding-top:20.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-11.6pt}.c100{margin-left:-7pt;padding-top:10.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c53{margin-left:-18.2pt;padding-top:10.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:228.2pt}.c69{margin-left:-18.2pt;padding-top:16.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:113.9pt}.c45{margin-left:47pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:4.2pt}.c120{margin-left:-13.7pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:3pt}.c59{margin-left:-18.2pt;padding-top:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:23.4pt}.c118{margin-left:-18.2pt;padding-top:6.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-8.7pt}.c19{margin-left:227.3pt;padding-top:61pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c81{margin-left:-18.2pt;padding-top:14.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c95{margin-left:13.1pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:29.3pt}.c164{margin-left:-9.2pt;padding-top:8.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:143.3pt}.c99{margin-left:13.1pt;padding-top:8.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:101.8pt}.c49{margin-left:25.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:9pt}.c154{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:27.1pt}.c56{margin-left:-15.4pt;padding-top:7.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:74.1pt}.c111{margin-left:4.3pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:30.9pt}.c15{margin-left:-9.2pt;padding-top:15.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:62.6pt}.c103{margin-left:-18.2pt;padding-top:12pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:130.7pt}.c76{margin-left:-4.8pt;padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c153{margin-left:-18.2pt;padding-top:14.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-6.8pt}.c116{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:0.2pt}.c47{margin-left:13.1pt;padding-top:8.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:87.6pt}.c135{margin-left:59.7pt;padding-top:9.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:121.4pt}.c145{margin-left:4.2pt;padding-top:9.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-21.6pt}.c55{margin-left:-13.7pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:0.9pt}.c130{margin-left:-9.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:22.6pt}.c129{margin-left:4.3pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:120.2pt}.c13{margin-left:4.2pt;padding-top:9.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.1pt}.c160{margin-left:-13.7pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3.7pt}.c87{margin-left:57.1pt;padding-top:14.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:38.4pt}.c70{margin-left:42.6pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:125.5pt}.c43{margin-left:-9.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:5pt}.c80{margin-left:-8.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:6.2pt}.c131{margin-left:15.8pt;padding-top:31.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:21.1pt}.c112{margin-left:-9.2pt;padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.1pt}.c5{margin-left:-7pt;padding-top:10.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c159{margin-left:145.7pt;padding-top:180.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:148.1pt}.c119{margin-left:-18.2pt;padding-top:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:155.9pt}.c62{margin-left:-18.2pt;padding-top:9.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-7pt}.c156{margin-left:-9.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-14.6pt}.c150{margin-left:3.5pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-3.4pt}.c107{margin-left:13.1pt;padding-top:9.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:68.9pt}.c162{margin-left:227.3pt;padding-top:59.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.7pt}.c46{margin-left:-9.2pt;padding-top:22.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.8pt}.c142{margin-left:-12.7pt;padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:6.4pt}.c152{margin-left:21.5pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:96.2pt}.c92{margin-left:3.5pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-3.1pt}.c36{margin-left:-13.7pt;padding-top:4.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:24.2pt}.c151{padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c16{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c161{padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c133{padding-top:8.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c149{padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c157{padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c134{padding-top:6.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c139{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c125{margin-left:-9.2pt;text-indent:18.1pt;margin-right:-16.1pt}.c147{margin-left:-9.2pt;margin-right:-16.1pt}.c77{margin-left:13.1pt;margin-right:-15.8pt}.c91{margin-left:-6.6pt;margin-right:36.5pt}.c20{margin-left:108.9pt;margin-right:-15.8pt}.c57{margin-left:-9.2pt;margin-right:107pt}.c128{margin-left:-18.2pt;margin-right:-7pt}.c106{margin-left:-9.2pt;margin-right:-12pt}.c41{margin-left:95.4pt;margin-right:105.4pt}.c166{margin-left:-18.2pt;margin-right:112.5pt}.c67{margin-left:-18.2pt;margin-right:44.8pt}.c44{margin-left:84pt;margin-right:86.2pt}.c126{margin-left:12.5pt;margin-right:25.8pt}.c93{margin-left:-18.2pt;margin-right:130.2pt}.c148{margin-left:-9.2pt;margin-right:-7.4pt}.c102{margin-left:28.8pt;margin-right:-3pt}.c31{margin-left:-3.9pt;margin-right:15.1pt}.c71{margin-right:-13.2pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c139"><p class="c7 c44"><span class="c155">Ricardo: Integrating R and Hadoop </span></p><p class="c131"><span class="c8">Sudipto Das</span><span class="c23">1</span><span class="c68">&lowast; </span><span class="c8">Yannis Sismanis</span><span class="c23">2 </span><span class="c8">Kevin S. Beyer</span><span class="c23">2 </span></p><p class="c25"><span class="c8">Rainer Gemulla</span><span class="c23">2 </span><span class="c8">Peter J. Haas</span><span class="c23">2 </span><span class="c8">John McPherson</span><span class="c23">2 </span></p><p class="c87"><span class="c11">1</span><span class="c90">University of California </span><span class="c11">2</span><span class="c90">IBM Almaden Research Center Santa Barbara, CA, USA San Jose, CA, USA sudipto@cs.ucsb.edu {syannis, kbeyer, rgemull, phaas, jmcphers}@us.ibm.com </span></p><p class="c81"><span class="c8">ABSTRACT </span><span class="c0">Many modern enterprises are collecting data at the most detailed level possible, creating data repositories ranging from terabytes to petabytes in size. The ability to apply sophisticated statistical anal- ysis methods to this data is becoming essential for marketplace competitiveness. This need to perform deep analysis over huge data repositories poses a significant challenge to existing statistical software and data management systems. On the one hand, statisti- cal software provides rich functionality for data analysis and mod- eling, but can handle only limited amounts of data; e.g., popular packages like R and SPSS operate entirely in main memory. On the other hand, data management systems&mdash;such as MapReduce-based systems&mdash;can scale to petabytes of data, but provide insufficient analytical functionality. We report our experiences in building Ri- cardo, a scalable platform for deep analytics. Ricardo is part of the eXtreme Analytics Platform (XAP) project at the IBM Almaden Research Center, and rests on a decomposition of data-analysis al- gorithms into parts executed by the R statistical analysis system and parts handled by the Hadoop data management system. This de- composition attempts to minimize the transfer of data across system boundaries. Ricardo contrasts with previous approaches, which try to get along with only one type of system, and allows analysts to work on huge datasets from within a popular, well supported, and powerful analysis environment. Because our approach avoids the need to re-implement either statistical or data-management func- tionality, it can be used to solve complex problems right now. </span></p><p class="c59"><span class="c8">Categories and Subject Descriptors </span><span class="c0">H.4 [Information Systems Applications]: Miscellaneous </span></p><p class="c119"><span class="c8">General Terms </span><span class="c0">Algorithms, Design </span></p><p class="c153"><span class="c12">&lowast;</span><span class="c2">The author conducted parts of this work at the IBM Almaden Re- search Center. </span></p><p class="c124"><span class="c18">Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGMOD&rsquo;10, June 6&ndash;11, 2010, Indianapolis, Indiana, USA. Copyright 2010 ACM 978-1-4503-0032-2/10/06 ...$10.00. </span></p><p class="c7 c57"><span class="c8">1. INTRODUCTION </span></p><p class="c17"><span class="c0">Many of today&rsquo;s enterprises collect data at the most detailed level possible, thereby creating data repositories ranging from terabytes to petabytes in size. The knowledge buried in these enormous datasets is invaluable for understanding and boosting business per- formance. The ability to apply sophisticated statistical analysis methods to this data can provide a significant competitive edge in the marketplace. For example, internet companies such as Amazon or Netflix provide personalized recommendations of products to their customers, incorporating information about individual prefer- ences. These recommendations increase customer satisfaction and thus play an important role in building, maintaining, and expanding a loyal customer base. Similarly, applications like internet search and ranking, fraud detection, risk assessment, microtargeting, and ad placement gain significantly from fine-grained analytics at the level of individual entities. This paper is about the development of industrial-strength systems that support advanced statistical analy- sis over huge amounts of data. </span></p><p class="c32"><span class="c0">The workflow for a data analyst comprises multiple activities. Typically, the analyst first explores the data of interest, usually via visualization, sampling, and aggregation of the data into summary statistics. Based on this exploratory analysis, a model is built. The output of the model is itself explored&mdash;often through visualization and also through more formal validation procedures&mdash;to determine model adequacy. Multiple iterations of model-building and evalua- tion may be needed before the analyst is satisfied. The final model is then used to improve business practices or support decision mak- ing. Feedback from model users can lead to further iterations of the model-development cycle. </span></p><p class="c30"><span class="c0">During this process, the data analyst&rsquo;s indispensable toolkit is a statistical software package such as R, SPSS, SAS, or Matlab. Each of these packages provides a comprehensive environment for sta- tistical computation, including a concise statistical language, well tested libraries of statistical algorithms for data exploration and modeling, and visualization facilities. We focus on the highly pop- ular R statistical analysis program. The Comprehensive R Archive Network (CRAN) contains a library of roughly 2000 add-in pack- ages developed by leading experts and covering areas such as lin- ear and generalized linear models, nonlinear regression models, time series analysis, resampling methods, classical parametric and nonparametric tests, classification, clustering, data smoothing, and many more [13]. We refer to the application of these sophisticated statistical methods as deep analytics. </span></p><p class="c30"><span class="c0">Most statistical software packages, including R, are designed to target the moderately-sized datasets commonly found in other areas of statistical practice (e.g., opinion polls). These systems operate on a single server and entirely in main memory; they simply fail when the data becomes too large. Unfortunately, this means that </span></p><p class="c63"><span class="c28">987 </span></p><p class="c16 c128"><span class="c0">data analysts are unable to work with these packages on massive datasets. Practitioners try to avoid this shortcoming either by ex- ploiting vertical scalability&mdash;that is, using the most powerful ma- chine available&mdash;or by working on only subsets or samples of the data. Both approaches have severe limitations: vertical scalabil- ity is inherently limited and expensive, and sampling methods may lose important features of individuals and of the tail of the data distribution [9]. </span></p><p class="c72"><span class="c0">In parallel to the development of statistical software packages, the database community has developed a variety of large-scale data management systems (DMSs) that can handle huge amounts of data. Examples include traditional enterprise data warehouses and newer systems based on MapReduce [11], such as Hadoop. The data is queried using high-level declarative languages such as SQL, Jaql, Pig, or Hive [14, 19, 23]. These systems leverage decades of research and development in distributed data management, and ex- cel in massively parallel processing, scalability, and fault tolerance. In terms of analytics, however, such systems have been limited pri- marily to aggregation processing, i.e., computation of simple ag- gregates such as SUM, COUNT, and AVERAGE, after using fil- tering, joining, and grouping operations to prepare the data for the aggregation step. For example, traditional reporting applications arrange high-level aggregates in cross tabs according to a set of hi- erarchies and dimensions. Although most DMSs provide hooks for user-defined functions and procedures, they do not deliver the rich analytic functionality found in statistical packages. </span></p><p class="c72"><span class="c0">To summarize, statistical software is geared towards deep ana- lytics, but does not scale to large datasets, whereas DMSs scale to large datasets, but have limited analytical functionality. Enter- prises, which increasingly need analytics that are both deep and scalable, have recently spurred a great deal of research on this prob- lem; see Section 6. Indeed, the work in this paper was strongly mo- tivated by an ongoing research collaboration with Visa to explore approaches to integrating the functionality of R and Hadoop [10]. As another example, Cohen at al. [9] describe how the Fox Audi- ence Network is using statistical functionality built inside a large- scale DMS. As discussed in Section 6, virtually all prior work at- tempts to get along with only one type of system, either adding large-scale data management capability to statistical packages or adding statistical functionality to DMSs. This approach leads to so- lutions that are often cumbersome, unfriendly to analysts, or waste- ful in that a great deal of well established technology is needlessly re-invented or re-implemented. </span></p><p class="c24"><span class="c0">In this paper, we report our experience in building Ricardo, a scalable platform for deep analytics. Ricardo&mdash;which is part of the eXtreme Analytics Platform (XAP) project at the IBM Almaden Research Center&mdash;is named after David Ricardo, a famous econo- mist of the early 19th century who studied conditions under which mutual trade is advantageous. Ricardo facilitates &ldquo;trading&rdquo; between R and Hadoop, with R sending aggregation-processing queries to Hadoop (written in the high-level Jaql query language), and Hadoop sending aggregated data to R for advanced statistical processing or visualization&mdash;each trading partner performs the tasks that it does best. In contrast to previous approaches, Ricardo has the following advantages: </span></p><p class="c76"><span class="c0">&bull; Familiar working environment. Analysts want to work within a statistical environment, and Ricardo lets them continue to do so. </span></p><p class="c48"><span class="c0">&bull; Data attraction. Ricardo uses Hadoop&rsquo;s flexible data store together with the Jaql query language. This combination al- lows analysts to work directly on any dataset in any format; this property of &ldquo;attracting&rdquo; data of any type has been iden- </span></p><p class="c7 c77"><span class="c0">tified as a key requirement for competitive enterprise analyt- ics [9]. </span></p><p class="c13"><span class="c0">&bull; Integration of data processing into the analytical workflow. Analysts traditionally handle large data by preprocessing and reducing it&mdash;using either a DMS or shell scripts&mdash;and then manually loading the result into a statistical package, e.g., as delimited text files. By using an integrated approach to data processing, Ricardo frees data analysts from this tedious and error-prone process, and allows them to leverage all available data. </span></p><p class="c78"><span class="c0">&bull; Reliability and community support. Ricardo is built from widely adopted open-source projects from both the data man- agement community and the statistical community. It lever- ages the efforts of both communities and has a reliable, well supported, and state-of-the-art code base. </span></p><p class="c13"><span class="c0">&bull; Improved user code. Ricardo facilitates more concise, more readable, and more maintainable code than is possible with previous approaches. </span></p><p class="c145"><span class="c0">&bull; Deep analytics. By exploiting R&rsquo;s functionality, Ricardo can handle many kinds of advanced statistical analyses, includ- ing principal and independent component analysis, k-means clustering, and SVM classification, as well as the fitting and application of generalized-linear, latent-factor, Bayesian, time- series, and many other kinds of statistical models. </span></p><p class="c13"><span class="c0">&bull; No re-inventing of wheels. By combining existing statistical and DMS technology, each of which represents decades of research and development, we can immediately start to solve many deep analytical problems encountered in practice. </span></p><p class="c83"><span class="c0">Ricardo is inspired by the work in [8], which shows that many deep analytical problems can be decomposed into a &ldquo;small-data part&rdquo; and a &ldquo;large-data part.&rdquo; In Ricardo, the small-data part is ex- ecuted in R and the large-data part is executed in the Hadoop/Jaql DMS. A key requirement for the success of this combined approach is that the amount of data that must be communicated between both systems be sufficiently small. Fortunately, this requirement holds for almost all of the deep analytics mentioned above. </span></p><p class="c32"><span class="c0">In the following sections, we show how Ricardo can facilitate some key tasks in an analyst&rsquo;s typical workflow: data exploration, model building, and model evaluation, all over a very large dataset. For illustrative purposes, we use the dataset provided for the Net- flix movie-recommendation competition [16]. Although the com- petition itself was based on a subset of just 100M movie ratings, our experiments on a Hadoop cluster in the Amazon Elastic Com- pute Cloud (EC2) indicate that Ricardo can scale R&rsquo;s functionality to handle the billions of ratings found in practice&mdash;over a terabyte of data in our case. </span></p><p class="c30"><span class="c0">We emphasize that an analyst who uses Ricardo need not neces- sarily be an expert in Jaql nor understand exactly how to decompose all the deep analytics appropriately. Ricardo can potentially deliver much of its deep-analytics functionality in the form of R packages and functions that hide most of the messy implementation details. For example, we describe in the sequel how Ricardo can be used to efficiently fit a latent-factor model of movie preferences over a massive dataset stored in a Hadoop cluster, as well as how Ri- cardo can be used for large-scale versions of principal component analysis (PCA) and generalized linear models (GLMs). This func- tionality can potentially be delivered to the analyst via high-level R functions called, e.g., </span><span class="c10">jaqlLF</span><span class="c0">, </span><span class="c10">jaqlPCA</span><span class="c0">, and </span><span class="c10">jaqlGLM</span><span class="c0">. Of course, if existing Ricardo packages do not meet the needs of a particular </span></p><p class="c65"><span class="c28">988 </span></p><p class="c7"><span class="c18">3</span><span class="c1">0 .4</span><span class="c26">&bull;&bull; </span></p><p class="c7"><span class="c18">Data </span><span class="c1">9.3</span><span class="c26">&bull;&bull;</span><span class="c60">&bull;&bull;</span><span class="c26">&bull;&bull;</span><span class="c18">Fit </span></p><p class="c7"><span class="c1">8.3</span><span class="c27">&bull;&bull;&bull;</span><span class="c1">7.3</span><span class="c27">&bull;</span><span class="c1">6.35.</span><span class="c18">Year of Release </span></p><p class="c7"><span class="c26">&bull;&bull;</span><span class="c27">&bull;&bull;</span><span class="c26">&bull;&bull;</span><span class="c60">&bull;</span><span class="c26">&bull;</span><span class="c60">&bull;&bull;&bull;</span><span class="c27">&bull;</span><span class="c26">&bull;</span><span class="c60">&bull;</span><span class="c26">&bull;&bull;&bull;&bull;&bull;</span><span class="c27">&bull;&bull;</span><span class="c26">&bull;</span><span class="c27">&bull;&bull;</span><span class="c26">&bull;</span><span class="c27">&bull;&bull;</span><span class="c1">g nitaRe garev</span><span class="c18">A</span><span class="c60">&bull;</span><span class="c26">&bull;&bull;&bull;</span><span class="c27">&bull;&bull;&bull;&bull;</span><span class="c26">&bull;</span><span class="c27">&bull;</span><span class="c26">&bull;</span><span class="c27">&bull;&bull; </span></p><p class="c7"><span class="c18">1950 1960 1970 1980 1990 2000 </span></p><p class="c7"><span class="c0">Figure 1: Average rating of a movie depending on its age </span></p><p class="c16"><span class="c0">analysis, then the analyst will have to implement the lower-level functionality. The hope is that, over time, users will develop a large library of Ricardo packages in the same way that the CRAN repos- itory has been developed for in-memory analytical packages. The XAP project team is trying to kick off this effort by providing pack- ages for the most common types of large-scale analyses. </span></p><p class="c16"><span class="c0">The remainder of this paper is structured as follows. In Section 2, we drill down into key aspects of a typical analytics workflow, us- ing movie recommendation data as a running example. Section 3 briefly describes the systems of interest: the R package for statis- tical computing, the Hadoop DMS, and the Jaql query language. In Section 4, we describe Ricardo and illustrate its use on several analysis tasks. Section 5 describes our experimental study on the Netflix data. We discuss prior work in Section 6 and give our con- clusions in Section 7. </span></p><p class="c7"><span class="c8">2. MOTIVATING EXAMPLES </span></p><p class="c7"><span class="c0">We motivate Ricardo in the context of three examples that cover key aspects of the analyst&rsquo;s workflow. The examples are centered around the Netflix competition [5]. This competition was estab- lished in 2006 in order to improve the recommender system that Netflix uses to suggest movies to its ten million customers. Since recommendations play an important role in establishing, maintain- ing and expanding a loyal customer base, such systems have be- come the backbone of many of the major firms on the web such as Amazon, eBay, and Apple&rsquo;s iTunes [15, 25]. Unlike search en- gines, which help find information that we know we are interested in, recommender systems help discover people, places, and things previously unknown to us. We classify the examples by the degree of complexity in the trading between R and Hadoop. </span><span class="c8">2.1 Simple Trading </span></p><p class="c16"><span class="c0">Our first two examples concern the exploration and evaluation phases of the analyst&rsquo;s workflow. During the exploration phase, an analyst tries to gain preliminary insights about the dataset of inter- est. For example, Figure 1 depicts how movies are perceived with respect to their age. We can see that, on average, older movies are perceived more positively than newer movies. To produce this sim- ple data visualization, the analyst first performs a linear regression and then calls upon a plotting facility to display the raw data and fitted model. The analyst might also want to formally verify that the fitted trend is statistically significant by looking at summary test statistics such as the t-statistic for the slope. Statistical soft- </span></p><p class="c16"><span class="c0">ware such as R offers a convenient and powerful environment for performing this type of exploration. However, such software can- not scale to the size of the datasets found in practice. Although, for the competition, Netflix published only a small subset of its rat- ings (100M), the actual number of (explicit and implicit) ratings encountered in practice is orders of magnitude larger. Ricardo al- lows such data to be efficiently preprocessed, aggregated, and re- duced by Hadoop, and then passed to R for regression analysis and visualization. </span></p><p class="c16"><span class="c0">During the evaluation phase the analyst wants to understand and quantify the quality of a trained model. In our second example, we assume that the analyst has built a model&mdash;such as a latent-factor recommendation model as described in the sequel&mdash;and wants to identify the top-k outliers, i.e., to identify data items on which the model has performed most poorly. Such an analysis, which must be performed over all of the data, might lead to the inclusion of addi- tional explanatory variables (such as movie age) into the model, to improve the model&rsquo;s accuracy. Ricardo enables such outlier analy- sis by allowing the application of complex R-based statistical mod- els over massive data. It does so by leveraging the parallelism of the underlying Hadoop DMS. </span></p><p class="c16"><span class="c0">The foregoing examples illustrate &ldquo;simple trading&rdquo; scenarios be- tween R and Hadoop. In the first case, data is aggregated and pro- cessed before it is passed to R for advanced analysis; in the second case, an R statistical model is passed to Hadoop for efficient paral- lel evaluation over the massive dataset. As discussed below, other analyses require more intricate exchanges, which Ricardo also sup- ports. </span></p><p class="c7"><span class="c8">2.2 Complex Trading </span></p><p class="c16"><span class="c0">Our third example illustrates the use of Ricardo during the mod- eling phase of an analyst&rsquo;s workflow. One approach to model build- ing over a huge dataset might use a simple-trading scheme in which Hadoop reduces the data by aggregation or sampling, and then passes the data to R for the model-building step. The downside of this approach is that detailed information, which must be ex- ploited to gain a competitive edge, is lost. As shown below, Ri- cardo permits a novel &ldquo;complex trading&rdquo; approach that avoids such information loss. </span></p><p class="c16"><span class="c0">The complex-trading approach, like simple trading, requires a decomposition of the modeling into a small-data part, which R han- dles, and a large-data part, which Hadoop handles&mdash;as mentioned in Section 1, many deep analytical problems are amenable to such a decomposition. Unlike simple trading, however, a complex-trading algorithm involves multiple iterations over the data set, with trading back and forth between R and Hadoop occurring at each iteration. </span></p><p class="c16"><span class="c0">As an instructive example, we consider the problem of building a latent-factor model of movie preferences over massive Netflix-like data. This modeling task is central to the winning Netflix com- petition technique [18], and enables accurate personalized recom- mendations for each individual user and movie, rather than global recommendations based on coarse customer and movie segments. Because such a model must discern each customer&rsquo;s preferences from a relatively small amount of information on that customer, it is clear that every piece of available data must be taken into ac- count. As with many other deep analytics problems, the sampling and aggregation used in a simple-trading approach is unacceptable in this setting. </span></p><p class="c16"><span class="c0">To understand the idea behind latent-factor models, consider the data depicted in Figure 2, which shows the ratings of three cus- tomers and three movies in matrix form. The ratings are printed in boldface and vary between 1 (hated the movie) and 5 (loved it). For example, Michael gave a rating of 5 to the movie &ldquo;About Schmidt&rdquo; </span></p><p class="c7"><span class="c28">989 </span></p><p class="c7 c102"><span class="c0">About Schmidt Lost in Translation Sideways </span></p><p class="c45"><span class="c0">(2.24) (1.92) (1.18) </span></p><p class="c142"><span class="c0">Alice ? 4 2 (1.98) (4.4) (3.8) (2.3) Bob 3 2 ? (1.21) (2.7) (2.3) (1.4) Michael 5 ? 3 </span></p><p class="c80"><span class="c0">(2.30) (5.2) (4.4) (2.7) </span></p><p class="c86"><span class="c0">Figure 2: A simple latent-factor model for predicting movie ratings. (Data points in boldface, latent factors and estimated ratings in italics.) </span></p><p class="c114"><span class="c0">and a rating of 3 to &ldquo;Sideways&rdquo;. In general, the ratings matrix is very sparse; most customers have rated only a small set of movies. The italicized number below each customer and movie name is a latent factor. In this example, there is just one factor per entity: Michael is assigned factor 2.30, the movie &ldquo;About Schmidt&rdquo; gets 2.24. In this simple example, the estimated rating for a particu- lar customer and movie is given by the product of the correspond- ing customer and movie factors.</span><span class="c68">1 </span><span class="c0">For example, Michael&rsquo;s rating of &ldquo;About Schmidt&rdquo; is approximated by 2.30 &middot; 2.24 &asymp; 5.2; the approximation is printed in italic face below the respective rating. The main purpose of the latent factors, however, is to predict rat- ings, via the same mechanism. Our estimate for Michael&rsquo;s rating of &ldquo;Lost in Translation&rdquo; is 2.30 &middot;1.92 &asymp; 4.4. Thus, our recommender system would suggest this movie to Michael but, in contrast, would avoid suggesting &ldquo;Sideways&rdquo; to Bob, because the predicted rating is 1.21 &middot; 1.18 &asymp; 1.4. </span></p><p class="c3"><span class="c0">Latent factors characterize the interaction between movies and customers and sometimes have obvious interpretations. For exam- ple, a movie factor might indicate the degree to which a movie is a &ldquo;comedy&rdquo; and a corresponding customer factor might indicate the degree to which the customer likes comedies. Usually, how- ever, such an interpretation cannot be given, and the latent fac- tors capture correlations between customers and movies without invoking domain-specific knowledge. Under the assumption that the preferences of individual customers remain constant, the more available feedback, the better the modeling power of a latent-factor model [15, 17]. Netflix and other companies typically collect bil- lions of ratings; taking implicit feedback such as navigation and purchase history into account, the amount of data subject to recom- mendation analysis is enormous. </span></p><p class="c3"><span class="c0">In Section 4 below, we discuss in detail how Ricardo handles the foregoing exploration, evaluation, and model-building example tasks that we have defined, and we indicate how Ricardo can be applied to other deep analytics tasks over massive data. </span></p><p class="c69"><span class="c8">3. PRELIMINARIES </span></p><p class="c22"><span class="c0">In this section, we briefly describe the main components of Ri- cardo: the R statistical software package, the Hadoop large-scale DMS, and the Jaql query language. </span></p><p class="c79"><span class="c12">1</span><span class="c2">In an actual recommender system, there is a vector of latent factors associated with each customer and movie, and the estimated rating is given by the dot product of the corresponding vectors. </span></p><p class="c7 c148"><span class="c8">3.1 The R Project for Statistical Computing </span></p><p class="c42"><span class="c0">R was originally developed by Ross Ihaka and Robert Gentle- man, who were at that time working at the statistics department of the University of Auckland, New Zealand. R provides both an open-source language and an interactive environment for statisti- cal computation and graphics. The core of R is still maintained by a relatively small set of individuals, but R&rsquo;s enormous popu- larity derives from the thousands of sophisticated add-on packages developed by hundreds of statistical experts and available through CRAN. Large enterprises such as AT&amp;T and Google have been supporting R, and companies such as REvolution Computing sell versions of R for commercial environments. </span></p><p class="c32"><span class="c0">As a simple example of R&rsquo;s extensive functionality, consider the following small program that performs the regression analysis and data visualization shown previously in Figure 1. It takes as input a &ldquo;data frame&rdquo; </span><span class="c10">df </span><span class="c0">that contains the mean ratings of the movies in the Netflix dataset by year of publication, performs a linear regression, and plots the result: </span></p><p class="c107"><span class="c10">fit &lt;- lm(df$mean ~ df$year) plot(df$year, df$mean) abline(fit) </span></p><p class="c136"><span class="c0">(A data frame is R&rsquo;s equivalent of a relational table.) Older movies appear to be rated more highly than recent ones; a call to the R function </span><span class="c10">summary(fit) </span><span class="c0">would validate that this linear trend is in- deed statistically significant by computing assorted test statistics. </span><span class="c8">3.2 Large-Scale Data Management Systems </span></p><p class="c42"><span class="c0">Historically, enterprise data warehouses have been the dominant type of large-scale DMS, scaling to hundreds of terabytes of data. These systems are usually interfaced by SQL, a declarative lan- guage for processing structured data. Such systems provide exten- sive support for aggregation processing as defined in Section 1. Im- plementations of enterprise data warehouses benefit from decades of experience in parallel SQL processing and, as indicated by the recent surge of new vendors of &ldquo;analytical databases,&rdquo; continue to provide innovative data management solutions. </span></p><p class="c32"><span class="c0">Besides being very expensive, these systems suffer from the dis- advantage of being primarily designed for clean and structured data, which comprises an increasingly tiny fraction of the world&rsquo;s data. Analysts want to be able to work with dirty, semi-structured or even unstructured data without going through the laborious cleans- ing process needed for data warehousing [9]. For these situations, MapReduce and its open-source implementation Apache Hadoop have become popular and widespread solutions. </span></p><p class="c30"><span class="c0">Hadoop comprises a distributed file system called HDFS bundled with an implementation of Google&rsquo;s MapReduce paradigm [11]. Hadoop operates directly on raw data files; HDFS takes care of the distribution and replication of the files across the nodes in the Hadoop cluster. Data processing is performed according to the MapReduce paradigm. The input files are split into smaller chunks, each of which is processed in parallel using a user-defined map function. The results of the map phase are redistributed (accord- ing to a user-defined criterion) and then fed into a reduce function, which combines the map outputs into a global result. Hadoop has been successfully used on petabyte-scale datasets and thousands of nodes; it provides superior scalability, elasticity and fault-tolerance properties on large clusters of commodity machines. Hadoop is an appealing alternative platform for massive data storage, manipula- tion and parallel processing. </span></p><p class="c19"><span class="c28">990 </span></p><p class="c7 c67"><span class="c8">3.3 Jaql: A JSON Query Language </span></p><p class="c35"><span class="c0">Perhaps the main drawback of Hadoop is that its programming interface is too low-level for most of its users. For this reason, a variety of projects aim at providing higher-level query interfaces on top of Hadoop; notable examples include Jaql [14], Pig [19], Hive [23] and Cascading</span><span class="c68">2</span><span class="c0">. Ricardo uses Jaql as its declarative inter- face to Hadoop. Jaql is an open-source dataflow language with rich data processing features such as transformation, filtering, join pro- cessing, grouping and aggregation. Jaql scripts are automatically compiled into MapReduce jobs, which are then executed in paral- lel by Hadoop. Since Jaql is built around the JSON data model and is highly extensible, it enhances Hadoop&rsquo;s usability while retaining all of Hadoop&rsquo;s flexibility. </span></p><p class="c3"><span class="c0">Although Jaql operates directly on user data files, it makes use of JSON views to facilitate data processing. For example, consider the following JSON view over the Netflix dataset: </span></p><p class="c53"><span class="c6">[ </span></p><p class="c21"><span class="c6">{ </span></p><p class="c128 c157"><span class="c6">customer: &quot;Michael&quot;, movie: { name: &quot;About Schmidt&quot;, year: 2002 }, rating: 5 },</span><span class="c38">... </span><span class="c6">]</span><span class="c18">,</span><span class="c2">Square brackets denote arrays (all ratings) and curly braces enclose </span><span class="c0">structured objects (individual rating or movie). The following Jaql query computes average ratings as a function of movie age; this data can then be &ldquo;traded&rdquo; to R, which can in turn process the data to produce Figure 1. </span></p><p class="c61"><span class="c10">read(&quot;ratings&quot;) -&gt; group by year = $.movie.year </span></p><p class="c96"><span class="c10">into { year, mean: avg($[*].rating) } -&gt; sort by [ $.year ]</span><span class="c0">. </span></p><p class="c84"><span class="c0">The operator </span><span class="c10">-&gt; </span><span class="c0">pipes the output of the expression on the left hand side into the right hand side; the current record is accessed via reference to the special variable </span><span class="c10">$</span><span class="c0">. Jaql provides functionality that would be tedious to implement using the native Hadoop API. </span></p><p class="c37"><span class="c8">4. TRADING WITH RICARDO </span></p><p class="c121"><span class="c0">In this section, we describe Ricardo in more detail, using the examples of Section 2. </span><span class="c8">4.1 Architecture Overview </span></p><p class="c35"><span class="c0">Figure 3 gives an overview of Ricardo&rsquo;s design. Ricardo consists of three components: an R driver process operated by the data ana- lyst, a Hadoop cluster that hosts the data and runs Jaql (and possi- bly also some R sub-processes), and an R-Jaql bridge that connects these two components. </span></p><p class="c3"><span class="c0">R serves as an interactive environment for the data analyst, but in contrast to classical R usage, the data itself is not memory-resident in R. Instead, the base data is stored in a distributed file system within the Hadoop cluster, which comprises a set of worker nodes that both store data and perform computation. As described previ- ously, Jaql provides a high-level language to process and query the Hadoop data, compiling into a set of low-level MapReduce jobs. </span></p><p class="c158"><span class="c0">The third, novel component of Ricardo is the R-Jaql bridge, which provides the communication and data conversion facilities needed to integrate these two different platforms. The bridge not only al- lows R to connect to a Hadoop cluster, execute Jaql queries, and </span></p><p class="c85"><span class="c12">2</span><span class="c143">http://www.cascading.org </span></p><p class="c16 c147"><span class="c0">receive the results of such queries as R data frames, but also allows Jaql queries to spawn R processes on Hadoop worker nodes. The bridge comprises an R package that provides integration of Jaql into R and a Jaql module that integrates R into Jaql. The latter module allows worker nodes to execute R scripts in parallel and R functions to be used inside Jaql queries. The key functions</span><span class="c68">3 </span><span class="c0">provided by the R package are: </span></p><p class="c64"><span class="c10">jaqlConnect() </span><span class="c0">Opens a channel connecting R to a </span></p><p class="c29"><span class="c0">Hadoop cluster. </span><span class="c10">jaqlSave() </span><span class="c0">Saves data from R to HDFS. </span><span class="c10">jaqlValue() </span><span class="c0">Returns to R arbitrarily complex JSON </span></p><p class="c92"><span class="c0">results from Jaql queries. </span><span class="c10">jaqlTable() </span><span class="c0">Like </span><span class="c10">jaqlValue() </span><span class="c0">but optimized for </span></p><p class="c150"><span class="c0">data frames (i.e. tabular data). </span><span class="c10">jaqlExport() </span><span class="c0">Exports R objects from the R driver process to R processes running on the worker nodes. </span></p><p class="c125 c161"><span class="c0">The components of Ricardo work together to support an analyst&rsquo;s workflow. The analyst typically starts by issuing Jaql queries from inside a top-level R process. These queries sample or aggregate the data to allow viewing and visualization of the data from within R. Next, the analyst builds models of the data. The building of these models&mdash;e.g., by training the model or fitting parameters&mdash;as well as the validation of their quality&mdash;e.g., by visualization, by cross validation, or by computing measures of fit or test statistics&mdash;is performed as a joint effort of R, Jaql, and the R-Jaql bridge. In the following subsections, we give specific examples of how Ricardo can be used in an analytic workflow. </span><span class="c8">4.2 Simple Trading </span></p><p class="c17"><span class="c0">First recall the data-exploration example of Section 2.1. The fol- lowing R script shows how the </span><span class="c10">jaqlTable() </span><span class="c0">function is used to push the aggregation of the data to the Hadoop cluster using Jaql. It computes the data frame </span><span class="c10">df </span><span class="c0">that holds the average ratings for movies of different ages; R then processes </span><span class="c10">df </span><span class="c0">using the code given in Section 3.1 to fit a regression model and produce Figure 1. As discussed in Section 3, R can use the </span><span class="c10">summary(fit) </span><span class="c0">command to determine the adequacy of the regression model. </span></p><p class="c99"><span class="c10">ch &lt;- jaqlConnect() df &lt;- jaqlTable(ch, &rsquo; </span></p><p class="c98"><span class="c10">read(&quot;ratings&quot;) -&gt; group by year = $.movie.year </span></p><p class="c146"><span class="c10">into { year, mean: avg($[*].rating) } -&gt; sort by [ $.year ] &rsquo;) </span></p><p class="c125 c133"><span class="c0">Now recall the second example of Section 2.1, in which the an- alyst wants to apply a trained model over all the data in order to compute the top-k outliers. The following R script shows how a trained model can be invoked in parallel using </span><span class="c10">jaqlExport()</span><span class="c0">. In this example, we use the linear model computed above. </span></p><p class="c47"><span class="c10">mymodel &lt;- function(x) { </span></p><p class="c51"><span class="c10">fit$coeff[2]*x[1] + fit$coeff[1] }</span><span class="c143">jaqlExport(ch, fit) </span><span class="c10">jaqlExport(ch, mymodel) res &lt;- jaqlTable(ch, &rsquo; </span></p><p class="c97"><span class="c10">read(&quot;ratings&quot;) -&gt; transform { $.*, </span></p><p class="c54"><span class="c12">3</span><span class="c2">At the time of this writing, the implementation of </span><span class="c143">jaqlExport </span><span class="c2">is still underway. </span></p><p class="c65"><span class="c28">991 </span></p><p class="c159"><span class="c0">Figure 3: Overview of Ricardo&rsquo;s architecture. </span></p><p class="c89"><span class="c10">error: pow( R(&quot;mymodel&quot;, [$.movie.year]) </span></p><p class="c163"><span class="c10">- rating, 2) } -&gt; top 10 by [ $.error desc ] &rsquo;) </span></p><p class="c128 c149"><span class="c0">Here </span><span class="c10">mymodel() </span><span class="c0">is an R function that implements the linear model evaluation, outputting an estimated average rating corresponding an input movie age. The </span><span class="c10">jaqlExport() </span><span class="c0">invocations ensure that both the computed model </span><span class="c10">fit </span><span class="c0">and the function </span><span class="c10">mymodel() </span><span class="c0">are &ldquo;known&rdquo; by the R-processes that are spawned on the worker nodes. During execution of the Jaql query, the worker nodes, in parallel, each execute the </span><span class="c10">mymodel() </span><span class="c0">function and compute the correspond- ing squared errors. The errors are sorted in descending order (us- ing the MapReduce framework), and the top-k outliers are returned back to R as a data frame. Note that the function </span><span class="c10">mymodel() </span><span class="c0">is simple enough to be implemented entirely in Jaql, without requir- ing R to evaluate it. In practice, however, this is seldom the case. See, for example, the time-series models discussed in Section 4.4. </span><span class="c8">4.3 Complex Trading </span></p><p class="c35"><span class="c0">In this section, we describe in detail how Ricardo can be used for building statistical models over massive data when simple trad- ing approaches do not suffice. We first illustrate our approach us- ing the latent-factor model mentioned in Section 2.2, and then in Section 4.4 we briefly indicate other types of analysis that can be handled by Ricardo. </span></p><p class="c72"><span class="c0">Recall that complex-trading scenarios arise when the model of interest must incorporate all of the available (massive) data. Be- cause data reduction prior to modeling is not allowed, the data will not fit in main memory. Ricardo does require that the model itself&mdash; e.g., the set of latent factors in our Netflix example&mdash;fit into the main memory of the analyst&rsquo;s R process. This assumption holds for all but extremely large models (such as those in PageRank algo- rithms). In the case of Netflix, the latent factors occupy on the order of tens to hundreds of megabytes of memory, even after scaling up the numbers of customers and movies used in the competition by several orders of magnitude. A model of this size can easily fit in main memory. In contrast, the base data on customers and movies ranges from hundreds of gigabytes to terabytes and is thus signifi- cantly larger than the model. Ricardo places no restrictions on the base-data size, as long as the Hadoop cluster is equipped with a sufficient number of worker nodes to process the data. </span></p><p class="c7 c91"><span class="c4">4.3.1 Computing a Latent-Factor Model </span></p><p class="c42"><span class="c0">Recall from Section 2 that the latent-factor model for movie recommendations assigns a set of latent factors to each customer and each movie. These factors are used to predict ratings that are unknown; movies with high predicted ratings can then be recom- mended to the customer. This general scheme also applies to other recommendation problems. In general, our goal is to recommend items (movies) to users (customers) based on known relationships between items and users. </span></p><p class="c32"><span class="c0">Indexes u and i denote users and items. Denote by p</span><span class="c12">u </span><span class="c0">the latent factor associated with user u and by q</span><span class="c9">i </span><span class="c2">the latent factor for item i. </span><span class="c0">Once learned, these factors allow us to compute a predicted rating &circ;r</span><span class="c9">ui </span><span class="c2">of u for i via a simple product: </span></p><p class="c14"><span class="c0">&circ;r</span><span class="c9">ui </span><span class="c2">= p</span><span class="c9">u</span><span class="c2">q</span><span class="c9">i</span><span class="c2">. (1) </span></p><p class="c138"><span class="c0">The value of &circ;r</span><span class="c9">ui </span><span class="c2">predicts how much user u will like (or dislike) item </span><span class="c0">i. As mentioned previously, this latent-factor model is highly sim- plified for aid in understanding Ricardo; an industrial-strength rec- ommender system would (1) maintain a vector of factors for each user and items, (2) use additional information about the user and item (such as demographic information), and (3) would account for shifts of preferences over time [17]. All of these extensions can be handled by Ricardo. </span></p><p class="c32"><span class="c0">Recent work [4, 7, 16] has suggested that the latent factors be learned via an optimization-based approach that tries to minimize the sum of squared errors between the observed ratings and the corresponding predicted ratings from the model. Thus we want to solve the problem </span></p><p class="c135"><span class="c0">min </span><span class="c9">{p</span><span class="c113">u</span><span class="c12">},{q</span><span class="c113">i</span><span class="c12">} </span></p><p class="c7 c41"><span class="c10">&sum;</span><span class="c9">u,i </span></p><p class="c7 c41"><span class="c10">&sum;</span><span class="c9">u,i </span></p><p class="c7 c20"><span class="c0">(r</span><span class="c9">ui </span><span class="c2">&minus; p</span><span class="c9">u</span><span class="c2">q</span><span class="c9">i</span><span class="c2">)</span><span class="c11">2</span><span class="c2">, (2) </span></p><p class="c7 c20"><span class="c0">(r</span><span class="c9">ui </span><span class="c2">&minus; p</span><span class="c9">u</span><span class="c2">q</span><span class="c9">i</span><span class="c2">)</span><span class="c11">2</span><span class="c2">, (2) </span></p><p class="c7 c20"><span class="c0">(r</span><span class="c9">ui </span><span class="c2">&minus; p</span><span class="c9">u</span><span class="c2">q</span><span class="c9">i</span><span class="c2">)</span><span class="c11">2</span><span class="c2">, (2) </span></p><p class="c104"><span class="c0">where the sum is taken over the set of observed ratings.</span><span class="c68">4 </span></p><p class="c32"><span class="c0">The above optimization problem is large and sparse, and has to be solved using numerical methods. The general algorithm is as follows: </span></p><p class="c50"><span class="c0">1. Pick (random) starting points for the values of p</span><span class="c9">u </span><span class="c2">and q</span><span class="c9">i</span><span class="c2">. </span></p><p class="c144"><span class="c12">4</span><span class="c2">In fact, (2) has been simplified; our actual implementation makes use of &ldquo;regularization&rdquo; techniques to avoid overfitting. For Netflix data, a simple weight decay scheme&mdash;which amounts to adding a penalty term to (2)&mdash;has proven successful [16]. </span></p><p class="c65"><span class="c28">992 </span></p><p class="c7"><span class="c0">2. Compute the squared error and the corresponding gradients with respect to each p</span><span class="c9">u </span><span class="c2">and q</span><span class="c9">i</span><span class="c2">, using the current values of p</span><span class="c9">u </span><span class="c2">and q</span><span class="c9">i</span><span class="c2">. </span></p><p class="c7"><span class="c0">3. Update the values of each p</span><span class="c9">u </span><span class="c2">and q</span><span class="c9">i </span><span class="c2">according to some up- </span></p><p class="c7"><span class="c0">date formula. </span></p><p class="c7"><span class="c0">4. Repeat steps 2 and 3 until convergence. </span></p><p class="c16"><span class="c0">The idea behind this &ldquo;gradient descent&rdquo; algorithm is to use the gra- dient information to try and reduce the value of the squared error as much as possible at each step. Observe that step 2 corresponds to a summation over the entire dataset and is thus data-intensive, while step 3 requires the use of a sophisticated optimization algorithm that is able to deal with a large number of parameters. </span></p><p class="c7"><span class="c4">4.3.2 The R Component </span></p><p class="c16"><span class="c0">We next describe how R contributes to the model-fitting proce- dure described in Section 4.3.1 by using its sophisticated optimiza- tion algorithms. We assume the existence of two functions </span><span class="c10">e </span><span class="c0">and </span><span class="c10">de</span><span class="c0">, which compute the squared error and the corresponding gradi- ents for specific values of the latent factors. Both functions take a single argument </span><span class="c10">pq</span><span class="c0">, which holds the concatenation of the latent factors for users and items. (The description of how we implement these functions is deferred to Sections 4.3.3 and 4.3.4.) R can then use its </span><span class="c10">optim </span><span class="c0">function to drive the overall optimization process. The </span><span class="c10">optim </span><span class="c0">function operates by using the current squared error value and gradient computed by </span><span class="c10">e </span><span class="c0">and </span><span class="c10">de</span><span class="c0">. Then </span><span class="c10">optim </span><span class="c0">uses this information to update the </span><span class="c10">pq </span><span class="c0">values in such a manner as to reduce the squared error as much as possible. This process is repeated until the squared error no longer decreases significantly. The up- dating process can be viewed as taking a &ldquo;downhill step&rdquo; in the space of possible </span><span class="c10">pq </span><span class="c0">values. Both the direction and size of the step are determined by the specific optimization method that is chosen, and can be quite complex. As discussed below, R has a number of optimization methods to choose from. </span></p><p class="c7"><span class="c0">The following snippet contains all of the high-level R code that is required to run the optimization: </span></p><p class="c7"><span class="c10">model &lt;- optim( pq, e, de, </span></p><p class="c7"><span class="c10">method=&quot;L-BFGS-B&quot;, ... ) </span></p><p class="c16"><span class="c0">where </span><span class="c10">pq </span><span class="c0">is the starting point and </span><span class="c10">model </span><span class="c0">is the resulting solution. The first three arguments thus describe the input to the optimiza- tion problem. The </span><span class="c10">method </span><span class="c0">argument determines the optimization method to be used. The </span><span class="c10">optim </span><span class="c0">function also takes a number of ad- ditional arguments that allow fine-grained control of the optimiza- tion algorithm, such as setting error tolerances and stopping criteria or bounding the number of iterations. </span></p><p class="c16"><span class="c0">Some popular optimization algorithms provided by R include the conjugate-gradient method [3] and L-BFGS [6]. Optimization is a complex field and there are many different methods, each with its own strengths and weaknesses. We experimented with many differ- ent optimization methods and we found that the L-BFGS method is very effective. L-BFGS is an optimization technique for solv- ing large non-linear optimization problems; it implements a lim- ited memory quasi-Newtonian optimization algorithm based on the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method. The details of this method are beyond the scope of this paper; in essence, it makes use of the values of </span><span class="c10">e </span><span class="c0">and </span><span class="c10">de </span><span class="c0">to learn the &ldquo;curvature&rdquo; of the objective function (the squared error in our case) by maintaining a low-memory approximation of the inverse of the Hessian Matrix. This information is then used to estimate the best direction and size for the next optimization step. </span></p><p class="c7"><span class="c4">4.3.3 The Hadoop and Jaql Component </span></p><p class="c16"><span class="c0">In Ricardo, Hadoop and Jaql are responsible for data storage and large-scale aggregation processing. For the latent-factor example, this latter processing corresponds to the computation of </span><span class="c10">e </span><span class="c0">and </span><span class="c10">de </span><span class="c0">at each step of the R-driven optimization algorithm. For the Net- flix data, the ratings are stored in a table </span><span class="c10">r </span><span class="c0">and the latent factors are stored in tables </span><span class="c10">p </span><span class="c0">and </span><span class="c10">q</span><span class="c0">. For simplicity, we assume that the following JSON views have been defined over these datasets: </span></p><p class="c7"><span class="c10">r: schema { u: long, i: long, rating: double } p: schema { u: long, factor: double } q: schema { i: long, factor: double } </span></p><p class="c16"><span class="c0">Each record in the ratings table corresponds to a single rating by user u of item i. Each record in p or q corresponds to a latent factor. </span></p><p class="c16"><span class="c0">We start with a simple &ldquo;naive&rdquo; Jaql implementation of the </span><span class="c10">e </span><span class="c0">and </span><span class="c10">de </span><span class="c0">functions and then discuss some potential optimizations. Our goal is to compute the following quantities: </span></p><p class="c7"><span class="c0">e = </span><span class="c34">&sum;</span><span class="c12">u,i </span></p><p class="c7"><span class="c0">(r</span><span class="c12">ui </span><span class="c0">&minus; p</span><span class="c12">u</span><span class="c0">q</span><span class="c12">i</span><span class="c0">)</span><span class="c23">2</span><span class="c0">, </span></p><p class="c7"><span class="c0">&part;e &part;p</span><span class="c9">u </span><span class="c0">= &minus;2 </span><span class="c34">&sum;</span><span class="c12">i </span></p><p class="c7"><span class="c0">(r</span><span class="c12">ui </span><span class="c0">&minus; p</span><span class="c12">u</span><span class="c0">q</span><span class="c12">i</span><span class="c0">)q</span><span class="c12">i</span><span class="c0">, </span></p><p class="c7"><span class="c0">&part;e &part;q</span><span class="c9">i </span><span class="c0">= &minus;2 </span><span class="c34">&sum;</span><span class="c12">u </span></p><p class="c7"><span class="c0">(r</span><span class="c12">ui </span><span class="c0">&minus; p</span><span class="c12">u</span><span class="c0">q</span><span class="c12">i</span><span class="c0">)p</span><span class="c12">u</span><span class="c0">, </span></p><p class="c16"><span class="c0">where, as before, the sums are taken over the observed ratings. For the computation of the squared error e, we have to bring together the rating from the </span><span class="c10">r </span><span class="c0">table and the corresponding factors from the </span><span class="c10">p </span><span class="c0">and </span><span class="c10">q </span><span class="c0">tables. The following Jaql query does the trick: </span></p><p class="c7"><span class="c6">join r, p, q </span></p><p class="c7"><span class="c6">where r.u == p.u and r.i == q.i into { </span><span class="c18">p</span><span class="c9">u</span><span class="c38">: p.factor, </span><span class="c6">-&gt; transform { error: -&gt; sum( $.error ); </span></p><p class="c7"><span class="c6">pow( </span><span class="c94">q</span><span class="c9">i</span><span class="c6">: $.</span><span class="c18">r</span><span class="c6">q.factor, </span><span class="c9">ui </span><span class="c38">- $.</span><span class="c94">p</span><span class="c9">u</span><span class="c38">*$.</span><span class="c94">q</span><span class="c18">r</span><span class="c9">ui</span><span class="c6">: r.rating </span><span class="c9">i</span><span class="c38">, 2 )} </span></p><p class="c7"><span class="c6">} </span></p><p class="c16"><span class="c0">This query first performs a three-way join between </span><span class="c10">r</span><span class="c0">, </span><span class="c10">p</span><span class="c0">, and </span><span class="c10">q</span><span class="c0">, then computes the squared error for each of the observed ratings, and fi- nally accumulates all of these squared errors. The computation of the gradients is performed in a similar way, but requires an addi- tional grouping step. The following query computes the derivatives with respect to the user factors: </span></p><p class="c7"><span class="c6">join r, p, q </span></p><p class="c7"><span class="c6">where r.u == p.u and r.i == q.i -&gt; into { </span><span class="c18">u</span><span class="c6">: </span><span class="c18">p</span><span class="c12">u</span><span class="c6">: r.u, p.factor, </span><span class="c18">r</span><span class="c9">ui</span><span class="c6">: transform { </span><span class="c18">u</span><span class="c6">: $.u, </span></p><p class="c7"><span class="c6">r.rating, </span></p><p class="c7"><span class="c18">q</span><span class="c9">i</span><span class="c38">: q.factor } </span></p><p class="c7"><span class="c6">gradient: -&gt; group by u = $.</span><span class="c18">u </span></p><p class="c7"><span class="c6">($.</span><span class="c18">r</span><span class="c9">ui</span><span class="c38">-$.</span><span class="c94">p</span><span class="c9">u</span><span class="c38">*$.</span><span class="c94">q</span><span class="c9">i</span><span class="c38">)*$.</span><span class="c94">q</span><span class="c9">i </span><span class="c38">} </span></p><p class="c7"><span class="c6">into { u, gradient: -2.0*sum($[*].gradient)}; </span></p><p class="c16"><span class="c0">The query returns a table containing pairs of users </span><span class="c10">u </span><span class="c0">and the cor- responding gradient &part;e/&part;p</span><span class="c12">u</span><span class="c0">. The query for the gradients with re- spect to the movie factors is similar. </span></p><p class="c16"><span class="c0">In our actual implementation, we used an optimized query that computes the error and gradients simultaneously. This has the ad- vantage that the number of MapReduce jobs generated by Jaql&mdash; and thus the number of passes over the data&mdash;is reduced from three to one. The optimized query is given by: </span></p><p class="c7"><span class="c6">r -&gt; hashJoin( fn(r) r.i, q, fn(q) q.i, </span></p><p class="c7"><span class="c6">-&gt; hashJoin( fn(r,q) fn(r) { r.*, r.u, p, fn(p) </span><span class="c18">q</span><span class="c9">i</span><span class="c38">: q.factor </span><span class="c6">p.u, </span></p><p class="c7"><span class="c38">} ) </span></p><p class="c7"><span class="c6">fn(r,p) { r.*, </span><span class="c18">p</span><span class="c9">u</span><span class="c38">: p.factor } ) </span><span class="c6">-&gt; transform { $.*, diff: $.rating-$.</span><span class="c18">p</span><span class="c12">u</span><span class="c6">*$.</span><span class="c18">q</span><span class="c9">i </span><span class="c38">} </span></p><p class="c7"><span class="c28">993 </span></p><p class="c7 c126"><span class="c6">-&gt; expand [ { value: pow( $.diff, 2.0 ) }, </span></p><p class="c127"><span class="c6">{ $.u, value: -2.0*$.diff*$.</span><span class="c18">q</span><span class="c9">i </span><span class="c38">}, </span><span class="c6">{ $.i, value: -2.0*$.diff*$.</span><span class="c18">p</span><span class="c9">u </span><span class="c38">} ] </span><span class="c6">-&gt; group by g = { $.u, $.i } </span></p><p class="c49"><span class="c6">into { g.u?, g.i?, total: sum($[*].value) } </span></p><p class="c74"><span class="c0">This query, though compact in terms of lines of Jaql code, is rather subtle in the way it exploits the features of the underlying map- combine-reduce framework. To avoid going into onerous detail, we simply mention that </span><span class="c10">hashJoin </span><span class="c0">is an efficient function that in- cludes in its argument list a key-extractor function for the probe side, a dataset for the build side, a key extractor for the build side, and a function that pairs matching tuples of the join. The first hash join joins </span><span class="c10">r </span><span class="c0">and </span><span class="c10">p</span><span class="c0">, and the second hash join takes the result and joins it with </span><span class="c10">q</span><span class="c0">. The remainder of the query performs all of the ag- gregations at once. It works with three different kind of records: (1) records with only a </span><span class="c10">u </span><span class="c0">field, (2) records with only an </span><span class="c10">i </span><span class="c0">field, and (3) records with neither </span><span class="c10">u </span><span class="c0">nor </span><span class="c10">i</span><span class="c0">. The records are distinguished us- ing the existence operator (denoted as a question mark in the query above). During the aggregation, records of type (1) contribute to the gradients of user factors, records of type (2) to the gradients of item factors, and records of type (3) to the overall squared error. The execution plan resembles the execution of a &ldquo;grouping-sets&rdquo; SQL query. We expect future versions of Jaql to automatically per- form some of the above optimizations. </span></p><p class="c56"><span class="c4">4.3.4 Integrating the Components </span></p><p class="c35"><span class="c0">In what follows, we show how to integrate the &ldquo;large-data&rdquo; com- putation of Section 4.3.3 with the &ldquo;small-data&rdquo; computation of Sec- tion 4.3.2, using the bridge. In particular, the naive version of the squared-error function </span><span class="c10">e </span><span class="c0">that is passed to R&rsquo;s optimization function </span><span class="c10">optim </span><span class="c0">has the following form: </span></p><p class="c129"><span class="c10">e &lt;- function(pq) { </span></p><p class="c111"><span class="c10">jaqlSave(ch, pq[1:np], p); jaqlSave(ch, pq[(np+1):(np+nq)], q); jaqlValue(ch, &quot;&lt;query&gt;&quot;); } </span></p><p class="c122"><span class="c0">The argument </span><span class="c10">pq </span><span class="c0">given to the error function is a concatenation of the current customer and movie factors; it is provided to </span><span class="c10">e </span><span class="c0">by </span><span class="c10">optim</span><span class="c0">. The first two lines of the function body split </span><span class="c10">pq </span><span class="c0">into its components and then save the result in Jaql via a call to </span><span class="c10">jaqlSave</span><span class="c0">. Here, variables </span><span class="c10">p </span><span class="c0">and </span><span class="c10">q </span><span class="c0">point to locations in the Hadoop cluster. The final line in the function body then executes the Jaql query that performs the actual computation and returns the result as a float- ing point value&mdash;here </span><span class="c10">&quot;&lt;query&gt;&quot; </span><span class="c0">is an abbreviation for the naive Jaql code for computing </span><span class="c10">e </span><span class="c0">that was given in Section 4.3.3. The naive function </span><span class="c10">de </span><span class="c0">for computing the gradients looks similar, but the call to </span><span class="c10">jaqlValue() </span><span class="c0">is replaced by </span><span class="c10">jaqlTable()</span><span class="c0">, since multiple numbers are returned. The result is automatically converted into an R data frame. All functions provided by the bridge automatically perform any necessary conversions of data types. </span></p><p class="c141"><span class="c0">In our implementation we use the optimized Jaql query (described in Section 4.3.3) to compute the squared error and the correspond- ing gradients concurrently. Thus, the implementation of </span><span class="c10">e </span><span class="c0">and </span><span class="c10">de </span><span class="c0">must be adjusted slightly. For brevity we omit the details. </span><span class="c8">4.4 Other Deep Analytics </span></p><p class="c22"><span class="c0">In the previous sections, we provided examples of how Ricardo enhances the analyst&rsquo;s workflow by scaling deep analytics to mas- sive datasets, exploiting Hadoops parallelism while staying in the analyst-friendly environment that R provides. We discussed how Ricardo handles &ldquo;simple trading&rdquo; and &ldquo;complex trading&rdquo; scenarios during the key phases of an analyst&rsquo;s workflow. </span></p><p class="c16 c125"><span class="c0">For purposes of exposition, all of our examples so far have been in the setting of the Netflix dataset, so that our focus has been a bit narrow. E.g., the latent-factor model, while certainly nontrivial and providing a good demonstration of the power of the Ricardo framework, is actually somewhat simple in terms of statistical so- phistication. In this section we briefly outline how Ricardo can be used to perform a number of other, more complex analyses, by leveraging R&rsquo;s extensive library of statistical functions. </span></p><p class="c32"><span class="c0">As one example, we experimented with R&rsquo;s time-series analy- sis functions and incorporated them in the recommender system. In this scenario, we took the time of the rating into account, since people often change preferences over time. (For simplicity, we take the &ldquo;time&rdquo; for a rating to be the total number of movies that the cus- tomer has rated up to and including the current movie.) The follow- ing R/Jaql code shows how one computes, in parallel, an &ldquo;autore- gressive integrated moving average&rdquo; (ARIMA) time-series model for each individual customer: </span></p><p class="c101"><span class="c6">my.arima &lt;- function(x) { </span></p><p class="c95"><span class="c6">library(forecast) auto.arima(x[[1]][ sort.list(x[[2]]) ]) }</span><span class="c38">jaqlExport(ch, my.arima) </span><span class="c6">arima.models &lt;- jaqlTable(ch, &rsquo; </span></p><p class="c152"><span class="c6">r -&gt; group by g={ $.u } </span></p><p class="c70"><span class="c6">into { g.u, </span></p><p class="c132"><span class="c6">model: R(&quot;my.arima&quot;, [ </span></p><p class="c137"><span class="c6">$[*].rating, $[*].date ] ) } &rsquo;) </span></p><p class="c112"><span class="c0">Here </span><span class="c10">my.arima() </span><span class="c0">is an R function that first orders the ratings vec- tor of a customer in increasing time order and then computes an ARIMA model by automatically fitting the model parameters. The embedded Jaql query groups the ratings by customer u and then constructs the corresponding rating and rating-date vectors in par- allel. Other extensions of the recommender system are possible, such as incorporation of demographic and movie genres informa- tion. By exploiting R&rsquo;s machine learning library functions, one could easily train, in parallel, decision trees and self-organizing maps that capture important properties of demographic and movie genre segments. </span></p><p class="c32"><span class="c0">We have also demonstrated that Ricardo can perform large-scale principal component analysis (PCA) and generalized linear models (GLM). For PCA, we use the covariance method: Hadoop/Jaql first shifts and scales the data by subtracting the empirical mean and dividing by the empirical standard deviation (after using Hadoop to compute these quantities), and then computes the empirical co- variance matrix. Next, R performs an eigenvector decomposition of the covariance matrix, and finally Hadoop/Jaql projects the orig- inal data onto the lower-dimensional space spanned by the major eigenvectors. </span></p><p class="c32"><span class="c0">GLMs generalize classical linear regression models by (1) allow- ing the response variable to be expressed as a nonlinear function&mdash; called the link function&mdash;of the usual linear combination of the in- dependent variables, and (2) by allowing the variance of each noisy measurement to be a function of its predicted value. To fit a GLM model to a massive dataset, Ricardo exploits Hadoop, together with R&rsquo;s library of distributions and link functions (which includes their derivatives and inverses). The model-fitting algorithm is similar to the algorithm for fitting a latent-factor model, except that R is used not only for the driver process, but also at the worker nodes. </span></p><p class="c32"><span class="c0">More generally, as pointed out in [8], many deep analytics can be decomposed into a small-data part and a large-data part. Such decompositions are possible not only for linear regression, latent- factor, PCA, and GLM analyses, but also for k-means clustering, </span></p><p class="c65"><span class="c28">994 </span></p><p class="c16 c128"><span class="c0">independent component analysis, support vector machines, classi- fication, and many more types of analysis. The general approach to scaling up these analytical tasks is to reformulate them in terms of &ldquo;summation forms&rdquo;. These sums, which are inherently distributive, can be executed in parallel. Then the results flow into algorithms that typically perform small matrix operations, such as (pseudo)- inversions or eigenvector decompositions, depending upon the spe- cific analysis. Thus, although a relatively small collection of scal- able analytics has been implemented so far, Ricardo can potentially be applied much more generally. </span></p><p class="c39"><span class="c8">4.5 Implementation Details </span></p><p class="c35"><span class="c0">The design philosophy of Ricardo is to minimize the transfer of functionality between R and Hadoop&mdash;letting each component do what it does best&mdash;as well as the amount of data transferred. The data that crosses the boundary between R and Jaql is usu- ally aggregated information. As shown by our latent-factor ex- ample, complex trading involves multiple iterations over the data, with aggregates crossing the R-Jaql bridge many times. The bridge must therefore be very efficient so as not to become a performance bottleneck. The design of the bridge turned out to be rather in- tricate, for a couple of reasons. One challenge is that R is im- plemented entirely in C and Fortran, whereas Hadoop and Jaql belong to the Java world. An even greater challenge arises be- cause the data formats of R and Jaql differ considerably. For ex- ample, R uses a column-oriented structure (data frames), whereas Jaql and Hadoop are designed for row-oriented access to data. As another example, R freely intermingles references to data items by position (e.g., </span><span class="c10">customer[[3]]</span><span class="c0">) and references by association (e.g., </span><span class="c10">customer$income</span><span class="c0">), whereas JSON objects represent data in a way that maintains either position or association, but not both. In this section, we discuss some of the design choices that we made in implementing the R-Jaql bridge and their impacts on the perfor- mance of data transfers across the bridge. </span></p><p class="c128 c134"><span class="c0">Bridging C with Java The problem of interfacing Java and C has been well studied, and the Java Native Interface (JNI) is a common solution for bridging these languages. A straightforward solution would be to use JNI to submit the query from R to the Jaql engine, and when Jaql finishes computation, use JNI calls to iteratively fetch data from Jaql into R, one tuple at a time. But this approach is inefficient for transferring even a few hundred data tuples, i.e., a few megabytes of data. For the Netflix latent-factor model, com- puting an aggregate rating for a movie (over the customers who rated the movie) requires about 17K data tuples to be transferred from Jaql to R. Using a JNI call for each tuple, the transfer takes tens of minutes, making this approach infeasible for any real-world applications. We therefore designed the bridge to pass only meta- and control-information using JNI calls, and wrappers on both the R side and the Jaql side use this information to perform bulk data transfers using shared files. For instance, when transferring results of a Jaql query to R, a Jaql wrapper materializes the result into a file in a format that can be directly loaded into R, and passes the location of this file to the R process through JNI. A wrapper on the R side reads this file and constructs an R object from the data, which can then be used for further R processing and analysis. This indirection through files (or even through shared memory) results in orders-of-magnitude improvement in performance&mdash;the forego- ing 17K tuple transfer now takes only a few seconds. A similar approach is used for transferring data from R to Jaql. </span></p><p class="c118"><span class="c0">Handling data-representation incompatibilities In R, large data objects resembling relational tables are primarily manipulated as data frames, which are column-oriented structures. On the other </span></p><p class="c7 c31"><span class="c0">Property Value </span><span class="c10">mapred.child.java.opts </span><span class="c0">-Xmx700m </span><span class="c10">io.sort.factor </span><span class="c0">100 </span><span class="c10">io.file.buffer.size </span><span class="c0">1048576 </span><span class="c10">io.sort.mb </span><span class="c0">256 </span><span class="c10">mapred.reduce.parallel.copies </span><span class="c0">20 </span><span class="c10">mapred.job.reuse.jvm.num.tasks </span><span class="c0">-1 </span><span class="c10">fs.inmemory.size.mb </span><span class="c0">100 </span></p><p class="c88"><span class="c0">Table 1: Hadoop configuration used in experiments </span></p><p class="c46"><span class="c0">hand, Jaql and Hadoop are primarily row-oriented. Surprisingly, when large R objects are serialized in a row-oriented format, the cost within R of reading these objects from disk (or from any I/O channel) grows super-linearly in the number of columns transferred. This realization led us to design the bridge to perform the transla- tion from row-oriented to column-oriented formats and vice versa. The Jaql wrapper translates the row-oriented results and outputs one file per column of data, while the wrapper on the R side reads the files one column at a time, and constructs the data frame from the individual columns. This approach ensures that cost of data transfer is linear with respect to the number of rows and the num- ber of columns. Similar approaches were used to circumvent the remaining incompatibilities between R and JSON objects. </span></p><p class="c15"><span class="c8">5. EXPERIMENTAL STUDY </span></p><p class="c42"><span class="c0">In this section, we report experimental results and share our ex- periences in implementing the latent factor model for movie recom- mendations. Our discussion focuses on performance and scalability aspects, although, as we have argued, Ricardo&rsquo;s technical approach has many advantages beyond simply providing a workable solution. </span></p><p class="c164"><span class="c8">5.1 Test Setup </span></p><p class="c17"><span class="c0">All performance experiments were conducted on a cluster of 50 nodes in the Amazon Elastic Compute Cloud (EC2). Each node was of type &ldquo;High CPU Extra Large,&rdquo; i.e., a 64-bit virtual machine with 7 GB of memory, 20 EC2 Compute Units CPU capacity (8 vir- tual cores with 2.5 EC2 Compute Units each), and 1690GB of lo- cal storage. Aggregated over the cluster, this amounts to about 400 CPU cores, 70TB of aggregated disk space, and 350 GB of main memory. We installed Hadoop version 0.20.0 on each of the nodes. One of the nodes was selected as master and ran the Hadoop name- node and jobtracker processes; the rest of the nodes were worker nodes and provided both storage and CPU. Each worker node was configured to run up to seven concurrent map tasks and one reduce task. This map-heavy configuration was chosen because the bulk of the processing is performed during the map phase; reducers merely aggregate the map output. Table 1 lists other Hadoop parameters that we chose; the remaining parameters match the default config- uration of Hadoop. The R driver itself was running on a separate EC2 gateway node of type &ldquo;Standard Extra Large&rdquo;, which differs from the remaining nodes in that it had more memory (15GB) but less CPU power (8 EC2 Compute Units: 4 virtual cores with 2 EC2 Compute Units each). </span></p><p class="c32"><span class="c0">Our experiments were performed on &ldquo;inflated&rdquo; versions of the original Netflix competition dataset [5]. The original dataset con- tains data for 100M ratings, including anonymized customer iden- tifiers (about 480K distinct), information about the movies (such as title and date of release; about 18K distinct), and the date that the rating was entered. The inflation process allowed us to evaluate the performance of Ricardo on datasets having real-world scales. </span></p><p class="c65"><span class="c28">995 </span></p><p class="c7"><span class="c0">Number of Rating Tuples Data Size in GB </span></p><p class="c16"><span class="c0">500M 104.33 1B 208.68 3B 625.99 5B 1,043.23 </span></p><p class="c7"><span class="c0">Table 2: Data set sizes used in the scale out experiments </span></p><p class="c16"><span class="c0">Specifically, we added six additional attributes to each tuple in or- der to simulate a real recommender system, where the logged in- formation contains fine-grained information such as demographics and navigation history. We also increased the number of ratings, with the largest inflated dataset containing 5B ratings (roughly 1TB of raw data). Table 2 summarizes the various dataset sizes used in the scale-up experiments. The numbers of distinct customers and distinct movies were retained from the original data, which amounts to a space requirement of about 4MB per factor of the model. </span></p><p class="c7"><span class="c8">5.2 Performance and Scalability </span></p><p class="c16"><span class="c0">Virtually all of the processing time for computing the latent fac- tor model was spent on the Jaql/Hadoop side, rather than in R. This is not surprising, since scanning, joining, and aggregating large datasets is both I/O and CPU intensive, whereas R&rsquo;s adjustment of latent-factor values during the optimization procedure is compa- rably cheap. Nevertheless, the total system performance depends on the time required per iteration over the data and on the number of iterations until convergence. Thus both the DMS and R play an integral part in overall performance. </span></p><p class="c16"><span class="c0">Figure 4 shows the performance results for a single iteration&mdash; i.e., the computation of the squared error and the corresponding gradient&mdash;for various dataset sizes. Recall that we presented the latent factor model using only a single latent factor per user and per movie. In the experiments, we actually fit a vector of latent factors per user and per movie during a single pass. For simplic- ity we omit the details, since the overall response time was only slightly affected by the vector size. We used two different imple- mentations: a baseline implementation using a hand-tuned Hadoop job and a Jaql implementation as used by Ricardo. As can be seen in the figure, both implementations scale linearly with the size of the dataset and thus provide a scalable solution. The execution plans used by raw Hadoop and Jaql are similar, but Jaql requires about twice as much time as raw Hadoop. The reason for this dif- ference lies in Jaql&rsquo;s higher level of abstraction: Jaql is designed to handle arbitrary queries, data formats, and data types, whereas our Hadoop implementation is specifically tailored to the problem at hand. This is also reflected in code complexity: the Jaql code comprises roughly 10 lines per query, whereas the Hadoop imple- mentation takes around 200 lines. </span></p><p class="c16"><span class="c0">The execution times as shown in Figure 4 should be taken with a grain of salt, as both Hadoop and Jaql are under active develop- ment. Each iteration currently takes a relatively large amount of time, up to an hour for large (1TB) data. We believe that with in- creasing maturity, both the relative difference between systems as well as the total execution time will decrease further. Indeed, our initial work on Ricardo has already led to multiple improvements in Jaql, e.g., better schema support, efficient serialization code, and runtime tuning. </span></p><p class="c7"><span class="c26">&bull; </span><span class="c27">&bull; </span><span class="c94">Jaql </span></p><p class="c7"><span class="c1">s dnocesn ie mi</span><span class="c18">T</span><span class="c1">0 </span></p><p class="c7"><span class="c1">0520051</span><span class="c18">Hadoop (handtuned) </span></p><p class="c7"><span class="c26">&bull; </span></p><p class="c7"><span class="c1">005</span><span class="c26">&bull; </span></p><p class="c7"><span class="c26">&bull; </span><span class="c18">01 2 3 4 5 </span></p><p class="c7"><span class="c18">Number of ratings in billions </span></p><p class="c7"><span class="c0">Figure 4: Performance comparison (single iteration) </span></p><p class="c7"><span class="c26">&bull; </span></p><p class="c7"><span class="c18">Conjugate gradient L&minus; BFGS </span></p><p class="c7"><span class="c18">0 5 10 15 20 </span></p><p class="c7"><span class="c0">Figure 5: Number of passes and RMSE using one latent factor </span></p><p class="c7"><span class="c8">5.3 Optimization Evaluation </span></p><p class="c16"><span class="c0">We experimented with many different optimization routines, in- cluding naive gradient descent as well as more sophisticated al- gorithms such as the conjugate-gradient method or L-BFGS. This experimentation was easy to perform, since we could directly and easily take advantage of R&rsquo;s large optimization library.</span><span class="c68">5 </span></p><p class="c16"><span class="c0">Figure 5 displays the root-mean-squared error (RMSE) of the la- tent-factor model as a function of the number of iterations of the optimization algorithm, i.e., the number of passes over the data. Results are given for both the conjugate-gradient and L-BFGS op- timization methods. The conjugate gradient method performs a gradient computation followed by a line search and thus requires multiple passes over the data to arrive at a new point estimate. The line searches account for the fluctuations in the RMSE curve that can be seen in Figure 5. In contrast, L-BFGS is a quasi-Newton method with low memory footprint and exhibits a completely dif- ferent behavior. Both methods converge, but L-BFGS converges faster for this particular problem. </span></p><p class="c7"><span class="c12">5</span><span class="c75">http://cran.r-project.org/web/views/Optimization.html </span></p><p class="c7"><span class="c28">996 </span></p><p class="c7"><span class="c1">r orree rauqsn aemt oo</span><span class="c18">R</span><span class="c60">&bull; </span><span class="c1">0 1.150.1</span><span class="c26">&bull; </span></p><p class="c7"><span class="c26">&bull; </span></p><p class="c7"><span class="c26">&bull; </span><span class="c27">&bull; </span></p><p class="c7"><span class="c26">&bull; </span></p><p class="c7"><span class="c26">&bull; </span><span class="c60">&bull; </span></p><p class="c7"><span class="c26">&bull; </span><span class="c60">&bull; </span><span class="c26">&bull; </span><span class="c27">&bull; </span></p><p class="c7"><span class="c1">00.</span><span class="c18">1Number of passes </span></p><p class="c7 c166"><span class="c8">6. RELATED WORK </span></p><p class="c121"><span class="c0">This section discusses prior work on alternative approaches to Ricardo. Unlike Ricardo, these approaches do not trade data and functionality between a DMS and a statistical package, but rather try to either scale out R or incorporate deep analytics into a DMS. </span><span class="c8">6.1 Scaling Out R </span></p><p class="c22"><span class="c0">The R community is very active in improving the scalability of R and there are dozens of approaches that aim at parallelizing R across multiple processors or nodes. The main drivers for this work are increased dataset sizes and the increasing demands of scientific research and high-performance computing [21]. </span></p><p class="c72"><span class="c0">The existing approaches to parallelizing R can be classified by their degree of abstraction. These range from low-level message passing to task- and data-parallel processing to high-level (largely) automatic parallelization. </span></p><p class="c3"><span class="c0">Packages of the message-passing type include Rmpi and rpvm, both of which are wrappers for their respective parallel computing middlewares. Direct use of message-passing systems requires sig- nificant expertise in parallel programming. More importantly, these packages require a &ldquo;tightly&rdquo; connected cluster, where fault toler- ance, redundancy and elasticity are not provided. This restriction limits scalability to relatively small clusters. In contrast, Hadoop (and hence Ricardo), provides all of the foregoing functionality, and hence can scale to very large clusters. </span></p><p class="c3"><span class="c0">Higher-level task- and data-parallel computing systems for par- allelizing R are usually built on top of a message-passing package, and are easier to use. The most popular representative R package of this type is SNOW [24] (for Simple Network Of Workstations). SNOW provides functionality to spawn a cluster (using, for exam- ple, MPI), to distribute values across the cluster, and to apply in par- allel a given function to a large set of alternative arguments. SNOW can be used to implement task parallelism (arguments are tasks) or data parallelism (arguments are data). SNOW&rsquo;s data-parallelism functionality is similar to the map operator in Hadoop; reduce op- erations, however, require more elaborate solutions. </span></p><p class="c72"><span class="c0">In general, SNOW appears to be too low-level for conveniently implementing scalable deep analytics. This assertion is based on our experience in using the SNOW package to implement the com- putation of the latent-factor model for movie recommendations. The implementation involved the following steps: </span></p><p class="c105"><span class="c0">1. Distribution of data. The base data had to be distributed across the cluster so that worker nodes can access it. This was achieved by using a distributed file system and by split- ting the data into smaller chunks, which are then handled by the individual nodes. </span></p><p class="c100"><span class="c0">2. Data processing. Operations such as the filter, group-by and </span></p><p class="c115"><span class="c0">join were implemented in R. </span></p><p class="c5"><span class="c0">3. Parallelization. The data processing operations were broken down into fragments, executed in parallel, and then the re- sults where combined. </span></p><p class="c62"><span class="c0">It is evident that the above steps constitute the core functionality of a large-scale DMS. Our implementation of latent-factor computa- tion using SNOW required 50% more code than the implementa- tion in Ricardo, while at the same time being much more compli- cated and error prone. Additional features such as scalability to a large number of nodes, fault tolerance and elasticity were so tedious to implement using R and SNOW that they were dropped from our implementation. A recent package called RHIPE [12] provides SNOW-like functionality using Hadoop as the runtime environment </span></p><p class="c16 c147"><span class="c0">and thus alleviates some of these problems. Data processing oper- ators and their parallelization are not yet implemented, however. Thus, analysis tasks such as latent-factor modeling are currently easier to implement in Ricardo, and the Ricardo implementations have better runtime properties. </span></p><p class="c108"><span class="c0">At the highest level, one would like a system that supports auto- matic parallelization of high-level R commands. Recently, there have been some promising results in this direction. Not all se- quential R code can be parallelized automatically, of course, but linear-algebra operations can be handled by some systems, includ- ing pR [20] and RIOT [26]. Both packages are similar to Ricardo in spirit, but instead of offloading data processing, they offload par- allel matrix computations to systems specifically designed for this purpose. </span><span class="c8">6.2 Deepening a DMS </span></p><p class="c42"><span class="c0">Whereas the R community has tried to incorporate large-scale DMS functionality, the DMS community has tried to incorporate analytics. Early efforts in this direction incorporated some sim- ple statistical analyses into parallel relational database systems [1]; indeed, the SQL standard now covers computation of basic sum- mary statistics such as standard deviation and covariance, as well as simple (single-variable) linear regression. Moreover, the ma- jor commercial vendors have all incorporated some simple vendor- specific analytic extensions. Cohen et al. [9] describe an attempt to incorporate into a relational DBMS such analytical functionality as vector and matrix operations, ordinary least squares, statistical tests, resampling techniques, and parameter fitting via the conju- gate-gradient method. The implementation exploits the extensible datatype facilities of Postgres. Although the resulting SQL queries are closer to what an analyst would use than classical SQL, it is un- clear whether an analyst would ever be truly comfortable enough with the quasi-SQL syntax to abandon the familiar constructs of R, SPSS, SAS, or Matlab. The statistical functionality would probably need to be hidden in stored procedures. </span></p><p class="c32"><span class="c0">Moving away from relational databases, we come to the Mahout project [2], which directly implements various machine-learning algorithms in Hadoop. The Mahout implementation currently nei- ther exploits high-level data processing languages built on top of Hadoop nor does it make use of any statistical software. However, Mahout is still at an early stage. As more and more sophisticated methods are added, leveraging such existing functionality adds to the stability and simplicity of the implementation. </span></p><p class="c30"><span class="c0">Arguably, the functionality of statistically enriched DMSs will always lag behind that of statistical software packages, and will not enjoy the same level of community support as can be found, for example, in the CRAN repository. Statistical packages also pro- vide analysts with a more comfortable interface to access this func- tionality. In contrast, expressing statistical functionality in a query language is tedious. For example, the SQL implementation of the Mann-Whitney U Test in [9] is much more complex and difficult to read than a call to a function like </span><span class="c10">wilcox.test </span><span class="c0">as provided by R. For these reasons, it is highly likely that data analysts will always want to work with statistical software. </span></p><p class="c117"><span class="c0">We conclude this section by mentioning SciDB [22]. This sys- tem embodies a very interesting and radical approach that attempts to completely redesign the data model&mdash;by using multidimensional arrays for storage and making vectors and arrays first-class objects&mdash; and scale both data management and computation by executing functions and procedures in parallel and as close to the data as pos- sible. </span></p><p class="c162"><span class="c28">997 </span></p><p class="c7 c93"><span class="c8">7. CONCLUSION </span></p><p class="c35"><span class="c0">We have presented Ricardo, a scalable platform for deep analyt- ics. Ricardo combines the data management capabilities of Hadoop and Jaql with the statistical functionality provided by R. Compared to previous approaches, Ricardo provides a feature-rich and scal- able working environment for statistical computing, and benefits from decades of experience from both the statistical and the data management communities. Although our experiments indicated that the performance of both Hadoop and Jaql is still suboptimal, we expect significant performance improvements in the future as this technology matures. Our work has focused on Hadoop and R, but it is evident that our approach is potentially applicable to other statistical packages and other DMSs. </span></p><p class="c3"><span class="c0">In ongoing work, we are identifying and integrating additional statistical analyses that are amenable to the Ricardo approach. Al- though Ricardo currently requires knowledge of the Jaql query lan- guage, a tighter and transparent integration into the R language is possible using packages and functions that hide the underly- ing data-management details from the analyst. For certain types of analyses, it may be possible to combine our work with auto- matic compilation technology to integrate R and Hadoop even more seamlessly. Our ultimate vision is to enable analysts to work with massive data just as they work with main-memory-size data today; Ricardo can be seen as a first step in this direction. </span></p><p class="c103"><span class="c8">8. REFERENCES </span></p><p class="c36"><span class="c0">[1] N. R. Alur, P. J. Haas, D. Momiroska, P. Read, N. H. </span></p><p class="c160"><span class="c0">Summers, V. Totanes, and C. Zuzarte. DB2 UDB&rsquo;s High Function Business Intelligence in e-Business. IBM Redbook Series, ISBN 0-7384-2460-9, 2002. [2] Apache Mahout. </span><span class="c10">http://lucene.apache.org/mahout/</span><span class="c0">. [3] M. Avriel. Nonlinear Programming: Analysis and Methods. </span></p><p class="c73"><span class="c0">Dover Publishing, 2003. [4] R. Bell, Y. Koren, and C. Volinsky. Modeling relationships at </span></p><p class="c120"><span class="c0">multiple scales to improve accuracy of large recommender systems. In KDD, pages 95&ndash;104, 2007. [5] J. Bennett and S. Lanning. The Netflix prize. In KDD Cup </span></p><p class="c66"><span class="c0">and Workshop, 2007. [6] R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited memory </span></p><p class="c55"><span class="c0">algorithm for bound constrained optimization. SIAM J. Sci. Comput., 16(5):1190&ndash;1208, 1995. [7] J. Canny. Collaborative filtering with privacy via factor </span></p><p class="c109"><span class="c0">analysis. In SIGIR, pages 238&ndash;245, 2002. [8] C.-T. Chu, S. K. Kim, Y.-A. Lin, Y. Yu, G. R. Bradski, A. Y. Ng, and K. Olukotun. Map-Reduce for machine learning on multicore. In NIPS, pages 281&ndash;288, 2006. [9] J. Cohen, B. Dolan, M. Dunlap, J. M. Hellerstein, and </span></p><p class="c7 c106"><span class="c0">C. Welton. MAD skills: New analysis practices for big data. PVLDB, 2(2):1481&ndash;1492, 2009. [10] J. Cunningham. Hadoop@Visa, Hadoop World NY, 2009. </span></p><p class="c43"><span class="c10">http://www.slideshare.net/cloudera/ hw09-large-scale-transaction-analysis</span><span class="c0">. [11] J. Dean and S. Ghemawat. MapReduce: simplified data </span></p><p class="c58 c71"><span class="c0">processing on large clusters. In OSDI, pages 137&ndash;150, 2004. [12] S. Guha. RHIPE - R and Hadoop Integrated Processing </span></p><p class="c58"><span class="c0">Environment. </span><span class="c10">http://ml.stat.purdue.edu/rhipe/</span><span class="c0">. [13] K. Hornik. The R FAQ, 2009. </span></p><p class="c82"><span class="c10">http://CRAN.R-project.org/doc/FAQ/R-FAQ.html</span><span class="c0">. [14] JAQL: Query Language for JavaScript Object Notation </span></p><p class="c40"><span class="c0">(JSON). </span><span class="c10">http://code.google.com/p/jaql</span><span class="c0">, 2009. [15] J. A. Konstan. Introduction to recommender systems. In </span></p><p class="c130"><span class="c0">SIGMOD Conference, pages 1373&ndash;1374, 2008. [16] Y. Koren. Factorization meets the neighborhood: a </span></p><p class="c33"><span class="c0">multifaceted collaborative filtering model. In KDD, pages 426&ndash;434, 2008. [17] Y. Koren. Collaborative filtering with temporal dynamics. In </span></p><p class="c165"><span class="c0">KDD, pages 447&ndash;456, 2009. [18] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization </span></p><p class="c154"><span class="c0">techniques for recommender systems. Computer, 42(8):30&ndash;37, 2009. [19] C. Olston, B. Reed, U. Srivastava, R. Kumar, and </span></p><p class="c106 c151"><span class="c0">A. Tomkins. Pig latin: a not-so-foreign language for data processing. In SIGMOD, pages 1099&ndash;1110, 2008. [20] N. F. Samatova. pR: Introduction to Parallel R for Statistical </span></p><p class="c140"><span class="c0">Computing. In CScADS Scientific Data and Analytics for Petascale Computing Workshop, pages 505&ndash;509, 2009. [21] M. Schmidberger, M. Morgan, D. Eddelbuettel, H. Yu, </span></p><p class="c156"><span class="c0">L. Tierney, and U. Mansmann. State of the art in parallel computing with R. Journal of Statistical Software, 31(1):1&ndash;27, June 2009. [22] M. Stonebraker, J. Becla, D. J. DeWitt, K.-T. Lim, D. Maier, O. Ratzesberger, and S. B. Zdonik. Requirements for science data bases and SciDB. In CIDR, 2009. [23] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, </span></p><p class="c116"><span class="c0">S. Anthony, H. Liu, P. Wyckoff, and R. Murthy. Hive - a warehousing solution over a Map-Reduce framework. PVLDB, 2(2):1626&ndash;1629, 2009. [24] L. Tierney, A. J. Rossini, N. Li, and H. Sevcikova. snow: </span></p><p class="c123"><span class="c0">Simple network of workstations. </span><span class="c10">http://cran.r-project.org/web/packages/snow/</span><span class="c0">. [25] M. Wedel, R. T. Rust, and T. S. Chung. Up close and </span></p><p class="c52"><span class="c0">personalized: a marketing view of recommendation systems. In RecSys, pages 3&ndash;4, 2009. [26] Y. Zhang, H. Herodotou, and J. Yang. RIOT: I/O-efficient numerical computing without SQL. In CIDR, 2009. </span></p><p class="c110"><span class="c28">998 </span></p></body></html>