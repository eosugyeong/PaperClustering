<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c39{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.8pt;font-family:"Arial";font-style:normal}.c107{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:5.4pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c86{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Arial";font-style:normal}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.4pt;font-family:"Arial";font-style:normal}.c50{color:#000000;font-weight:700;text-decoration:none;vertical-align:sub;font-size:12.1pt;font-family:"Arial";font-style:normal}.c56{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.2pt;font-family:"Arial";font-style:normal}.c49{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:11.4pt;font-family:"Arial";font-style:normal}.c87{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:3.8pt;font-family:"Arial";font-style:normal}.c132{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:11.6pt;font-family:"Arial";font-style:normal}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.8pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.1pt;font-family:"Arial";font-style:normal}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:7.2pt;font-family:"Arial";font-style:normal}.c68{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.1pt;font-family:"Arial";font-style:normal}.c11{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:11.5pt;font-family:"Arial";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.8pt;font-family:"Arial";font-style:normal}.c30{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Courier New";font-style:normal}.c42{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:5.8pt;font-family:"Arial";font-style:normal}.c32{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.8pt;font-family:"Arial";font-style:normal}.c29{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:7.6pt;font-family:"Arial";font-style:normal}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.5pt;font-family:"Arial";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.9pt;font-family:"Arial";font-style:normal}.c47{margin-left:-17.6pt;padding-top:1.2pt;text-indent:35.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.5pt}.c80{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:6.8pt;font-family:"Arial";font-style:normal}.c105{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:18.3pt;font-family:"Arial";font-style:normal}.c46{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.8pt;font-family:"Arial";font-style:normal}.c27{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.8pt;font-family:"Arial";font-style:normal}.c33{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c104{color:#000000;font-weight:700;text-decoration:none;vertical-align:sub;font-size:13.2pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.9pt;font-family:"Arial";font-style:normal}.c58{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.8pt;font-family:"Arial";font-style:normal}.c65{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.3pt;font-family:"Arial";font-style:normal}.c94{color:#000000;font-weight:700;text-decoration:none;vertical-align:sub;font-size:12pt;font-family:"Arial";font-style:normal}.c101{margin-left:-22.3pt;padding-top:1pt;text-indent:35.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9pt}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.7pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:normal}.c128{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:normal}.c89{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c21{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.1pt;font-family:"Arial";font-style:normal}.c73{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:6pt;font-family:"Arial";font-style:normal}.c76{margin-left:-26.6pt;padding-top:1.4pt;text-indent:35.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-15.4pt}.c110{margin-left:-17.6pt;padding-top:1.2pt;text-indent:35.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:12.2pt}.c45{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.3pt;font-family:"Arial";font-style:normal}.c119{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Courier New";font-style:normal}.c75{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:7.6pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c79{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.4pt;font-family:"Arial";font-style:normal}.c36{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c35{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c69{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.2pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c117{margin-left:-17.6pt;padding-top:1pt;text-indent:35.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-22.1pt}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.3pt;font-family:"Arial";font-style:normal}.c67{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:9.9pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:normal}.c108{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.9pt;font-family:"Arial";font-style:normal}.c54{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.6pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c93{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c92{margin-left:-17.6pt;padding-top:3.8pt;text-indent:26.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.5pt}.c22{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:12pt;font-family:"Arial";font-style:normal}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.6pt;font-family:"Arial";font-style:normal}.c88{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:12.1pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c82{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Arial";font-style:normal}.c59{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:7.2pt;font-family:"Arial";font-style:normal}.c44{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c55{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c85{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.4pt;font-family:"Arial";font-style:normal}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8pt;font-family:"Arial";font-style:normal}.c51{margin-left:-26.6pt;padding-top:1.7pt;text-indent:35.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-15.9pt}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.1pt;font-family:"Arial";font-style:normal}.c43{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.4pt;font-family:"Arial";font-style:normal}.c118{margin-left:-17.6pt;padding-top:1.7pt;text-indent:26.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.5pt}.c53{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.3pt;font-family:"Arial";font-style:normal}.c66{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:9.6pt;font-family:"Arial";font-style:normal}.c84{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:9.7pt;font-family:"Arial";font-style:normal}.c41{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:6.6pt;font-family:"Arial";font-style:normal}.c37{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.3pt;font-family:"Arial";font-style:normal}.c130{margin-left:-26.6pt;padding-top:4.1pt;text-indent:35.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-15.4pt}.c70{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8pt;font-family:"Arial";font-style:normal}.c63{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.9pt;font-family:"Arial";font-style:normal}.c74{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:3.6pt;font-family:"Arial";font-style:normal}.c61{margin-left:-17.6pt;padding-top:8.9pt;text-indent:26.7pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-24.5pt}.c81{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.8pt;font-family:"Arial";font-style:normal}.c122{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.3pt;font-family:"Arial";font-style:normal}.c64{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.6pt;font-family:"Arial";font-style:normal}.c71{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c62{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:11.3pt;font-family:"Arial";font-style:normal}.c131{margin-left:-26.6pt;padding-top:22.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.2pt}.c100{margin-left:-22.3pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.4pt}.c97{margin-left:-17.5pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-0.3pt}.c102{margin-left:-17.6pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:96.2pt}.c91{margin-left:-17.6pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-21.6pt}.c103{margin-left:-26.6pt;padding-top:14.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-1.3pt}.c129{margin-left:-17.6pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7.9pt}.c114{margin-left:-17.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.2pt}.c25{margin-left:-17.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-21.4pt}.c109{margin-left:-26.6pt;padding-top:9.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.4pt}.c14{margin-left:-28.8pt;padding-top:9.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-15.4pt}.c126{margin-left:-17.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-20.2pt}.c38{margin-left:-17.6pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-23pt}.c96{margin-left:-17.6pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-24.2pt}.c95{margin-left:-22.3pt;padding-top:1.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:14.6pt}.c52{margin-left:-17.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-23.8pt}.c124{margin-left:218.2pt;padding-top:36.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c115{margin-left:-17.6pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12.5pt}.c116{margin-left:-17.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:3.8pt}.c121{margin-left:-17.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12.7pt}.c77{margin-left:218.2pt;padding-top:25.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c57{margin-left:-17.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17pt}.c72{margin-left:-22.3pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:0.2pt}.c78{margin-left:-19.8pt;padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-24.5pt}.c112{margin-left:-17.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:5.5pt}.c17{margin-left:-17.6pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-7.7pt}.c99{margin-left:-17.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.5pt}.c83{margin-left:-19.8pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-24.5pt}.c133{margin-left:-17.6pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9.8pt}.c111{margin-left:-17.6pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-24.5pt}.c125{margin-left:-17.6pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-24.2pt}.c90{margin-left:-28.8pt;padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-15.4pt}.c113{margin-left:0.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.3pt}.c98{margin-left:-26.6pt;padding-top:22.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-15.4pt}.c120{margin-left:-17.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-13.9pt}.c106{margin-left:-13.3pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18.7pt}.c60{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c34{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c134{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c48{margin-left:-13.3pt;margin-right:-18pt}.c127{margin-left:-8.2pt;margin-right:-24.5pt}.c123{text-indent:35.4pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c134"><p class="c8"><span class="c89">Dismantling Complicated Query Attributes with Crowd </span></p><p class="c8"><span class="c44">Matan Laadan </span><span class="c33">Tel Aviv University </span><span class="c93">matanlaa@post.tau.ac.il </span></p><p class="c8"><span class="c33">Tel Aviv, Israel </span><span class="c44">Tova Milo </span><span class="c33">Tel Aviv University </span><span class="c93">milo@cs.tau.ac.il </span><span class="c33">Tel Aviv, Israel </span><span class="c44">ABSTRACT </span><span class="c0">We study the problem of query evaluation with the help of the crowd, when the value of the queried attributes is not available in the database and is also hard for the crowd to estimate. Rather than asking users directly about these at- tributes, we propose a novel alternative approach that first uses the crowd to dismantle the query attributes into finer related ones (whose value estimation is easier), then assem- ble them to yield better estimation for the query attributes. We show that it is sometimes beneficial not to only disman- tle the query attributes themselves, but rather to continue dismantling newly discovered attributes. We provide a care- ful statistical analysis to estimate the potential benefit (and cost) of dismantling each of the so-far-discovered attributes. Building on this analysis, we present an effective algorithm that balances between attributes dismantling and obtaining essential statistics about them (for estimating properties like &ldquo;difficulty&rdquo; and &ldquo;contribution&rdquo; of attributes) to decide how many crowd members should be asked about each attribute and how the answers should be assembled together. A thor- ough experimental analysis demonstrates the feasibility and effectiveness of the approach. </span></p><p class="c8"><span class="c44">1. INTRODUCTION </span></p><p class="c34"><span class="c0">We consider the problem of query evaluation with the help of the crowd, when the value of the query attributes is hard to estimate. Rather than asking users directly about these attributes, we propose a novel alternative approach that first uses the crowd to dismantle the query attributes into finer related ones (whose value estimation is easier), then assem- ble them to yield better estimation for the query attributes. To illustrate, assume that we want to evaluate a query over a database of objects, testing and retrieving the val- ues of certain attributes. This is a standard task when the attribute values are available explicitly in the database, but becomes challenging when they are not. For example, consider an imaginary cooking website CrowdCooking.com (CC) - a large recipes website where people can post their </span></p><p class="c15"><span class="c4">(c) 2015, Copyright is with the authors. Published in Proc. 18th Inter- national Conference on Extending Database Technology (EDBT), March 23-27, 2015, Brussels, Belgium: ISBN 978-3-89318-067-7, on OpenPro- ceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0. </span></p><p class="c15"><span class="c0">own recipes and other people can search and use them. Up until now, CC only allowed basic keyword search, but they now wish to upgrade their search capabilities to in- clude more sophisticated searches, allowing people to search, e.g., for dessert recipes that are easy to make, have less than X calories and contain a certain amount of proteins. While NLP/text-analysis techniques could be used to eval- uate some of these search criteria, this may be costly and inaccurate. An alternative approach that emerged in recent years is to use the crowd of web users for finding the value of the missing query attributes - Is a dish a dessert or not? How many calories/proteins does it contains? Etc. As crowd an- swers may be erroneous, a common approach is to ask mul- tiple users about each missing query attribute and compute some aggregation (usually average/median) of the answers. The number of crowd members that need to be asked about a given attribute is typically determined by the difficulty of the question and the budget constraints</span><span class="c36">1</span><span class="c0">. For example, three users probably suffice to determine with a high prob- ability that a certain dish is a dessert, but more are likely to be required to determine its number of calories. In fact, a key problem of this approach is that some attributes (like protein amount) are so difficult or un-intuitive for the crowd to evaluate, that the convergence to the final answer might be slow and thus require high budget [31]. Another disad- vantage is that query attributes are handled separately and potential mutual information (for example between dessert and calories) which could be used to reduce the number of required questions, or to improve accuracy, is ignored. </span></p><p class="c15"><span class="c0">A first solution to this problem was proposed in [27]. In- stead of asking the crowd directly about the attributes men- tioned in the query, it was suggested to also ask for the value of other (usually simpler) related attributes, and then derive the value of the query attributes from the answers. For example, to estimate the amount of protein in a certain dish one may ask what quantities of high protein ingredi- ents (such as meat, dairy, eggs, nuts and soy) does it con- tains. In this solution, queries are processed in two steps: (1) An o</span><span class="c55">ffl</span><span class="c0">ine preprocessing phase that, given a query, de- termines which object&rsquo;s attributes should be asked about, how many users should estimate each of those attributes and how the obtained values should be assembled together, and (2) an online query evaluation phase, where each object in the database is processed using the scheme derived in the o&#9999;ine step. For example, consider a query about the protein </span></p><p class="c8"><span class="c13">1</span><span class="c1">In common crowdsourcing platforms, crowd questions have some (small) monetary cost, and thus the number of ques- tions per object is typically bounded by the allocated budget </span></p><p class="c8"><span class="c35">409 10.5441/002/edbt.2015.36 </span></p><p class="c98"><span class="c0">amount in recipes in CC. Assuming a budget of 20 questions per recipe, the preprocessing phase may derive a formula of the form: 0.5protein amount</span><span class="c36">(10) </span><span class="c0">+ 0.13grams of meat</span><span class="c36">(3) </span><span class="c0">+ 0.15grams of dairy</span><span class="c36">(4) </span><span class="c0">+ 3number of eggs</span><span class="c36">(3)</span><span class="c0">. In the formula, attribute name</span><span class="c36">(n) </span><span class="c0">denotes the average value of n users an- swers when asked about the given attribute. The formula indicates that rather than using the full budget to ask users directly about protein amount, a better estimation would be obtained by asking only 10 crowd members, then averaging the derived value with a linear combination of estimated values for three other related attributes - grams of meat, grams of dairy, number of eggs (computed by asking 3, 4 and 3 crowd members about them, respectively). We will explain later how such formulas are derived. </span></p><p class="c76"><span class="c0">While this approach is shown to provide results supe- rior to those obtained by asking the same overall number of questions only about the query attributes [27], a great obstacle is that it requires the use a domain expert that provides the list of related attributes for each query. This use of experts-in-the-loop limits the scalability of the ap- proach and its applicability to fully-automated crowd-only platforms. In contrast, in the present paper, we provide a solution which is entirely crowd-based. Our goal is to replace the domain-expert by the Wisdom of Crowds, asking users to assist in &ldquo;dismantling&rdquo; difficult attributes and identifying those related attributes that can assist in query evaluation. Note that even harder related attributes may improve re- sults they force different ways for estimations, which is one of the foundations of the wisdom of crowds principle. </span></p><p class="c76"><span class="c0">As we will show, a successful solution will need to ad- dress two main challenges. The first is determining which object attributes should we best ask the crowd to disman- tle. We show that it is sometimes beneficial to not only dis- mantle the query attributes themselves, but rather to con- tinue dismantling newly discovered attributes. We provide a fine hypothetical analysis to estimate the potential ben- efit (and cost) of dismantling each of the so-far-discovered attributes and thereby determining which questions to ask the crowd. The second related challenge that we address is budget management. Given a budget (e.g., number of questions that can be asked to the crowd) for the o&#9999;ine preprocessing step, we need to use it both for dismantling the query attributes, as well as for obtaining some statistics about them (e.g., the distribution of user answers, the cor- relations between attributes value, etc.). Such statistics are required to estimate properties like &ldquo;difficulty&rdquo; and &ldquo;contri- bution&rdquo; of each attribute in order to decide how many crowd members should be asked about each attribute and how the answers should be assembled together. Our algorithm pro- vides a careful analysis that allows to balance the budget between these two complimentary tasks. </span></p><p class="c97"><span class="c0">We present in this article the following contributions: </span></p><p class="c90"><span class="c0">1. We propose a simple and generic model for modeling a database of objects with infinite unknown attribute names and values, the type of questions that can be posed to the crowd and the characteristics of those answers. </span></p><p class="c14"><span class="c0">2. Given an online per-object budget and an o</span><span class="c55">ffl</span><span class="c0">ine prepro- cessing budget, we use the model to present an algorithm that ideally uses the o&#9999;ine budget for deriving linear for- mulas (like the one illustrated above) that best exploit the online budget for deriving the values of query attributes. Our algorithm consists of five inter-related components </span></p><p class="c15 c127"><span class="c0">for which we explain what type of information is required and what crowd questions may be used to obtain it. We provide a generic black-box description for each compo- nent (which allows to plug-in different implementations) and propose a concrete implementation. </span></p><p class="c78"><span class="c0">3. Since the success of our framework depends on the dis- covery of relevant attributes, we focus our attention on this problem. We formally show how the potential gain (and cost) of each possible attribute dismantling ques- tion can be estimated and how this estimation can then be used to design an iterative algorithm that optimally chooses which crowd questions should be asked at each point. The estimation is based on a careful analysis of the already gathered information as well as on predic- tions about the potential effect of each question on the following algorithm components. </span></p><p class="c78"><span class="c0">4. Of particular challenge are queries with more than one attribute. There is a fine tradeoff here between the gain that one may obtain by discovering underlying correla- tions between attributes, and the cost (in terms of crowd questions) required for such discovery. Our algorithm uses a fine analysis of the current data to predict potential con- tributions and to balance the two. </span></p><p class="c83"><span class="c0">5. Finally, we present a thorough experimental analysis of our approach over two real-life data sets as well as syn- thetic data. We examine the various parts of our al- gorithm and its performance as a whole. We compare our algorithm to several existing/alternative approaches, showing that it consistently outperforms them in achiev- ing lower average error for the same budget. Our experi- ments also demonstrate the necessity of different parts of the algorithm for accurate attribute dismantling, which later translates to accurate attribute value estimation. </span></p><p class="c61"><span class="c0">The paper is organized as follows. The model and all rel- evant notations are presented in Section 2. To simplify the presentation we first consider in Section 3 the case where the query contains a single attribute. Queries with multiple attributes are then considered in Section 4. Our experimen- tal study is presented in Section 5. We discuss related work in Section 6 and conclude in Section 7. </span></p><p class="c102"><span class="c44">2. PRELIMINARIES </span></p><p class="c92"><span class="c0">We start by describing our model and notations, then for- mally define the problem that we study. </span></p><p class="c111"><span class="c3">Objects, attributes and queries. </span><span class="c0">Our data set consists of a set of objects. We use O to denote the possibly infinite domain of objects, and o, o</span><span class="c27">i </span><span class="c1">to denote an individual ob- </span><span class="c0">ject in O. In our running example, O is the set of all food recipes. An object may have attributes. We use A to de- note the domain of attribute names and a,a</span><span class="c27">i </span><span class="c1">to denote an </span><span class="c0">individual attribute name in A. We focus here on numer- ical attributes. Boolean attributes may be viewed here as numerical attributes with a value between 0 and 1, whereas multi-value attributes can be modeled by one such boolean attribute per value. In our running example, A includes all possible recipe properties such as time to prepare, is soup, is tasty, protein amount, is brown, number of eggs, etc. </span></p><p class="c118"><span class="c0">For an object o 2 O, an attribute name a 2 A, a set of objects O &#8674; O and a set of attribute names A &#8674; A, we use </span></p><p class="c77"><span class="c35">410 </span></p><p class="c15"><span class="c0">the following notations: (1) o.a denotes the value of the at- tribute a of the object o, (2) o.a</span><span class="c36">(&#8676;) </span><span class="c0">denotes an estimation of that value (3) o.A &#8984; {o.a | a 2 A} denotes the set of values for attributes in A of object o, (4) O.a &#8984; {o.a | o 2 O} is the set of values of attribute a of the objects in O. We will sometimes abuse notations and consider these sets as ran- dom variables over objects in O (to be explained later). Fi- nally (5) D</span><span class="c13">O&#8677;A </span><span class="c0">denotes a data table with rows correspond- ing to objects in O and columns to attributes in A, along with some representation for each object. </span></p><p class="c15"><span class="c0">Given a query Q over some data table D, we define A(Q) as the set of Q&rsquo;s query attributes. W.l.o.g one may think of Q as an SQL query and of A(Q) as the set of attribute names appearing in Q. In our running example Q might be, for in- stance, select number of calories, protein amount from CC where dessert=true. In this example, A(Q) = {is dessert, number of calories, protein amount}. As some attributes (and their values) may be missing from the data table D, we will need to learn them from the crowd. </span></p><p class="c8"><span class="c3">Crowd questions. </span><span class="c0">Crowd workers may be asked four types of questions: </span></p><p class="c15"><span class="c0">Attribute Value Questions (for brevity, value questions) - Here a crowd member is asked to provide an estimation of the value of an o.a. An example of a value question in our running example is showing a worker a recipe and asking her for the value of number of eggs. For an ob- ject o 2 O and an attribute a 2 A, o.a</span><span class="c36">(1) </span><span class="c0">denotes the random variable representing the estimation of one ran- dom worker&rsquo;s when asked about o.a. We use the </span><span class="c36">(1) </span><span class="c0">nota- tion also for estimations of groups of values (for example, O.A</span><span class="c36">(1) </span><span class="c0">&#8984; {o.a</span><span class="c36">(1) </span><span class="c0">| o 2 O, a 2 A}). </span></p><p class="c15"><span class="c0">Attribute Dismantling Questions (for brevity, disman- tling questions) - A crowd member is given here an at- tribute&rsquo;s name and requested to give another attribute&rsquo;s name that may provide some information about the value of the former. We assume (as later confirmed in our exper- iments) that workers are more likely to provide attributes that are correlative with the attribute in question. An example for a dismantling question may be which recipe&rsquo;s attribute may help estimate its number of calories. An answer may be is dietetic. For simplicity we assume that answers that refer to the same property (like large, big, grand) can be reasonably identified and merged to a single representative. This may be done, e.g., using a common thesaurus/NLP tools. (We will show however that our technique can work even without this). </span></p><p class="c15"><span class="c0">Dismantling Verification Questions (for brevity, verifi- cation questions) - Here we use crowd workers to verify that a previously suggested attribute a</span><span class="c27">i </span><span class="c1">may indeed help </span><span class="c0">in estimating the value of another attribute a</span><span class="c13">j</span><span class="c0">. An ex- ample for a verification question is does knowing if a dish is black may help in determining its number of calories. The likely crowd answer here is No. </span></p><p class="c15"><span class="c0">Example Questions - Here workers are given some at- tributes&rsquo; names and are asked to provide an example of an object o 2 O along with its values for the attributes. An example for such a question is asking a user to upload a recipe along with its calorie value. For simplicity we will assume below that the given value is the correct one (otherwise the it can be estimated via value questions). </span></p><p class="c8"><span class="c0">For all tasks we assume workers are independent and that spam filters are employed to avoid malicious workers. </span></p><p class="c15"><span class="c3">Other notations. </span><span class="c0">We further adopt and use some common notations. From statistics we use </span><span class="c55">E</span><span class="c13">X</span><span class="c0">[f(x)] for expectation, Var</span><span class="c27">X</span><span class="c1">[f(x)] for variance, &sigma;</span><span class="c27">X</span><span class="c1">(f(x)) for standard deviation, </span><span class="c0">Cov</span><span class="c13">X,Y </span><span class="c0">(f(X),g(Y )) for covariance and &rho;</span><span class="c13">X,Y </span><span class="c0">(f(x),g(y)) for correlation. The lower indexes specify the random variables and we omit them when they are clear from context. From algebra, we use M</span><span class="c36">T </span><span class="c0">to denote a matrix M&rsquo;s transpose, M </span><span class="c36">1 </span><span class="c0">to denote M&rsquo;s inverse and Diag(f(i)) for diagonal matrix where the i&rsquo;th value of the diagonal is equal to f(i). </span></p><p class="c15"><span class="c3">Problem definition. </span><span class="c0">A user allocates a per-object budget B</span><span class="c13">obj </span><span class="c0">which is the number of value questions that can be asked on a given object, in the online query evaluation phase, for estimating the value of the query attributes. To deter- mine how to best use this budget, the user also allocates a preprocessing budget B</span><span class="c27">prc </span><span class="c13">2</span><span class="c1">. An (o&#9999;ine) preprocessing phase </span><span class="c0">uses this to gather some information from the crowd (using the type of questions described above) and consequently de- rive a set of formulas of the form o.a</span><span class="c36">(&#8676;) </span><span class="c0">= </span><span class="c9">P</span><span class="c13">A </span><span class="c9">l</span><span class="c36">a</span><span class="c9">(a</span><span class="c36">i</span><span class="c9">)o.a</span><span class="c36">(b(a</span><span class="c5">i</span><span class="c36">)) </span><span class="c13">i </span><span class="c1">for each a 2 A(Q), which determine how objects should be </span><span class="c0">processed in the online query evaluation phase. The seman- tics of such formula is to first ask the crowd b(a</span><span class="c27">i</span><span class="c1">) value </span><span class="c0">questions about each attribute o.a</span><span class="c27">i</span><span class="c1">, then calculate the av- </span><span class="c0">erage answer for each o.a</span><span class="c27">i </span><span class="c1">(denoted o.a</span><span class="c13">(b(a</span><span class="c7">i</span><span class="c13">)) </span></p><p class="c34"><span class="c13">i </span><span class="c9">) and finally </span><span class="c0">calculate an estimation for o.a using a linear regression[12] with predictors l</span><span class="c27">a</span><span class="c1">(a</span><span class="c27">i</span><span class="c1">). An example of such a formula, for </span><span class="c0">the attribute protein amount, was given in the Introduction. The function b in the formulas determines how many ques- tions (if any) will be asked about each object&rsquo;s attribute, and intuitively reflects the &ldquo;difficulty&rdquo; of each attribute. Since the total value questions per object need to obey the B</span><span class="c27">obj </span><span class="c0">budget constraint, b must satisfy </span><span class="c9">P</span><span class="c13">a2A </span><span class="c9">b(a) &#63743; B</span><span class="c13">obj</span><span class="c0">. We call such function b a budget distribution of size B</span><span class="c27">obj</span><span class="c1">. </span></p><p class="c15"><span class="c0">For a given budget distribution function b and a linear regression formula l, we define an error in the estimation of a single attribute&rsquo;s value as Er(o.a</span><span class="c36">(&#8676;) </span><span class="c0">| b, l) = (o.a </span><span class="c1">P</span><span class="c27">A </span><span class="c0">l</span><span class="c27">a</span><span class="c1">(a</span><span class="c27">i</span><span class="c1">)o.a</span><span class="c13">(b(a</span><span class="c7">i</span><span class="c13">)) </span></p><p class="c15"><span class="c13">i </span><span class="c9">)</span><span class="c36">2</span><span class="c0">. We then define the error of an at- tribute estimation as the mean square error over all objects Er(O.a</span><span class="c36">(&#8676;) </span><span class="c0">| b, l) = </span><span class="c55">E</span><span class="c13">O</span><span class="c0">[Er(o.a</span><span class="c36">(&#8676;) </span><span class="c0">| b, l)] and the query er- ror as Er(Q(D)</span><span class="c36">(&#8676;) </span><span class="c0">| b, l) = </span><span class="c9">P</span><span class="c13">a2A(Q) </span><span class="c9">Er(O.a</span><span class="c36">(&#8676;) </span><span class="c0">| b, l). Note that for simplification we assume the errors of all attributes to be of equal importance. All our results also apply to a weighted error definition, as discussed later. Our goal here will be to minimize the query error Er(Q(D)</span><span class="c36">(&#8676;)</span><span class="c0">). Namely, to find b and l which minimize it, and to do this using at most a budget B</span><span class="c27">prc</span><span class="c1">. </span></p><p class="c8"><span class="c44">3. OUR SOLUTION </span></p><p class="c34"><span class="c0">We start by presenting a high-level informal description of our algorithm, what data do we collect and what is that data used for. Next, we provide a detailed formal descrip- tion of the functionality of the different components, as well as references to existing solutions for some of them. We then return to the components that are in the heart of our contribution and provide concrete novel solutions for them. To simplify the presentation we will assume below that the query contains only a single attribute, that we call the </span></p><p class="c8"><span class="c13">2</span><span class="c1">W.l.o.g. it is assumed that B</span><span class="c27">prc </span><span class="c1">&gt;&gt; B</span><span class="c27">obj </span></p><p class="c8"><span class="c35">411 </span></p><p class="c8"><span class="c0">target attribute, namely A(Q) = {a</span><span class="c27">t</span><span class="c1">}. We consider the gen- </span><span class="c0">eral case afterwards. We also assume that no attributes are initially available for the objects in the queried data table. For instance, in our running example this means that we are only given the recipes and no explicit attributes for them. The algorithm can be naturally extended to the general set- ting.</span><span class="c1">Data: Q, B</span><span class="c27">obj</span><span class="c1">,B</span><span class="c27">prc </span><span class="c0">1 E</span><span class="c13">B </span><span class="c0">GetExamples(N</span><span class="c13">1</span><span class="c0">,k); 2 while CollectingAttributesCondition = True do 3 a GetNextAttribute(A, S, B</span><span class="c27">obj</span><span class="c1">); </span><span class="c0">4 A A [ a; 5 S UpdateStatistics(S, a, E</span><span class="c27">B</span><span class="c1">); </span><span class="c0">6 b FindBudgetDistribution(S); 7 E</span><span class="c27">L </span><span class="c1">GetExamples(N</span><span class="c27">2</span><span class="c1">,b); </span><span class="c0">8 l FindRegression(b, E</span><span class="c13">L</span><span class="c0">); 9 return l, b </span></p><p class="c8"><span class="c0">Algorithm 1: The base case solution </span></p><p class="c8"><span class="c0">Algorithm 1 depicts a general description of our solution. Table 1 shows the different information items being collected throughout its execution. It contains objects (first column), true values of objects&rsquo; attributes (second column), and sets of workers&rsquo; answers to value questions (denoted {o</span><span class="c27">i</span><span class="c1">.a</span><span class="c13">(1) j </span><span class="c9">}</span><span class="c36">n</span><span class="c13">1 </span><span class="c1">in all other columns where n is the answers set size). We </span><span class="c0">use this table to illustrate what is done by Algorithm 1. We later explain how exactly this is done (what crowd tasks are involved, etc.). Note that some notations in Table 1 may not yet be clear at this point, but will be explained later. </span></p><p class="c15"><span class="c0">At the beginning, the only information available is the name of the query attribute (Table 1&rsquo;s &ldquo;True Values for A(Q)&rdquo; header). We first collect a set of example objects E</span><span class="c27">B </span><span class="c1">= {e</span><span class="c27">1</span><span class="c1">,...,e</span><span class="c27">N</span><span class="c7">1</span><span class="c0">} 2 O along with their true value for a</span><span class="c27">t</span><span class="c1">. </span><span class="c0">Those objects and values are shown in Table 1a. We then iteratively add new attribute columns in Table 1a by disman- tling existing attributes, thereby discovering new attribute names (the &ldquo;Value Questions Answers for A</span><span class="c27">final</span><span class="c1">&rdquo; headers) </span><span class="c0">and then obtaining values for them from workers. During this iterative process, we also use the crowd answers to cal- culate some statistics (not shown in the table) on the discov- ered attributes, which we use for deciding which attributes need to be dismantled next. When this collection process ends (we will explain later how this is determined), we use the statistics again to calculate a budget distribution b. Fi- nally, to compute the linear regression l we collect a second set of examples E</span><span class="c27">L</span><span class="c1">, along with their value for a</span><span class="c27">t </span><span class="c1">(Table 1b&rsquo;s </span><span class="c0">Objects and True Value columns). We use b to collect crowd answers for the remaining attributes, then use all the gath- ered information to learn the linear regression l. Now that we derived both b and l, the preprocessing phase ends. </span></p><p class="c15"><span class="c0">Later, in the query evaluation phase, b and l are used to collect estimations about the objects in the queried data table D (the Answers in 1c) and use it to calculate and return Q(D) (the &ldquo;True Values&rdquo; column in 1c). </span></p><p class="c8"><span class="c44">3.1 The Algorithm Components </span></p><p class="c8"><span class="c0">Algorithm 1 consists of five logical components: finding relevant attributes (lines 3-4), collecting statistics about them (lines 1 and 5), calculating a budget distribution (line 6), learning a linear regression (lines 7-8) and managing the preprocessing budget (line 2). We discuss them next. </span></p><p class="c8"><span class="c53">True Values Value Questions Answers objects </span></p><p class="c8"><span class="c53">for A(Q) for A</span><span class="c10">final </span><span class="c69">a</span><span class="c10">1 </span><span class="c69">a</span><span class="c10">1 </span><span class="c69">a</span><span class="c10">2 </span><span class="c69">&middot;&middot;&middot; a</span><span class="c10">l </span><span class="c53">e</span><span class="c2">1 </span><span class="c69">... </span><span class="c53">e</span><span class="c2">1</span><span class="c53">.a</span><span class="c2">1 </span><span class="c53">.</span><span class="c69">.. </span><span class="c53">{e</span><span class="c2">1</span><span class="c53">.a.</span><span class="c69">.. </span><span class="c21">(1) </span></p><p class="c8"><span class="c2">1 </span><span class="c56">}</span><span class="c21">k</span><span class="c2">1 </span><span class="c53">{e</span><span class="c2">1</span><span class="c53">.a.</span><span class="c69">.. </span><span class="c21">(1) </span></p><p class="c8"><span class="c2">2 </span><span class="c56">}</span><span class="c21">k</span><span class="c2">1 </span><span class="c53">... .</span><span class="c69">.. </span><span class="c53">{e</span><span class="c2">1</span><span class="c53">.a.</span><span class="c69">.. </span></p><p class="c8"><span class="c21">(1) </span><span class="c2">l </span><span class="c56">}</span><span class="c21">k</span><span class="c2">1 </span><span class="c53">e</span><span class="c10">N</span><span class="c80">1 </span><span class="c53">e</span><span class="c10">N</span><span class="c80">1</span><span class="c53">.a</span><span class="c10">1 </span><span class="c69">{e</span><span class="c10">N</span><span class="c80">1</span><span class="c53">.a</span><span class="c21">(1) </span></p><p class="c8"><span class="c2">1 </span><span class="c56">}</span><span class="c21">k</span><span class="c2">1 </span><span class="c56">{e</span><span class="c2">N</span><span class="c80">1</span><span class="c53">.a</span><span class="c21">(1) </span></p><p class="c8"><span class="c2">2 </span><span class="c56">}</span><span class="c21">k</span><span class="c2">1 </span><span class="c56">... {e</span><span class="c2">N</span><span class="c80">1</span><span class="c53">.a</span><span class="c21">(1) </span></p><p class="c8"><span class="c2">l </span><span class="c56">}</span><span class="c21">k</span><span class="c2">1 </span></p><p class="c8"><span class="c4">(a) Data used to calculate b </span></p><p class="c8"><span class="c19">objects </span></p><p class="c8"><span class="c19">True Values Value Questions Answers </span></p><p class="c8"><span class="c19">for A(Q) for A</span><span class="c59">final </span></p><p class="c8"><span class="c19">a</span><span class="c59">1 </span><span class="c58">a</span><span class="c59">1 </span><span class="c58">a</span><span class="c59">2 </span><span class="c58">&middot;&middot;&middot; a</span><span class="c59">l </span><span class="c19">e</span><span class="c59">1 </span><span class="c58">... e</span><span class="c59">1</span><span class="c58">.a</span><span class="c59">1 </span><span class="c19">.</span><span class="c58">.. {e</span><span class="c59">1</span><span class="c58">.a</span><span class="c45">(1) </span></p><p class="c8"><span class="c45">1 </span><span class="c19">.</span><span class="c58">.. </span><span class="c6">}</span><span class="c18">b(a</span><span class="c45">1 </span><span class="c74">1</span><span class="c18">) </span></p><p class="c8"><span class="c6">{e</span><span class="c45">1</span><span class="c19">.a</span><span class="c18">(1) </span></p><p class="c8"><span class="c45">2 </span><span class="c19">.</span><span class="c58">.. </span><span class="c6">}</span><span class="c18">b(a</span><span class="c45">1 </span><span class="c74">2</span><span class="c18">) </span></p><p class="c8"><span class="c6">... </span><span class="c19">.</span><span class="c58">.. </span><span class="c6">{e</span><span class="c45">1</span><span class="c19">.a</span><span class="c18">(1) </span><span class="c45">l </span><span class="c19">.</span><span class="c58">.. </span></p><p class="c8"><span class="c6">}</span><span class="c18">b(a</span><span class="c45">1 </span><span class="c74">l</span><span class="c18">) </span></p><p class="c8"><span class="c19">e</span><span class="c59">N</span><span class="c73">2 </span><span class="c19">e</span><span class="c59">N</span><span class="c73">2</span><span class="c19">.a</span><span class="c59">1 </span><span class="c58">{e</span><span class="c59">N</span><span class="c73">2</span><span class="c19">.a</span><span class="c18">(1) </span></p><p class="c8"><span class="c45">1 </span><span class="c6">}</span><span class="c18">b(a</span><span class="c45">1 </span><span class="c74">1</span><span class="c18">) </span></p><p class="c8"><span class="c6">{e</span><span class="c45">N</span><span class="c73">2</span><span class="c19">.a</span><span class="c18">(1) </span></p><p class="c8"><span class="c45">2 </span><span class="c6">}</span><span class="c18">b(a</span><span class="c45">1 </span><span class="c74">2</span><span class="c18">) </span></p><p class="c8"><span class="c6">... {e</span><span class="c45">N</span><span class="c73">2</span><span class="c19">.a</span><span class="c18">(1) </span></p><p class="c8"><span class="c45">l </span><span class="c6">}</span><span class="c18">b(a</span><span class="c45">1 </span></p><p class="c8"><span class="c74">l</span><span class="c18">) </span></p><p class="c8"><span class="c4">(b) Data used to learn l </span></p><p class="c8"><span class="c16">objects </span></p><p class="c8"><span class="c16">True Values Value Questions Answers </span></p><p class="c8"><span class="c16">for A(Q) for A</span><span class="c75">final </span></p><p class="c60"><span class="c16">a</span><span class="c75">1 </span><span class="c23">a</span><span class="c75">1 </span><span class="c23">a</span><span class="c75">2 </span><span class="c23">&middot;&middot;&middot; a</span><span class="c75">l </span><span class="c16">o</span><span class="c75">1 </span><span class="c16">o</span><span class="c75">2 </span><span class="c23">... ? ? </span><span class="c16">.</span><span class="c23">.. {o{o</span><span class="c75">1</span><span class="c23">.a</span><span class="c75">2</span><span class="c23">.a</span><span class="c54">(1) 1 (1) 1 </span><span class="c16">.</span><span class="c23">.. </span><span class="c79">}}</span><span class="c29">b(a</span><span class="c54">1 </span><span class="c29">b(a</span><span class="c54">1 </span><span class="c87">1</span><span class="c29">) </span><span class="c87">1</span><span class="c29">) </span></p><p class="c34"><span class="c79">{o{o</span><span class="c54">1</span><span class="c16">.a</span><span class="c54">2</span><span class="c16">.a</span><span class="c29">(1) </span><span class="c54">2 </span><span class="c29">(1) </span><span class="c54">2 </span><span class="c16">.</span><span class="c23">.. </span><span class="c79">}}</span><span class="c29">b(a</span><span class="c54">1 </span><span class="c29">b(a</span><span class="c54">1 </span><span class="c87">2</span><span class="c29">) </span><span class="c87">2</span><span class="c29">) </span></p><p class="c15"><span class="c79">... ... </span><span class="c16">.</span><span class="c23">.. </span><span class="c79">{o{o</span><span class="c54">1</span><span class="c16">.a</span><span class="c54">2</span><span class="c16">.a</span><span class="c29">(1) </span><span class="c54">l </span><span class="c29">(1) </span><span class="c54">l </span><span class="c16">.</span><span class="c23">.. </span></p><p class="c34"><span class="c79">}}</span><span class="c29">b(a</span><span class="c54">1 </span><span class="c29">b(a</span><span class="c54">1 </span><span class="c87">l</span><span class="c29">) </span><span class="c87">l</span><span class="c29">) </span></p><p class="c60"><span class="c4">(c) Data used in the online phase </span><span class="c0">Table 1: Data collected during the algorithm </span></p><p class="c15"><span class="c3">Finding Attributes. </span><span class="c0">We identify here a set of attributes that may assist in estimating the value of the query at- tribute. This is done using dismantling questions, followed by corresponding verification questions. A key observation here is that it is sometimes beneficial to not only ask users to dismantle the query attribute itself, but rather to con- tinue dismantling newly discovered attributes. Indeed, since the human mind is associative, asking diverse questions is important for better learning a domain [7]. For example, in our CC example, when asking a user to dismantle pro- tein amount, we may get the attribute meat content as an answer, but the distinction between red meat and white meat (which have different protein amounts) may be only ob- tained when asking users to dismantle meat content. </span></p><p class="c34"><span class="c0">We denote by A</span><span class="c27">m </span><span class="c1">the set of known related attributes af- </span><span class="c0">ter m iterations (and respectively A</span><span class="c27">0 </span><span class="c1">= A(Q) and A</span><span class="c27">final </span><span class="c1">is </span><span class="c0">the final subset). We denote by A</span><span class="c27">m+1|a</span><span class="c0">random variable representing this set, </span><span class="c7">j </span><span class="c0">assuming = A</span><span class="c13">m </span><span class="c0">[ ans</span><span class="c13">j </span><span class="c0">the the next dismantling question is for attribute a</span><span class="c13">j</span><span class="c0">. Our goal will be to formally, choose we a</span><span class="c27">j </span><span class="c0">wish </span><span class="c1">such </span><span class="c0">to </span><span class="c1">that </span><span class="c0">find </span></p><p class="c8"><span class="c1">A</span><span class="c27">m|a</span><span class="c7">j </span><span class="c0">allows minimal error. More </span></p><p class="c8"><span class="c0">argmax a</span><span class="c27">j</span><span class="c71">E</span><span class="c27">A</span><span class="c7">m</span><span class="c30">|</span><span class="c7">aj </span><span class="c13">=A</span><span class="c0">[ min </span><span class="c27">l,b </span></p><p class="c8"><span class="c13">8a62A b(a)=0 </span></p><p class="c8"><span class="c0">Er(Q|b, l)] (1) </span></p><p class="c15"><span class="c0">As this choice obviously depends on how the budget distri- bution b and the linear regression l are selected, we leave the solution of expression 1 for section 3.2.1. For now we only note that after asking the selected dismantling question and getting a new attribute name for an answer, we use verifi- cation questions to ensure that the obtained new attribute name is indeed a relevant one. Here we use standard al- gorithms such as [25] to determine the required number of questions for making a decision. </span></p><p class="c15"><span class="c0">Since a dismantling question in our setting is always fol- lowed by corresponding verifications questions, from here on whenever we use the term dismantling question we also refer to its following verification questions. </span></p><p class="c15"><span class="c3">Collecting Statistics. </span><span class="c0">As mentioned, we need to collect some information about A</span><span class="c13">m </span><span class="c0">- the set of known related at- tributes after m iterations - and the way workers answer </span></p><p class="c8"><span class="c35">412 </span></p><p class="c8"><span class="c28">A(Q) A</span><span class="c37">m </span><span class="c46">a</span><span class="c12">1 </span><span class="c46">a</span><span class="c12">1 </span><span class="c46">a</span><span class="c12">2 </span><span class="c46">&middot;&middot;&middot; a</span><span class="c12">l </span></p><p class="c8"><span class="c28">A</span><span class="c37">m </span></p><p class="c8"><span class="c28">a</span><span class="c37">1 </span><span class="c28">S</span><span class="c37">c</span><span class="c28">[1] S</span><span class="c37">o</span><span class="c28">[1] S</span><span class="c37">a</span><span class="c28">[1,1] S</span><span class="c37">a</span><span class="c28">[1,2] &middot;&middot;&middot; S</span><span class="c37">a</span><span class="c28">[1,l] a</span><span class="c12">2 </span><span class="c46">S</span><span class="c12">c</span><span class="c46">[2] S</span><span class="c12">o</span><span class="c46">[2] S</span><span class="c12">a</span><span class="c46">[2,1] S</span><span class="c12">a</span><span class="c46">[2,2] &middot;&middot;&middot; S</span><span class="c12">a</span><span class="c46">[2,l] </span></p><p class="c34"><span class="c28">.</span><span class="c46">.. </span><span class="c28">.</span><span class="c46">.. </span><span class="c28">.</span><span class="c46">.. </span><span class="c28">.</span><span class="c46">.. </span><span class="c28">.</span><span class="c46">.. </span><span class="c28">.</span><span class="c46">.. </span><span class="c28">.</span><span class="c46">.. </span><span class="c28">a</span><span class="c12">l </span><span class="c46">S</span><span class="c12">c</span><span class="c46">[l] S</span><span class="c12">o</span><span class="c46">[l] S</span><span class="c12">a</span><span class="c46">[l,1] S</span><span class="c12">a</span><span class="c46">[l,2] &middot;&middot;&middot; S</span><span class="c12">a</span><span class="c46">[l, l] </span><span class="c0">Table 2: Statistics calculated during the algorithm </span></p><p class="c15"><span class="c0">them. Our main tool for finding those statistics is gath- ering samples of the crowd responses and analyzing them. We do so by asking value questions about a set of exam- ple objects collected using example questions. Formally, our goal is to find an accurate estimation for the trio S</span><span class="c13">A </span><span class="c0">= (S</span><span class="c27">oA</span><span class="c0">,S</span><span class="c27">aA</span><span class="c0">,S</span><span class="c27">cA</span><span class="c0">) (or just S when the index is clear from con- text), depicted in Table 2 and defined as follow (for reasons that will be clear later). </span></p><p class="c34"><span class="c0">S</span><span class="c27">c </span><span class="c1">- Statistics about agreement among crowd workers. More </span><span class="c0">precisely, a vector of the average variances of workers an- swers 1A1 where to value S</span><span class="c13">cA</span><span class="c0">[a] questions. = </span><span class="c55">E</span><span class="c13">O</span><span class="c0">[Var[o.aFormally, </span><span class="c36">(1)</span><span class="c0">]]). S</span><span class="c27">cA </span><span class="c0">For is instance, a vector of in size our example we can expect S</span><span class="c27">c</span><span class="c1">[healthy] &gt; S</span><span class="c27">c</span><span class="c1">[tomato] as it is </span><span class="c0">easier to identify if a recipe contains a tomato. This is the second column of Table 2. </span></p><p class="c15"><span class="c0">S</span><span class="c13">o </span><span class="c0">- Statistics about how informative are the attributes. More precisely, this is the covariance vector between work- ers answers to the different attribute and the query at- tribute. Formally, S</span><span class="c27">oA </span><span class="c0">is a vector of size 1A1 and S</span><span class="c27">oA</span><span class="c0">[a] = 1Cov</span><span class="c27">O</span><span class="c1">(o.a</span><span class="c13">(1)</span><span class="c1">, o.a</span><span class="c27">t</span><span class="c1">)1. For example, if a</span><span class="c27">t </span><span class="c1">= dessert, we can </span><span class="c0">expect </span><span class="c36">S</span><span class="c5">o</span><span class="c13">[sweet] </span></p><p class="c8"><span class="c13">(sweet) </span><span class="c9">&gt; </span><span class="c36">S</span><span class="c5">c</span><span class="c13">[cheese] </span></p><p class="c15"><span class="c13">(cheese) </span><span class="c9">as most desserts are sweet (and </span><span class="c0">most non-desserts are not) but cheese can be easily found both in desserts and in non-desserts. This is the third column of Table 2. </span></p><p class="c15"><span class="c0">S</span><span class="c13">a </span><span class="c0">- Statistics about how much distinctive are attributes (in comparison to the other attributes). More precisely, this is the covariance matrix over crowd&rsquo;s answers to different attributes. Formally, S</span><span class="c27">aA </span><span class="c0">is a matrix of size 1A1&#8677;1A1 and S</span><span class="c13">aA</span><span class="c0">[a</span><span class="c13">i</span><span class="c0">,a</span><span class="c13">j</span><span class="c0">] = 1Cov</span><span class="c13">O</span><span class="c0">(o.a</span><span class="c36">(1) </span></p><p class="c8"><span class="c13">i </span><span class="c9">, o.a</span><span class="c36">(1) </span></p><p class="c8"><span class="c13">j </span><span class="c9">)1. For example, we can </span><span class="c0">expect usually </span><span class="c36">S</span><span class="c5">a</span><span class="c13">[Spicy, Sugar] </span></p><p class="c8"><span class="c0">indicates </span><span class="c13">(Spicy) (Sugar) </span><span class="c0">a </span><span class="c9">&gt; </span><span class="c0">non-spicy </span><span class="c36">S</span><span class="c5">a</span><span class="c13">[Easy to make,Sugar] </span></p><p class="c34"><span class="c13">(Easy </span><span class="c0">food </span><span class="c13">to make) </span><span class="c0">but </span><span class="c13">(Sugar) </span><span class="c0">does </span><span class="c9">as sugar </span><span class="c0">not imply anything about the complexity of the recipe. This is the fourth column of Table 2. Our goal is to get relatively good estimations of these mea- sures for a low budget. We describe how this is done in section 3.2.2. </span></p><p class="c15"><span class="c3">Calculating a budget distribution . </span><span class="c0">Once the relevant at- tributes are identified and the relevant statistics are calcu- lated we run a strictly computational algorithm to find b. As it was shown in [27], when applying the best linear regression to some table D</span><span class="c13">O&#8677;A </span><span class="c36">(b)</span><span class="c9">(a notation that means D</span><span class="c13">O&#8677;A</span><span class="c36">(b)</span><span class="c9">[o, a] = </span><span class="c0">o.a</span><span class="c36">(b(a))</span><span class="c0">), the error is </span><span class="c55">E</span><span class="c0">[E.a</span><span class="c27">t</span><span class="c1">]</span><span class="c13">2 </span><span class="c0">The first element is independent </span><span class="c1">S</span><span class="c27">o</span><span class="c13">T </span><span class="c1">(S</span><span class="c27">a</span><span class="c1">+Diag( </span><span class="c0">of b, so we have </span><span class="c13">Sb(a) </span><span class="c7">c</span><span class="c27">(a) </span></p><p class="c34"><span class="c0">that </span><span class="c9">)) </span><span class="c36">1</span><span class="c0">S</span><span class="c27">o</span><span class="c1">. </span><span class="c0">the best budget distribution is </span></p><p class="c8"><span class="c0">argmax </span></p><p class="c8"><span class="c13">b </span><span class="c9">S</span><span class="c13">o</span><span class="c36">T </span><span class="c0">(S</span><span class="c27">a </span><span class="c1">+ Diag(</span><span class="c0">Sb(a) </span><span class="c27">c</span><span class="c1">(a) </span></p><p class="c8"><span class="c9">)) </span><span class="c36">1</span><span class="c0">S</span><span class="c27">o </span><span class="c1">(2) </span></p><p class="c15"><span class="c0">[27] also showed that finding this optimal b is Np-hard in B</span><span class="c27">obj </span><span class="c1">and therefore an approximating algorithm is appropri- </span><span class="c0">ate. They provide such algorithm, which is a variation of the well known greedy forward selection. We use this algorithm as the FindQuestionsDistribution method in Algorithm 1. </span></p><p class="c15"><span class="c3">Learning a Linear Regression. </span><span class="c0">The last part of the algo- rithm is deciding on a linear regression l. Since we can not find the overall best linear regression, we minimize the error over some training set representing the online phase data (meaning that the estimation of each attribute a is done ac- cording to b(a)). We get this set by using example questions (getting object and target values) and value questions (get- ting attribute values). To reduce costs we re-use previously collected data. When collecting objects we skip the first N</span><span class="c13">1 </span><span class="c0">example questions, and when collecting estimations we only ask b(a) k value questions for each e.a. This is line 7 in Algorithm 1 and how we collect Table 1b&rsquo;s data. </span></p><p class="c15"><span class="c0">Once such training set exists, further computations are applied to find a linear regression l that minimizes the error over it. The problem of finding a linear regression that min- imizes the mean square error is a well studied problem [12] and there are many algorithms for it that we can just use. Specifically, we used a singular value decomposition (SVD [15]) algorithm, but since it is used as a black box other algorithms can also fit. This is line 8 in Algorithm 1. </span></p><p class="c8"><span class="c3">Managing the Preprocessing Budget. </span><span class="c0">To fully understand where and how budget is spent, one needs to first see the actual implementation presented next. We thus postpone this discussion to section 3.2.3. </span><span class="c44">3.2 Concrete Solutions </span></p><p class="c15"><span class="c0">Finally, we can focus on our implementations. We wish to remind that although they are described separately, all the components are in fact intertwined. </span></p><p class="c8"><span class="c3">3.2.1 Finding Attributes </span></p><p class="c15"><span class="c0">Recall that our objective is to solve expression 1. Knowing now, that the error behaves like expression 2 we can state a more specific objective - finding </span></p><p class="c8"><span class="c0">argmax </span></p><p class="c8"><span class="c13">a</span><span class="c7">j </span><span class="c0">argmax </span></p><p class="c8"><span class="c13">b </span><span class="c9">S</span><span class="c13">o</span><span class="c36">T</span><span class="c13">A</span><span class="c7">m</span><span class="c30">|</span><span class="c7">aj </span><span class="c0">(S</span><span class="c27">aA</span><span class="c7">m</span><span class="c30">|</span><span class="c7">aj </span><span class="c0">+ Diag(</span><span class="c9">S</span><span class="c13">cA</span><span class="c0">b(a) </span><span class="c7">m</span><span class="c30">|</span><span class="c7">aj </span><span class="c0">(a) </span></p><p class="c8"><span class="c9">)) </span><span class="c36">1</span><span class="c0">S</span><span class="c27">oA</span><span class="c7">m</span><span class="c30">|</span><span class="c7">aj </span></p><p class="c8"><span class="c0">(3) </span></p><p class="c34"><span class="c0">As an exact solution can only be made after asking all que- tions S</span><span class="c27">A</span><span class="c7">m </span><span class="c0">and </span><span class="c7">1 </span><span class="c0">and calculating estimate the all Snext </span><span class="c27">A</span><span class="c7">m</span><span class="c30">|</span><span class="c7">a</span><span class="c0">statistics </span><span class="c7">j </span><span class="c0">, we We also calculate the probability of use the current statistics it remaining S</span><span class="c27">A</span><span class="c7">m</span><span class="c30">|</span><span class="c7">aj </span><span class="c0">, for the every same aas </span><span class="c27">j</span><span class="c1">. </span></p><p class="c15"><span class="c0">only a first seen answer will effet it. We then use those esti- mations and solve expression 3. Described here are general schemes of the estimations. Full calculations can be found in the our paper[22]. </span></p><p class="c15"><span class="c0">Pr(new </span><span class="c55">| </span><span class="c0">a</span><span class="c27">j</span><span class="c1">) - We need to estimate the probability to get a </span><span class="c0">new answer. We do so by assuming it depends only on the number of questions asked so far and then using a simple Bernoulli-Bayesian model with the number of questions asked about a</span><span class="c13">j </span><span class="c0">so far (n</span><span class="c13">j</span><span class="c0">). The results are </span></p><p class="c8"><span class="c0">Pr(new 1 a</span><span class="c13">j</span><span class="c0">) = </span><span class="c9">n</span><span class="c36">j </span><span class="c9">+ 1 </span></p><p class="c8"><span class="c0">n</span><span class="c36">2</span><span class="c13">j </span><span class="c9">+ 3n</span><span class="c13">j </span><span class="c0">+ 2 </span><span class="c9">(4) </span></p><p class="c34"><span class="c0">S</span><span class="c27">oA</span><span class="c0">answer </span><span class="c7">m</span><span class="c30">|</span><span class="c7">aj </span><span class="c0">- We need to estimate the covariance of the next and the target (S</span><span class="c27">o</span><span class="c1">[ans</span><span class="c27">j</span><span class="c1">]). By definition of corre- </span><span class="c0">lation we have S</span><span class="c13">o</span><span class="c0">[ans</span><span class="c13">j</span><span class="c0">] = and S</span><span class="c13">o</span><span class="c0">[a</span><span class="c13">j</span><span class="c0">] are known. &sigma;(ans</span><span class="c13">j</span><span class="c0">) </span><span class="c36">&#8674;(a</span><span class="c5">t</span><span class="c13">,ans</span><span class="c7">j</span><span class="c13">) </span></p><p class="c8"><span class="c13">&#8674;(a</span><span class="c0">is </span><span class="c7">t</span><span class="c27">,a</span><span class="c0">assumed </span><span class="c7">j</span><span class="c13">) </span></p><p class="c8"><span class="c13">(ans(a</span><span class="c7">jj</span><span class="c13">) ) </span></p><p class="c34"><span class="c9">S</span><span class="c36">o</span><span class="c9">[a</span><span class="c36">j</span><span class="c9">]. </span><span class="c0">to be </span><span class="c9">&sigma;(a</span><span class="c36">j</span><span class="c9">) </span><span class="c0">indepen- dent with a</span><span class="c27">j </span><span class="c1">and can therefore be ignored. That leaves </span></p><p class="c8"><span class="c35">413 </span></p><p class="c15"><span class="c0">only the correlations&rsquo; ratio. We previously assumed ans</span><span class="c27">j </span><span class="c0">is highly correlated to a</span><span class="c13">j</span><span class="c0">, we now approximate this as </span><span class="c55">E</span><span class="c0">[&rho;(a</span><span class="c13">j</span><span class="c0">,ans</span><span class="c13">j</span><span class="c0">)] &#8673; 0.5, which translates to We address this approximation later. This </span><span class="c36">&#8674;(a</span><span class="c13">&#8674;(a</span><span class="c0">results </span><span class="c5">t</span><span class="c13">,ans</span><span class="c7">t</span><span class="c27">,a</span><span class="c7">jj</span><span class="c13">) ) </span></p><p class="c8"><span class="c0">in </span><span class="c9">&#8673; 0.5. </span></p><p class="c8"><span class="c0">S</span><span class="c27">oA</span><span class="c7">m</span><span class="c30">|</span><span class="c7">aj </span><span class="c0">[a] &#8673; </span></p><p class="c8"><span class="c0">&#8674; S</span><span class="c27">0.5 </span></p><p class="c8"><span class="c27">oA</span><span class="c13">(a</span><span class="c7">jm </span><span class="c13">)</span><span class="c9">S</span><span class="c36">oA</span><span class="c7">1</span><span class="c0">[a] </span><span class="c5">m 1</span><span class="c9">[a</span><span class="c36">j</span><span class="c9">] a = ans</span><span class="c36">j </span></p><p class="c8"><span class="c0">otherwise </span><span class="c9">(5) </span></p><p class="c34"><span class="c0">S</span><span class="c27">cA</span><span class="c0">swer </span><span class="c7">m</span><span class="c30">|</span><span class="c7">a</span><span class="c0">(S</span><span class="c7">j </span><span class="c0">- We need to estimate the variance of the next an- </span><span class="c27">c</span><span class="c1">[ans</span><span class="c27">j</span><span class="c1">]). Since there is no reason for it to change </span><span class="c0">over different dismantling questions we can use the same distribution for every j. Because FindRegression is not analytic (as we will see later), instead of measuring an exact distribution (which will make it impossible to cal- culate) we take an &rsquo;optimism in the face of uncertainty&rsquo; approach [20] and assume a very low constant value for it (8j S</span><span class="c27">c</span><span class="c1">(ans</span><span class="c27">j</span><span class="c1">) &#8673; 0). We then get </span></p><p class="c8"><span class="c0">S</span><span class="c13">cA</span><span class="c7">m</span><span class="c30">|</span><span class="c7">aj </span><span class="c0">[a] &#8673; </span></p><p class="c8"><span class="c0">&#8674; </span><span class="c1">0 a = ans</span><span class="c27">j </span></p><p class="c8"><span class="c0">S</span><span class="c13">cA</span><span class="c7">m 1</span><span class="c0">[a] otherwise </span><span class="c9">(6) </span></p><p class="c34"><span class="c0">S</span><span class="c27">aA</span><span class="c0">the </span><span class="c7">m</span><span class="c30">|</span><span class="c7">a</span><span class="c0">new </span><span class="c7">j </span><span class="c0">- We need to estimate the covariances between answer and the previously discovered attributes (S</span><span class="c13">a</span><span class="c0">[a</span><span class="c13">i</span><span class="c0">,ans</span><span class="c13">j</span><span class="c0">], a</span><span class="c13">i </span><span class="c0">2 A</span><span class="c13">m 1</span><span class="c0">). Again, since there is no rea- son for this to change over different dismantling questions we can just take the same distribution for every j. For similar reasons (calculations practicality), we again take the &rsquo;optimism in the face of uncertainty&rsquo; approach. We (wrongfully) assume no correlation between the new and the existing attributes. This assumption cannot be taken for S</span><span class="c13">a</span><span class="c0">[ans</span><span class="c13">j</span><span class="c0">,ans</span><span class="c13">j</span><span class="c0">], but this factor cancelled anyway in the Er(Q) calculation. We then get </span></p><p class="c8"><span class="c0">S</span><span class="c27">aA</span><span class="c7">m</span><span class="c30">|</span><span class="c7">aj </span><span class="c0">[a</span><span class="c27">u</span><span class="c1">,a</span><span class="c27">v</span><span class="c1">] &#8673; </span></p><p class="c8"><span class="c0">8</span><span class="c1">&lt;: </span></p><p class="c15"><span class="c0">1 a</span><span class="c13">u </span><span class="c0">= a</span><span class="c13">v </span><span class="c0">= ans</span><span class="c13">j </span><span class="c0">0 ans</span><span class="c27">j </span><span class="c1">2 {a</span><span class="c27">u</span><span class="c1">,a</span><span class="c27">v</span><span class="c1">} </span><span class="c0">S</span><span class="c27">aA</span><span class="c7">m 1</span><span class="c0">[a</span><span class="c27">u</span><span class="c1">,a</span><span class="c27">v</span><span class="c1">] otherwise </span></p><p class="c8"><span class="c0">(7) </span></p><p class="c8"><span class="c0">By putting results 4, 5, 6 and 7 into expression 3 we get that the best next dismantling question is </span></p><p class="c8"><span class="c0">argmax </span></p><p class="c8"><span class="c13">a</span><span class="c7">j </span><span class="c0">Pr(new | a</span><span class="c27">j</span><span class="c1">)[G(a</span><span class="c27">j</span><span class="c1">) L(A</span><span class="c27">m 1</span><span class="c1">,B</span><span class="c27">obj</span><span class="c1">,1)] (8) </span></p><p class="c8"><span class="c0">where Pr(new | a</span><span class="c13">j</span><span class="c0">) was described before, G(a</span><span class="c13">j</span><span class="c0">) = </span><span class="c36">0.25S</span><span class="c5">o</span><span class="c13">[a</span><span class="c7">j</span><span class="c13">]</span><span class="c26">2 </span></p><p class="c8"><span class="c13">(a</span><span class="c7">j</span><span class="c13">)</span><span class="c26">2 </span><span class="c0">and L(A, u, v) = max</span><span class="c13">b of size u </span><span class="c0">S</span><span class="c13">oA</span><span class="c0">(S</span><span class="c13">aA</span><span class="c0">+Diag( maxIntuitively, </span><span class="c27">b of size u </span><span class="c0">when </span><span class="c27">v </span><span class="c1">S</span><span class="c27">oA</span><span class="c0">adding (S</span><span class="c27">aA </span><span class="c0">+ a new Diag( attribute, </span><span class="c36">S</span><span class="c5">cA</span><span class="c27">b </span><span class="c36">S</span><span class="c5">cA</span><span class="c0">)) </span><span class="c36">1</span><span class="c0">S</span><span class="c27">oA</span><span class="c0">. </span></p><p class="c8"><span class="c27">b </span><span class="c0">)) </span><span class="c36">1</span><span class="c0">S</span><span class="c13">oA </span></p><p class="c15"><span class="c0">some of the record budget moves from the old attributes to the new one. G measures the gain from the new attribute and L measures the loss caused by decreasing budget from the old attributes. The reasons for those being the only changes are the low cor- relation As all assumptions we took of those values can be while calculated, estimating this concludes S</span><span class="c13">A</span><span class="c7">m</span><span class="c30">|</span><span class="c7">aj </span><span class="c0">. </span></p><p class="c15"><span class="c0">the GetNextAttribute method in Algorithm 1. This is also how we get each &ldquo;Value Questions Answers&rdquo; header in Table 1. It is easy to see that one dismantling question at the end of each iteration is the only crowd task. </span></p><p class="c8"><span class="c3">3.2.2 Collecting Statistics </span></p><p class="c15"><span class="c0">As explained, our goal in this part is to find a good ap- proximation for (S</span><span class="c27">oA</span><span class="c0">,S</span><span class="c27">aA</span><span class="c0">,S</span><span class="c27">cA</span><span class="c0">) while using a minimal bud- get. Following our iterative process for finding attributes, we build S in an inductive way. Namely, for each new attribute </span></p><p class="c8"><span class="c0">we calculate S</span><span class="c27">A</span><span class="c7">m </span><span class="c0">based on S</span><span class="c27">A</span><span class="c7">m 1 </span><span class="c0">and questions about the new attribute. </span></p><p class="c8"><span class="c0">For our current simplified case, there exists an approx- imation method in [27] that have proven itself before and that we can easily adapt. When we later discuss the general case, we will return to this part and refine the calculation. The ideas we take from [27] are to estimate S</span><span class="c13">A</span><span class="c7">final </span><span class="c0">(which is defined over O) by calculating it over example set E</span><span class="c27">B </span><span class="c1">and </span><span class="c0">then, for each object, estimating the behavior of o.a</span><span class="c36">(1) </span><span class="c0">based on k sample answers (for a very small k). We use those ideas in an inductive way A </span><span class="c13">1 </span><span class="c0">- We leave S</span><span class="c13">c</span><span class="c0">,S</span><span class="c13">a</span><span class="c0">,S</span><span class="c13">o </span><span class="c0">empty but collect a set of examples E</span><span class="c27">B </span><span class="c1">with target a</span><span class="c27">t </span><span class="c1">value by asking N</span><span class="c27">1 </span><span class="c1">example questions </span><span class="c0">(N</span><span class="c13">1 </span><span class="c0">is a parameter studied in [27]). This is line 1 in Al- gorithm 1 and during it we collect Table 1a&rsquo;s objects and true values. </span></p><p class="c15"><span class="c0">A</span><span class="c27">m </span><span class="c1">- For the new attribute a we ask k values questions </span><span class="c0">about e.a for every e 2 E</span><span class="c27">B</span><span class="c1">. We then update S by keeping </span><span class="c0">all previous values and adding (1) S</span><span class="c13">o</span><span class="c0">[a] = </span><span class="c55">E</span><span class="c13">E</span><span class="c7">B</span><span class="c0">[e.a</span><span class="c36">(k)</span><span class="c0">&middot;e.a</span><span class="c13">t</span><span class="c0">], (2) S</span><span class="c27">a</span><span class="c1">[a, a</span><span class="c27">i</span><span class="c1">] = S</span><span class="c27">a</span><span class="c1">[a</span><span class="c27">i</span><span class="c1">,a] = </span><span class="c71">E</span><span class="c27">E</span><span class="c7">B</span><span class="c0">[e.a</span><span class="c36">(k) </span><span class="c0">&middot; e.a</span><span class="c36">(k) </span></p><p class="c8"><span class="c13">i </span><span class="c9">] for every </span><span class="c0">a</span><span class="c27">i </span><span class="c1">2 A</span><span class="c27">m </span><span class="c1">and (3) S</span><span class="c27">c</span><span class="c1">[a] = </span><span class="c71">E</span><span class="c27">E</span><span class="c7">B</span><span class="c0">[VarEst</span><span class="c27">k</span><span class="c1">(e.a</span><span class="c13">(1)</span><span class="c1">)]. This is </span><span class="c0">the UpdateStatistics method in algorithm 1. During each step we get a sub-column in the answers column of Table 1a, a row in Table 2 and a sub-column in the right column of Table 2. It should be easy to see how this algorithm is compliant with the ideas mentioned above. It should also be easy to see that during this part we use the crowd for N</span><span class="c13">1 </span><span class="c0">example questions and kN</span><span class="c27">1</span><span class="c1">|A</span><span class="c27">final</span><span class="c1">| value questions. </span></p><p class="c8"><span class="c3">3.2.3 Management of the Preprocessing Budget </span></p><p class="c8"><span class="c0">In our algorithm the preprocessing budget is used for </span></p><p class="c15"><span class="c0">&bull; Finding A</span><span class="c27">final </span><span class="c1">by asking attributes and attributes verifica- </span><span class="c0">tion questions. This costs n dismantle questions where n is the number of dismantling questions we choose to ask. </span></p><p class="c15"><span class="c0">&bull; Calculating the statistics S</span><span class="c27">A</span><span class="c7">final </span><span class="c0">by asking example and value questions. This costs N</span><span class="c27">1 </span><span class="c1">example questions and </span><span class="c0">kN</span><span class="c13">1</span><span class="c0">|A</span><span class="c13">final</span><span class="c0">| value questions, where |A</span><span class="c13">final</span><span class="c0">| depends on n. </span></p><p class="c8"><span class="c0">&bull; Collecting a training set of N</span><span class="c27">2 </span><span class="c1">samples for l&rsquo;s learning. </span><span class="c0">This costs (N</span><span class="c13">2 </span><span class="c0">N</span><span class="c13">1</span><span class="c0">) example questions and (N</span><span class="c13">2 </span><span class="c0">N</span><span class="c13">1</span><span class="c0">)B</span><span class="c13">obj</span><span class="c0">+ N</span><span class="c27">1</span><span class="c1">(B</span><span class="c27">obj </span><span class="c0">P</span><span class="c27">A </span><span class="c0">min{b(a),k}) value questions. As N</span><span class="c27">1 </span><span class="c1">and k are external parameters, the only variables are </span><span class="c0">n and N</span><span class="c27">2</span><span class="c1">. Therefore, the only open question is when to </span><span class="c0">stop asking dismantling questions (line 2 in algorithm 1). We solve this tradeoff by applying a common simple linear lower bound for N</span><span class="c13">2 </span><span class="c0">as a function of b ([16]). Note that as our only tradeoff is n vs. N</span><span class="c27">2</span><span class="c1">, this mechanism is also appropriate </span><span class="c0">when considering different costs for different crowd tasks. As to the case of different cost that may apply for different questions of the same type (for example, numeric vs. binary), the appropriate coefficients need to be added. In this case, we also follow [27] idea and, during all components, divide each attribute&rsquo;s contribution by its cost. </span></p><p class="c15"><span class="c3">Remarks. </span><span class="c0">We conclude this section by commenting on the correctness and complexity of the algorithm. First, it is easy to see that the algorithm operates within the preprocessing budget B</span><span class="c27">prc</span><span class="c1">. Second, it is also easy to see that the algorithm </span><span class="c0">running time is polynomial with respect to the two budgets B</span><span class="c27">prc </span><span class="c1">and B</span><span class="c27">obj</span><span class="c1">. </span></p><p class="c8"><span class="c35">414 </span></p><p class="c8"><span class="c44">4. EXTENDED SOLUTION </span></p><p class="c15"><span class="c0">We focused so far on the simplified case where the query has a single attribute. We next consider the general case of multiple query attributes. </span></p><p class="c15"><span class="c0">A naive solution is to equally split the online and o&#9999;ine budgets between the query attribute and solve the prob- lem for each one separately. This however ignores possible correlation between the query attributes and their compo- nents. For example, consider a query with two attributes A(Q) = {calories, is dessert}. It is easy to see that many related attributes (e.g., sugar, fat,...) are good indicators for both target attributes, and budget would be saved if we reuse values. To address this we consider all the query at- tributes together, extending Algorithm 1 to handle multiple target attributes. We first present below a simple exten- sion, then discuss its shortcomings, and then generalize it to overcome them. Our first extension generalized the algo- rithm components as follows. </span></p><p class="c15"><span class="c0">GetExamples - Instead of asking the crowd for examples with one value for the single query attribute we now ask for examples with multiple attribute values - one per query attribute. (We later discuss what to do if users cannot provide all these values simultaneously.) </span></p><p class="c8"><span class="c0">GetNextAttribute - Expression 8 is refined to consider all </span></p><p class="c8"><span class="c0">the attributes: </span></p><p class="c60"><span class="c0">argmax </span><span class="c13">a</span><span class="c7">j </span></p><p class="c8"><span class="c0">X</span><span class="c27">a</span><span class="c7">t </span></p><p class="c8"><span class="c0">Pr(new | a</span><span class="c13">j</span><span class="c0">)[G(a</span><span class="c13">t</span><span class="c0">,a</span><span class="c13">j</span><span class="c0">) L(a</span><span class="c13">t</span><span class="c0">,A</span><span class="c13">m 1</span><span class="c0">,B</span><span class="c27">obj</span><span class="c1">,1)] </span></p><p class="c8"><span class="c0">(9) </span></p><p class="c34"><span class="c0">UpdateStatistic - Instead of updating the S</span><span class="c27">o </span><span class="c1">statistics of </span><span class="c0">a</span><span class="c27">new </span><span class="c1">for a single query attribute, we now need to update </span><span class="c0">S</span><span class="c13">oa</span><span class="c0">we </span><span class="c7">t</span><span class="c0">added [a</span><span class="c13">new</span><span class="c0">] here for every the query a</span><span class="c27">t </span><span class="c0">attribute a</span><span class="c13">t </span><span class="c0">2 A(Q). Note that </span><span class="c1">notation to S</span><span class="c27">o </span><span class="c1">since there are now </span><span class="c0">several query attributes. S</span><span class="c13">a </span><span class="c0">and S</span><span class="c13">c </span><span class="c0">remained the same since they are independent of Q. </span></p><p class="c8"><span class="c0">FindQuestionsDistribution - Instead of equation 2 we </span></p><p class="c8"><span class="c0">now have a refined version </span></p><p class="c60"><span class="c0">argmax </span><span class="c13">b </span></p><p class="c8"><span class="c0">X </span><span class="c13">a</span><span class="c7">t</span><span class="c27">2A(Q)</span><span class="c0">S</span><span class="c27">o</span><span class="c13">T</span><span class="c27">(a</span><span class="c7">t</span><span class="c13">,A)</span><span class="c9">(S</span><span class="c13">aA </span><span class="c9">+ Diag(S</span><span class="c36">c</span><span class="c9">[a] </span></p><p class="c8"><span class="c0">b(a) </span><span class="c9">) </span><span class="c36">1</span><span class="c0">S</span><span class="c27">o(a</span><span class="c7">t</span><span class="c27">,A) </span></p><p class="c8"><span class="c0">(10) </span></p><p class="c8"><span class="c0">FindRegression - Since l is now a set of linear regressions, </span></p><p class="c34"><span class="c0">we need to run </span><span class="c1">are independent </span><span class="c0">this method |A(Q)| times. Since </span><span class="c1">this yields an optimal solution. </span><span class="c0">all l</span><span class="c27">a</span><span class="c7">t </span><span class="c0">Note that for GetExamples we assumed above that it is possible to ask workers for examples with several attributes values. This may be problematic in practice: If the number of query attributes is too large, workers may not be willing to make the effort of providing all of their values; It may also be the case that a single crowd member does not know the value of all attributes, even for their own examples. To overcome this, instead of using just one set of examples E</span><span class="c27">B </span><span class="c0">with all query attributes, we will collect multiple sets of examples subset thereof). E</span><span class="c27">Ba</span><span class="c7">t</span><span class="c0">, one for each query attribute (or a In this case, the collected data in Table small 1a is replaced by the one depicted in Table 3. </span></p><p class="c15"><span class="c0">Looking at this table we can see that although the in- formation can be used to derive b, it comes with an ad- ditional cost: It is easy to see that the amount of data </span></p><p class="c60"><span class="c43">True Values for A(Q) </span><span class="c64">a</span><span class="c70">1 </span><span class="c64">a</span><span class="c70">2 </span><span class="c43">Value Questions Answers </span><span class="c64">a</span><span class="c70">1 </span><span class="c64">&middot;&middot;&middot; </span><span class="c43">for </span><span class="c64">a</span><span class="c70">l </span><span class="c43">A</span><span class="c70">final </span><span class="c43">e</span><span class="c32">1 </span><span class="c43">e</span><span class="c32">1</span><span class="c43">.a</span><span class="c32">1 </span><span class="c64">... </span><span class="c43">.</span><span class="c64">.. </span><span class="c43">? .</span><span class="c64">.. </span><span class="c43">{e</span><span class="c32">1</span><span class="c43">.a.</span><span class="c64">.. </span><span class="c31">(1) </span></p><p class="c8"><span class="c32">1 </span><span class="c24">}</span><span class="c31">k</span><span class="c32">1 </span><span class="c24">... </span><span class="c43">.</span><span class="c64">.. </span><span class="c24">{e</span><span class="c31">1</span><span class="c24">.a</span><span class="c43">.</span><span class="c64">.. </span></p><p class="c8"><span class="c31">(1) </span><span class="c32">l </span><span class="c24">}</span><span class="c31">k</span><span class="c32">1 </span><span class="c43">e</span><span class="c70">N</span><span class="c41">1 </span><span class="c43">e</span><span class="c70">N</span><span class="c41">1</span><span class="c43">.a</span><span class="c32">1 </span><span class="c43">? e</span><span class="c70">N</span><span class="c41">1</span><span class="c64">... </span><span class="c32">+1 </span><span class="c43">? .</span><span class="c64">.. </span><span class="c43">e</span><span class="c70">N</span><span class="c41">1</span><span class="c32">+1</span><span class="c43">.a</span><span class="c32">2 </span><span class="c43">.</span><span class="c64">.. </span><span class="c43">{e{e</span><span class="c70">NN</span><span class="c41">1</span><span class="c32">+1</span><span class="c43">.a</span><span class="c41">1</span><span class="c43">.a.</span><span class="c64">.. </span><span class="c31">(1) </span></p><p class="c8"><span class="c32">1 </span><span class="c31">(1) </span><span class="c32">1 </span><span class="c24">}</span><span class="c31">k</span><span class="c32">1 </span><span class="c24">}</span><span class="c31">k</span><span class="c32">1 </span><span class="c24">... ... </span><span class="c43">.</span><span class="c64">.. </span><span class="c24">{e{e</span><span class="c32">NN</span><span class="c41">1</span><span class="c32">+1</span><span class="c43">.a</span><span class="c41">1</span><span class="c43">.a.</span><span class="c64">.. </span></p><p class="c8"><span class="c31">(1) </span><span class="c32">l </span><span class="c31">(1) </span></p><p class="c8"><span class="c32">l </span><span class="c24">}</span><span class="c31">k</span><span class="c32">1 </span><span class="c24">}</span><span class="c31">k</span><span class="c32">1 </span><span class="c43">e</span><span class="c70">2N</span><span class="c41">1 </span><span class="c43">? </span><span class="c1">Table 3: Data </span><span class="c43">e</span><span class="c70">N</span><span class="c41">1</span><span class="c32">+1</span><span class="c43">.a</span><span class="c32">2 </span><span class="c1">collected </span><span class="c43">{e</span><span class="c70">2N</span><span class="c1">in </span><span class="c41">1</span><span class="c43">.a</span><span class="c31">(1) </span></p><p class="c8"><span class="c32">1 </span><span class="c1">the </span><span class="c24">}</span><span class="c31">k</span><span class="c32">1 </span><span class="c24">... </span><span class="c1">general </span><span class="c24">{e</span><span class="c1">case </span></p><p class="c34"><span class="c32">2N</span><span class="c41">1</span><span class="c43">.a</span><span class="c31">(1) </span><span class="c32">l </span><span class="c24">}</span><span class="c31">k</span><span class="c32">1 </span><span class="c0">we need to collect now depends both on |A</span><span class="c27">final</span><span class="c1">| and on </span><span class="c0">|A(Q)|. Therefore, if we want to allow A</span><span class="c27">final </span><span class="c1">to grow with </span><span class="c0">A(Q), our cost will grow quadratically. It is also easy to see that most of this growth is due to cost of redundant value questions. For example, consider A(Q) = {is dessert, number of calories,protein amount, easy to make}. One can assume that although there is likely to be a correlation be- tween number of calories and is dessert this is not the case for easy to make and protein amount so collecting statistics for all pairs is a waste of budget. To reduce this redundant overhead we take two steps. First, we choose carefully which data to collect (i.e., which asked). Second, for pairs for E</span><span class="c27">Ba</span><span class="c0">which </span><span class="c7">t</span><span class="c0">.a value data questions had should we not been col- lected, we estimate We explain this next. </span></p><p class="c8"><span class="c0">S</span><span class="c27">oa</span><span class="c7">t</span><span class="c0">[a] based on the other collected data. </span></p><p class="c34"><span class="c3">Collection. </span><span class="c0">Our choice of which data to collect is based on the following observation. The two cases one wants to avoid are (1) missing a highly correlated attribute-target pair and (2) wasting budget on a poorly correlated attribute-target pair. Therefore, whenever we get a new attribute a</span><span class="c13">j </span><span class="c0">we pair it with all query attributes a</span><span class="c27">t </span><span class="c1">for which we have a reason </span><span class="c0">to believe we define half of the S</span><span class="c13">oa</span><span class="c0">that maximal </span><span class="c7">t</span><span class="c0">[a</span><span class="c13">j</span><span class="c0">] S</span><span class="c27">oa</span><span class="c7">t</span><span class="c0">as [avalue </span><span class="c27">j</span><span class="c1">] </span><span class="c0">negligible </span><span class="c1">is not negligible. </span><span class="c0">iff max</span><span class="c27">a2A(Q) </span><span class="c1">In our heuristic, </span><span class="c0">its value is less than a </span><span class="c1">S</span><span class="c27">oa</span><span class="c0">[a</span><span class="c27">j</span><span class="c1">]. Our estima- </span><span class="c0">tion earlier here in for section S</span><span class="c13">oa</span><span class="c7">t</span><span class="c0">[a</span><span class="c13">j</span><span class="c0">] 3.2.1. is done in the same This results in way we described the following rule - when asking a dismantling question about a</span><span class="c13">i </span><span class="c0">and get- ting an answer a</span><span class="c27">j</span><span class="c1">, ask </span><span class="c0">&rho;(a</span><span class="c27">i</span><span class="c1">,a</span><span class="c27">t</span><span class="c1">) &gt; 0.5 max</span><span class="c27">a2A(Q) </span><span class="c1">value questions &rho;(a</span><span class="c27">j</span><span class="c1">,a). </span></p><p class="c8"><span class="c1">about E</span><span class="c27">Ba</span><span class="c7">t</span><span class="c0">.a</span><span class="c27">j </span><span class="c1">iff </span></p><p class="c34"><span class="c3">Estimation. </span><span class="c0">Finally, to estimate the missing S</span><span class="c27">o</span><span class="c1">&rsquo;s values we </span><span class="c0">use a graph model and define G = (U,V,E) as a weighted bi- partite graph with U(G) = A(Q) and V (G) = A</span><span class="c13">m</span><span class="c0">. The idea is to make each edge&rsquo;s weight w(a, a</span><span class="c27">t</span><span class="c1">) represent the value of </span><span class="c0">S</span><span class="c13">oa</span><span class="c0">graph. </span><span class="c7">t</span><span class="c0">[a] and Ideally, then we estimating However, since S</span><span class="c27">o </span><span class="c0">missing edges by distances on a would </span><span class="c1">is </span><span class="c0">have defined w(a, a</span><span class="c27">t</span><span class="c1">) = not normalized and also not S</span><span class="c27">oa</span><span class="c1">a </span><span class="c7">t</span><span class="c1">dis- </span><span class="c0">[a]. </span></p><p class="c15"><span class="c0">tance function, this is impossible. To overcome this, we employ a method described in [29] and use angular dis- tance as our weight function - w(a</span><span class="c13">t</span><span class="c0">,a</span><span class="c13">j</span><span class="c0">) = (O.a</span><span class="c13">t</span><span class="c0">, O.a</span><span class="c13">j</span><span class="c0">) = arccos sider an </span><span class="c13">&sigma;(a</span><span class="c36">S</span><span class="c0">inner-product </span><span class="c5">oa</span><span class="c7">t</span><span class="c27">)&sigma;(a</span><span class="c7">t</span><span class="c13">[a</span><span class="c7">jj</span><span class="c13">] </span></p><p class="c34"><span class="c13">)</span><span class="c9">. The idea behind angular distance is to con- </span><span class="c0">space where the vectors are random variables and the inner-product is covariance. This allows to prove that is indeed a distance function that answer what we looked for. Using the fact that in the angular dis- tance space </span><span class="c13">1</span><span class="c0">+ </span><span class="c13">2 </span><span class="c0">= arccos(cos( </span><span class="c13">1</span><span class="c0">) cos( </span><span class="c13">2</span><span class="c0">)), we define our estimation of S</span><span class="c27">o </span><span class="c1">as </span></p><p class="c8"><span class="c0">S</span><span class="c27">oa</span><span class="c7">t</span><span class="c0">[a</span><span class="c27">j</span><span class="c1">] = &sigma;(a</span><span class="c27">t</span><span class="c1">) &middot; &sigma;(a</span><span class="c27">j</span><span class="c1">) &middot; </span></p><p class="c8"><span class="c0">8</span><span class="c1">&lt;: </span></p><p class="c34"><span class="c0">cos(w(a</span><span class="c13">j</span><span class="c0">,a</span><span class="c13">i</span><span class="c0">)) edge exists cos(S.P(a</span><span class="c27">t</span><span class="c1">,a</span><span class="c27">j</span><span class="c1">)) path exists </span><span class="c0">0 otherwise</span><span class="c1">(11) </span><span class="c0">where S.P stands for (multiplication) shortest path. </span></p><p class="c8"><span class="c35">415 </span></p><p class="c15"><span class="c3">Weighted query attributes. </span><span class="c0">To conclude, note that in our discussion so far we assumed the errors of all attributes to be of equal weight. In practice some normalization may be re- quired. For example, is healthy is on a scale of [0,1] whereas number of calories may reach thousands. In this case, each a</span><span class="c27">t </span><span class="c0">minimize the </span><span class="c1">2 </span><span class="c0">previous </span><span class="c1">A(Q) </span><span class="c9">P</span><span class="c1">is </span><span class="c13">a</span><span class="c0">calculation </span><span class="c7">t</span><span class="c1">associated </span><span class="c13">2A(Q) </span><span class="c9">!</span><span class="c13">t</span><span class="c55">E</span><span class="c0">[(O.athis </span><span class="c1">with </span><span class="c0">simply </span><span class="c36">(&#8676;) </span></p><p class="c34"><span class="c13">t </span><span class="c1">a weight !</span><span class="c27">t </span><span class="c1">and our goal is </span><span class="c9">O.a</span><span class="c13">t</span><span class="c0">)</span><span class="c36">2</span><span class="c0">]. When following results in adding weights </span><span class="c1">to </span></p><p class="c8"><span class="c0">to expression 9. </span></p><p class="c8"><span class="c44">5. EXPERIMENTS </span></p><p class="c15"><span class="c0">We analyze our solution experimentally along through di- mensions. We start with a general proof of concept - an examination of our algorithm as a whole. We then move to an analysis of its components, their necessity and their quality. We conclude with an analysis of how different as- sumptions and parameters can influence the results. </span></p><p class="c8"><span class="c44">5.1 Experiments Settings and Datasets </span></p><p class="c15"><span class="c0">We used three datasets - two with real life objects and real crowd answers, and one synthetic. Crowd answers from value and attribute questions were gathered through Crowd- Flower[3] - a platform for presenting small tasks to crowds. The answers collected in initial experiments was recorded in a database and reused in following experiments, so that re- sults of multiple runs/algorithms may be compared in equiv- alent settings. To compare our performance to [27] that used experts to obtain relevant attributes, we also added to our database the data collected in that work. For example ques- tions, in order to have a true &rsquo;gold standard&rsquo; (known target answers), we used our lab members as crowd. </span></p><p class="c15"><span class="c0">We designed our crowd interface and payment following the guidelines in [13] and the work of [27]. Our crowd tasks consist of a set of value (resp. dismantling) questions that a crowd member needs to answer. We set the payment for bi- nary value question to 0.1 </span></p><p class="c15"><span class="c0">for general numeric values. For dismantling and example questions, that were not studied in [27], we set the payment to 1.5 </span></p><p class="c15"><span class="c0">per answer, following our preliminary experiments that showed this to be the minimal price that kept workers&rsquo; feedback positive, and set the price of an example question to 5 </span></p><p class="c15"><span class="c0">as this is a relatively hard task. (We will show however in the sequel that the trends in our results are robust to changes in these numbers). As for other parameters we used the following: the number of value samples k used for estimating statistics when deriving budget distributions was 2, as this is the rec- ommended number for the corresponding black-box that we used[27]. The number of examples N</span><span class="c13">1 </span><span class="c0">was set to 200 to keep our costs low while still having many examples. For learn- ing the linear regression, the examples number N</span><span class="c13">2 </span><span class="c0">was set to 50+8&#8676;#attributes, a common practice in such tasks [16]. For attribute weights, unless otherwise stated, we gave each query attribute a weight in reverse proportion to its vari- ance (!</span><span class="c27">t </span><span class="c1">= </span><span class="c0">scale (standard </span><span class="c13">V ar(O.a1 </span></p><p class="c34"><span class="c0">deviations), </span><span class="c7">t</span><span class="c13">)</span><span class="c9">). This normalize all errors to a similar </span><span class="c0">so that no query attribute will be negligible. We will explicitly mention below where using other weights affects the results. </span></p><p class="c15"><span class="c3">Human Pictures Data Set. </span><span class="c0">In this set of experiments our objects are people and the only information available for them is their picture. The query attributes in the differ- ent experiments include Weight, Height, Age, Bmi (body </span></p><p class="c8"><span class="c0">and to 0.4 </span></p><p class="c8"><span class="c40">Ques- </span><span class="c40">Ques- </span></p><p class="c8"><span class="c40">tion </span><span class="c66">Answer Fre- </span></p><p class="c8"><span class="c40">quency </span></p><p class="c8"><span class="c40">tion </span><span class="c84">Answer Fre- </span></p><p class="c8"><span class="c40">quency </span></p><p class="c8"><span class="c40">Bmi </span></p><p class="c8"><span class="c40">Calories </span><span class="c40">Weight 33% </span></p><p class="c8"><span class="c40">Has Eggs 8% </span><span class="c40">Height 33% </span></p><p class="c8"><span class="c40">Low Calories 4% </span><span class="c40">Age 6% </span></p><p class="c8"><span class="c40">Dessert 2% </span><span class="c40">Attrctive 2% </span></p><p class="c8"><span class="c40">Healthy 2% </span><span class="c40">Age 22% </span></p><p class="c8"><span class="c40">Has Meat 13% </span></p><p class="c8"><span class="c40">Height </span></p><p class="c8"><span class="c40">Shoe Size Taller Then </span></p><p class="c8"><span class="c40">You 9% </span></p><p class="c8"><span class="c40">Protein </span><span class="c66">7% </span></p><p class="c8"><span class="c40">Number of </span></p><p class="c34"><span class="c40">Eggs </span><span class="c84">4% </span><span class="c40">High Protein 4% </span><span class="c40">Weight 6% </span></p><p class="c8"><span class="c40">Vegetarian 2% </span><span class="c40">Wrinkles 15% </span></p><p class="c8"><span class="c40">Age </span></p><p class="c8"><span class="c40">Gray Hair 10% Old 10% </span></p><p class="c8"><span class="c40">Healthy </span></p><p class="c8"><span class="c40">Children 3% </span></p><p class="c8"><span class="c40">Attractive </span></p><p class="c34"><span class="c40">Low Salt 8% Natural 8% Fat Amount 4% Bitter 4% </span><span class="c40">Good Facial </span></p><p class="c34"><span class="c40">Features </span><span class="c66">17% </span><span class="c40">Fat 6% Has Good Style 6% </span></p><p class="c34"><span class="c40">Easy To Make </span><span class="c40">Works Out 1% </span></p><p class="c8"><span class="c4">(a) Pictures Domain </span></p><p class="c8"><span class="c40">Number of Ingredients </span><span class="c84">17% </span><span class="c40">Fast 10% Tasty 5% Expensive 2% </span></p><p class="c8"><span class="c4">(b) Recipes Domain </span><span class="c0">Table 4: Attribute dismantling questions and their answers </span></p><p class="c8"><span class="c37">S</span><span class="c42">c </span><span class="c81">S</span><span class="c42">o </span><span class="c81">(a</span><span class="c42">i</span><span class="c81">) (a</span><span class="c42">j</span><span class="c81">) S</span><span class="c42">a </span><span class="c81">(a</span><span class="c42">i</span><span class="c81">) (a</span><span class="c42">j</span><span class="c81">) </span></p><p class="c34"><span class="c37">Bmi Age Bmi Weight Heavy Attractive Works Out Wrinkles Bmi 30 0.88 0.63 1 0.94 0.86 0.48 0.4 0.26 Weight 189 0.86 0.7 0.94 1 0.82 0.53 0.39 0.28 Heavy 0.14 0.89 0.6 0.86 0.82 1 0.44 0.46 0.27 Attractive 0.13 0.45 0.44 0.48 0.53 0.44 1 0.32 0.28 Works Out 0.11 0.36 0.29 0.4 0.39 0.46 0.32 1 0.15 Wrinkles 0.16 0.25 0.52 0.26 0.28 0.27 0.28 0.15 1 </span></p><p class="c8"><span class="c4">(a) Pictures Domain </span></p><p class="c8"><span class="c32">S</span><span class="c107">c </span><span class="c70">S</span><span class="c107">o </span><span class="c70">(a</span><span class="c107">i</span><span class="c70">) (a</span><span class="c107">j</span><span class="c70">) S</span><span class="c107">a </span><span class="c70">(a</span><span class="c107">i</span><span class="c70">) (a</span><span class="c107">j</span><span class="c70">) </span></p><p class="c34"><span class="c32">Calories Protein Calories Low Calorie Desset Healty Vegetarian Eggs Calories 80707 0.41 0.34 1 0.2 0.07 0.15 0.18 0.03 Low Calorie 0.06 0.18 0.08 0.2 1 0.1 0.26 0.1 0.13 Desset 0.08 0.26 0.5 0.07 0.1 1 0.44 0.34 0.38 Healthy 0.2 0.02 0.16 0.15 0.26 0.44 1 0.06 0.27 Vegetarian 0.13 0.26 0.52 0.18 0.1 0.34 0.06 1 0.14 </span></p><p class="c8"><span class="c32">Eggs 0.05 0.11 0.26 0.03 0.13 0.38 0.27 0.14 1 </span></p><p class="c60"><span class="c4">(b) Recipes Domain </span><span class="c0">Table 5: Examples for statistics in the different domains </span></p><p class="c8"><span class="c0">mass index, objects O were defined taken as from </span><span class="c13">height(m)</span><span class="c36">weight(kg) </span></p><p class="c34"><span class="c0">the </span><span class="c26">2 </span><span class="c9">) </span><span class="c0">publicly </span><span class="c9">and Attractiveness. </span><span class="c0">available </span><span class="c9">The </span><span class="c0">Photo- graphic Height/Weight Chart [4], where people post pictures of themselves announcing their own height and weight. We used reported value as the true values for Height, Weight and Bmi. For other target values, we used an average over many value question estimations. </span></p><p class="c15"><span class="c0">Examples of answers received when asking workers to dis- mantle various attributes are depicted in Table 4. The first column depicts the attribute to dismantle, the sec- ond column contains some related attributes suggested by the crowd, the last column shows the percentage of all an- swers that each attribute name was returned. Examples of statistics gathered for the attributes are depicted in Table 5 (this is a concrete example of Table 2, but unlike Table 2 it shows attributes correlation and not covariance to make things more intuitive for the reader). </span></p><p class="c15"><span class="c3">Recipes Data Set. </span><span class="c0">In this set of experiments our objects are recipes and the data available for them in the database is the recipe&rsquo;s name, picture and unstructured ingredients- list. The query attributes in the different experiments in- clude Proteins, Calories, Good for kids, Easy to make and Healthy. The objects are the 500 most popular recipes in allrecipes.com[1] website, normalized to one serving. We used nutritious values found in this website as true values for the matching query attributes. For other query attributes we again used average value derived from multiple value questions. Here again, examples of some answers obtained for dismantling questions and statistics on the attributes are depicted in tables 4 and 5 respectively. </span></p><p class="c8"><span class="c35">416 </span></p><p class="c8"><span class="c3">Synthetic Data. </span><span class="c0">To neutralize our own subjectivity/belief w.r.t which object attributes are hard/easy, we also ran ex- periments on a synthetically generated domain. For this we automatically generated a set of objects and attributes (with some dependencies between them) and mocked crowd answers about them (in compliance with the assumptions on crowd&rsquo;s answers mentioned in the paper). The details of this process can be found in the full paper [22]. The experi- ment results are consistent with those for real-life data and are thus omitted here. </span><span class="c44">5.2 Proof of concept </span></p><p class="c15"><span class="c0">We compared our algorithm, which we call DisQ (short for Dismantling queries), to existing practices. We use the following algorithms as baselines: </span></p><p class="c15"><span class="c0">NaiveAverage - In this common approach, the online phase simply asks questions about the attributes in A(Q) and returns their average the budget by the weights. o.a</span><span class="c36">(B</span><span class="c13">t </span><span class="c5">obj</span><span class="c36">) </span></p><p class="c34"><span class="c9">. </span><span class="c0">This </span><span class="c9">For |A(Q)| &gt; 1 we split </span><span class="c0">algorithm has no o&#9999;ine preprocessing phase. </span></p><p class="c15"><span class="c0">SimpleDisQ - This is a simplified version of our algorithm, which captures the best that can be done today without using an expert. It runs similar to DisQ, but without the attribute dismantling phase. </span></p><p class="c15"><span class="c0">We compared these two algorithms to our algorithm. We did so for all three data sets and for different query attributes and query sizes. We also tested with different preprocess- ing budgets B</span><span class="c27">prc </span><span class="c1">and different per-object budgets B</span><span class="c27">obj</span><span class="c1">. For </span><span class="c0">B</span><span class="c27">obj </span><span class="c1">we used the range of 0.4-10 </span></p><p class="c15"><span class="c0">. The lower bound was set to match 1 numeric value-question. The upper bound was set as it is a fairly large amount and as most of the ex- periment graphs show stagnation after it. For B</span><span class="c13">prc </span><span class="c0">we used the range of $10-35. We have taken those values since the graphs stagnate outside those boundaries. For each value we executed 30 experiments and took the average result. Note that although we took the average, all observations are true in general as most results are very close to the average. </span></p><p class="c15"><span class="c3">Varying </span><span class="c0">B</span><span class="c27">prc</span><span class="c105">. </span><span class="c1">Figure 1a shows results for a query with </span><span class="c0">A(Q) = {Bmi} (using the pictures data set) for varying preprocessing budgets. We will show more results later. We start with an example where |A(Q)| = 1 to isolate different effects. We fixed B</span><span class="c27">obj </span><span class="c1">to 4 </span></p><p class="c8"><span class="c0">and used different B</span><span class="c27">obj </span><span class="c1">values. </span><span class="c0">We used B</span><span class="c27">obj </span><span class="c1">= 4 </span></p><p class="c34"><span class="c0">as it is over the graph&rsquo;s knee (as we will see later). Note that since NaiveAverage does not involve learning and since the number of examples in SimpleDisQ is always N</span><span class="c13">1 </span><span class="c0">(since A</span><span class="c27">final </span><span class="c1">is very small), DisQ is the only al- </span><span class="c0">gorithm that changes with B</span><span class="c27">prc</span><span class="c1">. One can easily see that for </span><span class="c0">every B</span><span class="c27">prc </span><span class="c1">value our algorithm has the lowest average error. </span><span class="c0">The difference is especially significant for large B</span><span class="c13">prc </span><span class="c0">values as for those ranges A</span><span class="c27">final </span><span class="c1">is bigger. We can also see that </span><span class="c0">the improvement is slowly stagnating which is the expected result if the &ldquo;important&rdquo; attributes are found quickly. To ilustrate how an algorithm&rsquo;s output looks like, we provide here Bmi</span><span class="c36">(*) </span><span class="c0">an = example 0.6Bmifor </span><span class="c36">(5) </span><span class="c0">one + 11.9Heavyof the dismantles </span><span class="c36">(10) </span><span class="c0">+ 0.4Works when B</span><span class="c27">prc </span><span class="c0">Out</span><span class="c1">= </span><span class="c36">(1) </span><span class="c1">$25. </span><span class="c0">+ 0.2Age</span><span class="c36">(1) </span><span class="c0">2.7Attractive</span><span class="c36">(3) </span><span class="c0">0.2Tall</span><span class="c36">(2) </span><span class="c0">+ 10.6. </span></p><p class="c15"><span class="c3">Varying </span><span class="c0">B</span><span class="c27">obj</span><span class="c105">. </span><span class="c1">We continue with the same Bmi example but </span><span class="c0">now considering varying online per-object budget. Figure 1d show the errors for this case. We used B</span><span class="c27">prc </span><span class="c1">= $30 as it is </span></p><p class="c8"><span class="c13">0 </span><span class="c67">12 10 8 </span><span class="c108">SimpleDisQ DisQ </span><span class="c67">6 4 </span><span class="c67">2 </span><span class="c13">0.067 </span><span class="c104">MSE[Arbitrary] </span><span class="c13">0.075 0.085 0.095 </span><span class="c0">Figure 2: Necessary B</span><span class="c27">obj </span><span class="c1">for achieving target errors </span><span class="c0">again over the graphs&rsquo; knee, but similar behavior is shown for other values. First, note that all algorithms improve as B</span><span class="c27">obj </span><span class="c1">increases and that this improvement is slowly decaying. </span><span class="c0">This is what one could expect as a bigger B</span><span class="c27">obj </span><span class="c1">means a big- </span><span class="c0">ger crowd (and should therefore mean better accuracy) and since it is known that every additional worker has declining marginal utility. Second, note that both SimpleDisQ and DisQ achieve lower error than NaiveAverage. This clearly shows how combining artificial intelligence with the wisdom- of-crowds leads to improved results. Finally, It is easy to see that the average results from our algorithm are superior to those of the other algorithms. This is especially noticeable for lower B</span><span class="c27">obj </span><span class="c1">budget but is also true for higher B</span><span class="c27">obj</span><span class="c1">. And </span><span class="c0">again, although the we show results for the average case, this is true for most cases. One can see, for example, that in order to achieve an accuracy of less than 0.067 one needs to spend 10 </span></p><p class="c15"><span class="c0">per object in our algorithm. This statement is still true after including the extra budget for the preprocess phase, since the cost of learning a regression when B</span><span class="c27">obj </span><span class="c1">= 10 </span></p><p class="c8"><span class="c0">exceed the $30 used in our algorithm for B</span><span class="c27">obj </span><span class="c1">= 6 </span></p><p class="c15"><span class="c0">. In figure 2 we show more examples of the budget necessary for achieving different ac- curacies in the different algorithms. </span></p><p class="c15"><span class="c3">Other Examples. </span><span class="c0">Figures 1c and 1f show equivalent graphs for a case of two query attributes (Bmi, Age) and figures 1b and 1e show equivalent graphs in the recipes domain (for the query attribute Protein). It is easy to see that all of our observations about the first example (Bmi) are also true when adding to it a second attribute (Age). In the case of Protein, however, this is only partly true. Consider first fig- ure 1e. At a first glance it looks different from figure 1d. However, a closer look shows that all our observation still hold. The only difference is that NaiveAverage performs much worse and this changes the proportions of the graph. We believe this is because Protein is much less intuitive than Bmi. Next, consider figure 1b. Here, in addition to the dif- ferent proportions we also see a different trend. Unlike the previous cases where we saw that increasing B</span><span class="c27">obj </span><span class="c1">always de- </span><span class="c0">creased the error, in the case of proteins we see increase in error for B</span><span class="c13">obj </span><span class="c0">&gt; 4 </span></p><p class="c15"><span class="c0">. The reason for this lies in CollectingAt- tributesCondition. Since our stopping criteria for discover- ing new attributes depends on B</span><span class="c27">obj</span><span class="c1">, for higher B</span><span class="c27">obj </span><span class="c1">we have </span><span class="c0">less budget for the dismantling which in turn result in less attributes and therefore in a larger error. The effect of a smaller attributes set A</span><span class="c13">final </span><span class="c0">also exists for Bmi, but in that case its effect was smaller than the effect of the increased B</span><span class="c27">obj</span><span class="c1">. A reasonable conclusion is that for large B</span><span class="c27">obj </span><span class="c1">budget </span><span class="c0">one should also provide a large B</span><span class="c27">prc </span><span class="c1">budget. </span></p><p class="c15"><span class="c0">The experiments described above demonstrate the trends in all of our experiments: In all settings our algorithm out- performs the competing algorithms. Increasing improve- ments are observed when query attributes are difficult and </span></p><p class="c8"><span class="c35">417 </span></p><p class="c8"><span class="c0">per object in SimpleDisQ but only 6 </span></p><p class="c8"><span class="c5">0.26 </span><span class="c5">0.1 </span></p><p class="c8"><span class="c5">0.23 </span></p><p class="c8"><span class="c22">] yrartibrA[ES</span><span class="c82">M</span><span class="c5">0.09 </span></p><p class="c8"><span class="c88">] yrartibrA[ES</span><span class="c86">M</span><span class="c5">0.21 0.19 </span></p><p class="c8"><span class="c5">0.24 </span><span class="c43">NaiveAverage </span><span class="c43">SimpleDisQ </span></p><p class="c8"><span class="c5">0.22 </span></p><p class="c8"><span class="c43">DisQ </span></p><p class="c8"><span class="c43">NaiveAverage </span></p><p class="c8"><span class="c5">0.08 </span></p><p class="c8"><span class="c5">0.17 </span><span class="c5">0.15 </span></p><p class="c8"><span class="c5">0.2 </span></p><p class="c8"><span class="c43">SimpleDisQ </span><span class="c43">DisQ </span></p><p class="c8"><span class="c5">15 20 </span><span class="c82">B</span><span class="c5">prc</span><span class="c122">[$] </span></p><p class="c8"><span class="c5">25 30 35 </span></p><p class="c8"><span class="c5">10 15 20 </span><span class="c86">B</span><span class="c5">prc</span><span class="c86">[$] </span></p><p class="c8"><span class="c5">25 30 35 40 </span></p><p class="c34"><span class="c5">20 30 </span><span class="c50">B</span><span class="c85">prc</span><span class="c50">[$] </span><span class="c5">40 50 60 </span><span class="c4">(b) A(Q) = {Protein} (c) A(Q) = {Bmi, Age} </span></p><p class="c8"><span class="c5">0.05 </span></p><p class="c8"><span class="c5">0.18 </span><span class="c5">0.07 </span></p><p class="c8"><span class="c5">0.13 </span></p><p class="c8"><span class="c5">0.06 </span></p><p class="c8"><span class="c65">NaiveAverage SimpleDisQ </span></p><p class="c60"><span class="c5">0.11 0.09 </span><span class="c65">DisQ </span><span class="c5">0.16 0.14 </span></p><p class="c8"><span class="c5">0.12 </span></p><p class="c8"><span class="c128">(a) A(Q) = {Bmi} </span></p><p class="c8"><span class="c5">0.07 </span><span class="c5">0.05 </span></p><p class="c8"><span class="c5">0.05 </span></p><p class="c8"><span class="c5">0.1 </span></p><p class="c8"><span class="c5">0.13 </span></p><p class="c8"><span class="c5">0.3 </span></p><p class="c8"><span class="c5">0.45 </span></p><p class="c8"><span class="c5">0.12 </span></p><p class="c8"><span class="c5">0.11 </span></p><p class="c8"><span class="c43">NaiveAverage SimpleDisQ </span><span class="c65">DisQ </span></p><p class="c8"><span class="c22">] yrartibtA[ES</span><span class="c82">M</span><span class="c5">0.25 </span></p><p class="c8"><span class="c88">] yrartibrA[ES</span><span class="c82">M</span><span class="c5">0.4 </span></p><p class="c8"><span class="c43">NaiveAverage SimpleDisQ DisQ </span></p><p class="c8"><span class="c5">0 2 4 </span><span class="c94">B</span><span class="c7">obj</span><span class="c94">[&#8373;] </span></p><p class="c8"><span class="c5">6 8 10 </span><span class="c5">0 2 4 </span><span class="c82">B</span><span class="c5">obj</span><span class="c82">[&#8373;] </span></p><p class="c8"><span class="c5">6 8 10 </span></p><p class="c8"><span class="c5">0 2 4 </span><span class="c82">B</span><span class="c5">obj</span><span class="c82">[&#8373;] </span></p><p class="c8"><span class="c5">6 8 10 </span></p><p class="c8"><span class="c4">(d) A(Q) = {Bmi} </span></p><p class="c8"><span class="c4">(f) A(Q) = {Bmi, Age} </span><span class="c0">Figure 1: Error in query estimation for varying B</span><span class="c13">prc </span><span class="c0">(top row) and varying B</span><span class="c27">obj </span><span class="c1">(bottom row) </span></p><p class="c15"><span class="c0">the relative improvement normally grows with the budget al- located to the preprocessing increases, in particular in cases when the per-object online budget is small. </span></p><p class="c8"><span class="c44">5.3 Algorithm Components </span></p><p class="c15"><span class="c0">We next examine the individual algorithm components. In particular we analyze our attributes dismantling method and the processing of multiple query attributes. </span></p><p class="c8"><span class="c3">5.3.1 Dismantling Attributes </span></p><p class="c8"><span class="c0">We considered two dimensions here. </span></p><p class="c15"><span class="c3">Finding Relevant Attributes. </span><span class="c0">We first tested if the crowd can give good answers to attribute dismantling questions, and if so, then how. We created gold standard attributes sets for different data domains and query attributes, and tested the crowd coverage for these attributes (percentage of discovered attributes). We computed the performance of our dismantling process and of a naive approach that asks questions only about the attributes explicitly appearing in the query. For defining the gold standard in the pictures do- main (for the query attributes Height and Weight) we used the expert-provided attributes from [27]. For the gold stan- dard in the recipes domain (for the query attributes Proteins and textitCalories) we used an expert dietitian. </span></p><p class="c15"><span class="c0">For all queries our algorithm yielded over 80% coverage, and we compensated for the missing attributes by other discovered attributes not mentioned by the experts. This shows that the crowd could indeed replace the experts for this task. In contrast, the coverage for the naive algorithm fell below 50%, demonstrating the necessity of our choice to dismantle additional attributes. We further validated these observations by considering two additional real-life attribute domains: house prices (using [18] as a gold standard), and laptop prices (using [9]), obtaining similar results. </span></p><p class="c15"><span class="c3">The GetNextAttribute Method. </span><span class="c0">We next compared our technique for choosing the next attribute to dismantle to a simpler alternative where the only attributes considered </span></p><p class="c8"><span class="c5">0.35 </span><span class="c5">0.1 </span></p><p class="c8"><span class="c5">0.09 </span></p><p class="c8"><span class="c5">0.2 </span></p><p class="c8"><span class="c65">NaiveAverage </span></p><p class="c8"><span class="c5">0.3 </span></p><p class="c8"><span class="c5">0.08 </span></p><p class="c8"><span class="c5">0.07 </span></p><p class="c8"><span class="c5">0.15 </span></p><p class="c8"><span class="c65">SimpleDisQ </span></p><p class="c8"><span class="c5">0.25 </span><span class="c43">DisQ </span></p><p class="c8"><span class="c5">0.2 </span></p><p class="c8"><span class="c5">0.06 </span></p><p class="c8"><span class="c5">0.1 </span></p><p class="c8"><span class="c5">0.15 </span></p><p class="c8"><span class="c5">0.05 </span></p><p class="c8"><span class="c4">(e) A(Q) = {Protein} </span></p><p class="c8"><span class="c5">0.1 </span></p><p class="c15"><span class="c0">are the ones appearing explicitly in the query. We call this variant OnlyQueryAttributes. (We also considered variations of OnlyQueryAttributes and DisQ that chose questions at random, but since those variation are very naive and were consistently inferior to our algorithm we omitted those re- sults). Two example experiment, for the recipes domain with and the query attribute protein are shown in figure 3b (for B</span><span class="c13">prc </span><span class="c0">= 30 and varying B</span><span class="c13">obj</span><span class="c0">) and figure 3a (from B</span><span class="c27">obj </span><span class="c1">= 4 </span></p><p class="c15"><span class="c0">and varying B</span><span class="c27">prc</span><span class="c1">). First, it is easy to see that our </span><span class="c0">previous observations on DisQ in figures 1b and 1e also hold for OnlyQueryAttributes. Second, DisQ consistently outper- form OnlyQueryAttributes illustrating again the necessity of our approach. This intensifies as B</span><span class="c13">prc </span><span class="c0">grows since there exist enough budget to learn many attributes so the low variety of answers to the dismantling question only about protein becomes apparent. Similar trends were observed in all set- tings - different query attributes, query length and domains. The only thing to note is that in some specific cases, when the answers to the dismantling questions about the query attributes were varied enough the difference between the al- gorithms became noticeable only for large B</span><span class="c27">prc</span><span class="c1">. </span></p><p class="c8"><span class="c3">5.3.2 Statistic Estimation </span></p><p class="c15"><span class="c0">We next examine our method for collecting partial statis- tics in queries with multiple attributes. As the main issues here are which attributes should be paired with which query attributes, and how to compensate for the missing pairs, we compared our solution to the following baselines. </span></p><p class="c15"><span class="c0">TotallySeparated - This is the naive solution that solves the problem separately for each query attribute, splitting the budget equally between them. </span></p><p class="c15"><span class="c0">Full - This is a simplified variant of our algorithm that does not optimize the computation and simply gathers statis- tics for all attribute pairs. </span></p><p class="c15"><span class="c0">OneConnection - This is another simplified variant that does consider only some of the pairs, but uses a more naive heuristic for choosing them: When a new attribute is discovered, it is paired only with one query attribute. </span></p><p class="c8"><span class="c35">418 </span></p><p class="c8"><span class="c32">0.085 </span></p><p class="c8"><span class="c11">] yrartibrA[ES</span><span class="c63">M</span><span class="c32">0.075 0.08 </span></p><p class="c8"><span class="c32">0.07 </span></p><p class="c8"><span class="c68">DisQ OnlyQueryAttributes </span></p><p class="c8"><span class="c32">0.065 </span></p><p class="c8"><span class="c32">0.06 </span></p><p class="c8"><span class="c32">0.055 </span></p><p class="c8"><span class="c32">10 15 20 </span><span class="c63">B</span><span class="c32">prc</span><span class="c63">[$] </span></p><p class="c8"><span class="c32">25 30 35 40 </span></p><p class="c8"><span class="c4">(a) Changing B</span><span class="c27">prc </span><span class="c32">0.085 </span></p><p class="c8"><span class="c132">] yrartibtA[ES</span><span class="c63">M</span><span class="c32">0.075 0.065 0.08 </span></p><p class="c8"><span class="c32">0.07 </span></p><p class="c8"><span class="c32">0.06 </span></p><p class="c8"><span class="c68">DisQ OnlyQueryAttributes </span></p><p class="c8"><span class="c32">0.05 </span></p><p class="c8"><span class="c32">0 2 4 </span><span class="c63">B</span><span class="c32">obj</span><span class="c63">[&#8373;] </span></p><p class="c8"><span class="c32">6 8 10 </span></p><p class="c8"><span class="c1">Figure 3: Error in </span><span class="c4">(b) </span><span class="c1">estimation </span><span class="c4">Changing </span><span class="c1">for </span><span class="c4">B</span><span class="c27">obj </span><span class="c1">A(Q) = {Protein} </span></p><p class="c15"><span class="c0">NaiveEstimations - Finally, this variant selects the pairs using our technique, but rather than inferring individual values for the missing pairs, it assigns to all a default value that equal to the average S</span><span class="c27">o </span><span class="c1">value. </span></p><p class="c8"><span class="c0">A sample of the results, for the pictures domain and the query attributes Bmi and Age are shown in figures 4a (B</span><span class="c27">obj </span><span class="c1">= </span><span class="c0">4 </span></p><p class="c8"><span class="c32">0.055 </span></p><p class="c8"><span class="c0">, varying B</span><span class="c13">prc</span><span class="c0">) and figure 4b (B</span><span class="c13">prc </span><span class="c0">= $50, varying B</span><span class="c13">obj</span><span class="c0">). We set here B</span><span class="c27">prc </span><span class="c1">to $50 to highlight the trends, as we will </span><span class="c0">shortly discuss. First, note that all the variations follow the general trends of our algorithm as discussed above, in regard for their dependencies in B</span><span class="c13">prc </span><span class="c0">and B</span><span class="c27">obj</span><span class="c1">. Second, it is easy </span><span class="c0">to see the relatively bad performances of the TotallySeper- ated baseline (especially for lower B</span><span class="c27">prc</span><span class="c1">) which demonstrates </span><span class="c0">the advantage of asking about several query attributes to- gether. Third, in comparison to Full, our algorithm con- sistently achieves better (or at least as good) results when considering reasonable B</span><span class="c27">prc </span><span class="c1">(as in figure 4a). This effect was </span><span class="c0">even more noticeable in the synthetic domain where we could test large queries. For some queries, however, these trends change for very high B</span><span class="c27">prc </span><span class="c1">as the saved budget from the non- </span><span class="c0">important pairs is wasted on even more non-important new attributes. Next, in compare to OneConnection, our algo- rithm achieves at least as good results for all budgets, and better results for high B</span><span class="c27">prc</span><span class="c1">. The reason for that is that </span><span class="c0">for large budgets OneConnection saves budget in the be- ginning on redundant connections, but that budget is then referred to even more redundant attributes. In some cases, however, and for low B</span><span class="c13">prc</span><span class="c0">, OneConnection did get better results, but only very marginally. The reason is the tradeoff between B</span><span class="c27">obj </span><span class="c1">and flexible-B</span><span class="c27">prc </span><span class="c1">we discussed before which </span><span class="c0">effects DisQ more. Finally, our algorithm consistently out- performs NaiveEstimations. This is true for every budget since our estimation method incures no crowd cost. </span><span class="c44">5.4 Dependency on Assumptions </span></p><p class="c8"><span class="c0">Our last set of experiments examined the robustness of our algorithm to some changes in underlying our assump- tions. We briefly discuss the assumptions considered and our conclusions. A detailed description of the experiments and results can be found in our full paper [22]. Attributes Quality: We tested resilience to receiving also some irrelevant attributes in the dismantling process. This did not affect the previous trends, but as expected required </span></p><p class="c8"><span class="c20">0.18 </span><span class="c62">] yrartibrA[ES</span><span class="c39">M</span><span class="c20">0.17 0.16 0.15 0.14 0.13 0.12 </span></p><p class="c8"><span class="c13">DisQ </span><span class="c13">Full OneConnection </span></p><p class="c8"><span class="c20">0.11 </span></p><p class="c34"><span class="c13">TotalySeparated NaiveEstimation </span><span class="c20">0.1 </span></p><p class="c8"><span class="c20">20 25 30 35 40 </span><span class="c39">B</span><span class="c20">prc</span><span class="c39">[$] </span></p><p class="c8"><span class="c20">45 50 55 60 65 </span></p><p class="c8"><span class="c4">(a) Changing B</span><span class="c13">prc </span></p><p class="c8"><span class="c20">0.22 </span></p><p class="c8"><span class="c13">DisQ Full OneConnection TotalySeparated NaiveEstimation </span></p><p class="c8"><span class="c20">0.1 </span></p><p class="c8"><span class="c20">0 2 4 6 8 10 </span></p><p class="c8"><span class="c1">Figure 4: Error in </span><span class="c4">(b) </span><span class="c1">estimation </span><span class="c4">Changing </span><span class="c1">for </span><span class="c4">B</span><span class="c27">obj </span><span class="c1">A(Q) = {Bmi,Age} </span></p><p class="c8"><span class="c0">somewhat higher preprocessing budget (B</span><span class="c13">prc</span><span class="c0">) for obtaining same error rates. Normalization Mechanism: We tested the necessity of iden- tifying different answers about the same property as one. Here again, our algorithm can work with imperfect (or even no) unification and the trends stay the same. Somewhat higher B</span><span class="c27">prc </span><span class="c1">is again needed for obtaining same error rates. </span><span class="c0">Answer&rsquo;s Correlation Parameter: We used different con- stants (instead of just 0.5) for </span><span class="c55">E</span><span class="c0">[&rho;(a</span><span class="c27">j</span><span class="c1">,ans</span><span class="c27">j</span><span class="c1">)] when estimating </span><span class="c0">S</span><span class="c13">A</span><span class="c0">Crowd-Tasks </span><span class="c7">m</span><span class="c30">|</span><span class="c7">aj </span><span class="c0">. The results remained similar. </span></p><p class="c15"><span class="c0">Payment: We tested how a different pricing models for crowd tasks impact the results. Change in prices changed some of the gradients in the varying-B</span><span class="c27">prc </span><span class="c1">graphs </span><span class="c0">but the trends remained the same. </span></p><p class="c8"><span class="c44">6. RELATED WORK </span></p><p class="c15"><span class="c0">Using the crowd as a source of knowledge, and for solving problems, has attracted much research in recent years [11]. The crowd was shown to be a useful tool for many types of tasks, including, but not restricted to, value estimation [24], data filtering [25], information collection [5], natural languages processing [6] etc. However, to our knowledge, our work is the first to consider using the crowd for discovering query-related attribute names. There has also been much work dealing with the collection of data via various platforms (e.g., payment [2] or games [10]), and the effective collection of such data (e.g., how to best present questions [13], how to filter spam [19], when to stop asking [30] etc.). Our work exploits these platforms and previous results. The concept of removing experts from crowd processes was also researched before[14], but not in the context of query estimation. </span></p><p class="c15"><span class="c0">Our work is also influenced by previous work in machine learning, and on the use of supervised learning for regression learning (e.g., [12]). A more specific problem related to our challenge is feature selection [17] - how to effectively narrow a set of attributes for some learning process. Two models that are particularly interesting are budget learning ([23]), where the issue is deciding which is the most valuable feature to measure next under a limited budget, and meta-features (e.g., [28]), where the issue is trying to predict unseen fea- tures behavior based on some properties and similarity to other features. All of those problems, however, focus on a given predefined set of attributes. They also do not con- </span></p><p class="c8"><span class="c35">419 </span></p><p class="c8"><span class="c49">] yrartibrA[ES</span><span class="c39">M</span><span class="c20">0.2 0.18 </span></p><p class="c8"><span class="c20">0.16 </span></p><p class="c8"><span class="c20">0.14 </span></p><p class="c8"><span class="c20">0.12 </span></p><p class="c8"><span class="c39">B</span><span class="c20">obj</span><span class="c39">[&#8373;] </span></p><p class="c131"><span class="c0">sider the selection of the same attribute name more than once (required here due the uncertainty of crowd answers). </span></p><p class="c76"><span class="c0">Previous work has also dealt with the combination of crowd and learning (e.g., [8]). The common combinations are to use the crowd to label a data set (e.g., [32]) or for filling attributes values (e.g., [21], [26]).Closest to our work is that of [27] which also deals with estimating one attribute value by asking about others, but, as explained in the Intro- duction, requires experts-in-the-loop. While we use some of their results as basic building blocks, a major contribution here is our crowd-based attribute dismantling along with the careful statistical analysis that allows for an effective experts-free algorithm. </span></p><p class="c103"><span class="c44">7. CONCLUSION AND FUTURE WORK </span></p><p class="c130"><span class="c0">We studied in this paper the problem of query evaluation when the value of the queried attributes is not available in the database and is also hard for the crowd to estimate. We proposed a novel approach that uses the crowd to dismantle the query attributes into finer related ones (whose value es- timation is easier), then assembles them to yield better esti- mation for the query attributes. Given an online per-object budget and an o&#9999;ine preprocessing budget, we presented an algorithm that ideally uses the o&#9999;ine budget for dismantling the query attributes and deriving linear formulas that best exploit the online budget for deriving the values of query attributes. We have also demonstrated the effectiveness of the approach through experimental study on both real-life crowd and synthetic data. </span></p><p class="c51"><span class="c0">We focus in the paper on the minimization of the expected- mean-square-error. Other error measures may also be of in- terest for future research. For example, a recall-precision measurement may fit more for boolean query attributes like gluten free (for recipes), or for a categorical attribute like cousin type where the number of possibilities may be large. We also considered only linear formulas for assembling at- tributes values. While this has proved to provide good ex- perimental results, more general rules may be useful in cer- tain situations and we intend to study this in future work. In our development we assumed that we are given an on-line per-object budget and an o&#9999;ine preprocessing budget and used the later to optimize the usage of the former. Deter- mining automatically what these budgets should be and the ideal ratio between them is an intriguing future research. </span></p><p class="c109"><span class="c44">Acknowledgements </span><span class="c0">This work has been partially funded by the European Re- search Council under the FP7, ERC grant MoDaS, agree- ment 291071, and by the Israel Ministry of Science. </span><span class="c44">8. REFERENCES </span></p><p class="c95"><span class="c4">[1] AllRecipes.com. http://allrecipes.com. [2] Amazon Mechanical Turk. https://www.mturk.com. [3] Crowd Flower. http://www.crowdflower.com. [4] The height and weight chart. http: </span></p><p class="c101"><span class="c4">//www.cockeyed.com/photos/bodies/heightweight.html. [5] Y. Amsterdamer, Y. Grossman, T. Milo, and P. Senellart. </span></p><p class="c72"><span class="c4">Crowd mining. In SIGMOD Conference, 2013. [6] C. Biemann. Creating a system for lexical substitutions </span></p><p class="c100"><span class="c4">from scratch using crowdsourcing. Language resources and evaluation, 47(1), 2013. [7] W. Bousfield and C. Sedgewick. An analysis of sequences of restricted associative responses. The Journal of General Psychology, 30(2), 1944. </span></p><p class="c8 c48"><span class="c4">[8] A. Brew, D. Greene, and P. Cunningham. The interaction </span></p><p class="c106"><span class="c4">between supervised learning and crowdsourcing. In NIPS workshop on computational social science and the wisdom of crowds, 2010. [9] P. D. Chwelos, E. R. Berndt, and I. M. Cockburn. Faster, </span></p><p class="c52"><span class="c4">smaller, cheaper: an hedonic price analysis of pdas. Applied Economics, 40(22), 2008. [10] D. Deutch, O. Greenshpan, B. Kostenko, and T. Milo. </span></p><p class="c120"><span class="c4">Declarative platform for data sourcing games. In WWW, 2012. [11] A. Doan, R. Ramakrishnan, and A. Y. Halevy. </span></p><p class="c125"><span class="c4">Crowdsourcing systems on the world-wide web. Communications of the ACM, 54(4), 2011. [12] N. R. Draper and H. Smith. Applied regression analysis 2nd </span></p><p class="c25 c123"><span class="c4">ed. New York New York John Wiley and Sons 1981., 1981. [13] C. Eickho</span><span class="c119">ff </span><span class="c4">and A. P. de Vries. Increasing cheat robustness </span></p><p class="c96"><span class="c4">of crowdsourcing tasks. Inf. Retr., 16(2), 2013. [14] C. Gokhale, S. Das, A. Doan, J. F. Naughton, N. Rampalli, </span></p><p class="c91"><span class="c4">J. Shavlik, and X. Zhu. Corleone: Hands-o</span><span class="c119">ff </span><span class="c4">crowdsourcing for entity matching. In SIGMOD Conference, 2014. [15] G. H. Golub and C. Reinsch. Singular value decomposition </span></p><p class="c99"><span class="c4">and least squares solutions. Numerische Mathematik, 14(5), 1970. [16] S. B. Green. How many subjects does it take to do a </span></p><p class="c25"><span class="c4">regression analysis. Multivariate behavioral research, 26(3), 1991. [17] I. Guyon and A. Elissee</span><span class="c119">ff</span><span class="c4">. An introduction to variable and </span></p><p class="c114"><span class="c4">feature selection. Journal of Machine Learning Research, 3, 2003. [18] D. Harrison Jr and D. L. Rubinfeld. Hedonic housing prices </span></p><p class="c133"><span class="c4">and the demand for clean air. Journal of environmental economics and management, 5(1), 1978. [19] P. G. Ipeirotis, F. Provost, and J. Wang. Quality </span></p><p class="c38"><span class="c4">management on amazon mechanical turk. In Proceedings of the ACM SIGKDD Workshop on Human Computation, 2010. [20] L. P. Kaelbling. Learning in embedded systems. MIT press, </span></p><p class="c117"><span class="c4">1993. [21] E. Kamar, S. Hacker, and E. Horvitz. Combining human and machine intelligence in large-scale crowdsourcing. In AAMAS, 2012. [22] M. Laadan. Dismantling complicated query attributes with </span></p><p class="c116"><span class="c4">crowd. Master&rsquo;s thesis, Tel-Aviv University, 2014. [23] D. J. Lizotte, O. Madani, and R. Greiner. Budgeted </span></p><p class="c121"><span class="c4">learning of naive-bayes classifiers. CoRR, abs/1212.2472, 2012. [24] J. Noronha, E. Hysen, H. Zhang, and K. Z. Gajos. </span></p><p class="c115"><span class="c4">Platemate: crowdsourcing nutritional analysis from food photographs. In UIST, 2011. [25] A. G. Parameswaran, H. Garcia-Molina, H. Park, </span></p><p class="c129"><span class="c4">N. Polyzotis, A. Ramesh, and J. Widom. CrowdScreen: algorithms for filtering data with humans. In SIGMOD Conference, 2012. [26] G. Patterson and J. Hays. SUN attribute database: </span></p><p class="c57"><span class="c4">Discovering, annotating, and recognizing scene attributes. In CVPR, 2012. [27] S. Sabato and A. Kalai. Feature multi-selection among </span></p><p class="c47"><span class="c4">subjective features. In ICML (3), 2013. [28] B. Taskar, M. F. Wong, and D. Koller. Learning on the test </span></p><p class="c110"><span class="c4">data: Leveraging unseen features. In ICML, 2003. [29] A. Towsley, J. Pakianathan, and D. H. Douglass. </span></p><p class="c126"><span class="c4">Correlation angles and inner products: Application to a problem from physics. ISRN Applied Mathematics, 2011. [30] B. Trushkowsky, T. Kraska, M. J. Franklin, and P. Sarkar. </span></p><p class="c17"><span class="c4">Crowdsourced enumeration queries. In ICDE, 2013. [31] A. Wald. Sequential tests of statistical hypotheses. The </span></p><p class="c112"><span class="c4">Annals of Mathematical Statistics, 16(2), 1945. [32] P. Ye and D. Doermann. Combining preference and </span></p><p class="c113"><span class="c4">absolute judgements in a crowd-sourced setting. In ICML (3), 2013. </span></p><p class="c124"><span class="c35">420 </span></p></body></html>