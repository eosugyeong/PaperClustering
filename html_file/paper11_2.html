<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c87{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.7pt;font-family:"Arial";font-style:normal}.c39{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.7pt;font-family:"Arial";font-style:normal}.c43{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Arial";font-style:normal}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c97{margin-left:-33.1pt;padding-top:1.4pt;text-indent:42.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-9pt}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.3pt;font-family:"Arial";font-style:normal}.c75{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.5pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c29{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Courier New";font-style:normal}.c71{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.5pt;font-family:"Arial";font-style:normal}.c44{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.6pt;font-family:"Arial";font-style:normal}.c83{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9.1pt;font-family:"Arial";font-style:normal}.c95{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c65{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:7.1pt;font-family:"Arial";font-style:normal}.c79{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12pt;font-family:"Arial";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:normal}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c94{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.6pt;font-family:"Arial";font-style:normal}.c38{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8pt;font-family:"Arial";font-style:normal}.c63{margin-left:-33.1pt;padding-top:1.7pt;text-indent:42.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-9pt}.c66{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.1pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.4pt;font-family:"Arial";font-style:normal}.c53{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:7.8pt;font-family:"Arial";font-style:normal}.c55{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.5pt;font-family:"Courier New";font-style:normal}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.5pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.2pt;font-family:"Arial";font-style:normal}.c62{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.3pt;font-family:"Arial";font-style:normal}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c67{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:7pt;font-family:"Arial";font-style:normal}.c32{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.2pt;font-family:"Arial";font-style:normal}.c58{margin-left:-33.1pt;padding-top:1.7pt;text-indent:42.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-10.9pt}.c76{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.2pt;font-family:"Arial";font-style:normal}.c52{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.9pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.4pt;font-family:"Arial";font-style:normal}.c72{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4pt;font-family:"Arial";font-style:normal}.c59{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.8pt;font-family:"Arial";font-style:normal}.c74{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.4pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:9pt;font-family:"Arial";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:6pt;font-family:"Arial";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c37{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:7pt;font-family:"Arial";font-style:normal}.c25{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c85{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.3pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c73{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.6pt;font-family:"Arial";font-style:normal}.c26{margin-left:-33.1pt;padding-top:1.7pt;text-indent:42.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26.5pt}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11pt;font-family:"Arial";font-style:normal}.c48{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:7.3pt;font-family:"Arial";font-style:normal}.c89{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:15.2pt;font-family:"Arial";font-style:normal}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:normal}.c35{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:5.5pt;font-family:"Arial";font-style:normal}.c51{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:normal}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c36{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.4pt;font-family:"Arial";font-style:normal}.c49{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.6pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c56{margin-left:-24.1pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.8pt}.c40{margin-left:-24.1pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-6.7pt}.c45{margin-left:-28.6pt;padding-top:5.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:33.3pt}.c61{margin-left:-24.1pt;padding-top:22.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-17.8pt}.c93{margin-left:218.2pt;padding-top:69.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c21{margin-left:-5.4pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:82.3pt}.c17{margin-left:218.2pt;padding-top:66.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:217.9pt}.c70{margin-left:-33.1pt;padding-top:13.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-10.9pt}.c33{margin-left:-33.1pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-9pt}.c46{margin-left:-28.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-8.5pt}.c47{margin-left:-24.1pt;padding-top:13pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:126.7pt}.c42{margin-left:-24.1pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-14.4pt}.c57{margin-left:-28.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c77{margin-left:-33.1pt;padding-top:13.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:128.8pt}.c69{margin-left:-28.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-8.2pt}.c50{margin-left:-24.1pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:9.1pt}.c54{margin-left:-33.1pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-18.8pt}.c86{padding-top:3.8pt;text-indent:33pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c60{padding-top:4.1pt;text-indent:33pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c30{padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c78{padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c41{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c64{padding-top:146.4pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c22{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c80{margin-left:-24.1pt;text-indent:42.8pt;margin-right:26.2pt}.c92{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c82{margin-left:-24.1pt;margin-right:-8.2pt}.c81{margin-left:-15.2pt;margin-right:-18pt}.c68{margin-left:-24.1pt;margin-right:-12pt}.c91{margin-left:-24.1pt;margin-right:-5.3pt}.c90{margin-left:-28.6pt;margin-right:-7.8pt}.c27{margin-left:-24.1pt;margin-right:-18pt}.c88{margin-left:-28.6pt;margin-right:-6.8pt}.c96{margin-left:-28.6pt;margin-right:-6.6pt}.c84{margin-left:-14.2pt;margin-right:-9pt}.c98{margin-left:-24.1pt;margin-right:-13.7pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c92"><p class="c1"><span class="c95">Grid-Index Algorithm for Reverse Rank Queries </span></p><p class="c1"><span class="c19">Yuyang Dong </span><span class="c13">Department of Computer Science University of Tsukuba </span><span class="c24">touuyou@gmail.com </span></p><p class="c1"><span class="c13">Ibaraki, Japan </span><span class="c19">Hanxiong Chen </span><span class="c13">Department of Computer Science University of Tsukuba </span><span class="c24">chx@cs.tsukuba.ac.jp </span></p><p class="c1"><span class="c13">Ibaraki, Japan </span><span class="c19">Jeffrey Xu Yu </span><span class="c13">Department of System Engineering and Engineering Management The Chinese University of Hong Kong, China </span><span class="c24">yu@se.cuhk.edu.hk </span><span class="c19">Kazutaka Furuse </span><span class="c13">Department of Computer Science University of Tsukuba </span><span class="c24">furuse@cs.tsukuba.ac.jp </span></p><p class="c1"><span class="c13">Ibaraki, Japan </span><span class="c19">Hiroyuki Kitagawa </span><span class="c13">Department of Computer Science University of Tsukuba </span><span class="c24">kitagawa@cs.tsukuba.ac.jp </span><span class="c13">Ibaraki, Japan </span><span class="c19">ABSTRACT </span><span class="c2">In Rank-aware query processing, reverse rank queries have already attracted significant interests. Reverse rank queries can find matching customers for a given product based on individual customers&rsquo; preference. The results are used in numerous real-life applications, such as market analysis and product placement. Efficient processing of reverse rank queries is challenging because it needs to consider the combination on the given data set of user preferences and the data set of products. </span></p><p class="c5"><span class="c2">Currently, there are two typical reverse rank queries: Re- verse top-k and reverse k-ranks. Both prefer top-ranking products and the most efficient algorithms for them have a common methodology that indexes and prunes the data set using R-trees. This kind of tree-based algorithms suffers the problem that their performance in high-dimensional data de- clines sharply while high-dimensional data are significant for real-life applications. In this paper, we propose an efficient scan algorithm, named Grid-index algorithm (GIR), for pro- cessing reverse rank queries efficiently. GIR algorithm uses an approximate values index to save computations in scan- ning and only requires a little memory cost. Our theoretical analysis guarantees the efficiency and the experimental re- sults confirm that GIR has superior performance compared to tree-based methods in high-dimensional applications. </span></p><p class="c1"><span class="c19">CCS Concepts </span></p><p class="c1"><span class="c2">&bull;Theory of computation &rarr; Database query process- ing and optimization (theory); </span></p><p class="c5"><span class="c20">c 2017, Copyright is with the authors. Published in Proc. 20th Inter- national Conference on Extending Database Technology (EDBT), March 21-24, 2017 - Venice, Italy: ISBN 978-3-89318-073-8, on OpenProceed- ings.org. Distribution of this paper is permitted under the terms of the Cre- ative Commons license CC-by-nc-nd 4.0 </span></p><p class="c1"><span class="c3">Series ISSN: 2367-2005 306 </span><span class="c25">10.5441/002/edbt.2017.28 </span></p><p class="c5"><span class="c2">Figure 1: Example for RTK and RKR queries. (a): the top- 2 cell phones appreciated by users. (b): the RT-2 of each phone. (c): the rank list and the R1-R of each phone. </span></p><p class="c1"><span class="c19">Keywords </span><span class="c2">Reverse Rank Queries; High-dimensional Data Querying; </span></p><p class="c1"><span class="c19">1. INTRODUCTION </span></p><p class="c5"><span class="c2">Top-k queries retrieve top-k products based on a given user preference. As a user-view model, top-k queries are widely used in many applications as shown in [3,8]. As- suming that there is a dataset of user preferences, reverse rank queries (RRQ) have been proposed to retrieve the user preference that causes a given product to match the query condition. From the perspective of manufacturers, RRQ are essential to identify customers who may be interested in their products and to estimate the visibility of their fprod- ucts based on different user preferences. Not limited to the field of product (user) recommendations for e-commerce, this concept of user-product can be extended to a wider range of applications, such as business reviewing, dating and job hunting. </span></p><p class="c5"><span class="c2">Reverse top-k (RTK) [13,14] and reverse k-ranks (RKR) [22] are two typical RRQ queries. Figure 1 shows an exam- ple of RTK and RKR queries. In this example, five different cell phones are scored on how &ldquo;smart&rdquo; they are and the &ldquo;rat- ing&rdquo;. Also, there is a preferences database for three users. </span></p><p class="c1"><span class="c4">2</span><span class="c49">8 &middot;10</span><span class="c38">4 </span></p><p class="c1"><span class="c72">BBR SIM </span></p><p class="c1"><span class="c49">8 &middot;10</span><span class="c38">5 </span></p><p class="c1"><span class="c49">6644</span><span class="c4">2d 2-20 </span></p><p class="c1"><span class="c4">d 2-20 </span></p><p class="c1"><span class="c20">(a) BBR vs SIM. </span></p><p class="c1"><span class="c72">MPA SIM </span><span class="c49">) sm(emit</span><span class="c4">C</span><span class="c20">(b) MPA vs SIM. </span></p><p class="c1"><span class="c2">Figure 2: Performance of tree-base algorithms (BBR, MPA) and Simple scan on varying d (2-20). </span></p><p class="c5"><span class="c2">These preferences are based on a series of weights for each attribute. The score of a cell phone based on a user&rsquo;s pref- erence is found by a weighted sum function that computes the inner product of the cell phone attributes vector and the user preferences vector. Without loss of generality, we assume that minimum values are preferable. </span></p><p class="c1"><span class="c2">From the values in Figure 1, Tom&rsquo;s score for cell phone p</span><span class="c0">1 </span><span class="c2">is 0.6 &times; 0.8+0.7 &times; 0.2=0.62. All cell phones&rsquo; scores are calculated in the same way and ranked. If a cell phone is in the top-k of a user&rsquo;s rank list, then the user is in the result of the RTK query for that specific cell phone. In Figure 1 (b), the RT-2 results for each cell phone are shown. We can see that p</span><span class="c11">2</span><span class="c16">&rsquo;s RT-2 results are Tom, Jerry and Spike, </span><span class="c2">meaning that all users consider p</span><span class="c0">2 </span><span class="c2">as an element of their top-2 favorites. Notice that p</span><span class="c11">1 </span><span class="c16">and p</span><span class="c11">4 </span><span class="c16">have empty RT-2 </span><span class="c2">result sets, which means that every user prefers at least two other phones. [22] believed that it was not useful to return an empty answer and proposed RKR query, which find the top-k user preferences whose rank for the given product is highest among all users. In Figure 1(c), p</span><span class="c11">1 </span><span class="c16">is ranked 3rd </span><span class="c2">by Tom, 5th by Jerry, and 3rd by Spike. In other words, Tom (Spike) ranks p</span><span class="c0">1 </span><span class="c2">higher than other users, so he is in the answer of the R1-R of p</span><span class="c11">1</span><span class="c16">. </span><span class="c19">1.1 Notations and Problem Definition </span></p><p class="c41"><span class="c2">Each product p in the data set P is a d-dimensional vector, where each dimension is a numerical non-negative scoring attribute. p can be represented as a point p = (p[1], ..., p[d]), where p[i] is an attribute value on ith dimension of p. The data set of preferences, W, is defined in a similar way. w is a user preference vector for products where w &isin; W, and w[i] dimension, defined f1. </span><span class="c11">w</span><span class="c16">(p) </span><span class="c2">The is </span><span class="c16">= </span><span class="c2">the definitions as &sum;an user </span><span class="c11">di=1 </span><span class="c2">where inner w[i]&middot;p[i]. defined w[i] of product top-k &ge; weight Notations 0 query and of w value </span><span class="c8">&sum;</span><span class="c2">and and </span><span class="c0">d</span><span class="c11">i=1 </span><span class="c2">are for p, of w[i] summarized which the the = two attribute 1. is expressed The reverse in score on Table rank ith </span></p><p class="c1"><span class="c2">as is </span></p><p class="c1"><span class="c2">queries [13,22] are re-used here. </span></p><p class="c5"><span class="c2">Definition 1. (Top-k query): Given a positive integer k, a point set P and a user-defined weighting vector w, the resultant set TOP</span><span class="c11">k</span><span class="c16">(w) of the top-k query is a set of </span><span class="c2">points such that TOP</span><span class="c11">k</span><span class="c16">(w) &sube; P, |TOP</span><span class="c11">k</span><span class="c16">(w)| = k and &forall;p</span><span class="c11">i</span><span class="c16">,p</span><span class="c11">j</span><span class="c16">: </span><span class="c2">p</span><span class="c0">i </span><span class="c2">&isin; TOP</span><span class="c11">k</span><span class="c16">(w), p</span><span class="c11">j </span><span class="c16">&isin; P &minus; TOP</span><span class="c11">k</span><span class="c16">(w). Therefore, it holds that </span><span class="c2">f</span><span class="c11">w</span><span class="c16">(p</span><span class="c11">i</span><span class="c16">) &le; f</span><span class="c11">w</span><span class="c16">(p</span><span class="c11">j</span><span class="c16">). </span></p><p class="c5"><span class="c2">Definition 2. (RTK query): Given a query point q and k, as well as P and W (dataset of points and weighting vec- tors respectively), a weighting vector w</span><span class="c11">i </span><span class="c16">&isin; W belongs to the </span><span class="c2">reverse top-k result set of q, if and only if &exist;p &isin; TOP</span><span class="c11">k</span><span class="c16">(w</span><span class="c11">i</span><span class="c16">) </span><span class="c2">such that f</span><span class="c11">w</span><span class="c15">i</span><span class="c2">(q) &le; f</span><span class="c11">w</span><span class="c15">i</span><span class="c2">(p). </span></p><p class="c1"><span class="c49">) sm(emitUPUP</span><span class="c4">C2 8 12 16 20 </span></p><p class="c1"><span class="c4">2 8 12 16 20 </span></p><p class="c1"><span class="c62">Symbol Description d Data dimensionality P Data set of products (points) W Data set of weighting vectors q Query point f</span><span class="c32">w</span><span class="c62">(p) The score of p based on w, f</span><span class="c32">w</span><span class="c62">(p) = </span><span class="c71">&sum;</span><span class="c32">d</span><span class="c67">i=1</span><span class="c62">(w[i] &middot; q[i]). p[i] Value of a point p &isin; P on i th dimension p</span><span class="c37">(a) </span><span class="c62">Approximate index vector of a point p P </span><span class="c37">(A) </span><span class="c62">Approximate index vectors set &forall;p &isin; P n Number of partitions of value range Grid Grid-index L[f</span><span class="c32">w</span><span class="c62">(p)] Lower bound of score of p on w U[f</span><span class="c32">w</span><span class="c62">(p)] Upper bound of score of p on w q &#8826;</span><span class="c32">w </span><span class="c62">p q precedes p based on w </span></p><p class="c1"><span class="c2">Table 1: Notations and symbols </span></p><p class="c5"><span class="c2">Definition 3. (RKR query): Given a query point q and k, as well as P and W, reverse k-ranks returns a set S, where S &sube; W and |S| = k, such that &forall;w</span><span class="c11">i </span><span class="c16">&isin; S,&forall;w</span><span class="c11">j </span><span class="c16">&isin; (W &minus; S), </span><span class="c2">rank(w</span><span class="c0">i</span><span class="c2">,q) &le; rank(w</span><span class="c0">j</span><span class="c2">,q). </span></p><p class="c1"><span class="c2">The rank(w, q) is defined as the number of points with a smaller score than q for a given w. </span></p><p class="c1"><span class="c19">1.2 Motivation and Challenges </span></p><p class="c5"><span class="c2">To the best of our knowledge, the most efficient algorithm for processing RTK is the Branch-and-Bound (BBR) algo- rithm [17], and the most efficient algorithm for RKR is the Marked-Pruning-Approach (MPA) algorithm [22]. Both al- gorithms use a tree-based methodology, which uses an R-tree to index the data set and prune unnecessary entries through the use of MBRs (Minimum Bounding Rectangles). How- ever, as pointed out by [2,4,19], the use of R-tree or any other spatial indexes suffer from similar problems: When processing high-dimensional data sets, the performance de- clines to even worse than that of linear scan. </span></p><p class="c5"><span class="c2">Figure 2 shows the comparison of performance between tree-based algorithms (BBR, MPA) and the simple scan (SIM, linear scan). According to the results, SIM outper- forms these tree-based algorithms when processing RRQ in high dimensions. The reason for that inefficiency is that tree-based algorithms cannot divide data correctly in high dimensions, causing most of the MBRs to intersect with each other. Thus, even a small range query can overlap with a major proportion of the MBRs. </span></p><p class="c5"><span class="c2">Figure 3 shows a geometric view of processing RTK queries. In this example, suppose that we treat p</span><span class="c11">4 </span><span class="c16">as the query point </span><span class="c2">q, then a line that crosses q is perpendicular to Tom&rsquo;s weight vector. The points in the gray area have a greater rank than q. Tree-based methodology filters entries that are entirely in the gray area and counts the number of points contained in filtered entries to record the rank of q. However, because q is within overlapping parts of MBRs, the tree-based algo- rithm cannot filter any parts of MBRs containing the Tom or Jerry&rsquo;s preferences. As a result, it has to go through most entries one by one and compute the scores. In these cases, traversal of the tree-based spatial index is not an efficient method. </span></p><p class="c5"><span class="c2">For real-world applications, it is a natural requirement to process RRQ for high dimensional data (more than 3). Both the product&rsquo;s attributes and user&rsquo;s preferences are likely to be high-dimensional. For example, cell phones consumers care about many features, such as price, processor, storage, size, battery life, camera, etc. As another example, DIAN- </span></p><p class="c1"><span class="c3">307 </span></p><p class="c1"><span class="c23">e</span><span class="c15">3 </span></p><p class="c1"><span class="c23">T om(0.8, 0.2) </span></p><p class="c1"><span class="c20">(a) Tom </span></p><p class="c1"><span class="c23">e</span><span class="c15">3 </span></p><p class="c1"><span class="c23">Jerry(0.3, 0.7) </span></p><p class="c1"><span class="c20">(b) Jerry </span></p><p class="c1"><span class="c2">Figure 3: Tree-base methodology processing RTK and search space (gray). </span></p><p class="c5"><span class="c2">PING </span><span class="c14">1</span><span class="c2">, a Chinese business-reviewing website, ranks restau- rants by users&rsquo; reviews on overall rate, food flavor, cost, service, environment, waiting time, etc. Therefore, process- ing RRQ with a high-dimensional data set is a significant problem, and due to the so-called &ldquo;curse of dimensionality&rdquo;, simple scan offers a better performance than R-tree to solve it.</span><span class="c16">Despite its performances advantages on high-dimensional </span><span class="c2">queries, there are challenges in processing RRQ with the simple scan. RRQ are more complicated queries than sim- ple similarity searches such as the top-k query or the nearest neighbor search, and the time complexity of a naive simple scan method is O(|P|&times;|W|). RRQ require that every com- bination between P and W is checked before obtaining an answer. And this incurs a large number of pairwise com- putations. A comparison of 10K cell phones and 10K user preferences would necessitate 10K &times; 10K = 100M compu- tations. As a result, the enormous computational require- ments cause the CPU cost to outweigh the I/O cost, which is the opposite of what happen in normal situations. We hold a preliminary experiment to confirm this by measuring the elapsed time for reading different sizes of data, for process- ing RRQ queries and for the pairwise computations in the inner product. Table 2 shows that the time taken to read different sizes of data file is almost negligible in the RRQ processing. Rather, the major cost of processing RRQ is the pairwise computations. We also found that the propor- tion of pairwise computations in processing RRQ grew from about 50% in 6-dimensional data to 90% in 100-dimensional data. In conclusion, in contrast to the usual strategy of sav- ing I/O cost in other simple similarity searches, saving CPU computations is the key to process high-dimensional RRQ efficiently. </span></p><p class="c5"><span class="c2">For the above reasons, we develop an optimized version of the simple scan, called the Grid-index algorithm (GIR) which reduces the amount of multiplication of inner product in the processing. First, We pre-compute some approximate multiplication values and store them into a 2d array named Grid-index. Then we pre-process the data P and W and create the approximate vectors P </span><span class="c14">(A) </span><span class="c2">and W </span><span class="c14">(A) </span><span class="c2">which in- dicate the index. In the GIR algorithm, we first scan the approximate vectors P </span><span class="c14">(A) </span><span class="c2">and W </span><span class="c14">(A)</span><span class="c2">, then use them with the Grid-index to assemble upper and lower bounds, which help to filter most data without multiplications. After the filtering, we only need to refine few remaining data. In the </span></p><p class="c1"><span class="c0">1</span><span class="c16">http://www.dianping.com </span></p><p class="c1"><span class="c23">&bull;&bull;</span><span class="c28">p</span><span class="c23">3 </span></p><p class="c1"><span class="c15">p</span><span class="c23">&bull;</span><span class="c15">1 </span></p><p class="c1"><span class="c28">p</span><span class="c23">&bull;4 </span></p><p class="c1"><span class="c23">&bull;</span><span class="c28">p</span><span class="c23">3 </span></p><p class="c1"><span class="c15">p1 </span></p><p class="c1"><span class="c28">p</span><span class="c23">&bull;4 </span></p><p class="c1"><span class="c23">e</span><span class="c15">1 </span></p><p class="c1"><span class="c23">&bull; </span><span class="c15">p2 </span></p><p class="c1"><span class="c28">p</span><span class="c23">&bull;5 </span></p><p class="c1"><span class="c23">&bull; </span><span class="c15">p2 </span></p><p class="c1"><span class="c28">p</span><span class="c23">&bull;5 </span></p><p class="c1"><span class="c23">e</span><span class="c15">2 </span></p><p class="c1"><span class="c23">e</span><span class="c15">1 </span></p><p class="c1"><span class="c23">e</span><span class="c15">2 </span></p><p class="c1"><span class="c74">hhhhhhhhhhhhh </span><span class="c73">Elapsed time(ms) </span></p><p class="c1"><span class="c75">Data size </span><span class="c94">1K 10K 100K </span></p><p class="c22"><span class="c75">Reading data 5 26 146 Processing RRQ 240 9311 624318 &minus;Pairwise computations 103 5321 352511 </span></p><p class="c1"><span class="c2">Table 2: Time cost for reading data and processing reverse rank queries with 6-dimensional data. </span></p><p class="c1"><span class="c2">worst case, it costs the I/O time for reading the P </span><span class="c14">(A) </span><span class="c2">and W </span><span class="c14">(A)</span><span class="c2">, which is much less than original data and insignificant as concluded above. </span><span class="c19">1.3 Contributions </span></p><p class="c1"><span class="c2">The contributions of this paper are as follows: </span></p><p class="c5"><span class="c2">&bull; We elucidate that the simple scan is an appropriate way to process RRQ when processing high-dimensional data. We also demonstrate that CPU cost is the ma- jority cost and that it is much larger than I/O pro- cessing. We are the first to conclude that a better approach for processing RRQ is to optimize the scan method. </span></p><p class="c5"><span class="c2">&bull; We propose a Grid-index, which uses pre-calculated score bounds to reduce multiplications in the simple scan. Based on Grid-index, we propose GIR algo- rithm which processes RTK and RKR queries more efficiently. Our method outperforms tree-based algo- rithms in almost all cases and all data sets, except for those in very low (less than 4) dimensional cases. </span></p><p class="c5"><span class="c2">&bull; We analyze the filter performance of tree-based al- gorithms and establish the GIR performance model. Theoretical analysis clarifies the limitation of the tree- based methods. The performance model of proposed GIR guarantees the efficiency of the Grid-index method is achieved at a negligible memory cost. </span></p><p class="c5"><span class="c2">The rest of this paper is organized as follows: Section 2 summarizes the related work. Section 3 states the Grid- index concept and how to construct upper and lower bounds. In Section 4, we present the formal description of the GIR algorithm. Section 5 analyzes the performance of tree-based algorithms and gives a performance model for the Grid- index. Experimental results are shown in Section 6, and Section 7 concludes the paper. </span></p><p class="c1"><span class="c19">2. RELATED WORK </span></p><p class="c5"><span class="c2">For top-k queries, one possible approach to the top-k prob- lem is the Onion technique [3]. This algorithm precomputes and stores convex hulls of data points in layers like an onion. The evaluation of a linear top-k query is accomplished by starting from the outermost layer and processing these lay- ers inwardly. [8] proposed a system named PREFER that uses materialized views of top-k result sets that are very close to the scoring function in a query. </span></p><p class="c5"><span class="c2">Reverse rank queries (RRQ) are the reverse version of the top-k queries. A typical query of RRQ is the reverse top-k query. [13,14] introduced the reverse top-k query and presented two versions, namely monochromatic and bichro- matic, and proposed a reverse top-k Threshold Algorithm (RTA). [5] indexed a dataset with a critical k-polygon for monochromatic reverse top-k queries in two dimensions. [17] </span></p><p class="c1"><span class="c3">308 </span></p><p class="c54"><span class="c2">propose a tree-base, branch-and-bound (BBR) algorithm which is the state-of-the-art approach for reverse top-k query. BBR indexes both data sets P and W in two R-trees, and points and weighting vectors are pruned through the branch-and- bound methodology. For applications, reverse top-k query was used in [16] to identified the most influential products, and in [15] to monitor the popularity of locations based on user mobility. </span></p><p class="c63"><span class="c2">However, the reverse top-k query has a limitation that re- turns an empty result for an unpopular product. [22] intro- duced the reverse k-ranks query to ensure that any product in the data set can find their potential customers. Then pro- posed a tree-base algorithm named MPA (Marked Pruning Approach), which uses a d-dimensional histogram to index W and an R-tree to index P. Dong et al. [7] indicated that both reverse top-k and reverse k-rank queries were de- signed for only one product and cannot handle the product bundling. So they defined an aggregate reverse rank query that finds the top-k users for multiple query products. </span></p><p class="c97"><span class="c2">Other works also considered a given data point and aimed at finding the queries that have this data point in their re- sult set, such as the reverse (k) nearest neighbor (RNN or RKNN) [10,20] that finds points that consider the query point as the nearest neighbor. RKNN may looks similar to RRQ, but they are actually very different. RKNN evaluates relative L</span><span class="c0">p </span><span class="c2">distance in one Euclid space with between two certain points. On the other hand, RRQ focus on the abso- lute ranking value over all products, and the ranking scores are found through inner products of user preferences and products, from two different data spaces. </span></p><p class="c58"><span class="c2">For other reverse queries, the reverse furthest neighbor (RFN) [21] and its extension RKFN (reverse k furthest neigh- bor) [18] find points that consider a query point as their furthest neighbor. The reverse skyline query uses the ad- vantages of products to find potential customers based on the dominance of competitors products [6,11]. However, re- verse skyline query uses a desirable product data to describe the preference of a user. But in the definition of RRQ, the preference is described as a weighting vector. </span></p><p class="c63"><span class="c2">For the space-partition tree-based structure, R*-tree [1], a variation on R-tree, improves pruning performance by reducing overlap in the tree construction. [9] used Hilbert space-filling curves to impose a linear ordering on the data rectangles in R-tree and improve the performance. [2] in- vestigated and demonstrated the deficiencies of R-tree and R*-tree when dealing with high-dimensional data. As an improvement, a superior index structure named X-tree was proposed. X-tree uses a split algorithm to minimize over- lap and utilizes the concept of super-nodes. In our opinion, X-tree can be seen as a middle approach between the R- tree and simple scan methods, because it uses the spatial tree structure to process the disjoint parts, and uses linear scan with the overlapping parts. For high-dimensional data, there are very few disjoint parts, causing there to be almost no advantage to the construction and look-up features of the X-tree. </span></p><p class="c26"><span class="c2">It is well known that the overlapping nodes in high-dimensional space, is a shortcoming of tree structure. R. Weber et al. [19] proved that tree-based like [1,2] is worse than linear scan in high-dimensional data and proposed a VAFILE filtering strategy. They divided the data space into buckets equally and use these buckets&rsquo; upper and lower bounds to filter can- didates. The goal of using VAFILE is to save I/O cost by </span></p><p class="c5 c27"><span class="c2">Figure 4: Equally dividing value range into 4 partitions, allocating real values into approximate intervals and getting the approximate vector p</span><span class="c14">(a) </span><span class="c2">and w</span><span class="c14">(a)</span><span class="c2">. </span></p><p class="c27 c64"><span class="c2">Figure 5: 4&times;4 Grids for points and weighting vectors, map- ping p</span><span class="c14">(a) </span><span class="c2">and w</span><span class="c14">(a) </span><span class="c2">onto Grids. </span></p><p class="c61"><span class="c2">scanning the bit-compressed file of buckets. However, we purpose to save the CPU computing in RRQ. [4] proposed a technique by &ldquo;indexing the function&rdquo; that pre-computing some key values of the L</span><span class="c11">p</span><span class="c16">-distance function to avoid the </span><span class="c2">expensive computing in high-dimensional nearest neighbour search. </span></p><p class="c47"><span class="c19">3. GRID-INDEX </span></p><p class="c27 c86"><span class="c2">According the statement in Section 1.2, it stands to reason that using a simple scan with high-dimensional data is the most efficient approach. However, in this method, the multi- plications of inner products take most of the processing time. We were inspired to study a method that could enhance the efficiency of the simple scan by avoiding multiplications for the inner product. In this section, we introduce the concept of Grid-index, which stores pre-calculated approximate mul- tiplication values. The approximate values can form upper and lower bounds of a score and can be used in a filtering step for the simple scan approach. </span><span class="c19">3.1 Approximate Values in Grid-index </span></p><p class="c27 c60"><span class="c2">Concept of Grids. To confirm that the resultant score of the weighted sum function (inner product) is fair, all val- ues in p must be in the same range, so must all values in w. We use this feature to allocate values into value ranges. As Figure 4 shows, in this example we partition the value range into 4 equal intervals. For the given p = (0.62,0.15, 0.73), the first attribute p[1] = 0.62 falls into the third partition [0.5,0.75]. The second, p[2] = 0.15, falls into the first parti- tion [0, 0.25]. We will store the partition numbers as an ap- proximate vector, denoted as p</span><span class="c14">(a) </span><span class="c2">and w</span><span class="c14">(a)</span><span class="c2">, so p</span><span class="c14">(a) </span><span class="c2">= (2,0,2) and w</span><span class="c14">(a) </span><span class="c2">= (0,2,1). </span></p><p class="c30 c81"><span class="c2">Since the inner product is the sum of pairwise multipli- </span></p><p class="c17"><span class="c3">309 </span></p><p class="c5"><span class="c2">cations of p[i] and w[i], we combine the ranges of p and w to form the grids. Figure 5 illustrates the 4 &times; 4 grids in this example. We can map an arbitrary pair of (p[i],w[i]) onto a certain grid, and different (p[i],w[i]) pairs may share the same grid location. The purpose of mapping the pairs onto the grid is to use the grids&rsquo; corners to estimate the score of p[i]&middot; w[i]. By taking advantage of values having the same range, these grids can be re-used for mapping all pairs (p[i],w[i]), i &isin; [1,d], p &isin; P and w &isin; W. </span></p><p class="c5"><span class="c2">Construction of Grid-index. Assume that we divide the value range of p and w into n = 2</span><span class="c14">b </span><span class="c2">partitions, and the po- sition information of all elements in a vector are represented by a (n+1)-element vector &alpha;</span><span class="c11">p </span><span class="c16">for points and &alpha;</span><span class="c11">w </span><span class="c16">for weights. </span><span class="c2">In the example of Figure 4, &alpha;</span><span class="c0">p </span><span class="c2">= &alpha;</span><span class="c0">w </span><span class="c2">= (0,0.25,0.5,0.75,1). The Grid-index, denoted as Grid, is a 2-dimensional array and saves all multiplication results of all combinations be- tween &alpha;</span><span class="c0">p </span><span class="c2">and &alpha;</span><span class="c0">w</span><span class="c2">: </span></p><p class="c1"><span class="c2">Grid[i][j] = &alpha;</span><span class="c11">p</span><span class="c16">[i] &middot; &alpha;</span><span class="c11">w</span><span class="c16">[j], i,j &isin; [0,n] (1) </span></p><p class="c22"><span class="c2">Score Bounds and Precedence. According to the above Grid partition, we pre-store all approximate vectors for P and W, denoted as P </span><span class="c14">(A) </span><span class="c2">and W </span><span class="c14">(A)</span><span class="c2">. The approximate vector p</span><span class="c14">(a) </span><span class="c2">for a given p is calculated by p</span><span class="c14">(a)</span><span class="c2">[i] = &lfloor;p[i]&middot;n/r&rfloor;, where r is the range of p[i]&rsquo;s attribute value. w</span><span class="c14">(a) </span><span class="c2">is calcu- lated from w in the same way. Clearly, for a pair (p[i],w[i]) in the ith dimension, Grid[p</span><span class="c14">(a)</span><span class="c2">[i]][w</span><span class="c14">(a)</span><span class="c2">[i]] is the lower bound and Grid[p</span><span class="c14">(a)</span><span class="c2">[i] + 1][w</span><span class="c14">(a)</span><span class="c2">[i] + 1] is the upper bound. In the example, p[1] = 0.62, w[1] = 0.12 and p</span><span class="c14">(a)</span><span class="c2">[1] = 2, w</span><span class="c14">(a)</span><span class="c2">[1] = 0. Based on Equation (1), Grid[2][0] = 0.5 &times; 0, Grid[2+1][0+1] = 0.75&times;0.25, meaning 0.5&times;0 &le; p[1]&middot;w[1] &le; 0.75 &times; 0.25. For the inner properties of product the inner f</span><span class="c0">w</span><span class="c2">(p) product = and </span><span class="c8">&sum;</span><span class="c0">d</span><span class="c11">i=1 </span><span class="c2">features p[i] &middot; w[i], of based on the Grid- index, we know that: </span></p><p class="c1"><span class="c2">L[f</span><span class="c11">w</span><span class="c16">(p)] &le; f</span><span class="c11">w</span><span class="c16">(p) &le; U[f</span><span class="c11">w</span><span class="c16">(p)] (2) </span></p><p class="c1"><span class="c2">where L[f</span><span class="c0">w</span><span class="c2">(p)] and U[f</span><span class="c0">w</span><span class="c2">(p)], denoting the lower bound and the upper bound of f</span><span class="c11">w</span><span class="c16">(p), are given by </span></p><p class="c1"><span class="c2">L[f</span><span class="c0">w</span><span class="c2">(p)] = </span></p><p class="c1"><span class="c16">&sum;</span><span class="c0">d</span><span class="c11">i=1 </span></p><p class="c1"><span class="c2">Grid[p</span><span class="c14">(a)</span><span class="c2">[i]][w</span><span class="c14">(a)</span><span class="c2">[i]] (3) </span></p><p class="c1"><span class="c2">U[f</span><span class="c11">w</span><span class="c16">(p)] = </span></p><p class="c1"><span class="c16">&sum;</span><span class="c0">d</span><span class="c11">i=1 </span></p><p class="c1"><span class="c2">Grid[p</span><span class="c14">(a)</span><span class="c2">[i] + 1][w</span><span class="c14">(a)</span><span class="c2">[i] + 1] (4) </span></p><p class="c1"><span class="c2">The relationship between p and q can be classified into three cases with the help of L[f</span><span class="c11">w</span><span class="c16">(p)] and U[f</span><span class="c11">w</span><span class="c16">(p)]: </span></p><p class="c1"><span class="c2">&bull; Case 1 (p &#8826;</span><span class="c0">w </span><span class="c2">q): If U[f</span><span class="c0">w</span><span class="c2">(p)] &lt; f</span><span class="c0">w</span><span class="c2">(q), p precedes q, p has a higher rank than q with w. </span></p><p class="c1"><span class="c2">&bull; Case 2 (q &#8826;</span><span class="c11">w </span><span class="c16">p): If L[f</span><span class="c11">w</span><span class="c16">(p)] &gt; f</span><span class="c11">w</span><span class="c16">(q), q precedes p, p </span><span class="c2">does not affect the rank of q with w. </span></p><p class="c5"><span class="c2">&bull; Case 3 (p q): Otherwise, p and q are incomparable, i.e., L[f</span><span class="c11">w</span><span class="c16">(p)] &le; f</span><span class="c11">w</span><span class="c16">(q) &le; U[f</span><span class="c11">w</span><span class="c16">(p)]. The Grid-index </span><span class="c2">cannot define whether p or q ranks higher with w. </span></p><p class="c1"><span class="c2">Filtering Strategy. We scan the approximate vectors first, then use the Grid-index to obtain L[f</span><span class="c0">w</span><span class="c2">(p)] and U[f</span><span class="c0">w</span><span class="c2">(p)], and filter points that satisfy either Case 1 or Case 2 above. After scanning, if necessary, we carry out a refining phase, and compute the real score for all points in Case 3. Notice </span></p><p class="c1"><span class="c2">Figure 6: 6-bit string for compressing the p to p</span><span class="c14">(a)</span><span class="c2">. </span></p><p class="c1"><span class="c2">that throughout this process, we only calculated the sum and retrieved L[f</span><span class="c0">w</span><span class="c2">(p)] and U[f</span><span class="c0">w</span><span class="c2">(p)] of Equations (3) and (4). If a point p is in Case 1 or Case 2, we do not need to compute the real score f</span><span class="c0">w</span><span class="c2">(p), thus saving computational costs with multiplications to find the inner product. </span><span class="c19">3.2 Compress the Approximate Vectors </span></p><p class="c5"><span class="c2">Storing all approximate vectors incurs extra storage costs for data sets P and W. To compress this storage, each ap- proximate vector can be presented by a bit-string describing the interval which its elements fall. Figure 6 shows an ex- ample where the approximate vector p</span><span class="c14">(a) </span><span class="c2">is saved as a 6-bit string (100010), because 2 bits are needed to define 4 par- titions for each of the 3 dimensions. Generally, if we divide the value range into 2</span><span class="c14">b </span><span class="c2">partitions, then a (b &times; d)-bit string is needed to store an approximate vector. According to the analysis in Section 5.3, b = 6 is enough for a good filter- ing performance. Usually, the original data is a 64-bit float value, so the storage overhead by the compressed 6-bit data is less than 1/10 of the original data </span><span class="c14">2</span><span class="c2">. This kind of bit- string compressing technique is also used in [19]. </span></p><p class="c5"><span class="c2">Reading approximate vectors with bit-string binary com- pression only has half the time costs compared to regular I/O operations. However, the superiority of I/O cost can be ignored because the CPU cost is far greater than the I/O cost in RRQ, as discussed in Section 1.2. </span></p><p class="c5"><span class="c2">It may be argued that it would be the most efficient to store all the scores of each p and w directly. In reality, storing that amount of data is impossible due to the immense cost. For example, assume that there are 10K products and 10K weight vectors. For Grid-index, 20K tuples are needed to store the approximate vectors, but it would take 10K &times; 10K = 100M tuples to store all the scores. The storage overhead for storing all scores is thousands of times of the approximate vectors in the proposed Grid-index method. </span></p><p class="c1"><span class="c19">4. THE GIR ALGORITHM </span></p><p class="c1"><span class="c2">Next, we use the Grid-index methodology to propose two versions of Grid-indexing algorithm for RTK and RKR queries. The two algorithms can be implemented easily by using the GInTop-k function that efficiently obtains the rank of query point q on a certain input w. </span><span class="c19">4.1 GInTop-</span><span class="c2">k </span><span class="c19">Function Based on Grid-index </span></p><p class="c1"><span class="c2">Algorithm 1 describes the GInTop-k function based on Grid-index. GInTop-k scans each approximate vector p</span><span class="c14">(a) </span></p><p class="c1"><span class="c0">j </span><span class="c8">&isin; </span><span class="c2">P </span><span class="c14">(A) </span><span class="c2">&minus; Domin. Domin is a global variable denoting a buffer </span></p><p class="c5"><span class="c0">2</span><span class="c16">When n = 2</span><span class="c0">b</span><span class="c16">, then the storage cost for the approximate </span><span class="c2">vectors are |P </span><span class="c14">(A)</span><span class="c2">| = </span><span class="c14">b</span><span class="c0">64</span><span class="c8">|P| and |W </span><span class="c14">(A)</span><span class="c2">| = </span><span class="c14">b</span><span class="c0">64</span><span class="c8">|W|, if P and </span><span class="c2">W&rsquo;s attributes are float values. </span></p><p class="c1"><span class="c3">310 </span></p><p class="c1"><span class="c2">Algorithm 1 Grid-index checking q&rsquo;s rank (GInTop-k) </span></p><p class="c1"><span class="c2">Require: P</span><span class="c14">(A)</span><span class="c2">,w</span><span class="c14">(a) </span></p><p class="c1"><span class="c0">i </span><span class="c8">, q, k, Grid, Domin </span><span class="c2">Ensure: -1: discard w</span><span class="c0">i</span><span class="c2">, rnk: include w</span><span class="c0">i </span></p><p class="c1"><span class="c2">1: Cand &larr; &empty; 2: rnk &larr; Domin.size 3: for each p</span><span class="c14">(a) </span></p><p class="c1"><span class="c0">j </span><span class="c8">&isin; P </span><span class="c14">(A)</span><span class="c2">&minus; Domin do 4: Calculate U[f</span><span class="c11">w</span><span class="c16">(p</span><span class="c11">j</span><span class="c16">)] by Eq. (4) </span><span class="c2">5: if U[f</span><span class="c0">w</span><span class="c15">i</span><span class="c2">(p</span><span class="c0">j</span><span class="c2">)] &le; f</span><span class="c0">w</span><span class="c15">i</span><span class="c2">(q) then 6: rnk ++ // (p</span><span class="c11">j </span><span class="c16">&#8826;</span><span class="c11">w </span><span class="c16">q) </span><span class="c2">7: if p</span><span class="c0">j </span><span class="c2">&#8826; q then 8: Domin &larr; Domin &cup; {p</span><span class="c11">j</span><span class="c16">} </span><span class="c2">9: if rnk &ge; k then 10: return &minus;1 11: else 12: Calculate L[f</span><span class="c0">w</span><span class="c2">(p</span><span class="c0">j</span><span class="c2">)] by Eq. (3) 13: if L[f</span><span class="c11">w</span><span class="c15">i</span><span class="c2">(p</span><span class="c11">j</span><span class="c16">)] &le; f</span><span class="c11">w</span><span class="c15">i</span><span class="c2">(q) &le; U[f</span><span class="c11">w</span><span class="c15">i</span><span class="c2">(p</span><span class="c11">j</span><span class="c16">)] then </span><span class="c2">14: Cand &larr; Cand &cup; {p</span><span class="c0">j</span><span class="c2">} // (p</span><span class="c0">j </span><span class="c2">q) 15: Refine Cand : compare real score and updating rnk. 16: if rnk &ge; k then 17: return &minus;1 18: else 19: return rnk </span></p><p class="c1"><span class="c2">recording dominating points. If p is in Domin, then ev- ery attribute of a point p is smaller than the correspond- ing attribute in q (&forall;p[i],i &isin; (0,d) : p[i] &lt; q[i]). Once q is given, points in Domin always rank better than q. During scanning, the number of points that rank better than q are counted by rnk, which is initialized by the size of Domin (line 2). The upper bound for the score is obtained using Grid-index (line 4). If the upper bound is smaller than the score of q (Case 1, line 5), then p</span><span class="c11">j </span><span class="c16">must have a better rank </span><span class="c2">than q for the weighting vector w</span><span class="c0">i</span><span class="c2">, hence rnk increases by 1 (line 6). Anytime p</span><span class="c11">j </span><span class="c16">is found dominating q, denoted by </span><span class="c2">p</span><span class="c0">j </span><span class="c2">&#8826; q, p</span><span class="c0">j </span><span class="c2">will be appended to Domin (line 7-8). Whenever rnk reaches k (line 9), there are at least k points that rank better than q, thus the current w</span><span class="c11">i </span><span class="c16">is not a result of RTK of q </span><span class="c2">(&minus;1 is returned). Otherwise, we get a lower bound from the Grid-index (line 12). If q s score is between the lower bound and upper bound of p</span><span class="c0">j</span><span class="c2">&rsquo;s score (in Case 3, line 13), then p</span><span class="c0">j </span><span class="c2">is added to Cand for further refinement(line 14). After scan- ning P </span><span class="c14">(A)</span><span class="c2">, if the algorithm did not return a decision, then a refinement step is necessary to establish (line 15). We check the original data of the points held in Cand and refine rnk in the same way, terminating immediately when it reaches k.</span><span class="c16">Computing f</span><span class="c11">w</span><span class="c16">(p) requires d multiplication operations and </span><span class="c2">d addition operations. However, to find U[f</span><span class="c0">w</span><span class="c2">(p)] and L[f</span><span class="c0">w</span><span class="c2">(p)], it is only necessary to carry out d addition operations. There- fore, our approach will save d times of multiplication if U[f</span><span class="c0">w</span><span class="c2">(p)] &le; f</span><span class="c0">w</span><span class="c2">(q) and the algorithm uses the branch at lines 5-10. When U[f</span><span class="c11">w</span><span class="c16">(p)] &ge; f</span><span class="c11">w</span><span class="c16">(q), our approach requires </span><span class="c2">another d addition operations to find L[f</span><span class="c0">w</span><span class="c2">(p)], that is, an equivalent amount of additions to replace the multiplication operations in the evaluation of f</span><span class="c11">w</span><span class="c16">(p). In conclusion, using </span><span class="c2">this method will save computational cost if any point can be filtered by the Grid-index. Section 5.3 proves that a low cost Grid-index can be used to filter over 99% of points. </span></p><p class="c1"><span class="c19">4.2 Grid-index Algorithm </span></p><p class="c1"><span class="c2">Now we introduce how Grid-index is applied to RRQ. Al- </span></p><p class="c1"><span class="c2">gorithm 2 and Algorithm 3 give the implementation of RTK and RKR. </span></p><p class="c1"><span class="c2">For each approximate vector of w</span><span class="c14">(a) </span></p><p class="c5"><span class="c0">i </span><span class="c8">&isin; W </span><span class="c14">(A)</span><span class="c2">, Algorithm 2 receives the result of filtering performed by GInTop-k (line 4). If the current w</span><span class="c11">i </span><span class="c16">ranks q in its top-k, then w</span><span class="c11">i </span><span class="c16">will be </span><span class="c2">added into the result set of RTOPk(q) (Line 5-6). If there exists more than k dominating points of q, the algorithm returns an empty set because q cannot be part of the top-k for any weighting vector w (line 7-8). </span></p><p class="c5"><span class="c2">Unlike RTK, a heap structure of size k, denoted by heap, and a value minRank are introduced in Algorithm 3 for pro- cessing the RKR. For each w</span><span class="c14">(a) </span></p><p class="c5"><span class="c0">i </span><span class="c8">&isin; W </span><span class="c14">(A)</span><span class="c2">, function GInTop-k is called first, minRank is passed to GInTop-k and used for filtering (line 5). If q ranks in the top-minRank (line 6), we insert w</span><span class="c11">i </span><span class="c16">and rnk into the heap. The last rank of heap </span><span class="c2">is pushed out after it holds more than k elements (line 7). Meanwhile, minRank is updated by the current last rank of heap (line 8). This ensures a self-refined bound and keeps the current k best results from W in heap. Finally, when the algorithm terminates, heap is returned as the result set. </span></p><p class="c1"><span class="c2">Algorithm 2 Grid-index Reverse top-k (GIRTop-k) </span></p><p class="c1"><span class="c2">Input: P </span><span class="c14">(A)</span><span class="c2">,W </span><span class="c14">(A)</span><span class="c2">, q, k Output: RTK result set RTOPk(q) </span></p><p class="c1"><span class="c2">1: create Grid (Grid-index) 2: Domin &larr; {&empty;} 3: for each w</span><span class="c14">(a) </span></p><p class="c22"><span class="c0">i </span><span class="c8">&isin; W </span><span class="c14">(A) </span><span class="c2">do 4: rnk &larr; GInTop-k(P </span><span class="c14">(A)</span><span class="c2">,w</span><span class="c14">(a) </span></p><p class="c1"><span class="c0">i </span><span class="c8">, q, k, Grid, Domin) </span><span class="c2">5: if rnk = &minus;1 then 6: RTOPk(q) &larr; RTOPk(q) &cup; {w</span><span class="c11">i</span><span class="c16">} </span><span class="c2">7: if Domin.size &ge; k then 8: return {&empty;} 9: return RTOPk(q) </span></p><p class="c1"><span class="c2">Algorithm 3 Grid-index Reverse k-ranks (GIRk-Rank) </span></p><p class="c1"><span class="c2">Input: P </span><span class="c14">(A)</span><span class="c2">,W </span><span class="c14">(A)</span><span class="c2">, q, k Output: heap = RKR result set </span></p><p class="c1"><span class="c2">1: create Grid (Grid-index) 2: heap &larr; {&empty;}, Domin &larr; {&empty;} 3: minRank &larr; &infin; 4: for each w</span><span class="c14">(a) </span></p><p class="c22"><span class="c0">i </span><span class="c8">&isin; W </span><span class="c14">(A) </span><span class="c2">do 5: rnk &larr; GInTop-k(P</span><span class="c14">(A)</span><span class="c2">,w</span><span class="c14">(a) </span></p><p class="c1"><span class="c0">i </span><span class="c8">, q, minRank, Grid, </span><span class="c2">Domin) 6: if rnk = &minus;1 then 7: heap.insert (w</span><span class="c11">i</span><span class="c16">, rnk) </span><span class="c2">8: minRank &larr; heap&rsquo;s last rank. 9: return heap </span></p><p class="c1"><span class="c19">5. PERFORMANCE ANALYSIS </span></p><p class="c1"><span class="c2">In this section, we first analyze the weakness of tree-based algorithms for RRQ. We then build a cost model for Grid- index that finds the ideal number of grids (n &times; n), guaran- teeing that specified filtering performance. </span><span class="c19">5.1 The Difficulty of Space-division in High </span><span class="c24">Dimensional Data </span><span class="c2">We first observe the influence of the number of divisions through a space-division index. According to [22], MPA </span></p><p class="c1"><span class="c3">311 </span></p><p class="c1"><span class="c2">1 1 </span></p><p class="c1"><span class="c23">W</span><span class="c15">group</span><span class="c23">.u </span></p><p class="c1"><span class="c20">y </span></p><p class="c1"><span class="c23">W</span><span class="c15">group</span><span class="c23">.u </span><span class="c20">y </span></p><p class="c1"><span class="c23">W</span><span class="c15">group</span><span class="c23">.l </span></p><p class="c1"><span class="c28">&gamma; </span><span class="c23">W</span><span class="c15">group</span><span class="c23">.l </span></p><p class="c22"><span class="c20">R</span><span class="c0">p </span><span class="c15">&gamma; </span><span class="c20">R</span><span class="c0">p</span><span class="c20">x</span><span class="c11">i </span><span class="c20">p </span></p><p class="c1"><span class="c23">x</span><span class="c15">i</span><span class="c23">+x </span><span class="c15">2 </span></p><p class="c1"><span class="c15">i </span><span class="c51">x </span><span class="c11">i </span></p><p class="c1"><span class="c20">0 </span></p><p class="c1"><span class="c20">x </span></p><p class="c1"><span class="c2">1 </span></p><p class="c1"><span class="c20">0 </span><span class="c85">x = 1 &minus; &gamma; </span></p><p class="c1"><span class="c2">1 </span></p><p class="c1"><span class="c20">(a) Trapezoidal prism </span></p><p class="c1"><span class="c20">(b) Tetrahedron </span></p><p class="c1"><span class="c2">Figure 7: Two kinds of Filtering areas (gray) of R-tree. </span></p><p class="c5"><span class="c2">uses a d-dimensional histogram to group all weighting vec- tors W into buckets. Each dimension is partitioned into c equal-width intervals, in total, there are c</span><span class="c14">d </span><span class="c2">buckets. As [22] suggests, c = 5, If |W| = 100K with the 3-dimensional data, W is grouped in 5</span><span class="c14">3 </span><span class="c2">= 125 buckets. However, if d = 10, then there are 5</span><span class="c14">10 </span><span class="c2">&asymp; 9 million buckets. It is not logical to filter only 100K weight vectors by testing the upper and lower bounds of such a huge number of buckets. In this case, scan- ning one by one would be more efficient. </span></p><p class="c1"><span class="c19">5.2 Analysis of R-tree Filtering Performance </span></p><p class="c1"><span class="c2">We test some range queries (within 1% area of the data space) over different d with an R-tree and observe the MBRs. Table 3 shows the average value of accessed MBRs&rsquo; attributes. Not surprisingly, when d &gt; 6, all (100%) of MBRs overlap in the query range, which means that all entries will be ac- cessed during processing. As mentioned in Section 1.2, it is a shortcoming of tree-based algorithms that the MBRs will always overlap with each other when the data is high- dimensional. </span></p><p class="c5"><span class="c2">Besides the shortcoming from the tree-based index itself, we also found that the filterable space of RRQ with tree- based methodology reduces as the dimensionality increases. This conclusion is supported by the following estimation. </span></p><p class="c41"><span class="c2">Consider a tree-based algorithm that constructs an R-tree for the products P and assume that R</span><span class="c0">p </span><span class="c2">is a MBR of this R- tree. In query processing, for each group of w&rsquo;s (denoted as W</span><span class="c11">group</span><span class="c16">), points within R</span><span class="c11">p </span><span class="c16">are checked. The upper and </span><span class="c2">lower of W</span><span class="c11">group </span><span class="c2">bounds </span><span class="c16">and </span><span class="c2">of </span><span class="c16">R</span><span class="c2">f</span><span class="c11">p</span><span class="c16">. </span><span class="c11">W</span><span class="c15">group</span><span class="c16">As Figure </span><span class="c2">(R</span><span class="c11">p</span><span class="c16">) are 7 shows, determined The gray by the area borders is the </span><span class="c2">safely filtered space. The shape of the gray area can be a hyper-prism, a hyper-tetra or a combination of the two. It means that in some of the dimensions (denoted as g) the area will be a triangle, while a trapezoid in others. Assume that the two kinds of shapes are separated clearly; then the proportion of filtered values can be obtained by measuring the volume: </span></p><p class="c1"><span class="c2">V ol = V ol</span><span class="c11">T etraX </span><span class="c16">&middot; V ol</span><span class="c11">P rismX </span><span class="c16">+ V ol</span><span class="c11">T etraY </span><span class="c16">&middot; V ol</span><span class="c11">P rismY </span><span class="c16">(5) </span></p><p class="c5"><span class="c2">To give an analytical result, we assume that R</span><span class="c0">p </span><span class="c2">is in the centroid, so the two filtering areas are equal (V ol</span><span class="c11">T etraX </span><span class="c16">= </span><span class="c2">V ol</span><span class="c0">T etraY </span><span class="c2">). Then the volume becomes </span></p><p class="c1"><span class="c2">V ol = 2 &middot; V ol</span><span class="c11">T etra </span><span class="c16">&middot; V ol</span><span class="c11">P rism </span><span class="c16">(6) </span></p><p class="c1"><span class="c2">Firstly, the volume of hyper-tetra is: </span><span class="c14">3 </span></p><p class="c1"><span class="c2">V ol</span><span class="c11">T etra </span><span class="c16">= g!</span><span class="c2">1( </span></p><p class="c1"><span class="c16">&prod;</span><span class="c11">i=1 </span><span class="c0">g</span><span class="c2">x</span><span class="c11">i</span><span class="c16">) = g!</span><span class="c2">1(1 &minus; &gamma;)</span><span class="c14">g </span><span class="c2">(7) </span></p><p class="c1"><span class="c2">then, the volume of the hyper-prism (the area in Figure 7 (a)) is: </span></p><p class="c1"><span class="c2">S</span><span class="c11">i </span><span class="c16">= </span><span class="c2">1</span><span class="c16">2</span><span class="c2">(x</span><span class="c11">i </span><span class="c16">+ x </span><span class="c11">i</span><span class="c2">) &middot; H &le; (</span><span class="c8">1 &minus; </span><span class="c2">2 </span><span class="c8">&gamma; </span></p><p class="c5"><span class="c8">) &le; 1</span><span class="c2">2 </span><span class="c8">(8) </span><span class="c2">where H = 1 is the length of the side. Imagine a 3 dimen- sional trapezoidal prism in the figure, the volume is: </span></p><p class="c1"><span class="c2">V ol</span><span class="c11">P rism3d </span><span class="c16">= </span><span class="c2">1</span><span class="c16">3</span><span class="c2">(S</span><span class="c11">1 </span><span class="c16">+ S</span><span class="c11">2 </span><span class="c16">+ </span><span class="c2">&radic;</span><span class="c16">S</span><span class="c11">1</span><span class="c16">S</span><span class="c11">2</span><span class="c16">) &middot; H &le; </span><span class="c2">1</span><span class="c16">2 </span><span class="c2">(9) </span></p><p class="c1"><span class="c2">This result holds for higher dimensional trapezoidal prisms. Consequently, the maximum volume gives the filtered area. </span></p><p class="c1"><span class="c2">V ol</span><span class="c0">max </span><span class="c2">= 2 &middot; g!</span><span class="c8">1(1 &minus; &gamma;)</span><span class="c14">g </span><span class="c2">&middot; </span><span class="c8">1</span><span class="c2">2 </span><span class="c8">= </span><span class="c2">g!</span><span class="c8">1(1 &minus; &gamma;)</span><span class="c14">g </span><span class="c2">(10) </span></p><p class="c5"><span class="c2">It is reasonable to assume that in half of the dimensions the filtered area is hyper-tetra in shape. We will consider a dataset of d = 10, g = 5, according to Equation (10), R-tree based methods can only filter at most space. </span></p><p class="c1"><span class="c14">1</span><span class="c0">5! </span><span class="c8">=0.8% of the data </span></p><p class="c1"><span class="c2">This clearly shows that the space filtered by R-trees in RRQ becomes very small when encountering high-dimensional data. For all points in the space which can not be filtered, each w[i] &middot; p[i] must be calculated and compared with that of the query point. </span></p><p class="c1"><span class="c19">5.3 The Performance Model of Grid-index </span></p><p class="c5"><span class="c2">To build a model of our Grid-index, we make the following assumption about the d-dimensional point data set: Values in all dimensions are independent of each other, and the sub-score in each dimension (w[i] &middot; p[i]) follows a uniform distribution. Both value ranges of P and W are divided into n partitions for the Grid-index. </span></p><p class="c5"><span class="c2">Let the probability of a score S falling into a certain in- terval (a, b) be Prob(a&lt;S&lt;b), where (a, b) is created by Grid-index. Data points with scores outside of (a, b) can be filtered. We denote the filtering performance F by: </span></p><p class="c1"><span class="c2">F(a, b)=1 &minus; Prob(a&lt;S&lt;b). (11) </span></p><p class="c1"><span class="c2">For example, if the probability of a point falling in an inter- val is 5%, then we say that the filter performance is 95%. </span></p><p class="c5"><span class="c2">Obviously, F(a, b) from Grid-index depends on the den- sity of the grids (n &times; n). More partitions n lead to smaller Prob(a&lt;S&lt;b) and better filtering performance. However, larger n requires more memory, so it is important to find a suitable n that balances these factors. For this purpose, we first establish specific score properties and then define the relationship between F and n. </span></p><p class="c5"><span class="c2">For the case of one dimension, dividing the range into equally n</span><span class="c14">2 </span><span class="c2">partitions, the probability of a point p&rsquo;s score falling into a certain interval is obviously: </span></p><p class="c1"><span class="c2">Prob( n</span><span class="c8">k</span><span class="c14">2 </span><span class="c8">&lt; w &middot; p &lt; k + </span><span class="c2">n</span><span class="c14">2 </span><span class="c8">1 </span></p><p class="c1"><span class="c8">) = </span><span class="c2">n</span><span class="c8">1</span><span class="c14">2 </span><span class="c8">, k = 1,2, ..., n</span><span class="c14">2</span><span class="c2">. (12) </span></p><p class="c41"><span class="c0">3</span><span class="c16">Recall that the area of a right </span><span class="c2">tetrahedron has volume v = </span><span class="c14">x</span><span class="c0">3 </span><span class="c23">3</span><span class="c14">s</span><span class="c2">the volume is V</span><span class="c11">d&minus;1 </span><span class="c16">&sim; cx</span><span class="c0">d&minus;1 </span><span class="c16">then triangle is s </span><span class="c8">= </span><span class="c16">V</span><span class="c11">d </span><span class="c14">x</span><span class="c23">1</span><span class="c16">= </span><span class="c11">3&middot;2 </span><span class="c14">x</span><span class="c23">2</span><span class="c2">&int; </span><span class="c14">x</span><span class="c23">3 </span><span class="c16">V</span><span class="c2">. </span><span class="c11">d&minus;1</span><span class="c16">dx </span><span class="c2">if </span><span class="c16">= </span><span class="c2">for </span><span class="c0">x</span><span class="c15">1</span><span class="c2">(d-1) </span><span class="c11">2 </span><span class="c0">x</span><span class="c15">2 </span><span class="c2">, and a dim, </span><span class="c16">&sim; </span><span class="c0">cxd </span><span class="c28">d</span><span class="c8">. </span></p><p class="c1"><span class="c3">312 </span></p><p class="c22"><span class="c31">Dimensionality 3 6 9 12 15 18 21 24 #MBR 1501 1480 1470 1470 1439 1479 1458 1456 diagonal length 4057.7 11744.3 19559.1 23807.9 31010.9 33717.1 36979.2 40515 Shape</span><span class="c53">&lowast; </span><span class="c31">24.9 13.8 8.9 6.4 4.8 4.6 4.7 4.4 Overlaps in Query(1%) 30% 99.8% 100% 100% 100% 100% 100% 100% Volume 2.89 &times; e</span><span class="c53">9 </span><span class="c31">1.39 &times; e</span><span class="c53">21 </span><span class="c31">3.65 &times; e</span><span class="c53">33 </span><span class="c31">1.72 &times; e</span><span class="c53">45 </span><span class="c31">1.08 &times; e</span><span class="c53">58 </span><span class="c31">5.31 &times; e</span><span class="c53">69 </span><span class="c31">2.16 &times; e</span><span class="c53">81 </span><span class="c31">2.28 &times; e</span><span class="c53">93 </span><span class="c0">&lowast; </span><span class="c51">Shape is the ratio of the longest edge against the shortest one of an MBR. </span></p><p class="c1"><span class="c2">Table 3: Observation of accessed MBRs of R-tree in query. 100K points indexed in R-tree, each MBR has 100 entries. </span></p><p class="c1"><span class="c79">s erocsforebmu</span><span class="c43">N8,000 6,000 </span></p><p class="c1"><span class="c43">0 </span></p><p class="c1"><span class="c43">Value range </span></p><p class="c1"><span class="c2">The average score value of a point p is </span></p><p class="c1"><span class="c43">4,000 </span></p><p class="c1"><span class="c2">p &middot; w = </span><span class="c8">1</span><span class="c2">d </span></p><p class="c1"><span class="c2">Figure 8: Grid-index scores distribution in dimension d = 4, partitions n = 4, |P| = 100K, |W| = 100K. </span></p><p class="c1"><span class="c2">Now, p[i]) falling we want in a to score estimate range the obtained probability by Grid-index. of p&rsquo;s score (For </span><span class="c8">&sum;</span><span class="c0">d</span><span class="c11">i=1 </span><span class="c2">the </span></p><p class="c1"><span class="c2">w[i]&middot; </span></p><p class="c1"><span class="c2">discrete d dimension case: </span></p><p class="c1"><span class="c2">Prob( </span></p><p class="c1"><span class="c16">&sum;</span><span class="c0">d</span><span class="c2">(p[i] &middot; w[i]) (17) </span></p><p class="c1"><span class="c11">i=1</span><span class="c43">2,000 </span></p><p class="c1"><span class="c2">By the central limit theorem, we have the following ap- proximation when d is sufficiently large. </span></p><p class="c1"><span class="c2">Lemma 1. (Score Distribution). The following random variable </span></p><p class="c1"><span class="c2">Z = </span></p><p class="c5"><span class="c2">&radic;</span><span class="c16">&sigma; d</span><span class="c2">(p &middot; w &minus; &mu;) (18) follows the standard normal distribution (SND). In other words, Z &sim; N(0,1), where &mu; and &sigma; are as in Equation (16). </span></p><p class="c1"><span class="c2">Note that d &middot; p &middot; w is the score of point p. Representing </span></p><p class="c1"><span class="c16">&sum;</span><span class="c0">d</span><span class="c2">(w[i] &middot; p[i]) = s) (13) </span></p><p class="c41"><span class="c2">it by a random variable S, S follows a normal with mean &mu; = &mu;d and standard deviation &sigma; = distribution &sigma;</span><span class="c8">&radic;</span><span class="c2">d. By Equation (16), </span><span class="c11">i=1</span><span class="c2">This probability can be found by the so called &rdquo;Dice Prob- lems&rdquo;: Rolling d n</span><span class="c14">2</span><span class="c2">-sided dice and find the probability of </span></p><p class="c1"><span class="c2">&mu; = </span><span class="c8">1</span><span class="c2">2</span><span class="c8">rd &sigma; = </span></p><p class="c5"><span class="c2">obtaining s score. In this problem, a n</span><span class="c14">2</span><span class="c2">-sided die corre- sponds to the score range of a single dimension which is equally partitioned in n</span><span class="c14">2 </span><span class="c2">parts by Grid-index. The number of dice corresponds to the number of dimensions d, and the scores by rolling d dice becomes the point&rsquo;s score. </span></p><p class="c1"><span class="c2">The number of ways obtaining score s is the coefficient of x</span><span class="c14">s </span><span class="c2">in: </span></p><p class="c1"><span class="c2">t(x)=(x</span><span class="c14">1 </span><span class="c2">+ x</span><span class="c14">2 </span><span class="c2">+ ... + x</span><span class="c14">n</span><span class="c28">2</span><span class="c2">)</span><span class="c14">d </span><span class="c2">(14) </span></p><p class="c1"><span class="c2">By [12], the probability of obtaining s score on d n-sided dice is </span></p><p class="c1"><span class="c2">Prob(s, d, n) = n</span><span class="c8">1</span><span class="c14">2d </span></p><p class="c1"><span class="c2">2&radic;</span><span class="c8">&radic;</span><span class="c16">d </span></p><p class="c1"><span class="c2">3</span><span class="c8">r (19) </span></p><p class="c1"><span class="c2">From Lemma 1 and (11), we may now estimate the filter- ing performance. </span></p><p class="c1"><span class="c2">Lemma 2. (Filtering performance). The filtering perfor- mance of Grid-index, F, is given by </span></p><p class="c1"><span class="c2">F(x, x +&#8710;)=1 &minus; Prob(x&lt;S&lt;x + &#8710;) </span></p><p class="c1"><span class="c2">= 1 &minus; </span></p><p class="c1"><span class="c2">&int; </span><span class="c11">x+&#8710; </span></p><p class="c1"><span class="c0">x </span><span class="c8">f(x)dx (20) </span><span class="c2">where </span></p><p class="c1"><span class="c2">f(x) = &sigma; </span><span class="c8">&radic;1 </span></p><p class="c1"><span class="c2">2&pi;</span><span class="c8">exp(&minus;(x </span><span class="c2">2&sigma; </span><span class="c8">&minus; &mu; </span><span class="c14">2 </span><span class="c8">)</span><span class="c14">2 </span></p><p class="c1"><span class="c8">) (21) </span></p><p class="c1"><span class="c2">(&minus;1)</span><span class="c14">k</span><span class="c2">is the probability density function of N(&mu; , &sigma; ). </span></p><p class="c1"><span class="c2">It is difficult to calculate the integral, but by rewriting Z in Lemma 1, The above equation can be: </span></p><p class="c1"><span class="c2">Z = </span><span class="c8">d &middot; p </span><span class="c2">&sigma;</span><span class="c8">&middot; &radic;w </span><span class="c2">d </span><span class="c8">&minus; &mu;d </span></p><p class="c1"><span class="c8">= S &minus; &mu; </span></p><p class="c1"><span class="c2">&sigma; </span><span class="c8">(22) </span></p><p class="c1"><span class="c2">we can map S to Z &sim; N(0, 1) and need only to look up the SND table. </span></p><p class="c5"><span class="c2">We are now ready to estimate the filtering performance of the Grid-index methodology. Recall that the score of a point is the sum of d addends. The score&rsquo;s range in each dimension is [0,r), and it is equally divided into n</span><span class="c14">2 </span><span class="c2">partitions. Thus, the value range computed by Grid-index of a d-dimensional points corresponds to range &#8710;: </span></p><p class="c1"><span class="c2">&#8710; = n</span><span class="c8">r</span><span class="c14">2 </span><span class="c8">d (23) </span></p><p class="c1"><span class="c3">313 </span><span class="c8">(</span><span class="c2">d</span><span class="c16">k</span><span class="c0">&lfloor;(s&minus;d)/n</span><span class="c16">&sum;</span><span class="c28">2</span><span class="c0">&rfloor; </span><span class="c2">)(</span><span class="c16">s &minus; n</span><span class="c0">2</span><span class="c16">k &minus; 1 </span></p><p class="c1"><span class="c2">)</span><span class="c11">k=0 </span></p><p class="c1"><span class="c2">d &minus; 1 </span></p><p class="c41"><span class="c16">(15) </span><span class="c2">The filtering performance of Grid-index can be presented by 1&minus;Prob(s, d, n). However, it is difficult to analyse the re- lationship between n and the filtering performance by Equa- tion (15). On the other hand, we found that the distribution of scores approaches a normal distribution, even in low di- mensional cases, such as 4. Figure 8 shows the observation of distribution of scores computed by Grid-index with n = 4 partitions, and the dimension d = 4. This encourages us to approximate the feature by normal distribution. </span></p><p class="c1"><span class="c2">For a point p, p[i] &middot; w[i] obeys a uniform distribution with range [0, r), average value &mu; and standard deviation &sigma;, where </span></p><p class="c1"><span class="c2">&mu; = </span><span class="c8">1</span><span class="c2">2</span><span class="c8">r &sigma; = </span><span class="c2">2</span><span class="c8">&radic;1</span><span class="c2">3</span><span class="c8">r (16) </span></p><p class="c1"><span class="c59">[&mu; </span><span class="c9">(&#8710; = </span><span class="c36">n</span><span class="c48">rd</span><span class="c35">2</span><span class="c9">) </span><span class="c29">- </span><span class="c48">&#8710;</span><span class="c36">2 </span><span class="c9">,&mu; + </span><span class="c48">&#8710;</span><span class="c36">2 </span><span class="c9">] </span><span class="c76">0 </span></p><p class="c1"><span class="c59">&mu; = </span><span class="c48">1</span><span class="c36">2</span><span class="c9">&#1075;d </span><span class="c52">rd </span><span class="c20">(a) </span></p><p class="c1"><span class="c59">&Phi;(&alpha;) </span></p><p class="c1"><span class="c20">(b) </span></p><p class="c1"><span class="c2">Figure 9: (a): The normal distribution of point scores N(&mu; ,&sigma; ) &Phi;(&middot;) of the and SND the showing largest 1 probability &minus; </span><span class="c8">&int; </span><span class="c11">&minus;&alpha; </span><span class="c0">&alpha;</span><span class="c2">interval (gray). &middot; = 2&Phi;(&alpha;). </span></p><p class="c1"><span class="c2">(b): </span></p><p class="c41"><span class="c2">Our purpose is to find the number of partitions n which guarantees a certain filtering performance F in Lemma 2. To do this, it is sufficient to show the worst case. By Lemma 2, scores that fall within the interval illustrated by the gray part in Figure 9(a) which is located on either side of &mu;, have the largest probability and thus gives the worst F. Concen- trating on this worst interval [&mu; (22) and Equation (19), we find sponds to&minus; that </span><span class="c14">&#8710;</span><span class="c0">2 </span><span class="c8">,&mu; </span><span class="c2">S</span><span class="c11">&#8710; </span><span class="c8">+ </span><span class="c16">= </span><span class="c14">&#8710;</span><span class="c0">2 </span><span class="c8">], </span><span class="c16">&mu; </span><span class="c8">by </span><span class="c16">&plusmn; </span><span class="c8">Equation </span><span class="c0">&#8710;</span><span class="c11">2 </span><span class="c2">corre- </span></p><p class="c1"><span class="c16">Z</span><span class="c11">&#8710; </span><span class="c16">= </span><span class="c2">S</span><span class="c11">&#8710; </span><span class="c2">&sigma; </span><span class="c16">&minus; &mu; </span></p><p class="c1"><span class="c8">= &mu; &plusmn; </span><span class="c14">&#8710;</span><span class="c0">2 </span><span class="c8">&minus; &mu; </span></p><p class="c1"><span class="c2">&sigma; </span><span class="c8">= &plusmn;&radic;</span><span class="c2">3d </span></p><p class="c1"><span class="c2">n</span><span class="c14">2 </span><span class="c8">(24) </span></p><p class="c22"><span class="c2">From Lemma 1, Z &sim; N(0,1), the filtering performance in the worst case can be given by F(x, x+&#8710;) &gt; F</span><span class="c11">worst</span><span class="c16">(x, x+&#8710;) = 1&minus;</span><span class="c2">&int; </span><span class="c0">&mu; </span><span class="c11">&mu; </span><span class="c0">&minus; </span><span class="c11">+ </span><span class="c28">&#8710;</span><span class="c23">2 &#8710;</span><span class="c15">2 </span></p><p class="c1"><span class="c2">f(x)dx = 2&Phi;(</span><span class="c8">&radic;</span><span class="c2">3d </span></p><p class="c41"><span class="c2">n</span><span class="c14">2 </span><span class="c8">) </span><span class="c2">(25) where &Phi;(&middot;) is the area shown in Figure 9 (b). </span></p><p class="c1"><span class="c2">The above discussion leads to the following result. </span></p><p class="c5"><span class="c2">Theorem 1. Given &epsilon; &lt; 1, the filtering performance of n partitions is guaranteed to be above 1- &epsilon; in Grid-index such that </span></p><p class="c1"><span class="c2">n &gt; </span></p><p class="c1"><span class="c2">&radic;</span><span class="c16">2</span><span class="c2">&radic;</span><span class="c16">3d </span></p><p class="c1"><span class="c2">&delta; </span><span class="c8">(26) </span></p><p class="c1"><span class="c2">where &delta; is determined by looking up the SND table at (1 &minus; &epsilon;)/2, that is, </span></p><p class="c1"><span class="c2">&Phi;(2</span><span class="c8">&delta;) = 1 &minus; &epsilon; </span></p><p class="c1"><span class="c2">2 </span><span class="c8">(27) </span></p><p class="c22"><span class="c2">Proof. By Equation (26), </span><span class="c14">&delta;</span><span class="c0">2 </span><span class="c8">&gt; </span><span class="c0">&radic;</span><span class="c2">tonically decreasing function </span><span class="c11">3d </span></p><p class="c1"><span class="c0">n</span><span class="c28">2 </span><span class="c8">. </span><span class="c2">(Figure 9), </span><span class="c8">Since </span><span class="c2">&Phi;( </span></p><p class="c1"><span class="c0">&radic;</span><span class="c8">&Phi; is a mono- </span><span class="c2">Combining 1 &minus; &epsilon;Equation (25) and Lemma 2, we have </span><span class="c0">n</span><span class="c11">3d </span></p><p class="c1"><span class="c28">2 </span><span class="c2">F </span><span class="c8">) </span><span class="c2">&gt; </span><span class="c8">&gt; </span><span class="c2">2&Phi;( </span><span class="c8">&Phi;( </span><span class="c14">&delta;</span><span class="c0">2</span><span class="c14">&delta;</span><span class="c0">2</span><span class="c8">). </span></p><p class="c1"><span class="c8">) = </span></p><p class="c1"><span class="c16">Example. To ensure that Grid-index filters out over 99% </span><span class="c2">data, formance we set is guaranteed &epsilon; = 1% ( </span><span class="c14">(1&minus;&epsilon;) </span></p><p class="c1"><span class="c0">2 </span><span class="c2">to be </span><span class="c8">= </span><span class="c2">better </span><span class="c8">0.495), </span><span class="c2">than </span><span class="c8">thus </span><span class="c2">F</span><span class="c8">the </span><span class="c11">worst</span><span class="c16">(&delta;) </span><span class="c8">filtering </span><span class="c16">= 99%. </span><span class="c8">per- </span></p><p class="c5"><span class="c2">Looking up this value in the SND table, we have &Phi;(0.0125) = 0.495, hence, &delta; = 0.025. By Theorem 1, the sufficient num- ber of partitions n is calculated by </span></p><p class="c1"><span class="c2">&radic;</span><span class="c16">3d </span></p><p class="c1"><span class="c2">n</span><span class="c14">2 </span><span class="c8">&lt; &delta; = 0.0125 &minus;&rarr; n &gt; </span></p><p class="c1"><span class="c2">&radic;</span><span class="c16">2</span><span class="c2">&radic;&delta; </span><span class="c16">3d </span></p><p class="c1"><span class="c8">= </span></p><p class="c1"><span class="c2">&radic;</span><span class="c16">80</span><span class="c2">&radic;</span><span class="c16">3d (28) </span></p><p class="c1"><span class="c59">0 </span><span class="c52">&alpha; </span></p><p class="c1"><span class="c83">H</span><span class="c10">W </span></p><p class="c1"><span class="c89">HHHH </span><span class="c10">P </span><span class="c87">Uniform Normal Exponential </span><span class="c10">Uniform 99.3% 98.3% 99.0% Normal 98.8% 96.5% 98.7% Exponential 99.2% 97.5% 98.9% </span></p><p class="c1"><span class="c2">Table 4: Filtering performance of Grid-index with different distributions. |P| = 100K, |W| = 100K, d = 6, n = 32 </span></p><p class="c1"><span class="c2">. </span></p><p class="c1"><span class="c44">Parameter Values Data dimensionality d 2 &sim; 50, 6 Distribution of data set P UN,CL,AC,RE Data set cardinality |P| 50K,100K,1M,2M,5M Distribution of data set W UN,CL,RE Data set cardinality |W| 50K,100K,1M,2M,5M Experiment times Number of clusters 1000 </span></p><p class="c1"><span class="c39">&radic;</span><span class="c65">3</span><span class="c44">|P|, </span><span class="c39">&radic;</span><span class="c65">3</span><span class="c44">|W| Variance &sigma;</span><span class="c66">W </span><span class="c34">2</span><span class="c39">,&sigma;</span><span class="c66">P </span><span class="c34">2</span><span class="c44">Number of grids, n</span><span class="c34">2 </span><span class="c39">0.1</span><span class="c34">2 </span></p><p class="c1"><span class="c44">4</span><span class="c34">2</span><span class="c44">,8</span><span class="c34">2</span><span class="c44">,16</span><span class="c34">2</span><span class="c44">,32</span><span class="c34">2</span><span class="c44">,64</span><span class="c34">2</span><span class="c44">,128</span><span class="c55">2 </span><span class="c44">k (top-k and k-ranks) 100,200,300,400,500 </span></p><p class="c1"><span class="c2">Table 5: Experimental parameters and default values(in bold) . </span></p><p class="c5"><span class="c2">If d = 20 then n = 32 satisfies Equation (28) hence a 32 &times; 32 Grid-index is enough for filtering over 99% data. The necessary memory is less than 8 K (32 &times; 32 &times; 8) Bytes. </span></p><p class="c5"><span class="c2">Theorem 1 is still true when w[i] &middot; p[i] follows other dis- tributions. The only difference is </span><span class="c16">would have to be estimated, which </span><span class="c2">that a </span><span class="c16">would </span><span class="c2">new &mu;</span><span class="c11">i </span><span class="c16">lead to a and differ- </span><span class="c11">&radic;</span><span class="c0">&sigma;</span><span class="c15">i</span><span class="c11">d </span><span class="c2">ent partition n. We observed the filtering performance on some typical distributions, including the normal distribution (&sigma; = 10%) and exponential distribution (&lambda; = 2). The filter- ing power of the Grid-index is shown in Table 4. Different &sigma; between these distributions lead to slight differences in filtering power. But the filtering power is always efficient. </span></p><p class="c1"><span class="c19">6. EXPERIMENT </span></p><p class="c1"><span class="c2">In this section, we present the experimental evaluation. All algorithms are implemented in C++ and experiments are run on a Mac with a 2.6 GHz Intel Core i5 processor, 8GB RAM, 500GB flash storage space. We pre-read the R- tree, data sets P and W, approximated vectors P</span><span class="c0">A </span><span class="c2">and W</span><span class="c0">A </span><span class="c2">and the Grid-index into memory. According to Table 2, the I/O time is not relevant, so we focus on comparing our work only in terms of CPU processing time. </span><span class="c19">6.1 Experimental Setup </span></p><p class="c5"><span class="c2">Data sets. For data set P, both real data (RE) and syn- thetic data sets are employed. Synthetic data sets are uni- form (UN), anti-correlated (AC), and clustered (CL), with an attribute value range of [0,10K). The details on gener- ating UN, AC, and CL data are in related research [13,17]. To create weighting vectors W, there is additional UN and CL data that is generated in the same way. There are three real data sets, HOUSE, COLOR and DIANPING. HOUSE (Household) consists of 201,760 6-d tuples, repre- senting the distribution percentages of an American family&rsquo;s annual payment on gas, electricity, water, heating, insurance and property tax. COLOR consists of 68,040 9-d tuples and describes features of images in the HSV color space. HOUSE and COLOR were also used in related works [13,17]. DI- </span></p><p class="c1"><span class="c3">314 </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c20">(f) P : AC, W: UN. </span></p><p class="c1"><span class="c2">Figure 10: GIR vs BBR (a, b, c) for RTK, GIR vs MPA (d, e, f) for RKR. Performance on synthetic data with varying d (2-8), |P| = |W| = 100K, top-k = 100, k-ranks = 100, n = 32. </span></p><p class="c1"><span class="c7">10</span><span class="c12">6 </span></p><p class="c1"><span class="c7">10</span><span class="c12">3 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">10 20 30 40 50 </span></p><p class="c1"><span class="c7">SCAN </span></p><p class="c1"><span class="c20">(d) P,W: UN, d (10-50). </span></p><p class="c5"><span class="c2">Figure 11: Performance on synthetic data with high dimen- sional d (10 &minus; 50), |P| = |W| = 100K, top-k = 100, k-ranks = 100, n = 32. </span></p><p class="c5"><span class="c2">ANPING is a 6-d real world data set from a famous Chinese online business-reviewing website. It includes 3,605,300 re- views by 510,071 users on 209,132 restaurants about rate, food flavor, cost, service, environment and waiting time. We use the average scores of the reviews by the same user as his/her preference (w), and the average scores of the reviews on a restaurant as its attributes (p). RRQ can be anticipated to help to find target users for these restaurants. </span></p><p class="c5"><span class="c2">Algorithms. We implemented BBR, MPA and Simple Scan algorithms (SIM). In BBR [17], both data sets P and W are indexed by R-tree, points and weighting vectors are pruned through the branch-and-bound methodology. MPA [22] uses an R-tree to index P and a d-dimensional histogram to group W in order to avoid checking every weighting vec- tor. In SIM, for each w, all points in P are scanned and used to compute the score. SIM also maintains a Domin buffer to avoid unnecessary computing and terminates when cur- rent rank does not satisfy the conditions for RTK or RKR. </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">C</span><span class="c18">) sm(emit</span><span class="c7">10</span><span class="c12">3 </span><span class="c18">UP</span></p><p class="c1"><span class="c7">10</span><span class="c12">6 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c20">(a) P,W: UN, d (10-50). </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c20">(c) P,W: UN, d (10-50). </span></p><p class="c1"><span class="c7">BBR SIM </span></p><p class="c1"><span class="c7">SCAN </span></p><p class="c1"><span class="c7">d(10-50) </span></p><p class="c1"><span class="c20">(b) P,W: UN, d (10-50). </span></p><p class="c1"><span class="c7">MPA SIM </span></p><p class="c1"><span class="c7">10 20 30 40 50 </span></p><p class="c1"><span class="c7">d(10-50) </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">10</span><span class="c12">4 </span></p><p class="c1"><span class="c7">10</span><span class="c12">2 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c7">10</span><span class="c12">4 </span></p><p class="c1"><span class="c7">10</span><span class="c12">2 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c20">(d) P, W : UN. </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c20">(a) P, W : UN. </span></p><p class="c1"><span class="c7">BBR SIM </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c22"><span class="c7">2 4 6 8 d </span></p><p class="c1"><span class="c20">(c) P: AC, W: UN. </span></p><p class="c1"><span class="c7">MPA SIM </span></p><p class="c1"><span class="c7">2 4 6 8 </span></p><p class="c1"><span class="c7">d </span></p><p class="c1"><span class="c7">P</span><span class="c18">airwiseComputations BBR </span></p><p class="c1"><span class="c7">P</span><span class="c18">ariwiseComputations MPA </span></p><p class="c1"><span class="c7">10</span><span class="c12">6 </span></p><p class="c1"><span class="c7">10</span><span class="c12">3 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">10</span><span class="c12">6 </span></p><p class="c1"><span class="c7">10</span><span class="c12">3 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">10 20 30 40 50 </span></p><p class="c1"><span class="c7">10 20 30 40 50 </span></p><p class="c1"><span class="c7">d(10-50) </span></p><p class="c1"><span class="c7">d(10-50) </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">10</span><span class="c12">4 </span></p><p class="c1"><span class="c7">10</span><span class="c12">2 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">10</span><span class="c12">4 </span></p><p class="c1"><span class="c7">10</span><span class="c12">2 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c20">(b) P, W: CL. </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c20">(e) P, W: CL. </span></p><p class="c1"><span class="c7">2 4 6 8 </span></p><p class="c1"><span class="c7">2 4 6 8 </span></p><p class="c1"><span class="c7">MPA SIM </span></p><p class="c1"><span class="c7">BBR SIM </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c20">(d) P, W: DIANPING, d = 6. </span></p><p class="c1"><span class="c2">Figure 12: GIR vs Tree-base with RE data on varying &rdquo;k&rdquo;, for RTK and RKR queries. n = 32. </span></p><p class="c5"><span class="c2">In conclusion, the only difference between SIM and GIR is that SIM computes a score for each p and w directly rather than using Grid-index for filtering. </span></p><p class="c5"><span class="c2">Parameters. Parameters are shown in Table 5 where the default values are d = 6, |P|=100K, |W|=100K, k=100, the number of Grids is 32</span><span class="c14">2</span><span class="c2">, and both P and W are UN data. </span></p><p class="c5"><span class="c2">Metrics. We did each experiment over 1000 times, and present the average value. The query point q is randomly selected from P. Besides the query execution time required by each algorithm, we also observe the number of pairwise computations and the percentage of accessed data. </span></p><p class="c1"><span class="c19">6.2 Experimental Results </span></p><p class="c5"><span class="c2">Synthesis data with varying d. Figure 10 shows the performance of P (UN, AC, CL) and W (UN, CL) on syn- thetic data sets, with |P|=100K and |W|=100K, k=100, n = 32. Figures 10a, 10b and 10c show the CPU time and cost comparisons for RTK in low dimensions (2 to 8). GIR out- performs BBR in all distributions (UN,CL,AC) when data </span></p><p class="c1"><span class="c3">315 </span></p><p class="c1"><span class="c7">d </span></p><p class="c1"><span class="c7">d </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c18">) sm(emitUP</span><span class="c7">C</span><span class="c20">(a) P : COLOR, W: UN, d = 9. </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c20">(c) P, W: DIANPING, d = 6. </span></p><p class="c1"><span class="c7">10</span><span class="c12">4 </span></p><p class="c1"><span class="c7">10</span><span class="c12">2 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">10</span><span class="c12">4 </span></p><p class="c1"><span class="c7">10</span><span class="c12">2 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">BBR SIM </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c22"><span class="c7">100 200 300 400 500 top-k </span></p><p class="c1"><span class="c20">(b) P: HOUSE, W: UN d = 6. </span></p><p class="c1"><span class="c7">BBR SIM </span></p><p class="c1"><span class="c7">100 200 300 400 500 </span></p><p class="c1"><span class="c7">top-k </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">10</span><span class="c12">4 </span></p><p class="c1"><span class="c7">10</span><span class="c12">2 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">10</span><span class="c12">4 </span></p><p class="c1"><span class="c7">10</span><span class="c12">2 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">2 4 6 8 </span></p><p class="c1"><span class="c7">2 4 6 8 </span></p><p class="c1"><span class="c7">MPA SIM </span></p><p class="c1"><span class="c7">BBR SIM </span></p><p class="c1"><span class="c7">d </span></p><p class="c1"><span class="c7">d </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">10</span><span class="c12">4 </span></p><p class="c1"><span class="c7">10</span><span class="c12">2 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">10</span><span class="c12">4 </span></p><p class="c1"><span class="c7">10</span><span class="c12">2 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">100 200 300 400 500 </span></p><p class="c1"><span class="c7">100 200 300 400 500 </span></p><p class="c1"><span class="c7">MPA BBR </span></p><p class="c1"><span class="c7">MPA BBR </span></p><p class="c1"><span class="c7">k-ranks </span></p><p class="c1"><span class="c7">k-ranks </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c20">(d) RKR. </span></p><p class="c5"><span class="c2">Figure 13: Scalability for all algorithms with varying |P| (a,b) and |W| (c,d), top-k = 100, k-ranks = 100, n = 32, d = 6. </span></p><p class="c5"><span class="c2">has over 4 dimensions. SIM outperforms BBR when data has more than 6 dimensions, with the exception of CL data, since R-tree can group and prune more points when the data set is clustered. GIR always exceeds SIM at least 2 times be- cause GIR uses score bounds from Grid-index to skip most data without doing multiplications. The results of RKR are shown in Figures 10d, 10e, 10f, GIR outperforms simple scan SIM at all times and outperforms the tree-based MPA with 4 to 8-dimensional data. </span></p><p class="c5"><span class="c2">In high-dimensions (10-50), as shown in Figures 11a and 11c, the query time taken by tree-based method increases rapidly for the two reasons we presented in Sections 1.2 and 5.2: overlapping MBRs and little space to prune. Figure 11b, 11d present the number of pairwise computations for all algorithms, both BBR and MPA use more computations than the simple scan. Notice that the computation numbers for GIR and SIM are equal and are both titled &ldquo;SCAN&rdquo; in the figures. On the other hand, GIR is the most stable method and only grows slightly. This confirms that GIR is only slightly affected by increasing dimensionality. </span></p><p class="c5"><span class="c2">Real data with varying &ldquo;k&rdquo;. For the performance of these algorithms on real data sets (RE) with varying k. No- tice that k has a different meaning, it is a query condition in RTK and a result size in RKR. Figures 12a, 12b show the results from data set HOUSE and COLOR, and data set W is generated as UN data. We process COLOR with RTK and HOUSE with RKR. Clearly, GIR is consistently superior to tree-based algorithms (BBR and MPA) and SIM, though all are stable for various k values. For the DIANPING dataset, P and W contain the average scores vectors from the reviews of users and restaurants. We peform RTK and RKR queries on DIANPING data and the Figures 12c and 12d show the comparison results. As we expected, the GIR algorithm is the most efficient for this real-world application data set. </span></p><p class="c5"><span class="c2">Scalability with varying |P| and |W|. According Fig- ure 13, as the cardinality of data set increases P (Figures 13a and 13b) or W (Figures 13c and 13d), GIR becomes significantly superior to tree-based algorithms (BBR, MPA) and SIM. n = 32 is sufficient to filter more than 99% of points for a 6-d dataset based our Theorem 1. Thus, the </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">10</span><span class="c12">4 </span></p><p class="c1"><span class="c7">10</span><span class="c12">2 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">10</span><span class="c12">4 </span></p><p class="c1"><span class="c7">10</span><span class="c12">2 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">BBR SIM </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c7">100K 1M 5M </span></p><p class="c1"><span class="c20">(b) RKR. </span></p><p class="c1"><span class="c7">BBR SIM </span></p><p class="c1"><span class="c7">100K 1M 5M </span></p><p class="c1"><span class="c7">Cardinality |W| </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c7">Cardinality |P| </span></p><p class="c1"><span class="c20">(a) RTK. </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c20">(c) RTK. </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">C</span><span class="c18">PUtime(ms) </span></p><p class="c1"><span class="c7">10</span><span class="c12">6 </span></p><p class="c1"><span class="c7">10</span><span class="c12">3 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">10</span><span class="c12">6 </span></p><p class="c1"><span class="c7">10</span><span class="c12">3 </span></p><p class="c1"><span class="c7">10</span><span class="c12">0 </span></p><p class="c1"><span class="c7">100K 1M 5M </span></p><p class="c1"><span class="c7">100K 1M 5M </span></p><p class="c1"><span class="c7">MPA SIM </span></p><p class="c1"><span class="c7">MPA SIM </span></p><p class="c1"><span class="c7">Cardinality |W| </span></p><p class="c1"><span class="c7">Cardinality |P| </span></p><p class="c1"><span class="c7">GIR </span></p><p class="c1"><span class="c38">) sm(emit</span><span class="c6">10</span><span class="c12">4 </span><span class="c6">10</span><span class="c12">2 </span><span class="c38">UP</span><span class="c6">C10</span><span class="c12">0 </span></p><p class="c1"><span class="c20">(b) RKR. </span></p><p class="c1"><span class="c2">Figure 14: GIR vs Tree-base with varying k, for RTK and RTK queries. P, W: UN, n = 32, d = 6. </span></p><p class="c1"><span class="c18">50 0</span><span class="c7">5 15 25 35 45 Real Theoretical </span></p><p class="c1"><span class="c20">(b) Filtered data, d = 20. </span></p><p class="c5"><span class="c2">Figure 15: (a) Visited data for all algorithms on varying d, (b) Filtering data (%) of Grid-index on varying n. |P| = 100K, |W| = 100K. P, W: UN. </span></p><p class="c1"><span class="c2">CPU cost increased only slightly as the scale increased. </span></p><p class="c5"><span class="c2">Effect on &ldquo;k&rdquo;. Figures 12, 14 also show the performance changes when k increases from 100 to 500. All algorithms are insensitive to k because k &#8810; |P| and k &#8810; |W|. </span></p><p class="c5"><span class="c2">Accessed data points. Figure 15a shows the percentage of visited data in the leaf nodes of the R-tree and original data points on UN data. As predicted by our analysis, R- tree degenerated to a simple scan through all leaf nodes with high-dimensional data. However, GIR accesses a relatively small amount of data after filtering with Grid-index. </span></p><p class="c5"><span class="c2">Effect on value range partitions n. Figure 15b shows the percent of 20-d data which can be filtered with Grid- index with various Grid numbers (n&times; n). We created Grid- index with different n from 4 to 128 and observed the filter- ing of data points. The results confirm the analytical result guaranteed by Theorem 1. n = 32 is enough to guarantee a high Grid-index efficiency. </span></p><p class="c1"><span class="c19">7. CONCLUSION </span></p><p class="c5"><span class="c2">Reverse rank queries are useful in many applications. In marketing analysis, they can be used to help manufactur- ers recognize their consumer base by matching their prod- uct features with user preferences. The state-of-the-art ap- proaches for both reverse top-k (BBR) and reverse k-ranks (MPA) are tree-based algorithms, and are not designed to deal with high-dimensional data. In this paper, we pro- posed the Grid-index and the GIR algorithm to overcome the cost of high-dimensional computing when processing re- verse rank queries. Theoretical analysis and experimental results confirmed the efficiency of the proposed algorithm when compared to the tree-based algorithms especially in high-dimensional cases. </span></p><p class="c5"><span class="c2">In future work, there are two extensions for GIR algo- rithm. The first is to find a heuristic method to adapt GIR to different data distributions by using non-equal-width Grid- index. This is easy to implement by merging and splitting </span></p><p class="c1"><span class="c3">316 </span></p><p class="c1"><span class="c18">d etisivfaeL/stnioPSIM TREE GIR </span></p><p class="c1"><span class="c7">100 </span></p><p class="c1"><span class="c18">s tniopderetlfi</span><span class="c7">%</span><span class="c20">(a) Visited data, n = 32. </span></p><p class="c1"><span class="c7">100 </span></p><p class="c1"><span class="c18">80 </span></p><p class="c1"><span class="c7">%604 16 64 </span></p><p class="c1"><span class="c7">d </span></p><p class="c1"><span class="c7">Number of Partition (n) </span></p><p class="c1"><span class="c7">GIR BBR SIM </span></p><p class="c22"><span class="c6">100 200 300 400 500 k </span></p><p class="c1"><span class="c20">(a) RTK. </span></p><p class="c41"><span class="c6">10</span><span class="c38">) sm(emitUP</span><span class="c6">C10</span><span class="c12">4 2 </span></p><p class="c1"><span class="c6">10</span><span class="c12">0 </span></p><p class="c1"><span class="c6">100 200 300 400 500 </span></p><p class="c1"><span class="c7">BBR SIM </span></p><p class="c1"><span class="c6">k </span></p><p class="c33"><span class="c2">some grids of the equal-width Grid-index based on the dis- tributions of the given P and W. The challenging point is the model of filtering performance with varied distributions in different dimensions. The second extension is to do opti- mization when the user preferences data w &isin; W has many zero entry, i.e., when W is sparse. Since in practice, a user is normally interested in a few attributes of the products. </span></p><p class="c70"><span class="c19">Acknowledgement </span><span class="c2">This work was partly supported by JSPS KAKENHI Grant- in-Aid for Scientific Research (B) (Grant number: 26280037). </span></p><p class="c77"><span class="c19">8. REFERENCES </span></p><p class="c45"><span class="c2">[1] N. Beckmann, H. Kriegel, R. Schneider, and </span></p><p class="c46"><span class="c2">B. Seeger. The r*-tree: An efficient and robust access method for points and rectangles. In Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data, Atlantic City, NJ, May 23-25, 1990., pages 322&ndash;331, 1990. [2] S. Berchtold, D. A. Keim, and H. Kriegel. The x-tree : </span></p><p class="c78 c90"><span class="c2">An index structure for high-dimensional data. In VLDB&rsquo;96, Proceedings of 22th International Conference on Very Large Data Bases, September 3-6, 1996, Mumbai (Bombay), India, pages 28&ndash;39, 1996. [3] Y. Chang, L. D. Bergman, V. Castelli, C. Li, M. Lo, </span></p><p class="c57"><span class="c2">and J. R. Smith. The onion technique: Indexing for linear optimization queries. In Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, May 16-18, 2000, Dallas, Texas, USA., pages 391&ndash;402, 2000. [4] H. Chen, J. Liu, K. Furuse, J. X. Yu, and N. Ohbo. </span></p><p class="c78 c96"><span class="c2">Indexing expensive functions for efficient multi-dimensional similarity search. Knowl. Inf. Syst., 27(2):165&ndash;192, 2011. [5] S. Chester, A. Thomo, S. Venkatesh, and </span></p><p class="c69"><span class="c2">S. Whitesides. Indexing reverse top-k queries in two dimensions. In Database Systems for Advanced Applications, 18th International Conference, DASFAA 2013, Wuhan, China, April 22-25, 2013. Proceedings, Part I, pages 201&ndash;208, 2013. [6] E. Dellis and B. Seeger. Efficient computation of </span></p><p class="c46"><span class="c2">reverse skyline queries. In Proceedings of the 33rd International Conference on Very Large Data Bases, University of Vienna, Austria, September 23-27, 2007, pages 291&ndash;302, 2007. [7] Y. Dong, H. Chen, K. Furuse, and H. Kitagawa. Aggregate reverse rank queries. In Database and Expert Systems Applications - 27th International Conference, DEXA 2016, Porto, Portugal, September 5-8, 2016, Proceedings, Part II, pages 87&ndash;101, 2016. [8] V. Hristidis, N. Koudas, and Y. Papakonstantinou. </span></p><p class="c78 c88"><span class="c2">PREFER: A system for the efficient execution of multi-parametric ranked queries. In Proceedings of the 2001 ACM SIGMOD international conference on Management of data, Santa Barbara, CA, USA, May 21-24, 2001, pages 259&ndash;270, 2001. [9] I. Kamel and C. Faloutsos. Hilbert r-tree: An </span></p><p class="c78 c84"><span class="c2">improved r-tree using fractals. In VLDB&rsquo;94, Proceedings of 20th International Conference on Very Large Data Bases, September 12-15, 1994, Santiago de Chile, Chile, pages 500&ndash;509, 1994. </span></p><p class="c1 c82"><span class="c2">[10] F. Korn and S. Muthukrishnan. Influence sets based </span></p><p class="c56"><span class="c2">on reverse nearest neighbor queries. In Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, May 16-18, 2000, Dallas, Texas, USA., pages 201&ndash;212, 2000. [11] X. Lian and L. Chen. Monochromatic and bichromatic reverse skyline search over uncertain databases. In Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008, pages 213&ndash;226, 2008. [12] J. V. Uspensky. Introduction to Mathematical </span></p><p class="c30 c80"><span class="c2">Probability. New York: McGraw-Hill, 1937. [13] A. Vlachou, C. Doulkeridis, Y. Kotidis, and </span></p><p class="c78 c91"><span class="c2">K. N&oslash;rv&aring;g. Reverse top-k queries. In Proceedings of the 26th International Conference on Data Engineering, ICDE 2010, March 1-6, 2010, Long Beach, California, USA, pages 365&ndash;376, 2010. [14] A. Vlachou, C. Doulkeridis, Y. Kotidis, and </span></p><p class="c40"><span class="c2">K. N&oslash;rv&aring;g. Monochromatic and bichromatic reverse top-k queries. IEEE Trans. Knowl. Data Eng., 23(8):1215&ndash;1229, 2011. [15] A. Vlachou, C. Doulkeridis, and K. N&oslash;rv&aring;g. </span></p><p class="c30 c68"><span class="c2">Monitoring reverse top-k queries over mobile devices. In Proceedings of the Tenth ACM International Workshop on Data Engineering for Wireless and Mobile Access, MobiDE 2011, Athens, Greece, June 12, 2011, pages 17&ndash;24, 2011. [16] A. Vlachou, C. Doulkeridis, K. N&oslash;rv&aring;g, and </span></p><p class="c50"><span class="c2">Y. Kotidis. Identifying the most influential data objects with reverse top-k queries. PVLDB, 3(1):364&ndash;372, 2010. [17] A. Vlachou, C. Doulkeridis, K. N&oslash;rv&aring;g, and </span></p><p class="c40"><span class="c2">Y. Kotidis. Branch-and-bound algorithm for reverse top-k queries. In Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2013, New York, NY, USA, June 22-27, 2013, pages 481&ndash;492, 2013. [18] S. Wang, M. A. Cheema, X. Lin, Y. Zhang, and </span></p><p class="c27 c30"><span class="c2">D. Liu. Efficiently computing reverse k furthest neighbors. In 32nd IEEE International Conference on Data Engineering, ICDE 2016, Helsinki, Finland, May 16-20, 2016, pages 1110&ndash;1121, 2016. [19] R. Weber, H. Schek, and S. Blott. A quantitative </span></p><p class="c30 c98"><span class="c2">analysis and performance study for similarity-search methods in high-dimensional spaces. In VLDB&rsquo;98, Proceedings of 24rd International Conference on Very Large Data Bases, August 24-27, 1998, New York City, New York, USA, pages 194&ndash;205, 1998. [20] S. Yang, M. A. Cheema, X. Lin, and Y. Zhang. </span></p><p class="c42"><span class="c2">SLICE: reviving regions-based pruning for reverse k nearest neighbors queries. In IEEE 30th International Conference on Data Engineering, Chicago, ICDE 2014, IL, USA, March 31 - April 4, 2014, pages 760&ndash;771, 2014. [21] B. Yao, F. Li, and P. Kumar. Reverse furthest </span></p><p class="c27 c78"><span class="c2">neighbors in spatial databases. In Proceedings of the 25th International Conference on Data Engineering, ICDE 2009, March 29 2009 - April 2 2009, Shanghai, China, pages 664&ndash;675, 2009. [22] Z. Zhang, C. Jin, and Q. Kang. Reverse k-ranks query. </span></p><p class="c21"><span class="c2">PVLDB, 7(10):785&ndash;796, 2014. </span></p><p class="c93"><span class="c3">317 </span></p></body></html>