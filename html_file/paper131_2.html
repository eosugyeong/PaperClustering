<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c54{margin-left:-25.7pt;padding-top:13.4pt;text-indent:36pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-20.7pt}.c123{margin-left:-279.7pt;padding-top:7.4pt;text-indent:288.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:237.6pt}.c97{margin-left:-13pt;padding-top:1.2pt;text-indent:25.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:15.1pt}.c165{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c158{margin-left:-279.7pt;padding-top:1.7pt;text-indent:288.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:237.6pt}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.6pt;font-family:"Arial";font-style:normal}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.7pt;font-family:"Arial";font-style:normal}.c130{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-11pt}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:7.8pt;font-family:"Arial";font-style:normal}.c41{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17pt}.c151{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c64{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-8.9pt}.c153{margin-left:-279.7pt;padding-top:1.4pt;text-indent:288.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:235.4pt}.c101{margin-left:-16.6pt;padding-top:0.5pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c60{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.7pt;font-family:"Arial";font-style:normal}.c110{margin-left:-6pt;padding-top:10.1pt;text-indent:89pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:3pt}.c38{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.4pt;font-family:"Arial";font-style:normal}.c58{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.1pt}.c93{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c73{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:9pt;font-family:"Arial";font-style:normal}.c70{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.4pt;font-family:"Arial";font-style:normal}.c92{margin-left:-25.7pt;padding-top:5.5pt;text-indent:30.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.2pt}.c143{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c25{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.6pt;font-family:"Arial";font-style:normal}.c65{margin-left:-25.7pt;padding-top:3.8pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c188{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c32{margin-left:-25.7pt;padding-top:1.4pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c35{margin-left:-16.6pt;padding-top:9.8pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.4pt}.c28{margin-left:237.4pt;padding-top:1.4pt;text-indent:-228.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-279.4pt}.c43{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.7pt;font-family:"Arial";font-style:normal}.c80{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.2pt}.c118{margin-left:-25.7pt;padding-top:58.1pt;text-indent:28.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c182{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c108{margin-left:-16.6pt;padding-top:10.6pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.4pt}.c98{margin-left:-16.6pt;padding-top:11pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c90{margin-left:-16.6pt;padding-top:4.1pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.2pt}.c87{margin-left:-13pt;padding-top:1pt;text-indent:25.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3.4pt}.c102{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c163{margin-left:-279.7pt;padding-top:1.4pt;text-indent:288.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:237.6pt}.c94{margin-left:-16.6pt;padding-top:1.7pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c61{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.4pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c154{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:3.6pt}.c42{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Arial";font-style:normal}.c103{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9.6pt}.c47{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c104{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12.7pt}.c96{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-4.8pt}.c193{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:7.7pt}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.1pt;font-family:"Arial";font-style:normal}.c131{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-21.1pt}.c138{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3.6pt}.c9{margin-left:-16.6pt;padding-top:1.4pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c85{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:3.8pt}.c21{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c75{margin-left:-25.7pt;padding-top:1.7pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c33{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Courier New";font-style:normal}.c141{margin-left:-16.6pt;padding-top:1.2pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-14.6pt}.c107{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.2pt}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.4pt;font-family:"Arial";font-style:normal}.c127{margin-left:-25.7pt;padding-top:3.8pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.2pt}.c74{margin-left:-25.7pt;padding-top:3.8pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.1pt}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c29{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.9pt;font-family:"Arial";font-style:normal}.c149{margin-left:-25.7pt;padding-top:10.1pt;text-indent:36pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c189{margin-left:-16.6pt;padding-top:1pt;text-indent:32.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.2pt}.c177{margin-left:-25.7pt;padding-top:1.4pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.2pt}.c147{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:9pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Courier New";font-style:normal}.c4{margin-left:-11.1pt;padding-top:13.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:2.6pt}.c109{margin-left:-102.1pt;padding-top:88.1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:258.2pt}.c19{margin-left:-25.7pt;padding-top:10.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c119{margin-left:-13pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-11.3pt}.c23{margin-left:249.1pt;padding-top:5.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-267.7pt}.c192{margin-left:65.8pt;padding-top:28.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:66pt}.c132{margin-left:-134.7pt;padding-top:2.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:308.4pt}.c100{margin-left:218.2pt;padding-top:635.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c190{margin-left:-25.7pt;padding-top:6.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.2pt}.c89{margin-left:-226.9pt;padding-top:7.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:290.2pt}.c171{margin-left:-25.7pt;padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.2pt}.c116{margin-left:-6pt;padding-top:6.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:3pt}.c55{margin-left:36.6pt;padding-top:13.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:25.7pt}.c8{margin-left:33.5pt;padding-top:5.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12.7pt}.c7{margin-left:-3.9pt;padding-top:5.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-12.7pt}.c155{margin-left:-15.4pt;padding-top:6.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.2pt}.c148{margin-left:100.1pt;padding-top:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:100.1pt}.c72{margin-left:-16.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-8.4pt}.c180{margin-left:-25.7pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c181{margin-left:237.4pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-279.4pt}.c117{margin-left:-16.6pt;padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c82{margin-left:34.5pt;padding-top:5.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-13.7pt}.c45{margin-left:-1.5pt;padding-top:6.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-10.3pt}.c52{margin-left:31.8pt;padding-top:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:23pt}.c66{margin-left:-8.5pt;padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17pt}.c81{margin-left:-21.1pt;padding-top:5.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-11.8pt}.c185{margin-left:-25.7pt;padding-top:12.7pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c53{margin-left:-25.7pt;padding-top:56.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c114{margin-left:-16.6pt;padding-top:30.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c174{margin-left:-22.8pt;padding-top:10.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:106.5pt}.c12{margin-left:-147.9pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:304.1pt}.c133{margin-left:-13pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7.9pt}.c84{margin-left:-16.6pt;padding-top:22.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c40{margin-left:237.4pt;padding-top:13pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-279.4pt}.c78{margin-left:-25.7pt;padding-top:9.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c16{margin-left:-17.5pt;padding-top:5.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-8.5pt}.c3{margin-left:-25.7pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:148.7pt}.c152{margin-left:-25.7pt;padding-top:14.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:62.3pt}.c95{margin-left:218.2pt;padding-top:68.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c137{margin-left:-25.7pt;padding-top:21.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c115{margin-left:-16.6pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-23pt}.c170{margin-left:-279.7pt;padding-top:10.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:237.6pt}.c172{margin-left:27.6pt;padding-top:10.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:20.1pt}.c173{margin-left:-25.7pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:44.8pt}.c183{margin-left:-25.7pt;padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:17pt}.c20{margin-left:30.5pt;padding-top:24.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:39.5pt}.c44{margin-left:-16.6pt;padding-top:6.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-26.2pt}.c145{margin-left:-13pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-14.9pt}.c91{margin-left:-16.6pt;padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.2pt}.c175{margin-left:-25.7pt;padding-top:13.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:54.6pt}.c37{margin-left:218.2pt;padding-top:66pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c63{margin-left:53.4pt;padding-top:7.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:44.6pt}.c48{margin-left:-16.6pt;padding-top:20.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c167{margin-left:-16.3pt;padding-top:12pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:51pt}.c144{margin-left:-16.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.2pt}.c146{margin-left:-25.7pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:106.5pt}.c57{margin-left:-23.8pt;padding-top:77.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24pt}.c113{margin-left:-25.7pt;padding-top:13.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c187{margin-left:-16.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-23.8pt}.c184{margin-left:-16.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:3.8pt}.c136{margin-left:-16.6pt;padding-top:12pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:112.3pt}.c22{margin-left:-13pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-13.7pt}.c139{margin-left:-25.7pt;padding-top:29.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c11{margin-left:-3.9pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:23.8pt}.c27{margin-left:-21.1pt;padding-top:5.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-11.8pt}.c162{margin-left:-13pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-19.4pt}.c194{margin-left:-16.6pt;padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:81.8pt}.c39{margin-left:-17.5pt;padding-top:5.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-8.2pt}.c164{margin-left:-25.7pt;padding-top:6.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:148pt}.c106{margin-left:30.5pt;padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:39.8pt}.c76{margin-left:-16.6pt;padding-top:14.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:106.6pt}.c67{margin-left:-0.6pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:120.5pt}.c69{margin-left:-7.3pt;padding-top:12pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:104.2pt}.c122{margin-left:-16.6pt;padding-top:20.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c121{margin-left:-25.7pt;padding-top:15.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:142.7pt}.c140{margin-left:-279.7pt;padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:237.6pt}.c112{margin-left:34.3pt;padding-top:19.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:43.4pt}.c62{margin-left:-25.7pt;padding-top:6.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c125{margin-left:218.2pt;padding-top:64.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c160{margin-left:-20.2pt;padding-top:13.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:21.3pt}.c156{margin-left:-13pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12pt}.c135{margin-left:-25.7pt;padding-top:9.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.2pt}.c56{margin-left:-194pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:349.9pt}.c120{margin-left:-279.7pt;padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:237.6pt}.c31{margin-left:-25.7pt;padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c77{margin-left:-16.6pt;padding-top:6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.2pt}.c88{margin-left:-13pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-24.5pt}.c68{margin-left:37.4pt;padding-top:13.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:30.7pt}.c34{margin-left:237.4pt;padding-top:27.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-279.4pt}.c128{margin-left:-2.6pt;padding-top:17pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:4.5pt}.c150{margin-left:-279.7pt;padding-top:19.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:237.6pt}.c83{margin-left:-16.6pt;padding-top:6.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c179{margin-left:-16.6pt;padding-top:3.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-27.6pt}.c129{padding-top:9.1pt;text-indent:27pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c168{padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c30{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c157{padding-top:13.7pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c176{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c13{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c178{padding-top:24pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c142{padding-top:4.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c86{padding-top:11pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c134{padding-top:5.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c79{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c169{margin-left:-279.7pt;text-indent:288.8pt;margin-right:237.6pt}.c36{margin-left:-2.2pt;text-indent:6pt;margin-right:187.6pt}.c71{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c99{margin-left:-25.7pt;text-indent:34.8pt;margin-right:-16.4pt}.c111{margin-left:-1.5pt;text-indent:87.4pt;margin-right:-10.3pt}.c50{margin-left:-4.9pt;margin-right:22.8pt}.c191{margin-left:29.3pt;margin-right:21.8pt}.c46{margin-left:-4.9pt;margin-right:-13.7pt}.c124{margin-left:-16.6pt;margin-right:-25.4pt}.c105{margin-left:-134.7pt;margin-right:308.2pt}.c166{margin-left:14.2pt;margin-right:141.8pt}.c161{margin-left:-8.5pt;margin-right:12.5pt}.c49{margin-left:33.5pt;margin-right:-12.7pt}.c159{margin-left:34.5pt;margin-right:-13.7pt}.c59{margin-left:5pt;margin-right:-6pt}.c51{margin-left:-3.9pt;margin-right:-12.7pt}.c126{text-indent:25.8pt}.c186{text-indent:27pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c71"><p class="c148"><span class="c151">Flexible Caching in Trie Joins </span></p><p class="c192"><span class="c10">Oren Kalinsky Yoav Etsion Benny Kimelfeld </span><span class="c188">Technion &ndash; Israel Institute Of Technology </span><span class="c93">{okalinsk@campus, yetsion@tce, bennyk@cs}.technion.ac.il </span></p><p class="c53"><span class="c10">ABSTRACT </span><span class="c6">While traditional algorithms for multiway join are based on re- ordering binary joins, more recent approaches have instantiated a new breed of &ldquo;worst-case-optimal&rdquo; in-memory algorithms wherein all relations are scanned simultaneously. Veldhuizen&rsquo;s Leapfrog Trie Join (LFTJ) is an example. An important advantage of LFTJ is its small memory footprint, due to the fact that intermediate results are full tuples that can be dumped immediately. However, since the algorithm does not store intermediate results, recurring joins must be reconstructed from the source relations, resulting in excessive memory traffic. In this paper, we address this problem by incorpo- rating caches into LFTJ. We do so by adopting recent developments in join optimization, tying variable ordering to a tree decomposi- tion of the query. While the traditional usage of tree decomposition computes the entire result for each bag, our proposed approach in- corporates caching directly into LFTJ and can dynamically adjust the size of the cache. Consequently, our solution balances between memory usage and repeated computation. Our experimental study over the SNAP dataset compares between various (traditional and novel) caching policies, and shows significant speedups over state- of-the-art algorithms on both join evaluation and join counting. </span></p><p class="c3"><span class="c10">CCS Concepts </span></p><p class="c92"><span class="c6">&bull;Information systems &rarr; Join algorithms; Query optimization; Main memory engines; </span></p><p class="c173"><span class="c10">Keywords </span><span class="c6">Databases, trie joins, tree decomposition, caching </span></p><p class="c146"><span class="c10">1. INTRODUCTION </span></p><p class="c74"><span class="c6">Traditional optimization of multiway joins has been based on de- composing the query into smaller join queries, and combining in- termediate relations. This approach has roots in Selinger&rsquo;s pairwise- join enumeration [26], and it includes the application of the algo- rithm of Yannakakis [30] over a tree decomposition of the query [13, 14]. Recent approaches have developed a new breed of in-memory algorithms wherein all relations are scanned simultaneously [1, 10, </span></p><p class="c118"><span class="c6">c </span><span class="c47">2017, Copyright is with the authors. Published in Proc. 20th Inter- national Conference on Extending Database Technology (EDBT), March 21-24, 2017 - Venice, Italy: ISBN 978-3-89318-073-8, on OpenProceed- ings.org. Distribution of this paper is permitted under the terms of the Cre- ative Commons license CC-by-nc-nd 4.0 </span></p><p class="c30 c124"><span class="c6">15,16,21,28,29], featuring the complexity guarantee of worst-case optimality. This yardstick of efficiency has been introduced by Ngo et al. [21], and it states that for every join query, no algorithm can be asymptotically faster on the space of all databases; in that work they presented the first worst-case optimal algorithm, later termed NPRR [21]. Effectively, the running time is bounded by the AGM bound [5] that determines the maximal number of tuples in the mul- tiway join of relations with given sizes. </span></p><p class="c94"><span class="c6">Leapfrog Trie Join (LFTJ) [29] is another worst-case-optimal al- gorithm, introduced by LogicBlox and implemented in the com- pany&rsquo;s product [3]. It operates in a manner of variable elimination where there is a linear order over the variables, and query results are generated one by one by incrementally assigning values to each variable in order. Trie-structured indices over the relations allow to efficiently determine whether the next variable in consideration can be assigned a value that is consistent with the assignments to the previous variables. (We give a detailed description of LFTJ in Section 2). Beyond being worst-case optimal, LFTJ has two impor- tant features. First, it avoids the potential generation of intermedi- ate results that may be substantially larger than the final output size (which is a key property in guaranteeing worst-case optimality). Second, LFTJ is very well suited for in-memory join evaluation, since besides the trie indices it has a close to zero memory con- sumption. Of course, memory is required for buffering the tuples in the final result, but these are never read and can be safely dumped to higher storage upon need. Moreover, these tuples are not even needed in the case of common aggregate queries (e.g., count the number of tuples in the result). </span></p><p class="c9"><span class="c6">Yet, intermediate results have the advantage that their tuples can be reused, and this is especially substantial in the presence of a sig- nificant skew. In our experiments, we have found that LFTJ often loses its advantage to the built-in caching of intermediate results of the traditional approaches, and in particular, LFTJ is often required to apply many repetitions of computations. The repeated traversals back and fourth on the trie index generate excessive memory traffic, which has detrimental impact on the performance of database sys- tems [2]. For example, our analysis of the memory load induced by LFTJ found that running a single count 5-cycle query on the SNAP ca-GrQc dataset generates over 45&middot;10</span><span class="c102">9 </span><span class="c6">memory accesses, whereas running the same query using tree decomposition and Yannakakis&rsquo;s join generates less than 16 &middot; 10</span><span class="c102">9 </span><span class="c6">accesses. (The implementation of both algorithms is discussed in Section 5.) </span></p><p class="c94"><span class="c6">Our goal in this work is to accelerate LFTJ by incorporating caching in a way that (a) allows for computation reuse, and (b) does not compromise its key advantages. In particular, our goal is to incorporate caching in LFTJ so that it can utilize whatever memory it has at its disposal towards memoization. However, it is not clear how LFTJ can cache intermediate results (without com- </span></p><p class="c57"><span class="c21">Series ISSN: 2367-2005 282 </span><span class="c165">10.5441/002/edbt.2017.26 </span></p><p class="c78"><span class="c6">puting and storing full results of subqueries as done in other algo- rithms [1, 28]). Intuitively, the challenge lies in the fact that every iteration involves a different partial assignment, and variables are interdependent through the query structure. Our solution is inspired by recent developments in the theory of join optimization, relating to worst-case optimality and tree decomposition [15, 16, 28]. But unlike existing work, we do not apply the join algorithm on each bag independently (which would result in high memory consump- tion due to intermediate results), but rather execute LFTJ as origi- nally designed. </span></p><p class="c32"><span class="c6">Specifically, to enable effective caching our approach applies the following steps. We first build a Tree Decomposition (TD) for the query, in a manner that we discuss later on. Intuitively, a TD trans- forms the query into a tree structure by grouping together several relations, where each group is called a bag. We then execute LFTJ as usual, but throughout the execution we use caches (deploying a caching/eviction policy) for partial assignments. More formally, each bag of the TD is assigned a cache, and the application of the cache happens when the iteration over the variables enters a new bag. The correctness of the cache usage (i.e., the fact that the inter- mediate assignments are consistent with the current assignment in construction) is crucially based on two properties. </span></p><p class="c180"><span class="c6">1. The variable ordering is required to be compatibile with the TD. Intuitively, compatibility means that the variable order is consistent with the preorder of the TD. (The formal definition is in Section 2.) 2. Each cache applies to partial assignments only for the vari- ables it contains (for evaluation) or the subtree underneath (for counting). For TD computation, there is a plethora of algorithms with differ- ent quality guarantees. The classical graph-theoretic measure refers to the maximal size of a bag, and a generalization to hypergraphs is based on the notion of a hypertree width. The optimal values of those (i.e., realizing the tree width and the hypertree width, respec- tively) are both NP-hard problems [4, 13], and efficient algorithms exist for special cases and different approximation guarantees [8]. Other notions include decompositions that approximate the mini- mal fractional hypertree width [15,19]. In our case, a TD defines a caching scheme, and various factors determine the effectiveness of this scheme. Caches are more reusable in the presence of skewed data, and hence, data statistics can be used to estimate the good- ness of a TD. Importantly, our caches correspond to the adhesions (parent-child intersections); in order to better capture opportunities of a high skew (and a high hit rate), we give precedence to keys from a domain of a smaller dimension, and hence, we favor smaller adhesions. Due to these arguments, we chose not to use any specific algorithm that generates a single tree decomposition, but rather to explore a large space of such decompositions. We devise a heuris- tic algorithm for enumerating TDs, tailored primarily towards small adhesions. Once such a collection of TDs are generated, we deploy a cost function that takes various factors into account, including the skew-based cost model of Chu et al. [10]. </span></p><p class="c75"><span class="c6">We experiment on three types of queries: paths, cycles and ran- dom. In par with recent studies on join algorithms, we base our ex- periments on datasets from the SNAP [18] and IMDB workloads. We explore several attributes of our cached LFTJ, such as the cache size and the eviction policy. We also experiment with the count ver- sion of the queries. Our experiments compare among LFTJ, with and without caching, and Yannakakis&rsquo;s algorithm over the TD (as in DunceCap [24,28]), as well as other various systems and engines (LogicBlox [3], PostgreSQL [27] and EmptyHeaded [1]). The re- sults show consistent improvement compared to LFTJ (in orders of magnitude on large queries), as well as general improvement </span></p><p class="c30 c124"><span class="c6">Figure 1: Mass-count disparity plots for value accesses on the evaluation of a 5-path (left) and a 5-cycle (right) over the SNAP ca-GrQc dataset; the double-headed arrows indicate that 80% of the accesses are applied to just 20% (left) and 5% (right) of the nodes </span></p><p class="c48"><span class="c6">compared to the examined algorithms and systems. The only al- ternative that outperforms our implementation on a large portion of the count queries is EmptyHeaded, as it implements a parallel implementation using the Single Instruction Multiple Data (SIMD) parallelization model (while our implementation applies standard sequential computation). We defer hardware utilization of this sort to future research. </span></p><p class="c94"><span class="c6">While retaining the inherent features of LFTJ, our caching dra- matically reduces the memory accesses. For illustration, running a 5-cycle count query generates only 1.4 &middot; 10</span><span class="c102">9 </span><span class="c6">memory accesses, which is over 30&times; fewer accesses than vanilla LFTJ (and over 10&times; fewer accesses than TD with Yannakakis&rsquo;s algorithm). Figure 1 provides some intuition on why we are able to establish such a dramatic improvement with a modest memory usage. The figure depicts mass-count disparity plots [11] for value accesses on the evaluation of a 5-path (left) and a 5-cycle (right) over the SNAP ca- GrQc dataset, which has a graph structure. The x-axis corresponds to the number of accesses. A tick at number n refers to the nodes that are accessed at most n times by our algorithm (node popular- ity); the dashed curve shows the fraction of such nodes among all nodes, and the solid curve shows the fraction of accesses to such nodes among all accesses. On the left plot we can see, for example, that 80% of the accesses are directed to around 20% of the most popular nodes (as indicated by the double-headed arrow), and on the right one we can see that 80% of the accesses are applied to 5% of the most popular nodes! </span></p><p class="c94"><span class="c6">To summarize, our contributions are as follows. First, we extend LFTJ with caching, without compromising the key benefits. Our caching is executed alongside LFTJ, and its size can be determined dynamically according to memory availability. This is achieved by combining LFTJ with a TD, a suitable variable ordering, and a suitable set of target variables for each cache. Second, we devise a heuristic approach to enumerating tree decompositions of a CQ; this approach favors small adhesions, and is based on enumerat- ing graph separating sets by increasing size. Third, we present a thorough experimental study that evaluates the effect of caching on LFTJ, on both evaluation and counting, and compares the results to state-of-the-art join algorithms. </span></p><p class="c76"><span class="c10">2. BACKGROUND </span></p><p class="c90"><span class="c6">In this section we give preliminary definitions and notation that we use throughout the paper. </span></p><p class="c194"><span class="c10">2.1 Conjunctive Queries </span></p><p class="c90"><span class="c6">We study the problem of evaluating a Conjunctive Query (CQ), and the problem of counting the number of tuples in the result of </span></p><p class="c37"><span class="c21">283 </span></p><p class="c30"><span class="c6">a CQ. As in recent work on worst-case optimal joins [21, 22, 29], we focus here on full CQs, which are CQs without projection. For- mally, a full CQ is a sequence &phi;</span><span class="c17">1</span><span class="c15">,...,&phi;</span><span class="c17">m </span><span class="c15">where each &phi;</span><span class="c17">i </span><span class="c15">is a sub- </span><span class="c6">goal of the form R(&tau;</span><span class="c17">1</span><span class="c15">,...,&tau;</span><span class="c17">k</span><span class="c15">) with R being a k-ary relation name </span><span class="c6">and each &tau;</span><span class="c2">j </span><span class="c6">being either a constant or a variable. In the remainder of this paper, we say simply &ldquo;CQ&rdquo; instead of &ldquo;full CQ.&rdquo; We denote by vars(&phi;</span><span class="c2">j</span><span class="c6">) the set of variables that occur in &phi;</span><span class="c2">j</span><span class="c6">, and we denote by vars(q) the union of the sets vars(&phi;</span><span class="c17">j</span><span class="c15">) over all atoms &phi;</span><span class="c17">j </span><span class="c15">in q (i.e., </span><span class="c6">the set of all variables appearing in q). </span></p><p class="c30"><span class="c6">Let q be a CQ. A partial assignment for q is function &mu; that maps every variable in vars(q) to either a constant value or null (denoted &perp;). If &mu; is a partial assignment for q, then we denote by q</span><span class="c17">[&mu;] </span><span class="c15">the CQ </span><span class="c6">that is obtained from q by replacing every variable x with &mu;(x), if &mu;(x) = &perp;, and leaving x intact if &mu;(x) = &perp;. If X is a subset of vars(q), then we denote by &mu;</span><span class="c17">|X </span><span class="c6">the restriction of &mu; to X; that is, &mu;</span><span class="c17">|X </span><span class="c15">is defined only over X, and &mu;</span><span class="c17">|X</span><span class="c15">(x) = &mu;(x) for all x &isin; X. </span></p><p class="c30"><span class="c6">For a CQ q, a partial assignment that maps every variable to a (nonnull) constant is called a complete assignment. Let D be a database over the same relation names as q. Evaluating q over D is the task of producing the set q(D), which consists of all complete assignments &mu; such that all the ground subgoals of q</span><span class="c17">[&mu;] </span><span class="c15">are facts </span><span class="c6">(tuples) of D; such an assignment is also called an answer (for q over D). Counting q over D is the task of computing the number of answers, that is, |q(D)|. </span></p><p class="c30"><span class="c6">The Gaifman graph of a CQ q is the undirected graph that has vars(q) as its node set and an edge between every two variables that co-occur in a subgoal of q. </span></p><p class="c13"><span class="c6">E</span><span class="c42">XAMPLE </span><span class="c6">2.1. Our running example uses the following CQ q over a single binary relation R. </span></p><p class="c13"><span class="c6">R(x</span><span class="c2">1</span><span class="c6">,x</span><span class="c2">2</span><span class="c6">),R(x</span><span class="c2">2</span><span class="c6">,x</span><span class="c2">3</span><span class="c6">),R(x</span><span class="c2">2</span><span class="c6">,x</span><span class="c2">4</span><span class="c6">),R(x</span><span class="c2">3</span><span class="c6">,x</span><span class="c2">4</span><span class="c6">),R(x</span><span class="c2">3</span><span class="c6">,x</span><span class="c2">5</span><span class="c6">),R(x</span><span class="c2">4</span><span class="c6">,x</span><span class="c2">6</span><span class="c6">) </span></p><p class="c30"><span class="c6">Observe that q does not have constant terms. This CQ is illustrated in the graph of Figure 2(a); in this case the graph is also the Gaif- man graph of q (since q is binary). The graph is also the Gaifman graph of the following CQ: </span></p><p class="c13"><span class="c6">R(x</span><span class="c2">1</span><span class="c6">,x</span><span class="c2">2</span><span class="c6">),S(x</span><span class="c2">2</span><span class="c6">,x</span><span class="c2">3</span><span class="c6">,x</span><span class="c2">4</span><span class="c6">),R(x</span><span class="c2">3</span><span class="c6">,x</span><span class="c2">4</span><span class="c6">),R(x</span><span class="c2">3</span><span class="c6">,x</span><span class="c2">5</span><span class="c6">),R(x</span><span class="c2">4</span><span class="c6">,x</span><span class="c2">6</span><span class="c6">) </span></p><p class="c13"><span class="c6">Let &mu; be the partial assignment that maps x</span><span class="c2">1 </span><span class="c6">and x</span><span class="c2">2 </span><span class="c6">to the constants 1 and 2, respectively, and the other variables to &perp;. Then q</span><span class="c17">[&mu;] </span><span class="c6">is </span></p><p class="c13"><span class="c6">R(1, 2),R(2,x</span><span class="c2">3</span><span class="c6">),R(2,x</span><span class="c2">4</span><span class="c6">),R(x</span><span class="c2">3</span><span class="c6">,x</span><span class="c2">4</span><span class="c6">),R(x</span><span class="c2">3</span><span class="c6">,x</span><span class="c2">5</span><span class="c6">),R(x</span><span class="c2">4</span><span class="c6">,x</span><span class="c2">6</span><span class="c6">) . </span></p><p class="c30"><span class="c6">Our example database D, depicted in Figure 2(b), consists of a single relation. It can verified that q(D) contains the following assignments &mu;</span><span class="c2">1 </span><span class="c6">and &mu;</span><span class="c2">2</span><span class="c6">: </span></p><p class="c13"><span class="c6">&bull; &mu;</span><span class="c17">1</span><span class="c15">: x</span><span class="c17">1 </span><span class="c15">&#8614;&rarr; 1, x</span><span class="c17">2 </span><span class="c15">&#8614;&rarr; 2, x</span><span class="c17">3 </span><span class="c15">&#8614;&rarr; 1, x</span><span class="c17">4 </span><span class="c15">&#8614;&rarr; 2, x</span><span class="c17">5 </span><span class="c15">&#8614;&rarr; 3, x</span><span class="c17">6 </span><span class="c15">&#8614;&rarr; 1 </span></p><p class="c13"><span class="c6">&bull; &mu;</span><span class="c17">2</span><span class="c15">: x</span><span class="c17">1 </span><span class="c15">&#8614;&rarr; 1, x</span><span class="c17">2 </span><span class="c15">&#8614;&rarr; 2, x</span><span class="c17">3 </span><span class="c15">&#8614;&rarr; 2, x</span><span class="c17">4 </span><span class="c15">&#8614;&rarr; 1, x</span><span class="c17">5 </span><span class="c15">&#8614;&rarr; 1, x</span><span class="c17">6 </span><span class="c15">&#8614;&rarr; 3 </span><span class="c6">If we remove from &mu;</span><span class="c2">1 </span><span class="c6">and &mu;</span><span class="c2">2 </span><span class="c6">the assignments for x</span><span class="c2">1 </span><span class="c6">and x</span><span class="c2">2</span><span class="c6">, then we get answers in q</span><span class="c17">[&mu;]</span><span class="c15">(D) for the above defined &mu;. </span></p><p class="c13"><span class="c10">2.2 Ordered Tree Decompositions </span></p><p class="c30"><span class="c6">Let q = &phi;</span><span class="c17">1</span><span class="c15">,...,&phi;</span><span class="c17">m </span><span class="c15">be a CQ. A Tree Decomposition (TD) of q </span><span class="c6">is a pair &#12296;t, &chi;&#12297; where t is a tree and &chi; is a function that maps every node of t to a subset &chi;(v) of vars(q), called a bag, such that both of the following hold. </span></p><p class="c13"><span class="c6">&bull; For each &phi;</span><span class="c2">j </span><span class="c6">there is a node v of t with vars(&phi;</span><span class="c2">j</span><span class="c6">) &sube; &chi;(v). </span></p><p class="c13"><span class="c6">&bull; For each x in vars(q), the nodes v with x &isin; &chi;(v) induce a connected subtree of t. An ordered TD of a CQ q is pair &#12296;t, &chi;&#12297; defined similarly to a TD, except that t is a rooted and ordered tree. We denote the root of t by root(t). Let v be a node of t. We denote by t</span><span class="c17">|v </span><span class="c15">the subtree </span><span class="c6">of t that is rooted at v and contains all of the descendants of v. If </span></p><p class="c13"><span class="c26">x</span><span class="c24">1 </span></p><p class="c13"><span class="c61">x</span><span class="c73">3 </span><span class="c61">x</span><span class="c73">4 </span><span class="c26">x</span><span class="c24">5 </span><span class="c26">x</span><span class="c24">6</span><span class="c26">x</span><span class="c24">1 </span></p><p class="c13"><span class="c26">x</span><span class="c24">2 </span><span class="c26">x</span><span class="c24">2 </span></p><p class="c13"><span class="c26">x</span><span class="c24">2 </span></p><p class="c79"><span class="c26">x</span><span class="c24">3 </span><span class="c26">x</span><span class="c24">4 </span><span class="c26">x</span><span class="c24">3 </span><span class="c26">x</span><span class="c24">5 </span><span class="c26">x</span><span class="c24">4 </span><span class="c26">x</span><span class="c24">6 </span><span class="c26">(a) </span></p><p class="c13"><span class="c26">R 1 2 1 3 2 1 2 2 3 1 </span></p><p class="c13"><span class="c26">(b) </span></p><p class="c13"><span class="c61">v </span><span class="c26">x</span><span class="c24">2</span><span class="c26">x</span><span class="c24">3 </span></p><p class="c13"><span class="c26">x</span><span class="c24">4 </span><span class="c26">(c) </span></p><p class="c13"><span class="c6">Figure 2: (a) Example of a CQ q; (b) A database D with a single relation; (c) An ordered tree decomposition of q </span></p><p class="c30"><span class="c6">v is a non-root node, then the parent adhesion of v (or simply the adhesion of v) is the set &chi;(p) &cap; &chi;(v) where p is the parent of v, and is denoted by adhesion(v). Every set adhesion(u), where u is a non-root node of t, is called an adhesion of &#12296;t, &chi;&#12297;. </span></p><p class="c30"><span class="c6">E</span><span class="c42">XAMPLE </span><span class="c6">2.2. We continue with our running example. Fig- ure 2(c) depicts an ordered tree decomposition &#12296;t, &chi;&#12297; of the query q of Figure 2(a). The tree t has four nodes, and the order is top down, left to right. The root is the top node with the bag {x</span><span class="c2">1</span><span class="c6">,x</span><span class="c2">2</span><span class="c6">}. To verify that it is indeed a tree decomposition of q, the reader needs to check that every edge in Figure 2(a) is contained in some bag of &#12296;t, &chi;&#12297;. The adhesions of &#12296;t, &chi;&#12297; are shown in the gray boxes. Let v be the node of t with &chi;(v) = {x</span><span class="c17">2</span><span class="c15">,x</span><span class="c17">3</span><span class="c15">,x</span><span class="c17">4</span><span class="c15">}. The parent adhesion of </span><span class="c6">v, which we denote by adhesion(v), is the singleton {x</span><span class="c2">2</span><span class="c6">}. </span></p><p class="c30"><span class="c6">Let q be a CQ, and let &#12296;t, &chi;&#12297; be an ordered TD of q. The preorder of t is the order &#8826; over the nodes of t such that for every node v with a child c preceding another child c , and nodes u and u in t</span><span class="c17">|c </span><span class="c6">and t</span><span class="c17">|c </span><span class="c6">, respectively, we have v &#8826; u &#8826; u . We denote the preorder of t by &#8826;</span><span class="c17">pre</span><span class="c15">. For a variable x in vars(q), the owner bag </span><span class="c6">of x, denoted owner(x), is the minimal node v of t, under &#8826;</span><span class="c2">pre</span><span class="c6">, such that x &isin; &chi;(v). For a node v of t, we denote by owned(v) the set of variables x that have v as the owner. We say that &#12296;t, &chi;&#12297; is compatible with an ordering &#12296;x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">n</span><span class="c15">&#12297; if i &lt; j whenever </span><span class="c6">owner(x</span><span class="c17">i</span><span class="c15">) &#8826;</span><span class="c17">pre </span><span class="c15">owner(x</span><span class="c17">j</span><span class="c15">). We may also say that the ordering </span><span class="c6">&#12296;x</span><span class="c2">1</span><span class="c6">,...,x</span><span class="c2">n</span><span class="c6">&#12297; is compatible with &#12296;t, &chi;&#12297; if the latter is compatible with the former. </span></p><p class="c30"><span class="c6">E</span><span class="c42">XAMPLE </span><span class="c6">2.3. Consider the given ordering &#12296;x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">6</span><span class="c15">&#12297; of the </span><span class="c6">variables in our running example (Figure 2), and the TD &#12296;t, &chi;&#12297; of Figure 2(c). The preorder of t is given by {x</span><span class="c17">1</span><span class="c15">,x</span><span class="c17">2</span><span class="c15">}, {x</span><span class="c17">2</span><span class="c15">,x</span><span class="c17">3</span><span class="c15">,x</span><span class="c17">4</span><span class="c15">}, </span><span class="c6">{x</span><span class="c17">3</span><span class="c15">,x</span><span class="c17">5</span><span class="c15">}, {x</span><span class="c17">4</span><span class="c15">,x</span><span class="c17">6</span><span class="c15">}. We have owner(x</span><span class="c17">3</span><span class="c15">) = owner(x</span><span class="c17">4</span><span class="c15">) = v, and </span><span class="c6">owned(v) = {x</span><span class="c17">3</span><span class="c15">,x</span><span class="c17">4</span><span class="c15">}. Note that owner(x</span><span class="c17">2</span><span class="c15">) = v since x</span><span class="c17">2 </span><span class="c15">occurs </span><span class="c6">already in the root of t (and therefore owner(x</span><span class="c17">2</span><span class="c15">) = root(t)). </span></p><p class="c13"><span class="c10">2.3 Trie Join </span></p><p class="c30"><span class="c6">We now describe the Leapfrog Trie Join (LFTJ) algorithm [29]. Our description is abstract enough to apply to the tributary join of Chu et al. [10]. Let q = &phi;</span><span class="c17">1</span><span class="c15">,...,&phi;</span><span class="c17">m </span><span class="c15">be a CQ. The execution of </span><span class="c6">LFTJ is based on a predefined ordering &#12296;x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">n</span><span class="c15">&#12297; of vars(q). </span><span class="c6">The correctness and theoretical efficiency of LFTJ are guaranteed on every order of choice, but in practice the order may have a sub- stantial impact on the execution cost [10]. Moreover, in our instan- tiation of LFTJ we will use orderings with specific properties. </span></p><p class="c30"><span class="c6">For every subgoal &phi;</span><span class="c17">k</span><span class="c15">, LFTJ maintains a trie structure on the </span><span class="c6">corresponding relation r. Each level i of the trie corresponds to a variable x</span><span class="c17">j </span><span class="c15">in vars(&phi;</span><span class="c17">k</span><span class="c15">), and holds values that can be matched </span><span class="c6">against x</span><span class="c2">j</span><span class="c6">. Whenever x</span><span class="c2">j </span><span class="c6">is in a level above x</span><span class="c17">j </span><span class="c15">it holds that j&lt;j . </span><span class="c6">Moreover, every path from root to leaf corresponds to a unique </span></p><p class="c13"><span class="c21">284 </span></p><p class="c13"><span class="c26">1 x</span><span class="c24">1</span><span class="c26">1 2 3 </span></p><p class="c13"><span class="c26">2 3 </span></p><p class="c13"><span class="c61">x</span><span class="c73">2 </span></p><p class="c13"><span class="c70">x</span><span class="c147">2 </span></p><p class="c13"><span class="c26">2 3 </span></p><p class="c13"><span class="c26">21 1 2 3 1 2 1 </span><span class="c61">x</span><span class="c73">3</span><span class="c6">Figure 3: The trie structures for subgoals R(x</span><span class="c17">1</span><span class="c15">,x</span><span class="c17">2</span><span class="c15">) and </span><span class="c6">R(x</span><span class="c2">2</span><span class="c6">,x</span><span class="c2">3</span><span class="c6">), respectively, in the running example </span></p><p class="c13"><span class="c6">tuple of r and vice versa. Sibling values in the trie are stored in a sorted manner. </span></p><p class="c30"><span class="c6">E</span><span class="c42">XAMPLE </span><span class="c6">2.4. Figure 3 depicts two of the tries used for eval- uating the CQ q of Example (2.1), of our running example. The left trie is for R(x</span><span class="c17">1</span><span class="c15">,x</span><span class="c17">2</span><span class="c15">) and R(x</span><span class="c17">2</span><span class="c15">,x</span><span class="c17">3</span><span class="c15">). (The reader should ignore </span><span class="c6">the gray triangles for now; we discuss them in the next example.) In this case, the tries are identical (as they index the same rela- tion), but they are used differently during query evaluation. Each level of the trie corresponds to a variable and a corresponding at- tribute. The path root&rarr;1&rarr;2 corresponds to the tuple R(1,2), and root&rarr;2&rarr;1 corresponds to R(2,1). </span></p><p class="c30"><span class="c6">LFTJ applies a sequence of unary joins, called leapfrog joins, as follows. Each trie holds an iterator, initialized by pointing to the root. A mapping &mu;, which is initialized with nulls, is maintained throughout the execution. First, all the subgoals that contain x</span><span class="c17">1 </span><span class="c6">advance their iterators in the first level until a matching value a is found (i.e., all iterators point to a), and &mu;(x</span><span class="c17">1</span><span class="c15">) is set to a. The </span><span class="c6">matching value is found efficiently in a technique referred to as leapfrogging [29]. The algorithm then proceeds recursively</span><span class="c102">1 </span><span class="c6">with the CQ q</span><span class="c17">[&mu;] </span><span class="c6">by proceeding to the next matching value, and so on, until all variables are assigned values (and then &mu; is printed) or no matching values are found; then backtracking takes place by advancing the previous iterator. A balanced-tree storage of the sib- ling collections in the tries guarantees that alignment of the iterators on matching attributes is done efficiently (in an amortized sense), which in turn guarantees that LFTJ is worst-case optimal [21]. </span></p><p class="c30"><span class="c6">E</span><span class="c42">XAMPLE </span><span class="c6">2.5. Continuing Example 2.4, the gray triangles in Figure 3 show a possible positioning of the pointers on the tries during the execution. Here, the pointer for x</span><span class="c17">1 </span><span class="c15">is set on 2, the point- </span><span class="c6">ers of x</span><span class="c2">2 </span><span class="c6">in both tries is set on 1 (which is a matching value found), and next a matching value for x</span><span class="c17">3 </span><span class="c15">will be sought in under the pointed </span><span class="c6">node in the right trie (and the other tries). </span></p><p class="c30"><span class="c6">We refer the reader to the original publication [29] for more de- tails on LFTJ. In this paper, it suffices to regard LFTJ abstractly as depicted in Figure 4. We call the algorithm of Figure 4 trie join and denote it by TrieJoin. This algorithm updates the global partial assignment &mu; using the subroutine RJoin (Recursive Join). </span></p><p class="c13"><span class="c10">3. CACHING IN TRIE JOIN </span></p><p class="c13"><span class="c6">In this section we devise an algorithm that incorporates caching within TrieJoin (Figure 4). We first discuss the intuition. </span><span class="c10">3.1 Intuition </span></p><p class="c30"><span class="c6">The general idea is as follows. Let q be the evaluated CQ, and let &#12296;x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">n</span><span class="c15">&#12297; be vars(q) in the order of iteration. Consider a point </span><span class="c6">in the iteration where we complete the assignment for x</span><span class="c2">1</span><span class="c6">,...,x</span><span class="c2">j </span><span class="c6">(j&lt;n), and suppose that we have already encountered the assign- ment for x</span><span class="c2">i</span><span class="c6">,...,x</span><span class="c2">j </span><span class="c6">in the past for some i such that 1 &lt;i&lt;j. We </span><span class="c2">1</span><span class="c15">The actual algorithm of [29] is not recursive, but rather applies a single procedure call. Recursion simplifies our presentation. </span></p><p class="c13"><span class="c6">Algorithm TrieJoin(q,&#12296;x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">n</span><span class="c15">&#12297;,T ) </span></p><p class="c13"><span class="c6">1: for d = 1,...,n do 2: &mu;(x</span><span class="c17">d</span><span class="c15">) := &perp; </span><span class="c6">3: RJoin(1) </span></p><p class="c13"><span class="c6">Subroutine RJoin(d) </span></p><p class="c13"><span class="c6">1: if d = n + 1 then 2: print &mu; 3: return 4: for all matching values a for x</span><span class="c17">d </span><span class="c15">in T do </span><span class="c6">5: position T on x</span><span class="c17">d </span><span class="c15">&#8614;&rarr; a </span><span class="c6">6: &mu;(x</span><span class="c17">d</span><span class="c15">) := a </span><span class="c6">7: RJoin(d + 1) 8: reset x</span><span class="c17">d </span><span class="c15">pointers in T </span></p><p class="c13"><span class="c6">Figure 4: Trie join </span></p><p class="c30"><span class="c6">would like to be able to reuse the past assignments, at least for a few of the next variables, say x</span><span class="c2">j+1</span><span class="c6">,...,x</span><span class="c2">k</span><span class="c6">, instead of searching again for matches. Integrating simple memoization in the algorithm will not suffice. The problem is that the assignments for x</span><span class="c2">j+1</span><span class="c6">,...,x</span><span class="c17">k </span><span class="c15">may depend not just on those for x</span><span class="c17">i</span><span class="c15">,...,x</span><span class="c17">j</span><span class="c15">, but rather on the as- </span><span class="c6">signments for variables in x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">i&minus;1</span><span class="c15">, and so reusing past assign- </span><span class="c6">ments may lead to incorrect results (false assignments). </span></p><p class="c30"><span class="c6">The above problem is avoided as follows. First, we deploy an ordered TD &#12296;t, &chi;&#12297;, and use an ordering &#12296;x</span><span class="c2">1</span><span class="c6">,...,x</span><span class="c2">n</span><span class="c6">&#12297; that is com- patible with &#12296;t, &chi;&#12297; (as defined in Section 2.2). Second, cache keys are assignments to sequences x</span><span class="c2">i</span><span class="c6">,...,x</span><span class="c2">j </span><span class="c6">of variables only if the set {x</span><span class="c17">i</span><span class="c15">,...,x</span><span class="c17">j</span><span class="c15">} is an adhesion of some node v of t. Finally, we cache </span><span class="c6">assignments only for the variables x</span><span class="c17">j+1</span><span class="c15">,...,x</span><span class="c17">k </span><span class="c15">that are owned by </span><span class="c6">v. Due to the nature of the TD, we can rest assured that the as- signments to x</span><span class="c17">j+1</span><span class="c15">,...,x</span><span class="c17">k </span><span class="c15">are independent of the assignments to </span><span class="c6">x</span><span class="c2">1</span><span class="c6">,...,x</span><span class="c2">i&minus;1 </span><span class="c6">(once we know the assignments for x</span><span class="c2">i</span><span class="c6">,...,x</span><span class="c2">j</span><span class="c6">). </span></p><p class="c13"><span class="c6">E</span><span class="c42">XAMPLE </span><span class="c6">3.1. Consider again our running example around Fig- ure 2. At some point in the execution of TrieJoin we construct the assignment &mu; with &mu;(x</span><span class="c2">1</span><span class="c6">)=1 and &mu;(x</span><span class="c2">2</span><span class="c6">)=2, and then continue to the rest of the variables in order. The next assignments we con- struct are x</span><span class="c2">3 </span><span class="c6">&#8614;&rarr; 1 and x</span><span class="c2">4 </span><span class="c6">&#8614;&rarr; 2. Once we are done with the com- plete assignments for the extended &mu;, we construct the assignments x</span><span class="c17">3 </span><span class="c15">&#8614;&rarr; 2 and x</span><span class="c17">4 </span><span class="c15">&#8614;&rarr; 1, and later on x</span><span class="c17">3 </span><span class="c15">&#8614;&rarr; 2 and x</span><span class="c17">4 </span><span class="c15">&#8614;&rarr; 2. Later </span><span class="c6">in the execution, we encounter the assignment &mu; with &mu;(x</span><span class="c2">1</span><span class="c6">)=2 and &mu;(x</span><span class="c17">2</span><span class="c15">) = 2. Since adhesion(x) = {x</span><span class="c17">2</span><span class="c15">}, we check to see </span><span class="c6">whether there is a cache for x</span><span class="c2">2 </span><span class="c6">&#8614;&rarr; 1, and if so, then it tells us ex- actly where to position the pointers for x</span><span class="c17">3 </span><span class="c15">and x</span><span class="c17">4 </span><span class="c15">(which are the </span><span class="c6">variables owned by v) in each of the possibilities (which are (1,2), (2,1), (2,2) and nothing else). We may similarly have a cache for {x</span><span class="c17">3</span><span class="c15">} (lower left adhesion) and for {x</span><span class="c17">4</span><span class="c15">} (lower right adhesion). </span></p><p class="c30"><span class="c6">Caching could be obtained by computing the complete join for every bag (using TrieJoin), and then joining the intermediate re- sults using an algorithm for acyclic joins such as Yannakakis [30], as done in DunceCap [24, 28]. However, we wish to control the memory consumption and avoid storing the complete joins of sub- queries. Our algorithm executes TrieJoin ordinarily, yet caches re- sults during the execution based on a deployed caching policy. </span></p><p class="c13"><span class="c6">C</span><span class="c42">OMMENT </span><span class="c6">3.2. Compatibility of the variable ordering with the TD has implications on the trie structures, which need to be consis- </span></p><p class="c13"><span class="c21">285 </span></p><p class="c128"><span class="c6">Algorithm CacheTrieJoin(q,&#12296;x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">n</span><span class="c15">&#12297;,&#12296;t, &chi;&#12297;,T ) </span></p><p class="c167"><span class="c6">1: for d = 1,...,n do 2: &mu;(x</span><span class="c17">d</span><span class="c15">) := &perp; </span><span class="c6">3: for all nodes v of t do 4: cache</span><span class="c2">v </span><span class="c6">:= &empty; 5: CacheRJoin(1)</span><span class="c15">Subroutine CacheRJoin(d) </span></p><p class="c160"><span class="c6">1: if d = n + 1 then 2: print &mu; 3: return 4: v := owner(x</span><span class="c17">d</span><span class="c15">) </span><span class="c6">5: {x</span><span class="c17">l</span><span class="c15">,x</span><span class="c17">l+1</span><span class="c15">,...,x</span><span class="c17">k</span><span class="c15">} := owned(v) </span><span class="c6">6: &alpha; := adhesion(v) 7: if v = root(t) and d = l then 8: if &mu;</span><span class="c17">|&alpha; </span><span class="c15">is a cache hit in cache</span><span class="c17">v </span><span class="c15">then </span><span class="c6">9: for all cached entries &mu; in cache</span><span class="c2">v</span><span class="c6">(&mu;</span><span class="c17">|&alpha;</span><span class="c15">) do </span><span class="c6">10: for i = l,...,k do 11: &mu;(x</span><span class="c17">i</span><span class="c15">) := &mu; (x</span><span class="c17">i</span><span class="c15">) </span><span class="c6">12: AdjustTries(T ,&mu; ) 13: CacheRJoin(k + 1) 14: reset x</span><span class="c17">l</span><span class="c15">,...,x</span><span class="c17">k </span><span class="c15">pointers in T </span><span class="c6">15: return 16: for all matching values a for x</span><span class="c17">d </span><span class="c15">in T do </span><span class="c6">17: position T on x</span><span class="c17">d </span><span class="c15">&#8614;&rarr; a </span><span class="c6">18: &mu;(x</span><span class="c17">d</span><span class="c15">) := a </span><span class="c6">19: CacheRJoin(d + 1) 20: if v = root(t) and d = k then 21: ApplyCachePolicy(cache</span><span class="c2">v</span><span class="c6">,&mu;</span><span class="c17">|&alpha;</span><span class="c15">,&mu;</span><span class="c17">|owned(v)</span><span class="c15">) </span><span class="c6">22: reset x</span><span class="c17">d </span><span class="c15">pointers in T </span></p><p class="c112"><span class="c6">Figure 5: TrieJoin with caching </span></p><p class="c139"><span class="c6">tent with the variable ordering [29]. Therefore, similarly to Empty- Headed [15], the design of our tries depends on the TD. As building the trie may take considerable time, our approach matches the sce- nario where the join is known in advance, but not the data (which is common in Web applications where queries arise due to user interaction with the UI). Another matching scenario is where the relations are narrow (e.g., graphs), and then we can compute in ad- vance multiple trie structures (which is the design choice of Emp- tyHeaded [15]) and load the proper ones upon need. </span></p><p class="c121"><span class="c10">3.2 Algorithm </span></p><p class="c65"><span class="c6">We now turn to a more formal description of our algorithm, which we call CacheTrieJoin, and is depicted in Figure 5. The algorithm extends upon the algorithm of Figure 4 in the sense that when no caching takes place, the two algorithms coincide. The al- gorithm takes as input a CQ q, a variable ordering &#12296;x</span><span class="c2">1</span><span class="c6">,...,x</span><span class="c2">n</span><span class="c6">&#12297;, an ordered TD &#12296;t, &chi;&#12297; that is compatible with &#12296;x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">n</span><span class="c15">&#12297;, and a </span><span class="c6">trie structure T for a database D. The algorithm prints all tuples in q(D). The algorithm uses a cache, denoted cache</span><span class="c17">v</span><span class="c15">, for every </span><span class="c6">node v of t, for caching computed assignments for the variables owned by v. The algorithm CacheTrieJoin simply initializes a global partial assignment &mu; and each cache</span><span class="c17">v</span><span class="c15">, and calls the sub- </span><span class="c6">routine CacheRJoin (the caching version of RJoin of Figure 4), which we describe next. </span></p><p class="c30 c124 c126"><span class="c6">The first part of the algorithm, lines 1&ndash;3, tests whether we are done with the variable scan (that is, the algorithm is called with the index n + 1) and, if so, prints &mu;. Now assume that d &le; n. So the currently iterated variable is x</span><span class="c17">d</span><span class="c15">. We denote by v the owner of x</span><span class="c17">d</span><span class="c15">, </span><span class="c6">and by &alpha; the adhesion of v (as defined in Section 2). Moreover, we assume that the nodes owned by v are x</span><span class="c17">l</span><span class="c15">,...,x</span><span class="c17">k </span><span class="c15">in ascend- </span><span class="c6">ing indices. Observe that owned(v) is indeed a consecutive set of variables, since the order is compatible with t. </span></p><p class="c94"><span class="c6">In lines 8&ndash;15 we handle the case where we have just entered v from a different node of t, which means that x</span><span class="c2">d </span><span class="c6">is the first node x</span><span class="c2">l </span><span class="c6">owned by v, and v is not the root (that is, v &gt; 1). From our con- struction, the adhesion of v is already assigned values in &mu; (again due to compatibility), and we check whether there is a cache hit for &mu;</span><span class="c17">|&alpha; </span><span class="c15">(the restriction of &mu; to &alpha;) in cache</span><span class="c17">v</span><span class="c15">. If indeed there is a cache </span><span class="c6">hit, then in lines 9&ndash;15 we scan the cache that contains all assign- ments &mu; that we have already computed for &mu;</span><span class="c17">|&alpha;</span><span class="c15">. For each such &mu; , </span><span class="c6">we extend &mu; with &mu; and adjust the trie structure T according to &mu; . By adjusting T we consider every variable x</span><span class="c17">i </span><span class="c15">in x</span><span class="c17">l</span><span class="c15">,...,x</span><span class="c17">k </span><span class="c15">and if </span><span class="c6">x</span><span class="c2">i </span><span class="c6">is later used for a join, then we position the pointer precisely where it should have been if we scanned the trie and got to &mu; (x</span><span class="c17">i</span><span class="c15">);</span><span class="c2">2 </span><span class="c6">and if x</span><span class="c17">i </span><span class="c15">is not used for a future join, then we do nothing. As an </span><span class="c6">example, in our running example (Figure 2), we ignore x</span><span class="c2">5 </span><span class="c6">if we have a cached assignment for it. </span></p><p class="c94"><span class="c6">Lines 16&ndash;22 are executed in the case where we have not just entered v, or we do but had a cache miss on line 8. In this case, we continue exactly as in RJoin (Figure 4), but we also test whether x</span><span class="c17">d </span><span class="c15">is the last variable owned by v (i.e., x</span><span class="c17">d </span><span class="c15">is x</span><span class="c17">k</span><span class="c15">). If so, we either cache </span><span class="c6">or do not cache the assignment &mu;</span><span class="c17">|owned(v) </span><span class="c15">based on the underlying </span><span class="c6">caching policy for &mu;</span><span class="c17">|&alpha;</span><span class="c15">. Observe that this action may lead to an </span><span class="c6">eviction of a previously stored entry for some &mu; </span><span class="c17">|&alpha;</span><span class="c6">. </span></p><p class="c124 c129"><span class="c6">E</span><span class="c42">XAMPLE </span><span class="c6">3.3. We will now show how the scenario of Exam- ple 3.1 is realized in the algorithm CacheTrieJoin, where we con- sider again our running example (Figure 2). The algorithm first calls CacheRJoin(1), and the execution is the same as in RJoin, all the way until we reach the call to CacheRJoin(3) where we have &mu;(x</span><span class="c17">1</span><span class="c15">)=1 and &mu;(x</span><span class="c17">2</span><span class="c15">)=2. Observe that owner(x</span><span class="c17">3</span><span class="c15">) = v, which is </span><span class="c6">a non-root node, owned(v) = {x</span><span class="c2">3</span><span class="c6">,x</span><span class="c2">4</span><span class="c6">} (hence, l = 3 and k = 4). Also note that adhesion(v) = {x</span><span class="c17">2</span><span class="c15">}. The test of line 7 is true, but </span><span class="c6">that of line 8 is false since cache</span><span class="c17">v </span><span class="c15">is empty at that point. So, we </span><span class="c6">continue to lines 16&ndash;19 and apply the different assignments for x</span><span class="c2">3</span><span class="c6">, starting with x</span><span class="c17">3 </span><span class="c15">&#8614;&rarr; 1. We then call CacheRJoin(4), where we find </span><span class="c6">the assignment x</span><span class="c2">4 </span><span class="c6">&#8614;&rarr; 2. The test of line 20 is true, since x</span><span class="c2">4 </span><span class="c6">is the last owned by v. Therefore, we may decide (based on the applied caching policy) to cache the entry x</span><span class="c17">2 </span><span class="c15">&#8614;&rarr; 2 in cache</span><span class="c17">v</span><span class="c15">, and then we </span><span class="c6">store there the assignment (x</span><span class="c17">3</span><span class="c15">,x</span><span class="c17">4</span><span class="c15">) &#8614;&rarr; (1,2). We later store in that </span><span class="c6">entry the assignment (x</span><span class="c17">3</span><span class="c15">,x</span><span class="c17">4</span><span class="c15">) &#8614;&rarr; (2,2). </span></p><p class="c101"><span class="c6">Later in the execution, we call CacheRJoin(3) when we have &mu;(x</span><span class="c17">1</span><span class="c15">)=2 and &mu;(x</span><span class="c17">2</span><span class="c15">)=2. We may then find out that in cache</span><span class="c17">v </span><span class="c6">we have cached the entry of x</span><span class="c2">2 </span><span class="c6">&#8614;&rarr; 2, and we simply use the two tuples &mu; that maps (x</span><span class="c17">3</span><span class="c15">,x</span><span class="c17">4</span><span class="c15">) to (1,2) and to (2,2), as in lines 9&ndash; </span><span class="c6">13. However, if there is a cache miss then we repeat the above first execution of CacheRJoin(3). </span></p><p class="c124 c126 c168"><span class="c6">The following theorem states the correctness of the algorithm CacheTrieJoin. The proof is by a fairly straightforward application of the basic separation properties of a tree decomposition. </span></p><p class="c168 c124 c186"><span class="c6">T</span><span class="c42">HEOREM </span><span class="c6">3.4. Let q be a CQ, &#12296;x</span><span class="c2">1</span><span class="c6">,...,x</span><span class="c2">n</span><span class="c6">&#12297; an ordering of vars(Q) and &#12296;t, &chi;&#12297; a TD compatible with &#12296;x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">n</span><span class="c15">&#12297;. Let D </span><span class="c6">be a database, and T a trie structure for TrieJoin. Algorithm CacheTrieJoin(q,&#12296;x</span><span class="c2">1</span><span class="c6">,...,x</span><span class="c2">n</span><span class="c6">&#12297;,&#12296;t, &chi;&#12297;,T ) prints q(D). </span></p><p class="c155"><span class="c2">2</span><span class="c15">Technically, this is done by storing the position with &mu; in cache</span><span class="c17">v</span><span class="c15">. </span></p><p class="c125"><span class="c21">286 </span></p><p class="c164"><span class="c10">3.3 Counting </span></p><p class="c65"><span class="c6">We now describe a variation of CacheTrieJoin (Figure 5) for counting the number of tuples in q(D). The counting algorithm, which we refer to as CacheTJCount, is depicted in Figure 5. The input is the same as that of CacheTrieJoin, and the flow is very sim- ilar. There are, however, a few key differences, and our explanation (next) will focus on these. </span></p><p class="c75"><span class="c6">The algorithm CacheTJCount uses some new global variables and data structures. The variable total counts the joined tuples throughout the execution, and in the end stores the required number. For every non-root node v of t we have a counter intrmd(v) that stores the intermediate count of the assignments to the variables owned by the nodes in t</span><span class="c17">|v </span><span class="c15">(i.e., the subtree of t that consists of v </span><span class="c6">and all of its descendants), given the assignment to adhesion(v) in the current iteration. More precisely, let i be the maximal num- ber such that x</span><span class="c17">i </span><span class="c15">is in the adhesion of v, and consider a partial </span><span class="c6">assignment &mu; that is nonnull on precisely x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">i</span><span class="c15">. In an iter- </span><span class="c6">ation where &mu; is constructed, intrmd(v) will eventually hold the number of assignments &mu; that TrieJoin can assign to the variables owned by the nodes in t</span><span class="c17">|v</span><span class="c15">. As &#12296;t, &chi;&#12297; is compatible with the or- </span><span class="c6">dering &#12296;x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">n</span><span class="c15">&#12297;, this number is the same for all assignments &mu; </span><span class="c6">that agree on the adhesion &alpha; of v. The counter intrmd(v) holds the correct value once we are done with the variables owned by v. Another fundamental difference from CacheTJCount is that now cache</span><span class="c2">v </span><span class="c6">stores a natural number (rather than a collection of assign- ments) for each assignment &mu;</span><span class="c17">&alpha;</span><span class="c15">; this number is precisely the value </span><span class="c6">of intrmd(v) once we are done with the variables in t</span><span class="c17">|v</span><span class="c15">. </span></p><p class="c30 c99"><span class="c6">Following the initialization, the algorithm calls the subroutine CacheRJoinCount, which is the counting version of CacheRJoin. The input takes not only the variable index d, but also a factor f that aggregates cached intermediate counts. When we are done scanning all of the variables (i.e., we reach line 2), the factor f is added to total. When we are at the first node owned by the current non-root owner v (lines 7&ndash;13), we reset the counter intrmd(v). If we have a cache hit for &mu;</span><span class="c17">|&alpha; </span><span class="c15">in cache</span><span class="c17">v</span><span class="c15">, then we copy the number </span><span class="c6">cache</span><span class="c17">v</span><span class="c15">(&alpha;) into intrmd(v), multiply f by this number, and jump </span><span class="c6">directly to the first index outside of t</span><span class="c17">|v </span><span class="c15">(line 12). This skipping </span><span class="c6">is where compatibility is required, since it ensures that the nodes owned by t</span><span class="c17">|v </span><span class="c15">constitute a consecutive interval in &#12296;1,...,n&#12297;. </span></p><p class="c30 c99"><span class="c6">As previously, lines 14&ndash;20 are executed in the case where we have not just entered v, or experience a cache miss. We then con- tinue as in RJoin. However, if x</span><span class="c17">d </span><span class="c15">is the last variable owned by v, </span><span class="c6">then we update the intermediate count by adding the product of the intermediate results intrmd(c) of the children c of v. (Note that this product is 1 when v is a leaf.) Finally, in lines 22&ndash;23 we con- sider again the case where we have just entered a node v. Then, we are about to go back to the previous node, and so we apply the caching policy to possibly cache the number intrmd(v) for &mu;</span><span class="c17">|&alpha; </span><span class="c15">in </span><span class="c6">cache</span><span class="c2">v</span><span class="c6">. (This is why we maintain intrmd(v) to begin with.) </span></p><p class="c54"><span class="c6">E</span><span class="c42">XAMPLE </span><span class="c6">3.5. We illustrate CacheTJCount on our running example (Figure 2). On CacheRJoinCount(1,1) we set (in lines 16&ndash; 17) &mu;(x</span><span class="c17">1</span><span class="c15">) = 1 and call CacheRJoinCount(2,1), where we set </span><span class="c6">&mu;(x</span><span class="c17">2</span><span class="c15">)=2 and call CacheRJoinCount(3,1). We reach line 8 and </span><span class="c6">initialize intrmd(v) to 0. We have a cache miss (as the cache is empty), and we reach line 14, where CacheRJoinCount(4,1) is called with&mu;(x</span><span class="c2">3</span><span class="c6">)=1. From there we call CacheRJoinCount(5,1) with &mu;(x</span><span class="c17">4</span><span class="c15">)=2. Let c</span><span class="c17">l </span><span class="c15">and c</span><span class="c17">r </span><span class="c15">be the left and right children of v, </span><span class="c6">respectively. When the call returns, we have intrmd(c</span><span class="c17">l</span><span class="c15">)=2 and </span><span class="c6">intrmd(c</span><span class="c2">r</span><span class="c6">)=2, as x</span><span class="c2">5 </span><span class="c6">can be mapped to 2 and 3 and x</span><span class="c2">6 </span><span class="c6">can be mapped to 1 and 2. At this point total is equal to 4, since the scan has ended four times. We then reach line 18 (since x</span><span class="c2">4 </span><span class="c6">is the last owned by v) and set intrmd(v)=0+2 &times; 2=4. Similarly, after </span></p><p class="c13 c59"><span class="c6">Algorithm CacheTJCount(q,&#12296;x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">n</span><span class="c15">&#12297;, &#12296;t, &chi;&#12297;,T ) </span></p><p class="c69"><span class="c6">1: for d = 1,...,n do 2: &mu;(x</span><span class="c17">d</span><span class="c15">) := &perp; </span><span class="c6">3: for all nodes v of t do 4: cache</span><span class="c2">v </span><span class="c6">:= &empty; 5: intrmd(v) := 0 6: total := 0 7: CacheRJoinCount(1,1) 8: return total </span></p><p class="c55"><span class="c6">Subroutine CacheRJoinCount(d, f) </span></p><p class="c4"><span class="c6">1: if d = n + 1 then 2: total := total + f 3: return 4: v := owner(x</span><span class="c2">d</span><span class="c6">) 5: {x</span><span class="c17">l</span><span class="c15">,x</span><span class="c17">l+1</span><span class="c15">,...,x</span><span class="c17">k</span><span class="c15">} := owned(v) </span><span class="c6">6: &alpha; := adhesion(v) 7: if v = root(t) and d = l then 8: intrmd(v) := 0 9: if &mu;</span><span class="c17">|&alpha; </span><span class="c15">is a cache hit in cache</span><span class="c17">v </span><span class="c15">then </span><span class="c6">10: intrmd(v) := cache</span><span class="c17">v</span><span class="c15">(&mu;</span><span class="c17">|&alpha;</span><span class="c15">) </span><span class="c6">11: m := max{i | owner(x</span><span class="c2">i</span><span class="c6">) is in t</span><span class="c17">|v</span><span class="c15">} </span><span class="c6">12: CacheRJoinCount(m + 1,f &middot; cache</span><span class="c17">v</span><span class="c15">(&mu;</span><span class="c17">|&alpha;</span><span class="c6">)) 13: return 14: for all matching values a for x</span><span class="c17">d </span><span class="c15">in T do </span><span class="c6">15: position T on x</span><span class="c17">d </span><span class="c15">&#8614;&rarr; a </span><span class="c6">16: &mu;(x</span><span class="c17">d</span><span class="c15">) := a </span><span class="c6">17: CacheRJoinCount(d + 1,f) 18: if v = root(t) and d = k then 19: let c</span><span class="c17">1</span><span class="c15">,...,c</span><span class="c17">k </span><span class="c15">be the children of v in t </span><span class="c6">20: intrmd(v) := intrmd(v) + </span><span class="c33">&prod;</span><span class="c2">k</span><span class="c17">i=1 </span><span class="c6">intrmd(c</span><span class="c17">i</span><span class="c15">) </span><span class="c6">21: reset x</span><span class="c2">d </span><span class="c6">pointers in T 22: if v = root(t) and d = l then 23: ApplyCachePolicy(cache</span><span class="c2">v</span><span class="c6">,&mu;</span><span class="c17">|&alpha;</span><span class="c15">,intrmd(v)) </span></p><p class="c52"><span class="c6">Figure 6: Cached count over trie join </span></p><p class="c114"><span class="c6">the call to CacheRJoinCount(4,1) with &mu;(x</span><span class="c17">3</span><span class="c15">)=2 there will be </span><span class="c6">a call with &mu;(x</span><span class="c2">4</span><span class="c6">)=1 and intrmd(v) will be incremented by an- other 4, and so will be the case with &mu;(x</span><span class="c17">4</span><span class="c15">)=2. So, when we reach </span><span class="c6">line 23 for d = 3, we may cache the number 12 as cache</span><span class="c2">v</span><span class="c6">(&mu;</span><span class="c2">&alpha;</span><span class="c6">). </span></p><p class="c101"><span class="c6">The next time CacheRJoinCount(3,1) is called with &mu;(x</span><span class="c17">2</span><span class="c15">) = </span><span class="c6">2 (i.e., when &mu;(x</span><span class="c2">1</span><span class="c6">) = 2), we check cache</span><span class="c2">v </span><span class="c6">and may find that cache</span><span class="c17">v</span><span class="c15">(&mu;</span><span class="c17">&alpha;</span><span class="c15">) exists (i.e., cache hit) with cache</span><span class="c17">v</span><span class="c15">(&mu;</span><span class="c17">&alpha;</span><span class="c15">) = 12. If so, </span><span class="c6">we reach line 12 and call CacheRJoinCount(7,1&times;12). As 7 &gt; n, we skip to line 2 and add 12 to total. If there is a cache miss for &mu;</span><span class="c17">&alpha; </span><span class="c15">in cache</span><span class="c17">v</span><span class="c15">, there might still be a cache hit when we call </span><span class="c6">CacheRJoinCount(5,1) with &mu;(x</span><span class="c2">3</span><span class="c6">) = 2, and then we immedi- ately call CacheRJoinCount(6,1 &times; 2) on line 12, as 6 is the mini- mal index outside the subtree of c</span><span class="c17">l </span><span class="c15">(which contains only c</span><span class="c17">l</span><span class="c15">). </span></p><p class="c35"><span class="c6">The following theorem states the correctness of the algorithm CacheTJCount. </span></p><p class="c98"><span class="c6">T</span><span class="c42">HEOREM </span><span class="c6">3.6. Let q be a CQ, &#12296;x</span><span class="c2">1</span><span class="c6">,...,x</span><span class="c2">n</span><span class="c6">&#12297; an ordering of vars(Q) and &#12296;t, &chi;&#12297; a TD that is compatible with &#12296;x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">n</span><span class="c15">&#12297;. Let </span><span class="c6">D be a database, and T a trie structure for TrieJoin. Algorithm CacheTJCount(q,&#12296;x</span><span class="c17">1</span><span class="c15">,...,x</span><span class="c17">n</span><span class="c15">&#12297;,&#12296;t, &chi;&#12297;,T ) computes |q(D)|. </span></p><p class="c125"><span class="c21">287 </span></p><p class="c13"><span class="c0">p</span><span class="c5">1 </span></p><p class="c13"><span class="c0">m</span><span class="c5">1 </span></p><p class="c13"><span class="c0">p</span><span class="c5">1 </span></p><p class="c13"><span class="c0">m</span><span class="c5">2 </span><span class="c60">p</span><span class="c5">2 </span></p><p class="c13"><span class="c0">p</span><span class="c5">1 </span><span class="c60">m</span><span class="c5">1</span><span class="c60">p</span><span class="c5">2 </span></p><p class="c13"><span class="c60">m</span><span class="c5">1 </span><span class="c0">m</span><span class="c5">2</span><span class="c60">m</span><span class="c5">2 </span></p><p class="c13"><span class="c0">p</span><span class="c5">1 </span></p><p class="c13"><span class="c0">p</span><span class="c5">2 </span></p><p class="c13"><span class="c0">m</span><span class="c5">1 </span></p><p class="c13"><span class="c0">p</span><span class="c5">2 </span></p><p class="c79"><span class="c0">p</span><span class="c5">1 </span><span class="c60">p</span><span class="c5">2</span><span class="c0">m</span><span class="c5">2 </span><span class="c0">m</span><span class="c5">3 </span><span class="c0">m</span><span class="c5">1 </span><span class="c60">m</span><span class="c5">2 </span></p><p class="c13"><span class="c0">m</span><span class="c14">1 </span></p><p class="c13"><span class="c0">p</span><span class="c5">2 </span></p><p class="c13"><span class="c0">p</span><span class="c14">1 </span><span class="c0">m</span><span class="c14">1 </span><span class="c0">p</span><span class="c5">3 </span></p><p class="c13"><span class="c0">m</span><span class="c14">1 </span><span class="c0">m</span><span class="c14">2 </span></p><p class="c13"><span class="c0">p</span><span class="c14">1 </span></p><p class="c13"><span class="c0">p</span><span class="c14">2 </span><span class="c0">p</span><span class="c14">3 </span></p><p class="c13"><span class="c0">p</span><span class="c14">1 </span></p><p class="c13"><span class="c0">p</span><span class="c5">1</span><span class="c0">p</span><span class="c5">3 </span></p><p class="c13"><span class="c60">p</span><span class="c5">1 </span></p><p class="c79"><span class="c0">p</span><span class="c5">3 </span><span class="c60">m</span><span class="c5">1 </span><span class="c0">m</span><span class="c5">2 </span><span class="c0">m</span><span class="c14">2 </span></p><p class="c13"><span class="c0">p</span><span class="c14">3 </span><span class="c0">m</span><span class="c14">1 </span><span class="c0">m</span><span class="c14">2 </span><span class="c0">p</span><span class="c14">3 </span><span class="c60">m</span><span class="c5">2 </span></p><p class="c13"><span class="c0">m</span><span class="c5">2 </span></p><p class="c13"><span class="c0">p</span><span class="c5">3</span><span class="c0">m</span><span class="c5">2 </span><span class="c60">p</span><span class="c5">3 </span></p><p class="c13"><span class="c0">m</span><span class="c14">2 </span><span class="c0">p</span><span class="c14">2 </span><span class="c0">p</span><span class="c14">3 </span></p><p class="c13"><span class="c0">m</span><span class="c14">2 </span></p><p class="c13"><span class="c0">p</span><span class="c14">3 </span></p><p class="c13"><span class="c0">m</span><span class="c14">3 </span></p><p class="c13"><span class="c0">p</span><span class="c5">2 </span></p><p class="c13"><span class="c60">p</span><span class="c5">3 </span></p><p class="c13"><span class="c60">m</span><span class="c5">2 </span><span class="c0">m</span><span class="c5">3 </span><span class="c0">m</span><span class="c14">3 </span></p><p class="c13"><span class="c0">p</span><span class="c14">2 </span><span class="c0">m</span><span class="c14">2 </span><span class="c0">m</span><span class="c14">3 </span></p><p class="c13"><span class="c6">Figure 7: 4-cycle (top) and 6-cycle (bottom) queries on IMDB, each with two isomorphic TDs </span></p><p class="c30"><span class="c6">The proof is more involved than Theorem 3.4, and has two steps. We first prove, by induction on time, that whenever we complete iterating the variables of v, the number intrmd(v) is correct (i.e., it is the number of intermediate results for t</span><span class="c17">|v </span><span class="c15">given the assignment </span><span class="c6">for adhesion(v)). In the second step we show that every unit added to total accounts for a unique tuple in q(D) and vice versa. </span></p><p class="c13"><span class="c10">4. DECOMPOSITION </span></p><p class="c30"><span class="c6">We now discuss the challenge of finding a TD &#12296;t, &chi;&#12297; and a com- patible variable ordering. A typical TD algorithm aims at opti- mising some specific cost function such as generalized/fractional hypertree width [12, 15]. In our case, an important factor in the ef- fectiveness of the caches in our algorithms is their dimensionality, which is determined by the size of the adhesions. To better cap- ture opportunities of a high skew and hit rate, we give precedence to keys from a domain of a smaller dimension, and hence, we fa- vor smaller adhesions. There are, however, additional criteria be- yond the topological properties of the TD. For example, we would like to use adhesions such that their corresponding subqueries have high skews in the data, and then caching a small number of in- termediate results can save a lot of repeated computation. More- over, we would like to have a TD that is compatible with an order that is estimated as good to begin with. For a (rather extreme) il- lustration, Figure 7 depicts two TDs of two queries, 4-cycle and 6-cycle, over the IMDB dataset (see Section 5), where m and p de- note movie and person identifiers, respectively. The left TD favors persons for caching and the right favors movies for caching. While the decompositions are isomorphic, their performance of counting varies greatly: 4-cycle took around 40 seconds with the left TD, and around 4,000 with the right one; and 6-cycle took around 600 seconds and 27,000 on the left and right TDs, respectively. </span></p><p class="c13"><span class="c6">We take the approach of generating many TDs, estimating a cost on each, and selecting the one with the best estimate. In our imple- mentation (described in the next section), we deploy a heuristic cost function that ranks TDs based on three criteria, in a lexicographic manner: the maximal size over the adhesions (lower is better), the number of bags (higher is better), the sum of adhesions (lower is better), and the cost function of Chu et al. [10] for some variable ordering that is compatible with the TD. </span><span class="c10">4.1 Enumerating TDs </span></p><p class="c13"><span class="c6">We now describe our technique for enumerating ordered TDs. In future work we plan to compare our enumeration to a recent one </span></p><p class="c13"><span class="c6">Subroutine GenericTD(g, C) </span></p><p class="c13"><span class="c6">1: &#12296;S, U&#12297; &larr; ConstrainedSep(g, C) 2: if S = &perp; then 3: return the singleton decomposition of g 4: &#12296;t</span><span class="c2">0</span><span class="c6">,&chi;</span><span class="c2">0</span><span class="c6">&#12297; := RecursiveTD(g[S &cup; U],C &cup; S) 5: let V</span><span class="c17">1</span><span class="c15">,...,V</span><span class="c17">k </span><span class="c15">be the connected comps. of g &minus; (S &cup; U) </span><span class="c6">6: for i = 1,...,k do 7: &#12296;t</span><span class="c2">i</span><span class="c6">,&chi;</span><span class="c2">i</span><span class="c6">&#12297; := RecursiveTD(g[S &cup; V</span><span class="c2">i</span><span class="c6">],S) 8: let t be obtained from t</span><span class="c17">0</span><span class="c15">,t</span><span class="c17">1</span><span class="c15">,...,t</span><span class="c17">k </span><span class="c15">by connecting the root </span></p><p class="c13"><span class="c6">of t</span><span class="c2">0 </span><span class="c6">to the root of t</span><span class="c2">i </span><span class="c6">for all i &gt; 1 9: &chi; := &cup;</span><span class="c102">k</span><span class="c2">i=0</span><span class="c182">&chi;</span><span class="c2">i </span><span class="c6">10: return (t, &chi;) </span></p><p class="c13"><span class="c6">Figure 8: Tree decomposition via adhesion selection </span></p><p class="c30"><span class="c6">by Carmeli et al. [9]. We begin with a simple method for gener- ating a single TD. The two common heuristics to generating TDs are graph separation and elimination ordering [7]. We adopt the former, as it will later allow us to plug in an algorithm for enumer- ating separating sets of a graph. The algorithm calls a method for solving the side-constrained graph separation problem, or just the constrained separation problem for short, which is defined as fol- lows. The input consists of an undirected graph g and a set C of nodes of g. The goal is to find a separating set S of g, that is, a set S of nodes such that g &minus;S (obtained by removing from g every node of S) is disconnected. In addition, S is required to have the property that at least one connected component in g &minus; S is disjoint from C. Hence, S is required to separate C from some nonempty set of nodes. We call S a C-constrained separating set. We denote a call for a solver of this problem by ConstrainedSep(g, C). We later discuss an actual solver. For convenience, we assume that a solver returns the pair &#12296;S, U&#12297;, where U is the set of all nodes in the connected components of g &minus; S that intersect with C. </span></p><p class="c30"><span class="c6">The algorithm, called GenericTD(g, C), is depicted in Figure 8. It takes as input a graph g and a set C of nodes that is empty on the first call. The algorithm returns an ordered TD of g with the prop- erty that the root bag contains all nodes in C. So, the algorithm first calls ConstrainedSep(g, C). Let &#12296;S, U&#12297; be the result. It may be the case that the subroutine decides that no (good) C-constrained sep- arating set exists, and then the returned S is null (&perp;). In this case, the algorithm returns the TD that has only the nodes of g as the single bag. This case is handled in lines 1&ndash;3. Suppose now that the returned &#12296;S, U&#12297; is such that S is a C-constrained separating set. Denote by V</span><span class="c2">1</span><span class="c6">,...,V</span><span class="c2">k </span><span class="c6">the connected components of g &minus; (S &cup; U). The algorithm is then applied recursively to construct several or- dered TDs. First, an ordered tree decomposition &#12296;t</span><span class="c2">U</span><span class="c6">,&chi;</span><span class="c2">U</span><span class="c6">&#12297; of the induced subgraph of S &cup; U, which we denote by g[S &cup; U], such that the root contains C &cup; S (line 4). Second, for i = 1,...,k, an ordered tree decomposition &#12296;t</span><span class="c17">i</span><span class="c15">,&chi;</span><span class="c17">i</span><span class="c15">&#12297; of g[S &cup;V</span><span class="c17">i</span><span class="c15">] (the induced graph </span><span class="c6">of S &cup; V</span><span class="c17">i</span><span class="c15">) such that the root bag contains S (lines 5&ndash;7). Finally, </span><span class="c6">in lines 8&ndash;10 the algorithm combines all of the tree decompositions into a single tree decomposition (returned as the result), by con- necting the root of each &#12296;t</span><span class="c2">i</span><span class="c6">,&chi;</span><span class="c2">i</span><span class="c6">&#12297; to &#12296;t</span><span class="c2">U</span><span class="c6">,&chi;</span><span class="c2">U</span><span class="c6">&#12297; as a child of the root. </span></p><p class="c30"><span class="c6">The algorithm GenericTD(g, C) of Figure 8 generates a sin- gle ordered TD. We transform it into an enumeration algorithm by replacing line 1 with a procedure that efficiently enumerates C- constrained separating sets, and then executing the algorithm on every such set. A key feature of the enumeration is that it is done by increasing size of the separating sets, and hence, if we stop the </span></p><p class="c13"><span class="c21">288 </span></p><p class="c78"><span class="c6">enumeration of separating sets after k sets have been generated (to bound the number of the generated TDs), it is guaranteed that we have seen the k smallest C-constrained separating sets. </span></p><p class="c75"><span class="c6">We are then left with the task of enumerating the C-constrained separating sets by increasing size. For that, we have devised an algorithm that establishes the following complexity result. </span></p><p class="c149"><span class="c6">T</span><span class="c42">HEOREM </span><span class="c6">4.1. The S-constrained separating sets of a graph g can be generated by increasing size with polynomial delay. </span></p><p class="c19"><span class="c6">Our algorithm uses the well known technique for ranked enumer- ation with polynomial delay, namely the Lawler-Murty&rsquo;s proce- dure [17, 20]</span><span class="c102">3 </span><span class="c6">that reduces a general ranked (or sorted) enumera- tion problem to an optimization problem with simple constraints. Roughly speaking, to apply the procedure to a specific setting, one needs just to design an efficient solution to a constrained optimiza- tion problem. Due to lack of space, we omit the details and defer them to the long version of the paper. </span></p><p class="c152"><span class="c10">5. EXPERIMENTAL STUDY </span></p><p class="c65"><span class="c6">Our experimental study examines the performance benefits of our approach and algorithms. We compare our implemented al- gorithms to state-of-the-art solutions, and explore the effect of a number of key parameters and design choices. </span></p><p class="c183"><span class="c10">5.1 Algorithms and Systems Evaluated </span></p><p class="c127"><span class="c6">Our evaluation compares between implementations of several join algorithms, as listed below. All implementations were com- piled using g++ 4.9.3 with the -O3 flag. </span></p><p class="c32"><span class="c6">Our algorithms are CacheTrieJoin for CQ evaluation (Figure 5) and CacheTJCount for CQ counting (Figure 6). These implemen- tations extend the vanilla implementation of LFTJ [29], which we describe below. We refer to the implementations by the acronyms CTJ-E and CTJ-C, respectively. The caches are implemented using STL&rsquo;s unordered_map. The computation of a TD is as described in Section 4. If no bound is mentioned for the cache size, then no eviction takes place (and every partial assignment is cached). We compare against the following alternatives. </span></p><p class="c171"><span class="c6">LFTJ: We use a vanilla implementation of LFTJ [29]. Our imple- mentation uses C++ STL map as the underlying Trie data structure. Notably, this implementation adheres to the complexity require- ments of LFTJ. </span></p><p class="c31"><span class="c6">YTD: This algorithm combines Yannakakis&rsquo;s acyclic join algo- rithm [30] with a TD, as described by Gottlob et al. [14]. The implementation is based on DunceCap [24]. For each intermediate join (bag) a worst-case optimal algorithm is used. The complex- ity requirement for the indices seekLowerBound is provided by a binary search, enabled through the use of the cascading vectors for the Trie. We use the query compiler from EmptyHeaded [1] (which uses a YTD-like algorithm) to generate the TD and variable order- ing. For queries with only two bags we use a regular join since, in this case, the Yannakakis reduction stage generates an unneces- sary overhead. Moreover, for count queries whose TDs yield more than two bags, we save the relevant result for the matching join at- tributes (rather than storing full intermediate results). Notably, we have experimented with alternative YTD implementations, but they all proved inferior to the one described above. </span></p><p class="c190"><span class="c6">YTD-Par: EmptyHeaded [1] is a state-of-the-art graph query en- gine that operates as a parallel implementation of DunceCap [24], </span></p><p class="c135"><span class="c2">3</span><span class="c15">Lawler-Murty&rsquo;s procedure is a generalization of Yen&rsquo;s algo- rithm [31] for finding the k shortest simple paths of a graph. </span></p><p class="c13 c161"><span class="c6">Dataset #Nodes #Edges Category </span></p><p class="c66"><span class="c6">ca-GrQc 5,242 14,496 Collaboration net p2p-Gnutella04 10,876 39,994 P2P net ego-Facebook 4,039 88,234 Social net wiki-Vote 7,115 103,689 Social net ego-Twitter 81,306 1,768,149 Social net imdb-Actresses 2,714,695 4,700,000 Movies imdb-Actors 3,539,013 7,000,000 Movies </span></p><p class="c68"><span class="c6">Table 1: Dataset (SNAP) statistics </span></p><p class="c122"><span class="c6">with optimizations for graph databases. We view it as a query en- gine rather than a pure algorithm, since the implementation is tied to the hardware: it parallelizes the execution through the Single- Instruction Multiple-Data (SIMD) model. Parallel operations are executed using the vector unit available on modern Intel proces- sor cores. Specifically, each core on our test platform (Intel Xeon E5-2630 v3) includes a 256-bit vector unit that executes 8 integer (4-byte) operations in parallel. </span></p><p class="c117 c126"><span class="c6">In addition to pure algorithms, we also experiment with full sys- tems. Pure algorithm implementations avoid the overhead associ- ated with a full DBMS. We make this comparison simply to provide a context for the recorded running times. </span></p><p class="c91"><span class="c6">LB-LFTJ: LogicBlox (LB) 4.3.18 [3]: A commercial DBMS con- figured to use LFTJ as an its join engine. </span></p><p class="c91"><span class="c6">LB-FAQ: LogicBlox (LB) 4.3.18 configured to use InsideOut [16] as its join engine. </span></p><p class="c83"><span class="c6">PGSQL: PostgreSQL [27] is an open-source relational DBMS (ver- sion 9.3.4). For query evaluation (as opposed to count), we use the curser API of PGSQL to avoid storage of join results in memory. </span></p><p class="c108"><span class="c6">Other popular DBMSs and graph engines were compared to the above systems in a previous study [22], and were shown to be in- ferior in performance. Hence, we omit the other DBMSs from our experimental study. We further emphasize that our experiments ex- plicitly restricted all algorithms and systems to utilize only a single core on the test machine, which does not affect YTD-Par SIMD parallelization. </span><span class="c10">5.2 Methodology </span></p><p class="c126 c179"><span class="c6">The setup and methodology we adopted in our experimental study are as follows. </span></p><p class="c117"><span class="c6">Workloads. In par with other studies on join algorithms our eval- uation is based, for the most part, on datasets from the SNAP col- lection [18], similarly to Nguyen et al. [22]. The datasets consist of wiki-Vote, p2p-Gnutella04, ca-GrQc, ego-Facebook and ego- Twitter. Table 1 gives some basic statistics on the datasets. As the distribution of values in SNAP dataset is highly skewed, we also use IMDB to explore the effect of datasets that are less skewed and whose data skew is not uniform across attributes. To this end, we partition IMDB&rsquo;s cast_info table into a male_cast and a fe- male_cast tables, each with attributes (person_id and movie_id). We exclude the TPC-C and TPC-H benchmarks as the join queries in these benchmarks are small. </span></p><p class="c77"><span class="c6">Queries. Our datasets can be viewed as graphs, and so, we ex- periment using 3 types of CQs (again, consistently with Nguyen et al. [22]). The first type, denoted n-path for n = 3,...,7, finds all paths of length n. For example, the 4-path CQ is </span></p><p class="c63"><span class="c6">E(x, y),E(y, z),E(z,w). </span></p><p class="c37"><span class="c21">289 </span></p><p class="c109"><span class="c18">c</span><span class="c43">a-GrQc</span><span class="c18">e</span><span class="c43">go-facebook</span><span class="c18">e</span><span class="c43">go-twitter</span><span class="c18">p</span><span class="c43">2p-Gnutella04</span><span class="c18">w</span><span class="c43">iki-Vote </span><span class="c18">5-rand(0.6) </span></p><p class="c170"><span class="c6">Figure 9: The speedups obtained with CTJ-E over LFTJ and YTD for full query evaluation. Bars that represent executions that timed out are marked as gray. </span></p><p class="c150"><span class="c6">The second type is n-cycle, where n = 3,...,6, and the query finds cycles of length 3 to 6. For example, the 4-cycle CQ is </span></p><p class="c89"><span class="c6">E(x, y),E(y, z),E(z,w),E(w, x) . </span></p><p class="c123"><span class="c6">The third type consists of random CQs. We generate such CQs by forming a graph pattern using the Erd&ouml;s-Reyni generator. The generator takes n nodes and adds an edge between every two nodes, independently, with a specified probability p. The graph is undi- rected and without self loops. We use only connected graphs with n = 5 and n = 6, and with p = 0.4 and p = 0.6. Random graph queries are denoted as n-rand(p). For each set of parameters we generate four different graphs. We do not examine clique queries as these cannot be decomposed, and hence our algorithms are the same as LFTJ in this case. </span></p><p class="c140"><span class="c6">Hardware and system setup. Our experimental platform uses Su- permicro 2028R-E1CR24N servers. Each server is configured with two Intel Xeon E5-2630 v3 processors running at 2.4 GHz, 64GB of DDR3 DRAM, and is running a stock Ubuntu 14.0.4 Linux. </span></p><p class="c120"><span class="c6">Testing protocol. Each experiment was run three times, and the average runtime is reported. We set an execution timeout of 10 hours. Executions that timed out are highlighted. </span><span class="c10">5.3 Experimental Results </span></p><p class="c142 c169"><span class="c6">We start by experimenting with unlimited caches on query eval- uation of CQs. Next, we compare different cache sizes, caching policies and other cache attributes. </span></p><p class="c163"><span class="c6">Query evaluation produces all the tuples in the result of the query. We focus our exploration of query evaluation on computing the ma- terialized result rather than storing it. With the help of the related parties, the algorithms and systems were configured to ignore the final result and not store it. The only exception is YTD-Par, for which we could not disable the materialization of the final result. It is therefore not shown in our examination of query evaluation. </span></p><p class="c153"><span class="c6">Figure 9 presents the results for running query evaluation of 5- path and 5-cycles queries. The figure shows that for 5-path queries, CTJ-E outperforms YTD by 4&times; and LFTJ by over 9&times;. The per- formance gap is attributed to CTJ-E&rsquo;s caching, which captures fre- quently used intermediate results. CTJ-E&rsquo;s caching eliminates re- dundant scans of the Trie structure that occur in LFTJ. CTJ-E also outperforms YTD by up to 4.6&times; (3.2&times; on average), because the computation of YTD becomes memory bound in the final join stages due to the memory complexity of the Yannakakis joins. </span></p><p class="c158"><span class="c6">For 5-cycle queries, Figure 9 shows that CTJ-E is faster than LFTJ by an average of 8&times; for ca-GrQc, twitter and wiki datasets. </span></p><p class="c79 c166"><span class="c18">c</span><span class="c43">a-GrQc</span><span class="c18">e</span><span class="c43">go-facebook</span><span class="c18">e</span><span class="c43">go-twitter</span><span class="c18">p</span><span class="c43">2p-Gnutella04</span><span class="c18">w</span><span class="c43">iki-Vote </span><span class="c18">5-path </span></p><p class="c56"><span class="c18">c</span><span class="c43">a-GrQc</span><span class="c18">e</span><span class="c43">go-facebook</span><span class="c18">e</span><span class="c43">go-twitter</span><span class="c18">p</span><span class="c43">2p-Gnutella04</span><span class="c18">w</span><span class="c43">iki-Vote </span><span class="c18">5-cycle </span></p><p class="c56"><span class="c18">c</span><span class="c43">a-GrQc</span><span class="c18">e</span><span class="c43">go-facebook</span><span class="c18">e</span><span class="c43">go-twitter</span><span class="c18">p</span><span class="c43">2p-Gnutella04</span><span class="c18">w</span><span class="c43">iki-Vote </span><span class="c18">5-cycle </span></p><p class="c12"><span class="c18">c</span><span class="c43">a-GrQc</span><span class="c18">e</span><span class="c43">go-facebook</span><span class="c18">e</span><span class="c43">go-twitter</span><span class="c18">p</span><span class="c43">2p-Gnutella04</span><span class="c18">w</span><span class="c43">iki-Vote </span><span class="c18">5-rand(0.4) </span></p><p class="c12"><span class="c18">c</span><span class="c43">a-GrQc</span><span class="c18">e</span><span class="c43">go-facebook</span><span class="c18">e</span><span class="c43">go-twitter</span><span class="c18">p</span><span class="c43">2p-Gnutella04</span><span class="c18">w</span><span class="c43">iki-Vote </span><span class="c18">5-rand(0.4) </span></p><p class="c12"><span class="c18">c</span><span class="c43">a-GrQc</span><span class="c18">e</span><span class="c43">go-facebook</span><span class="c18">e</span><span class="c43">go-twitter</span><span class="c18">p</span><span class="c43">2p-Gnutella04</span><span class="c18">w</span><span class="c43">iki-Vote </span><span class="c18">5-rand(0.4) </span></p><p class="c30 c51"><span class="c0">4-cycle 558 1.1</span><span class="c1">&times; </span><span class="c0">1.9</span><span class="c1">&times; </span><span class="c0">9</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span><span class="c0">4409</span><span class="c1">&times; </span><span class="c0">5-cycle 4125 41</span><span class="c1">&times; </span><span class="c0">8</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">19</span><span class="c1">&times; </span><span class="c0">11526</span><span class="c1">&times; </span><span class="c0">6-cycle 84248 9</span><span class="c1">&times; </span><span class="c0">14</span><span class="c1">&times; </span><span class="c0">40</span><span class="c1">&times; </span><span class="c0">37</span><span class="c1">&times; </span><span class="c0">564</span><span class="c1">&times; </span></p><p class="c30 c51"><span class="c0">4-cycle 558 1.1</span><span class="c1">&times; </span><span class="c0">1.9</span><span class="c1">&times; </span><span class="c0">9</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span><span class="c0">4409</span><span class="c1">&times; </span><span class="c0">5-cycle 4125 41</span><span class="c1">&times; </span><span class="c0">8</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">19</span><span class="c1">&times; </span><span class="c0">11526</span><span class="c1">&times; </span><span class="c0">6-cycle 84248 9</span><span class="c1">&times; </span><span class="c0">14</span><span class="c1">&times; </span><span class="c0">40</span><span class="c1">&times; </span><span class="c0">37</span><span class="c1">&times; </span><span class="c0">564</span><span class="c1">&times; </span></p><p class="c30 c51"><span class="c0">4-cycle 558 1.1</span><span class="c1">&times; </span><span class="c0">1.9</span><span class="c1">&times; </span><span class="c0">9</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span><span class="c0">4409</span><span class="c1">&times; </span><span class="c0">5-cycle 4125 41</span><span class="c1">&times; </span><span class="c0">8</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">19</span><span class="c1">&times; </span><span class="c0">11526</span><span class="c1">&times; </span><span class="c0">6-cycle 84248 9</span><span class="c1">&times; </span><span class="c0">14</span><span class="c1">&times; </span><span class="c0">40</span><span class="c1">&times; </span><span class="c0">37</span><span class="c1">&times; </span><span class="c0">564</span><span class="c1">&times; </span></p><p class="c30 c51"><span class="c0">4-cycle 558 1.1</span><span class="c1">&times; </span><span class="c0">1.9</span><span class="c1">&times; </span><span class="c0">9</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span><span class="c0">4409</span><span class="c1">&times; </span><span class="c0">5-cycle 4125 41</span><span class="c1">&times; </span><span class="c0">8</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">19</span><span class="c1">&times; </span><span class="c0">11526</span><span class="c1">&times; </span><span class="c0">6-cycle 84248 9</span><span class="c1">&times; </span><span class="c0">14</span><span class="c1">&times; </span><span class="c0">40</span><span class="c1">&times; </span><span class="c0">37</span><span class="c1">&times; </span><span class="c0">564</span><span class="c1">&times; </span></p><p class="c50 c86"><span class="c0">Query Algorithms Systems </span></p><p class="c13 c50"><span class="c0">Query Algorithms Systems </span></p><p class="c13 c50"><span class="c0">Query Algorithms Systems </span></p><p class="c13 c50"><span class="c0">Query Algorithms Systems </span></p><p class="c82"><span class="c0">CTJ-E YTD LFTJ LB-FAQ LB-LFTJ PGSQL </span></p><p class="c13 c159"><span class="c0">CTJ-E YTD LFTJ LB-FAQ LB-LFTJ PGSQL </span></p><p class="c13 c159"><span class="c0">CTJ-E YTD LFTJ LB-FAQ LB-LFTJ PGSQL </span></p><p class="c13 c159"><span class="c0">CTJ-E YTD LFTJ LB-FAQ LB-LFTJ PGSQL </span></p><p class="c46 c134"><span class="c0">3-path 72 1.5</span><span class="c1">&times; </span><span class="c0">3</span><span class="c1">&times; </span><span class="c0">14</span><span class="c1">&times; </span><span class="c0">17</span><span class="c1">&times; </span><span class="c0">18355</span><span class="c1">&times; </span><span class="c0">4-path 791 4</span><span class="c1">&times; </span><span class="c0">9</span><span class="c1">&times; </span><span class="c0">23</span><span class="c1">&times; </span><span class="c0">21</span><span class="c1">&times; </span><span class="c0">59157</span><span class="c1">&times; </span><span class="c0">5-path 29458 4</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">26</span><span class="c1">&times; </span><span class="c0">24</span><span class="c1">&times; </span><span class="c0">t/o 6-path 1.33e+06 4</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">26</span><span class="c1">&times; </span><span class="c0">t/o t/o 7-path t/o t/o t/o t/o t/o t/o </span></p><p class="c30 c46"><span class="c0">3-path 72 1.5</span><span class="c1">&times; </span><span class="c0">3</span><span class="c1">&times; </span><span class="c0">14</span><span class="c1">&times; </span><span class="c0">17</span><span class="c1">&times; </span><span class="c0">18355</span><span class="c1">&times; </span><span class="c0">4-path 791 4</span><span class="c1">&times; </span><span class="c0">9</span><span class="c1">&times; </span><span class="c0">23</span><span class="c1">&times; </span><span class="c0">21</span><span class="c1">&times; </span><span class="c0">59157</span><span class="c1">&times; </span><span class="c0">5-path 29458 4</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">26</span><span class="c1">&times; </span><span class="c0">24</span><span class="c1">&times; </span><span class="c0">t/o 6-path 1.33e+06 4</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">26</span><span class="c1">&times; </span><span class="c0">t/o t/o 7-path t/o t/o t/o t/o t/o t/o </span></p><p class="c30 c46"><span class="c0">3-path 72 1.5</span><span class="c1">&times; </span><span class="c0">3</span><span class="c1">&times; </span><span class="c0">14</span><span class="c1">&times; </span><span class="c0">17</span><span class="c1">&times; </span><span class="c0">18355</span><span class="c1">&times; </span><span class="c0">4-path 791 4</span><span class="c1">&times; </span><span class="c0">9</span><span class="c1">&times; </span><span class="c0">23</span><span class="c1">&times; </span><span class="c0">21</span><span class="c1">&times; </span><span class="c0">59157</span><span class="c1">&times; </span><span class="c0">5-path 29458 4</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">26</span><span class="c1">&times; </span><span class="c0">24</span><span class="c1">&times; </span><span class="c0">t/o 6-path 1.33e+06 4</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">26</span><span class="c1">&times; </span><span class="c0">t/o t/o 7-path t/o t/o t/o t/o t/o t/o </span></p><p class="c30 c46"><span class="c0">3-path 72 1.5</span><span class="c1">&times; </span><span class="c0">3</span><span class="c1">&times; </span><span class="c0">14</span><span class="c1">&times; </span><span class="c0">17</span><span class="c1">&times; </span><span class="c0">18355</span><span class="c1">&times; </span><span class="c0">4-path 791 4</span><span class="c1">&times; </span><span class="c0">9</span><span class="c1">&times; </span><span class="c0">23</span><span class="c1">&times; </span><span class="c0">21</span><span class="c1">&times; </span><span class="c0">59157</span><span class="c1">&times; </span><span class="c0">5-path 29458 4</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">26</span><span class="c1">&times; </span><span class="c0">24</span><span class="c1">&times; </span><span class="c0">t/o 6-path 1.33e+06 4</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">26</span><span class="c1">&times; </span><span class="c0">t/o t/o 7-path t/o t/o t/o t/o t/o t/o </span></p><p class="c23"><span class="c0">4-cycle 2192 0.7</span><span class="c1">&times; </span><span class="c0">1.7</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span><span class="c0">4</span><span class="c1">&times; </span><span class="c0">2312</span><span class="c1">&times; </span><span class="c0">5-cycle 27855 11</span><span class="c1">&times; </span><span class="c0">6</span><span class="c1">&times; </span><span class="c0">3</span><span class="c1">&times; </span><span class="c0">14</span><span class="c1">&times; </span><span class="c0">t/o 6-cycle 415783 4</span><span class="c1">&times; </span><span class="c0">17</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span><span class="c0">44</span><span class="c1">&times; </span><span class="c0">t/o </span></p><p class="c40"><span class="c6">Figure 10: CTJ-E runtimes (in msecs) for {3&ndash;7}-path and {4&ndash; 6}-cycle queries and relative runtimes for compared solutions (i.e., m&times; means m times slower than CTJ-E), for ca-GrQc (top) and Wiki (bottom) datasets. Timeout (t/o) means over 10 hours. YTD-Par is omitted from the comparison as it always stores the materialized result. </span></p><p class="c34"><span class="c6">For the facebook and p2p-Gnutella04 datasets, however, CTJ-E ex- periences a small slowdown. Finally, CTJ-E outperforms YTD by 26&times; on average. The reason is that YTD&rsquo;s Yannakakis and the worst-case optimal join algorithm used by YTD, favor the opposite attributes order, which dramatically affects its performance. </span></p><p class="c28"><span class="c6">CTJ-E also delivers performance benefits for sparse random pat- tern queries. Figure 9 shows the results for representative graphs (which are consistent with the results for the other graphs). Specif- ically, for 5-rand(0.4) queries, CTJ-E outperforms LFTJ by 5&times; on average. CTJ-E is also consistently 3&ndash;4&times; faster than YTD, with the exception of p2p-Gnutella04 for which the results are comparable. These trends are consistent for denser 5-rand(0.6) random graphs. Here too, the results demonstrate the effectiveness of CTJ-E, whose runtime is, on average, 10&times; faster than LFTJ and 7&times; than YTD (CTJ-E and LFTJ runtimes are comparable for p2p-Gnutella04). </span></p><p class="c181"><span class="c6">Figure 10 presents the results of query evaluation for {3&ndash;7}-path and {4&ndash;6}-cycle queries over the ca-GrQc (top) and Wiki (bottom) datasets for different algorithms and systems. For brevity, we show the results for only two datasets: ca-GrQc and Wiki. These results are consistent with the results obtained for the other SNAP datasets. The figure shows that the performance benefits of CTJ-E over LFTJ increase with the size of the query. CTJ-E is 10&times; faster than LFTJ for 7-path queries and 14&times; for 6-cycle queries. Compared to YTD, CTJ-E speedup is 4&times; for 7-path queries and 4&ndash;9&times; for 6- cycle queries. The only case where CTJ-E is slower than another algorithm is the small 4-cycle query, for which YTD is faster by 30% on the Wiki dataset. Importantly, these results are consistent across the other datasets, excluding p2p-Gnutella04 for which the algorithms are comparable. </span></p><p class="c28"><span class="c6">Figure 10 also compares the performance of our algorithms to that of full DBMSs (PGSQL, LB-LFTJ, and LB-FAQ). We observe that the speedups are even larger (as expected, due to system over- head). Notably, the ratio between the performance of LFTJ and LB-LFTJ is more or less constant, showing that the system over- head here accounts for around 2&times; slowdown. </span></p><p class="c13 c36"><span class="c18">0</span><span class="c43">1020304050 </span><span class="c18">s</span><span class="c43">peedup </span></p><p class="c13 c105"><span class="c38">Over LFTJ </span></p><p class="c132"><span class="c38">Over YTD </span></p><p class="c11"><span class="c0">Query Algorithms Systems </span></p><p class="c8"><span class="c0">CTJ-E YTD LFTJ LB-FAQ LB-LFTJ PGSQL </span></p><p class="c13 c49"><span class="c0">CTJ-E YTD LFTJ LB-FAQ LB-LFTJ PGSQL </span></p><p class="c13 c49"><span class="c0">CTJ-E YTD LFTJ LB-FAQ LB-LFTJ PGSQL </span></p><p class="c7"><span class="c0">3-path 23 2</span><span class="c1">&times; </span><span class="c0">1.3</span><span class="c1">&times; </span><span class="c0">46</span><span class="c1">&times; </span><span class="c0">34</span><span class="c1">&times; </span><span class="c0">10116</span><span class="c1">&times; </span><span class="c0">4-path 133 4</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span><span class="c0">17</span><span class="c1">&times; </span><span class="c0">26</span><span class="c1">&times; </span><span class="c0">27679</span><span class="c1">&times; </span><span class="c0">5-path 2222 4</span><span class="c1">&times; </span><span class="c0">8</span><span class="c1">&times; </span><span class="c0">23</span><span class="c1">&times; </span><span class="c0">19</span><span class="c1">&times; </span><span class="c0">t/o 6-path 78528 4</span><span class="c1">&times; </span><span class="c0">10</span><span class="c1">&times; </span><span class="c0">23</span><span class="c1">&times; </span><span class="c0">23</span><span class="c1">&times; </span><span class="c0">t/o 7-path 3265542 4</span><span class="c1">&times; </span><span class="c0">10</span><span class="c1">&times; </span><span class="c0">t/o t/o t/o </span></p><p class="c7"><span class="c0">4-cycle 558 1.1</span><span class="c1">&times; </span><span class="c0">1.9</span><span class="c1">&times; </span><span class="c0">9</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span><span class="c0">4409</span><span class="c1">&times; </span><span class="c0">5-cycle 4125 41</span><span class="c1">&times; </span><span class="c0">8</span><span class="c1">&times; </span><span class="c0">11</span><span class="c1">&times; </span><span class="c0">19</span><span class="c1">&times; </span><span class="c0">11526</span><span class="c1">&times; </span><span class="c0">6-cycle 84248 9</span><span class="c1">&times; </span><span class="c0">14</span><span class="c1">&times; </span><span class="c0">40</span><span class="c1">&times; </span><span class="c0">37</span><span class="c1">&times; </span><span class="c0">564</span><span class="c1">&times; </span></p><p class="c100"><span class="c21">290 </span></p><p class="c110"><span class="c6">5-path 5-cycle dataset 25% 10% 1% 25% 10% 1% </span></p><p class="c116"><span class="c6">ca-GrQc 1.7&times; 3&times; 18&times; 1.9&times; 3&times; 5&times; p2p-Gnutella04 4&times; 5&times; 6&times; 1.3&times; 1.3&times; 1.3&times; ego-twitter 6&times; 9&times; 18&times; 2&times; 2&times; 2&times; wiki-Vote 1.2&times; 1.3&times; 3&times; 4&times; 4&times; 4&times; </span></p><p class="c113"><span class="c6">Table 2: Slowdown due to cache sizes with LRU, over unlimited cache size for 5-path and 5-cycle query evaluation </span></p><p class="c99 c178"><span class="c6">On average, CTJ-E is over 20&times; faster than LB-FAQ and LB- LFTJ for all path queries, and 3-44&times; faster for all cycle queries. An even more extreme speedup is evident when comparing to PGSQL, where CTJ-E is consistently 3&ndash;5 orders of magnitude faster. </span></p><p class="c177"><span class="c6">To conclude, we have shown that CTJ-E is substantially faster than the alternatives. Furthermore, the performance benefits of CTJ-E increase with the size of the query. </span></p><p class="c174"><span class="c143">5.3.1 Cache Parameters </span></p><p class="c65"><span class="c6">Tuning the parameters of the CTJ-E cache (e.g., cache size, evic- tion policy, cache partitioning) do not affect the correctness of the CTJ-E. Instead, these parameters only affect the caching efficiency of CTJ-E and, by proxy, the performance of the algorithm. The caching of partial results in CTJ-E thus presents a tradeoff between memory consumption and performance. Interestingly, LFTJ and YTD represent the two extremes of this classic tradeoff. On one hand, LFTJ caches no partial or intermediate results but rather re- peatedly scans the Trie to regenerate partial results. On the other hand, YTD must maintain all intermediate results generated by the individual joins on each bag. As a result, its memory consumption is even higher than CTJ-E with an unbounded cache. </span></p><p class="c75"><span class="c6">In this section we explore the memory-performance tradeoff by examining the impact of the different cache parameters on the per- formance of LFTJ. Unless stated otherwise, the memory allocated for the cache is evenly partitioned across the individual caches. </span></p><p class="c31"><span class="c6">Cache size. The size of the CTJ-E cache is, naturally, the pri- mary parameter that affects caching performance. We explore this parameter&rsquo;s impact on performance by bounding the cache size to 1%, 10%, and 25% of the size needed to store all partial results. For example, a 5-cycle query running on the twitter dataset re- quires 476MB to cache all partial results. We thus examine CTJ-E performance when bounding the total cache size to 4.76MB (1%), 47.6MB (10%) and 119MB (25%). In this experiment, all bounded caches use the least-recently-used (LRU) eviction policy. </span></p><p class="c32"><span class="c6">Table 2 presents the performance of CTJ-E with bounded cache size for representative queries and datasets. The performance is presented as the slowdown over a run with an unbounded cache. </span></p><p class="c32"><span class="c6">As expected, the table shows that performance degrades when reducing the cache size. For example, the performance of a 5- path query on the ca-GrQc dataset (0.4MB unbounded cache) slows </span></p><p class="c20"><span class="c6">dataset 5-path 5-cycle </span></p><p class="c106"><span class="c6">ego-twitter 1.8&times; 11&times; ca-GrQc 1.3&times; 2&times; p2p-Gnutella04 1.3&times; -1.3&times; wiki-Vote -1.1&times; 4&times; </span></p><p class="c113"><span class="c6">Table 3: Speedup for LRU over RANDOM on cache bounded to 10% for 5-path and 5-cycle query evaluation </span></p><p class="c13 c111"><span class="c6">5-path 5-cycle dataset 1x 2x 5x 1x 2x 5x </span></p><p class="c45"><span class="c6">ca-GrQc 11.6&times; 3.4&times; 3&times; 10.4&times; 9.2&times; 8.3&times; wiki-Vote 2.3&times; 1.5&times; 1.6&times; 5.5&times; 5&times; 5&times; p2p-Gnutella04 5&times; 5&times; 5&times; 1.3&times; 1.3&times; 1.3&times; ego-twitter 14&times; 8&times; 5.5&times; 2.4&times; 2.4&times; 2.4&times; </span></p><p class="c124 c157"><span class="c6">Table 4: Slowdown due to different cache partitioning with LRU bounded to 5% of the full cache capacity, over unlimited cache size for 5-path and 5-cycle query evaluation) </span></p><p class="c84"><span class="c6">down by 1.7&times; when bounding the cache to 25% of full capacity, by 2.5&times; with 10% of full capacity, and by 18.4&times; with 1% of full ca- pacity. The performance degradation is less acute in other cases. A 5-cycle query running on ca-GrQc (8.8MB unbounded cache) slows down by 1.9&times;, 3&times;, and 5&times; for caches bounded at 25%, 10%, and 1%, respectively, of full capacity. In other cases, the performance impact of a bounded cache is fairly constant regard- less of the bound. For example, running a 5-cycle query on the twitter dataset (476MB unbounded cache) results in a slowdown of 2.4&ndash;2.5&times; for caches bounded at 1&ndash;25% of full capacity. </span></p><p class="c9"><span class="c6">In summary, Table 2 shows that bounding the cache size to 25% of its full capacity only yields an average slowdown of &sim;3&times; over an unbounded CTJ-E run. Bounding the cache size even further to 10% of an unbounded cache results in an average slowdown of &sim;2.7&times; over the performance obtained with an unbounded cache. Notably, the performance obtained with a 10% bound is still supe- rior to LFTJ, as well as to YTD 5-cycle queries, and comparable to YTD for 5-path queries. </span></p><p class="c117"><span class="c6">Eviction policy. We now turn to examine the impact of the cache eviction policy on overall CTJ-E performance. Specifically, we compare the performance obtained with both LRU and Random eviction policies (the Random policy, as its name suggests, ran- domly selects a cache entry to evict with uniform distribution). We note that we have experimented with other eviction and insertion policies, some based on statistical analysis of the datasets, but none provided much better results than the classic LRU policy. </span></p><p class="c9"><span class="c6">Table 3 presents the LRU performance as speedup over Random. For brevity, we only show results for a 10% cache bound. The table shows that for path queries LRU outperforms Random by 1.4&times; on average. This is because most cached values will be effective in path queries on the datasets we tested, and due to the overhead of the LRU bookkeeping. On cycle queries, LRU outperforms Ran- dom by 4.5&times; on average. We therefore choose to use the LRU eviction policy with bounded CTJ-E caches. </span></p><p class="c44"><span class="c6">Cache partitioning. The final parameter we explore is the allo- cation of memory among caches. We test the LRU performance speedup for bounded cache size, which is partitioned between the caches in three different configurations. The first configuration (1&times;) divides the allocated memory equally between the caches. The second (2&times;), divides the allocated memory between the caches, such that each level is bounded to 2&times; of the size of the level above it. Here, a cache level means the position in the pre-order of the TD. As an example, for the 5-cycle query on the twitter dataset, we allocate a total of 476MB. In the second configuration, the first cache will be bounded to 1/3 (158MB) and the second cache will be bounded to 2/3 (317MB). The last configuration (5&times;) is similar to the previous, but with a scale of 5&times; instead of 2&times;. </span></p><p class="c9"><span class="c6">Table 4 shows the results for CTJ-E with LRU eviction, bounded to 5%, over the different partitioning configurations. The results show that for small caches of equal size, the caches can become </span></p><p class="c37"><span class="c21">291 </span></p><p class="c13"><span class="c18">10</span><span class="c43">Timeout </span><span class="c29">7 </span><span class="c18">10</span><span class="c29">6 </span></p><p class="c13"><span class="c18">10</span><span class="c29">5 </span></p><p class="c13"><span class="c18">10</span><span class="c29">4 </span></p><p class="c13"><span class="c18">10</span><span class="c29">3 </span></p><p class="c13"><span class="c18">10</span><span class="c29">2 </span></p><p class="c13"><span class="c18">10</span><span class="c29">1 </span></p><p class="c13"><span class="c43">10</span><span class="c29">0 </span></p><p class="c13"><span class="c38">W</span><span class="c25">iki-Vote </span></p><p class="c13"><span class="c38">p</span><span class="c25">2p</span><span class="c18">5-path </span><span class="c25">-Gnutella04 </span></p><p class="c13"><span class="c38">e</span><span class="c25">go-facebook </span></p><p class="c13"><span class="c38">c</span><span class="c25">a-GrQc </span></p><p class="c13"><span class="c38">e</span><span class="c25">go-twitter </span></p><p class="c13"><span class="c38">W</span><span class="c25">iki-Vote </span></p><p class="c13"><span class="c38">p</span><span class="c25">2p</span><span class="c18">5-cycle </span><span class="c25">-Gnutella04 </span></p><p class="c13"><span class="c38">e</span><span class="c25">go-facebook </span></p><p class="c13"><span class="c38">c</span><span class="c25">a-GrQc </span></p><p class="c13"><span class="c38">e</span><span class="c25">go-twitter </span></p><p class="c13"><span class="c38">W</span><span class="c25">iki-Vote </span></p><p class="c13"><span class="c38">p</span><span class="c25">2p</span><span class="c18">5-rand(0.4) </span><span class="c25">-Gnutella04 </span><span class="c38">e</span><span class="c25">go-facebook </span></p><p class="c13"><span class="c38">c</span><span class="c25">a-GrQc </span></p><p class="c13"><span class="c38">e</span><span class="c25">go-twitter </span></p><p class="c13"><span class="c38">W</span><span class="c25">iki-Vote </span></p><p class="c13"><span class="c38">p</span><span class="c25">2p</span><span class="c18">5-rand(0.6) </span></p><p class="c13"><span class="c25">-Gnutella04 </span></p><p class="c13"><span class="c38">e</span><span class="c25">go-facebook </span></p><p class="c13"><span class="c38">c</span><span class="c25">a-GrQc </span><span class="c38">e</span><span class="c25">go-twitter </span><span class="c6">Figure 11: Runtimes for count queries using the different algorithms. Gray bars represent executions that timed out. </span></p><p class="c30"><span class="c6">ineffective due to thrashing. The results also show that a differ- ent cache partitioning that allocates more memory to greater level caches, such as 2&times; and 5&times;, can improve the performance by 10%- 3&times;. With these configurations CTJ-E outperforms LFTJ even with very small memory allocation. The reason we observed is that a cache in a greater level is accessed more often, and therefore ac- counts for a larger portion of the recurring joins. Note that differ- ent cache partitions do not affect queries on p2p-Gnutella04, since CTJ-E caches are less effective for this dataset. This crude alloca- tion depicts the importance of dynamically allocating the memory between the caches, which we plan to pursue in future research. </span></p><p class="c30"><span class="c6">Summary. We conclude that bounded caches enable CTJ-E to benefit from both worlds. On one hand, it delivers substantial speed- ups over LFTJ while preserving the bounded memory footprint property. On the other hand, it can execute in settings where tra- ditional join algorithms, which store all intermediate results, either cannot execute or suffer substantial slowdowns due to disk I/O. </span></p><p class="c13"><span class="c10">5.4 Results on Count Queries </span></p><p class="c30"><span class="c6">We now examine the performance benefits of CTJ-C for count queries. Figure 11 presents the runtime of 5-path, 5-cycle, and 5- rand queries on different datasets. It shows that CTJ-C executes the queries substantially faster than the alternatives for all datasets except p2p-Gnutella04. CTJ-C is faster than LFTJ by over an or- der of magnitude. When compared to YTD, CTJ-C is typically 2&ndash; 5&times; faster, with the exception of 5-rand(0.4) over p2p-Gnutella04, where CTJ-C results in a marginal slowdown. </span></p><p class="c30"><span class="c6">The distinction between the datasets is rooted in their value dis- tribution. Skewed value distributions are more amenable to caching. Specifically, when some values appear frequently in multiple tu- ples, caching partial walks through the LFTJ Trie will likely pre- vent redundant walks over the Trie. For example, the ego-Twitter dataset exhibits such skew. For this dataset, CTJ-C is consistently 2&ndash;5&times; faster than YTD and orders of magnitude faster than LFTJ. </span></p><p class="c30"><span class="c6">On the other hand, when the distribution of values across the dataset is not skewed, as is the case with p2p-Gnutella04, caching partial values have little benefit. Indeed, for this dataset the per- formance benefits of CTJ-C are moderate (for 5-rand queries, both YTD and LFTJ even marginally outperform CTJ-C). The results demonstrate the effectiveness of CTJ-C when running on datasets whose value distribution is skewed. </span></p><p class="c30"><span class="c6">Figure 11 compares the algorithms when running two represen- tative 5-rand random graph queries. Comparing CTJ-C with the LFTJ algorithm, we see that CTJ-C is consistently faster by or- ders of magnitude. The only exception is the p2p-Gnutella04 that, as discussed above, exhibits a balanced value distribution. When </span></p><p class="c30"><span class="c6">comparing CTJ-C to YTD, we observe an average speedup of &sim;8&times;. Again, the only exception is the p2p-Gnutella04 dataset. Notably, the results for 6-rand (not shown) are consistent with 5-rand. </span></p><p class="c30"><span class="c6">The performance benefits of CTJ-C are consistent across differ- ent query sizes. Figure 12 presents the runtimes for {3&ndash;7}-path and {3&ndash;6}-cycle queries. For brevity, we show the results for only two of the datasets. (The figure also shows the performance of DBMSs, which is discussed below.) The figure shows that for path queries CTJ-C is consistently 3&times; faster than YTD. Moreover, CTJ-C is or- ders of magnitude faster than LFTJ, and the performance benefits only increase with the size of the query. </span></p><p class="c176"><span class="c6">For {3&ndash;7}-cycle queries, Figure 12 shows that CTJ-C outper- forms LFTJ and YTD, especially on larger cycle queries. Inter- estingly, we see little difference in the running times for 3-cycle queries. The reason for that is there is no tree decomposition for triangles, and CTJ-C effectively behaves like LFTJ. Similarly, the performance of CTJ-C and YTD is comparable for 3-cycle queries. When comparing the benefits of CTJ-C over large cycle and path queries (Figure 12), we see that CTJ-C delivers better speedups for paths. This is attributed to the cache dimension property (the size of adhesions). Therefore, the cache dimension for paths is set to one, and for cycles it is set to two. Notably, a cache whose dimension is one is shown to be much more effective. 5-cycle queries present another interesting result. For these queries YTD performs worse than LFTJ (and CTJ-C). The reason is that YTD&rsquo;s Yannakakis and the worst-case optimal join algorithm used by YTD, favor the op- posite attributes order, which dramatically affects its performance. Figure 12 shows that the performance benefit of CTJ-C and YTD over LFTJ increase with the query size at an exponential rate. More- over, while CTJ-C and YTD have similar scaling trends for path queries, CTJ-C is an order of magnitude faster for {5&ndash;6}-cycle. </span></p><p class="c30"><span class="c6">Comparison to systems and engines. To explore the scaling trends of the pure algorithms compared to those of DBMSs, we ran the queries on PGSQL (using pairwise join), LB-LFTJ, LB-FAQ (worst-case optimal join algorithms) and YTD-Par (parallel imple- mentation of YTD). For brevity, we show the results for only two datasets: Wiki-Vote and ego-Facebook. Notably, these are consis- tent with the results obtained for the other SNAP datasets. </span></p><p class="c30"><span class="c6">Figure 12 shows the results for {3&ndash;7}-path count queries. The first thing to note in the table is that the scaling of vanilla LFTJ and LB-LFTJ are correlated. We attribute the 4&ndash;10&times; ratio in per- formance between the two to overheads associated with running a full DBMS vs. a pure algorithm. A comparison between YTD-Par and YTD shows that YTD-Par is much faster than YTD. This to be expected, as YTD-Par engine is a parallel implementation of YTD pure algorithm, using the processor&rsquo;s wide vector unit. Due to the </span></p><p class="c13"><span class="c21">292 </span></p><p class="c172"><span class="c0">Algorithms Systems and engines </span></p><p class="c27"><span class="c0">query CTJ-C YTD LFTJ LB-FAQ LB-LFTJ PGSQL YTD-Par </span></p><p class="c81"><span class="c0">3-path 36 3</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span><span class="c0">9</span><span class="c1">&times; </span><span class="c0">54</span><span class="c1">&times; </span><span class="c0">19</span><span class="c1">&times; </span><span class="c0">0.08</span><span class="c1">&times; </span><span class="c0">4-path 58 3</span><span class="c1">&times; </span><span class="c0">133</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span><span class="c0">615</span><span class="c1">&times; </span><span class="c0">364</span><span class="c1">&times; </span><span class="c0">0.05</span><span class="c1">&times; </span><span class="c0">5-path 78 3</span><span class="c1">&times; </span><span class="c0">4362</span><span class="c1">&times; </span><span class="c0">8</span><span class="c1">&times; </span><span class="c0">21113</span><span class="c1">&times; </span><span class="c0">11161</span><span class="c1">&times; </span><span class="c0">0.09</span><span class="c1">&times; </span><span class="c0">6-path 97 3</span><span class="c1">&times; </span><span class="c0">157691</span><span class="c1">&times; </span><span class="c0">7</span><span class="c1">&times; </span><span class="c0">t/o 402735</span><span class="c1">&times; </span><span class="c0">0.10</span><span class="c1">&times; </span><span class="c0">7-path 119 4</span><span class="c1">&times; </span><span class="c0">t/o 7</span><span class="c1">&times; </span><span class="c0">t/o t/o 0.13</span><span class="c1">&times; </span></p><p class="c81"><span class="c0">3-cycle 24 1</span><span class="c1">&times; </span><span class="c0">1</span><span class="c1">&times; </span><span class="c0">13</span><span class="c1">&times; </span><span class="c0">13</span><span class="c1">&times; </span><span class="c0">41</span><span class="c1">&times; </span><span class="c0">0.21</span><span class="c1">&times; </span><span class="c0">4-cycle 1474 0.85</span><span class="c1">&times; </span><span class="c0">3</span><span class="c1">&times; </span><span class="c0">7</span><span class="c1">&times; </span><span class="c0">7</span><span class="c1">&times; </span><span class="c0">3</span><span class="c1">&times; </span><span class="c0">0.16</span><span class="c1">&times; </span><span class="c0">5-cycle 9401 16</span><span class="c1">&times; </span><span class="c0">16</span><span class="c1">&times; </span><span class="c0">1.66</span><span class="c1">&times; </span><span class="c0">43</span><span class="c1">&times; </span><span class="c0">13</span><span class="c1">&times; </span><span class="c0">2</span><span class="c1">&times; </span><span class="c0">6-cycle 28615 11</span><span class="c1">&times; </span><span class="c0">235</span><span class="c1">&times; </span><span class="c0">1</span><span class="c1">&times; </span><span class="c0">617</span><span class="c1">&times; </span><span class="c0">242</span><span class="c1">&times; </span><span class="c0">0.90</span><span class="c1">&times; </span></p><p class="c86 c191"><span class="c0">Algorithms Systems and engines </span></p><p class="c16"><span class="c0">query CTJ-C YTD LFTJ LB-FAQ LB-LFTJ PGSQL YTD-Par </span></p><p class="c39"><span class="c0">3-path 26 5</span><span class="c1">&times; </span><span class="c0">4</span><span class="c1">&times; </span><span class="c0">14</span><span class="c1">&times; </span><span class="c0">39</span><span class="c1">&times; </span><span class="c0">22</span><span class="c1">&times; </span><span class="c0">0.12</span><span class="c1">&times; </span><span class="c0">4-path 48 4</span><span class="c1">&times; </span><span class="c0">62</span><span class="c1">&times; </span><span class="c0">10</span><span class="c1">&times; </span><span class="c0">308</span><span class="c1">&times; </span><span class="c0">174</span><span class="c1">&times; </span><span class="c0">0.06</span><span class="c1">&times; </span><span class="c0">5-path 94 4</span><span class="c1">&times; </span><span class="c0">818</span><span class="c1">&times; </span><span class="c0">7</span><span class="c1">&times; </span><span class="c0">7973</span><span class="c1">&times; </span><span class="c0">2116</span><span class="c1">&times; </span><span class="c0">0.07</span><span class="c1">&times; </span><span class="c0">6-path 119 3</span><span class="c1">&times; </span><span class="c0">15086</span><span class="c1">&times; </span><span class="c0">6</span><span class="c1">&times; </span><span class="c0">t/o 56534</span><span class="c1">&times; </span><span class="c0">0.08</span><span class="c1">&times; </span><span class="c0">7-path 150 3</span><span class="c1">&times; </span><span class="c0">t/o 6</span><span class="c1">&times; </span><span class="c0">t/o t/o 0.09</span><span class="c1">&times; </span></p><p class="c39"><span class="c0">3-cycle 48 1.1</span><span class="c1">&times; </span><span class="c0">1</span><span class="c1">&times; </span><span class="c0">12</span><span class="c1">&times; </span><span class="c0">12</span><span class="c1">&times; </span><span class="c0">42</span><span class="c1">&times; </span><span class="c0">0.13</span><span class="c1">&times; </span><span class="c0">4-cycle 569 1.31</span><span class="c1">&times; </span><span class="c0">1</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span><span class="c0">4</span><span class="c1">&times; </span><span class="c0">0.12</span><span class="c1">&times; </span><span class="c0">5-cycle 1785 74</span><span class="c1">&times; </span><span class="c0">8</span><span class="c1">&times; </span><span class="c0">3</span><span class="c1">&times; </span><span class="c0">37</span><span class="c1">&times; </span><span class="c0">26</span><span class="c1">&times; </span><span class="c0">12</span><span class="c1">&times; </span><span class="c0">6-cycle 4639 54</span><span class="c1">&times; </span><span class="c0">81</span><span class="c1">&times; </span><span class="c0">3</span><span class="c1">&times; </span><span class="c0">366</span><span class="c1">&times; </span><span class="c0">208</span><span class="c1">&times; </span><span class="c0">5</span><span class="c1">&times; </span></p><p class="c185"><span class="c6">Figure 12: CTJ-C runtimes (in msecs) for {3&ndash;7}-path and {3&ndash; 6}-cycle count queries and relative runtimes for compared so- lutions (i.e., m&times; means m times slower than CTJ-C), shown for Wiki (top) and Facebook (bottom) datasets. t/o indicates run- time over 10 hours (timeout). </span></p><p class="c137"><span class="c6">parallel implementation, YTD-Par is also faster than CTJ-C and LFTJ on path queries. Nevertheless, the sequential CTJ-C imple- mentation is comparable to YTD-Par for {5&ndash;6}-cycles queries (and is even faster on some datasets). </span></p><p class="c32"><span class="c6">On average, CTJ-C is over 39&times; faster than LB-LFTJ for all path queries, and 5-208&times; faster for all cycle queries. CTJ-C speedup over LB-FAQ is 7&times; and 4&times; on average for path and cycle queries, respectively. Compared to PGSQL, CTJ-C is consistently 3&ndash;5 or- ders of magnitude faster for big cycle and path queries. </span></p><p class="c175"><span class="c10">6. CONCLUDING REMARKS </span></p><p class="c99 c142"><span class="c6">We have studied the incorporation of caching in LFTJ by tying an ordered tree decomposition to the variable ordering. The re- sulting scheme retains the inherent advantages of LFTJ (worst case optimality, low memory footprint), but allows it to accelerate per- formance based on whatever memory it decides to (dynamically) allocate. Our experimental study shows that the result is consis- tently faster than LFTJ, by orders of magnitude on large queries, and usually faster than other state of the art join algorithms. </span></p><p class="c32"><span class="c6">This work gives rise to several directions for future work. These include further exploration of different caching strategies, different TD enumerations and cost functions, extension to general aggre- gate operators (e.g., based on the work of Joglekar et al. [15] and Khamis et al. [16]), and generalizing beyond joins [29]. A highly relevant work is that on factorized representations [6,23,25], which we can incorporate in two manners. First, our caches can hold fac- torized representations instead of flat tuples. Second, the final result can be factorized by itself, and in that case our caching is likely to become even more effective, since it will save the cycles then we effectively spend on de-factorizing our cached results. </span></p><p class="c62"><span class="c6">Acknowledgments. We are very grateful to LogicBlox for shar- ing their code, and especially to Hung Ngo and Martin Bravenboer from LogicBlox for helpful suggestions and help with their system. We are also grateful to Christopher Aberger and Chris R&eacute; from </span></p><p class="c30 c124"><span class="c6">Stanford University for insightful discussions and assistance in set- ting up EmptyHeaded. This research is supported by the Israeli Ministry of Science, Technology, and Space. Yoav Etsion is sup- ported by the Center for Computer Engineering at Technion. Benny Kimelfeld is a Taub Fellow supported by the Taub Foundation, and supported by the Israeli Science Foundation, Grants #1295/15 and #1308/15. </span></p><p class="c136"><span class="c10">7. REFERENCES </span></p><p class="c133"><span class="c0">[1] C. R. Aberger, S. Tu, K. Olukotun, and C. R&eacute;. Emptyheaded: A relational </span></p><p class="c162"><span class="c0">engine for graph processing. In SIGMOD, pages 431&ndash;446, 2016. [2] A. Ailamaki, D. J. DeWitt, M. D. Hill, and D. A. Wood. DBMSs on a modern </span></p><p class="c145"><span class="c0">processor: Where does time go? In VLDB, pages 266&ndash;277, 1999. [3] M. Aref, B. ten Cate, T. J. Green, B. Kimelfeld, D. Olteanu, E. Pasalic, T. L. </span></p><p class="c156"><span class="c0">Veldhuizen, and G. Washburn. Design and implementation of the logicblox system. In SIGMOD, pages 1371&ndash;1382, 2015. [4] S. Arnborg, D. Corneil, and A. Proskurowski. Complexity of finding </span></p><p class="c88"><span class="c0">embeddings in a k-tree. SIAM J. Alg. Disc. Meth., 8(2):277&ndash;284, 1987. [5] A. Atserias, M. Grohe, and D. Marx. Size bounds and query plans for relational </span></p><p class="c87"><span class="c0">joins. SIAM J. Comput., 42(4):1737&ndash;1767, 2013. [6] N. Bakibayev, T. Kocisk&yacute;, D. Olteanu, and J. Zavodny. Aggregation and </span></p><p class="c119"><span class="c0">ordering in factorised databases. PVLDB, 6(14):1990&ndash;2001, 2013. [7] H. L. Bodlaender and A. M. C. A. Koster. Treewidth computations i. upper </span></p><p class="c97"><span class="c0">bounds. Inf. Comput., 208(3):259&ndash;275, 2010. [8] V. Bouchitt&eacute;, D. Kratsch, H. M&uuml;ller, and I. Todinca. On treewidth </span></p><p class="c22"><span class="c0">approximations. Discrete Applied Mathematics, 136(2-3):183&ndash;196, 2004. [9] N. Carmeli, B. Kenig, and B. Kimelfeld. On the enumeration of all minimal </span></p><p class="c141"><span class="c0">triangulations. CoRR, abs/1604.02833, 2016. [10] S. Chu, M. Balazinska, and D. Suciu. From theory to practice: Efficient join </span></p><p class="c189"><span class="c0">query evaluation in a parallel database system. In SIGMOD, pages 63&ndash;78, 2015. [11] D. G. Feitelson. Metrics for mass-count disparity. In MASCOTS, pages 61&ndash;68, </span></p><p class="c104"><span class="c0">2006. [12] G. Gottlob, G. Greco, and F. Scarcello. Pure nash equilibria: Hard and easy </span></p><p class="c96"><span class="c0">games. J. Artif. Intell. Res. (JAIR), 24:357&ndash;406, 2005. [13] G. Gottlob, M. Grohe, N. Musliu, M. Samer, and F. Scarcello. Hypertree </span></p><p class="c144"><span class="c0">decompositions: Structure, algorithms, and applications. In WG, pages 1&ndash;15, 2005. [14] G. Gottlob, N. Leone, and F. Scarcello. Hypertree decompositions and tractable </span></p><p class="c41"><span class="c0">queries. In PODS, pages 21&ndash;32, 1999. [15] M. R. Joglekar, R. Puttagunta, and C. R&eacute;. AJAR: aggregations and joins over </span></p><p class="c107"><span class="c0">annotated relations. In PODS, pages 91&ndash;106, 2016. [16] M. A. Khamis, H. Q. Ngo, and A. Rudra. FAQ: questions asked frequently. In </span></p><p class="c138"><span class="c0">PODS, pages 13&ndash;28, 2016. [17] E. L. Lawler. A procedure for computing the </span><span class="c1">k </span><span class="c0">best solutions to discrete </span></p><p class="c72"><span class="c0">optimization problems and its application to the shortest path problem. Management Science, 18(7):401&ndash;405, 1972. [18] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset </span></p><p class="c193"><span class="c0">collection, 2014. [19] D. Marx. Approximating fractional hypertree width. ACM Trans. on </span></p><p class="c85"><span class="c0">Algorithms, 6(2), 2010. [20] K. G. Murty. An algorithm for ranking all the assignments in order of </span></p><p class="c131"><span class="c0">increasing cost. Operations Research, 16(3):682&ndash;687, 1968. [21] H. Q. Ngo, E. Porat, C. R&eacute;, and A. Rudra. Worst-case optimal join algorithms: </span></p><p class="c64"><span class="c0">[extended abstract]. In PODS, pages 37&ndash;48, 2012. [22] D. T. Nguyen, M. Aref, M. Bravenboer, G. Kollias, H. Q. Ngo, C. R&eacute;, and </span></p><p class="c115"><span class="c0">A. Rudra. Join processing for graph patterns: An old dog with new tricks. In GRADES, pages 2:1&ndash;2:8, 2015. [23] D. Olteanu and J. Z&aacute;vodn&yacute;. Size bounds for factorised representations of query </span></p><p class="c187"><span class="c0">results. ACM Trans. on Database Systems (TODS), 40(1):2, 2015. [24] A. Perelman and C. R&eacute;. DunceCap: Compiling worst-case optimal query plans. </span></p><p class="c58"><span class="c0">In SIGMOD, pages 2075&ndash;2076, 2015. [25] M. Schleich, D. Olteanu, and R. Ciucanu. Learning linear regression models </span></p><p class="c80"><span class="c0">over factorized joins. In SIGMOD, pages 3&ndash;18, 2016. [26] P. G. Selinger, M. M. Astrahan, D. D. Chamberlin, R. A. Lorie, and T. G. Price. </span></p><p class="c184"><span class="c0">Access path selection in a relational database management system. In SIGMOD, pages 23&ndash;34, 1979. [27] The PostgreSQL Global Development Group. PostgreSQL. </span></p><p class="c154"><span class="c0">www.postgresql.org. [28] S. Tu and C. R&eacute;. DunceCap: Query plans using generalized hypertree </span></p><p class="c130"><span class="c0">decompositions. In SIGMOD, pages 2077&ndash;2078, 2015. [29] T. L. Veldhuizen. Triejoin: A simple, worst-case optimal join algorithm. In </span></p><p class="c103"><span class="c0">ICDT, pages 96&ndash;106, 2014. [30] M. Yannakakis. Algorithms for acyclic database schemes. In VLDB, pages </span></p><p class="c103"><span class="c0">82&ndash;94, 1981. [31] J. Y. Yen. Finding the </span><span class="c1">k </span><span class="c0">shortest loopless paths in a network. Management </span></p><p class="c67"><span class="c0">Science, 17:712&ndash;716, 1971. </span></p><p class="c95"><span class="c21">293 </span></p></body></html>