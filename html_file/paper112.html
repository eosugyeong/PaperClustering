<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c109{margin-left:-25pt;padding-top:1.4pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c44{color:#ffffff;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:5.6pt;font-family:"Arial";font-style:normal}.c131{color:#c5000b;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.9pt;font-family:"Arial";font-style:normal}.c35{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:normal}.c117{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.5pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c70{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Courier New";font-style:normal}.c77{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c53{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:normal}.c136{margin-left:-16.2pt;padding-top:1.7pt;text-indent:25.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.7pt}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c42{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c130{margin-left:-25pt;padding-top:187pt;text-indent:56.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.1pt}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.9pt;font-family:"Arial";font-style:normal}.c68{color:#ff950e;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c33{margin-left:-25pt;padding-top:3.8pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c6{margin-left:-16.2pt;padding-top:1pt;text-indent:33.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:6.5pt}.c47{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11pt;font-family:"Arial";font-style:normal}.c26{margin-left:-21.1pt;padding-top:1pt;text-indent:34.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:12.4pt}.c83{margin-left:-25pt;padding-top:4.1pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c32{margin-left:-25pt;padding-top:1.7pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c62{color:#0000ff;font-weight:400;text-decoration:none;vertical-align:super;font-size:9.4pt;font-family:"Arial";font-style:normal}.c8{margin-left:-16.2pt;padding-top:1.4pt;text-indent:25.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.9pt}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.5pt;font-family:"Arial";font-style:normal}.c37{color:#008000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.6pt;font-family:"Arial";font-style:normal}.c25{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c92{color:#ff950e;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.9pt;font-family:"Arial";font-style:normal}.c120{margin-left:-16.2pt;padding-top:1pt;text-indent:33.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:4.3pt}.c22{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c0{margin-left:-16.2pt;padding-top:1.7pt;text-indent:25.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.9pt}.c58{margin-left:237.8pt;padding-top:1.4pt;text-indent:-228.7pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.9pt}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c67{margin-left:-16.2pt;padding-top:1pt;text-indent:33.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.8pt}.c45{margin-left:-25pt;padding-top:1.4pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c39{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Courier New";font-style:normal}.c60{color:#000000;font-weight:700;text-decoration:none;vertical-align:super;font-size:11pt;font-family:"Arial";font-style:normal}.c29{margin-left:-16.2pt;padding-top:3.8pt;text-indent:25.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.9pt}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c21{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.9pt;font-family:"Arial";font-style:normal}.c94{color:#999999;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c43{color:#999999;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c11{margin-left:-25pt;padding-top:3.8pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c59{color:#0000ff;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.6pt;font-family:"Arial";font-style:normal}.c119{color:#000000;font-weight:700;text-decoration:none;vertical-align:sub;font-size:11.5pt;font-family:"Arial";font-style:normal}.c81{margin-left:-16.2pt;padding-top:1.2pt;text-indent:33.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:15.8pt}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Courier New";font-style:normal}.c118{color:#c5000b;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c114{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c48{margin-left:-16.2pt;padding-top:14.6pt;text-indent:26.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.9pt}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c27{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.6pt;font-family:"Arial";font-style:normal}.c90{margin-left:-25pt;padding-top:8.6pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c19{margin-left:-25pt;padding-top:1.7pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c95{margin-left:-25pt;padding-top:7.2pt;text-indent:33.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c12{margin-left:-16.2pt;padding-top:1.4pt;text-indent:25.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.9pt}.c97{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c108{color:#008000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.9pt;font-family:"Arial";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c17{margin-left:-21.1pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:6.2pt}.c91{margin-left:-21.1pt;padding-top:3.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:5pt}.c98{margin-left:-21.1pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-13.3pt}.c133{margin-left:-25pt;padding-top:12pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-35.8pt}.c122{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-20.2pt}.c128{margin-left:-15.7pt;padding-top:11pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.9pt}.c61{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:20.2pt}.c124{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.4pt}.c55{margin-left:-13pt;padding-top:120.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-4.6pt}.c56{margin-left:52pt;padding-top:9.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:42.5pt}.c79{margin-left:0.9pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3.1pt}.c46{margin-left:-21.1pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c9{margin-left:-21.1pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-1.8pt}.c85{margin-left:-25pt;padding-top:13.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:104.1pt}.c36{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-23.3pt}.c87{margin-left:-21.1pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-7pt}.c104{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15.6pt}.c129{margin-left:-25pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c110{margin-left:-25pt;padding-top:18.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:103.8pt}.c100{margin-left:-25pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-14pt}.c40{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:5.3pt}.c126{margin-left:218.2pt;padding-top:54.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c74{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.3pt}.c15{margin-left:218.2pt;padding-top:441.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:217.9pt}.c112{margin-left:-25pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:1.1pt}.c78{margin-left:-25pt;padding-top:14.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:114.9pt}.c54{margin-left:-16.2pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-22.8pt}.c49{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-15.4pt}.c50{margin-left:-25pt;padding-top:9.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:5.4pt}.c89{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25.7pt}.c76{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-23pt}.c80{margin-left:-25pt;padding-top:14.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:113.7pt}.c30{margin-left:-2.2pt;padding-top:9.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12pt}.c69{margin-left:218.2pt;padding-top:53.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c96{margin-left:-25pt;padding-top:11pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:26.6pt}.c125{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-8.6pt}.c132{margin-left:-8.2pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-15pt}.c127{margin-left:-25pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-17.8pt}.c111{margin-left:-25pt;padding-top:8.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-26pt}.c63{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.8pt}.c115{margin-left:-25pt;padding-top:14.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.9pt}.c75{margin-left:-16.2pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-17.3pt}.c135{margin-left:-25pt;padding-top:9.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.9pt}.c73{margin-left:-16.2pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.3pt}.c34{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-22.8pt}.c86{margin-left:-21.1pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12.8pt}.c52{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-25pt}.c82{margin-left:-16.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-17.3pt}.c106{margin-left:218.2pt;padding-top:52.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-36.1pt}.c101{margin-left:-25pt;padding-top:4.8pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-16.9pt}.c41{margin-left:-25pt;padding-top:3.8pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-16.9pt}.c99{padding-top:5.3pt;text-indent:26.2pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c102{padding-top:8.2pt;text-indent:26.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c93{padding-top:12.2pt;text-indent:26.2pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c123{padding-top:7pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c71{padding-top:18.7pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c103{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c23{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c72{padding-top:148.6pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c105{padding-top:11.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c51{margin-left:-16.2pt;text-indent:25.3pt;margin-right:-25.9pt}.c84{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c57{margin-left:6.2pt;margin-right:106.1pt}.c66{margin-left:-16.2pt;margin-right:-25.9pt}.c116{margin-left:-147.4pt;margin-right:255.1pt}.c121{margin-left:114.4pt;margin-right:-20.4pt}.c107{margin-left:237.8pt;margin-right:-25.9pt}.c64{margin-left:-2pt;margin-right:108pt}.c134{margin-left:252pt;margin-right:-265.8pt}.c65{margin-left:-16.2pt;margin-right:-22.3pt}.c38{margin-left:136.7pt;margin-right:13.2pt}.c88{margin-left:-22.3pt;margin-right:105.8pt}.c113{margin-left:237.8pt;margin-right:-279.9pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c84"><p class="c3"><span class="c77">Context-aware Event Stream Analytics </span></p><p class="c3"><span class="c24">Olga Poppe</span><span class="c7">*</span><span class="c2">, </span><span class="c24">Chuan Lei</span><span class="c7">**</span><span class="c2">, </span><span class="c24">Elke A. Rundensteiner</span><span class="c7">* </span><span class="c2">and </span><span class="c24">Dan Dougherty</span><span class="c7">* </span></p><p class="c3"><span class="c10">*</span><span class="c13">Worcester Polytechnic Institute, 100 Institute Road, Worcester, MA 01609, USA </span></p><p class="c3"><span class="c10">**</span><span class="c13">NEC Labs America, 10080 N Wolfe Rd, Cupertino, CA 95014, USA </span></p><p class="c3"><span class="c2">opoppe|rundenst|dd@cs.wpi.edu, chuan@nec-labs.com </span></p><p class="c3"><span class="c24">ABSTRACT </span><span class="c2">Complex event processing is a popular technology for con- tinuously monitoring high-volume event streams from health care to traffic management to detect complex compositions of events. These event compositions signify critical &ldquo;appli- cation contexts&rdquo; from hygiene violations to traffic accidents. Certain event queries are only appropriate in particular con- texts. Yet state-of-the-art streaming engines tend to execute all event queries continuously regardless of the current appli- cation context. This wastes tremendous processing resources and thus leads to delayed reactions to critical situations. We have developed the first context-aware event process- ing solution, called </span><span class="c14">CAESAR</span><span class="c2">, which features the following key innovations. (1) The </span><span class="c14">CAESAR </span><span class="c2">model supports applica- tion contexts as first class citizens and associates appropriate event queries with them. (2) The </span><span class="c14">CAESAR </span><span class="c2">optimizer em- ploys context-aware optimization strategies including con- text window push-down strategy and query workload shar- ing among overlapping contexts. (3) The </span><span class="c14">CAESAR </span><span class="c2">infras- tructure allows for lightweight event query suspension and activation driven by context windows. Our experimental study utilizing both the Linear Road stream benchmark as well as real-world data sets demonstrates that the context- aware event stream analytics consistently outperforms the state-of-the-art strategies by factor of 8 on average. </span></p><p class="c3"><span class="c24">1. INTRODUCTION </span></p><p class="c1"><span class="c2">Complex Event Processing (CEP) has emerged as a promi- nent technology for supporting applications from financial fraud [30] to health care [32]. Traditionally, CEP systems consume event streams produced by smart digital devices like sensors and mobile phones and continuously evaluate the query workload to monitor the input event streams. </span></p><p class="c1"><span class="c2">In many stream-based applications, events convey partic- ular application contexts such that the system reaction to an event may significantly vary depending on the current context. Therefore, some event queries may only need to be executed under certain circumstances while others can </span></p><p class="c1"><span class="c97">&copy;</span><span class="c14">2016, Copyright is with the authors. Published in Proc. 19th Inter- national Conference on Extending Database Technology (EDBT), March 15-18, 2016 - Bordeaux, France: ISBN 978-3-89318-070-7, on OpenPro- ceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0 </span></p><p class="c1"><span class="c2">be safely suspended. The following examples highlight the challenges and opportunities of context-aware event stream processing that have been overlooked in the prior research. </span></p><p class="c1"><span class="c2">Motivating Example. Traffic has both a huge economic and environmental impact on our daily lives. Drivers trav- eling the 10-worst U.S. traffic corridors annually spend an average of 140 hours idling in traffic [2]. Due to pollution and noise, congestion in the USA&rsquo;s 83 largest urban areas in 2010 led to a related public health cost of $18 billion [3]. Further, road traffic injuries caused an estimated 1.24 mil- lion deaths worldwide in 2010 [4]. </span></p><p class="c1"><span class="c2">An intelligent traffic control center could reduce these crippling impacts. The center receives vehicle position re- ports, analyzes them, infers the current situation in the mon- itored road segments and reacts instantaneously to ensure safe and smooth traffic flow. Early detection and prompt re- action to critical situations are eminently important. They prevent time and fuel waste, reduce pollution, avoid prop- erty damage and in some cases even save human lives. </span></p><p class="c3"><span class="c21">switch </span><span class="c28">if traffic </span></p><p class="c3"><span class="c108">clear </span><span class="c119">switch </span><span class="c117">if traffic </span><span class="c28">flows smoothly </span></p><p class="c3"><span class="c28">flows smoothly </span><span class="c21">switch </span><span class="c28">if </span></p><p class="c3"><span class="c21">switch </span><span class="c28">if stopped cars </span></p><p class="c3"><span class="c28">many slow cars </span></p><p class="c3"><span class="c131">accident </span></p><p class="c3"><span class="c21">initiate </span><span class="c28">if many slow cars </span></p><p class="c3"><span class="c92">congestion </span><span class="c21">initiate </span><span class="c28">if stopped cars </span></p><p class="c3"><span class="c28">alarm computation </span></p><p class="c3"><span class="c28">toll computation </span><span class="c21">terminate </span><span class="c28">if stopped cars removed </span></p><p class="c3"><span class="c21">terminate </span><span class="c28">if few fast cars </span></p><p class="c3"><span class="c2">Figure 1: </span><span class="c14">CAESAR </span><span class="c2">model of traffic management </span></p><p class="c1"><span class="c2">System reaction to a position report should thus be mod- ulated depending on the current situation on the road (here referred to as context</span><span class="c7">1</span><span class="c2">). Indeed, if an accident is detected, all vehicles downstream should be warned and possibly al- ternative routes should be suggested (Figure 1). If a road segment becomes congested, drivers may be charged toll to discourage them from driving to control smooth traffic flow. If a road segment is clear, none of the above actions should take place. Clearly, current application contexts must be rapidly detected and continuously maintained to determine appropriate reactions of the system at all times. </span></p><p class="c1"><span class="c2">Conditions implying an application context can be com- plex. They are specified on both the event streams and the current contexts. For example, if over 50 cars per minute </span></p><p class="c3"><span class="c10">1</span><span class="c13">Here we utilize the term application context to refer to the state of a sub-network such as accident, congestion, etc. We deliberately avoid using the notion state since it is a too overloaded term in the CEP literature. </span></p><p class="c3"><span class="c16">Series ISSN: 2367-2005 413 </span><span class="c42">10.5441/002/edbt.2016.38 </span></p><p class="c3"><span class="c2">structs move with an average speed less then 40 mph and the cur- </span></p><p class="c3"><span class="c2">to capture the semantics of processes and in that rent context is no congestion then the context deriving query </span></p><p class="c3"><span class="c2">sense express application contexts. However, these models, updates the context to congestion for this road segment. To </span></p><p class="c3"><span class="c2">targeting business process specification, were not designed save resources and thus to ensure prompt system responsive- </span></p><p class="c3"><span class="c2">for event stream processing. Thus, they neglect its core ness, such complex context detection should happen once. </span></p><p class="c3"><span class="c2">peculiarities such as the event-driven nature of context de- Its results must be available on-time and shared among all </span></p><p class="c3"><span class="c2">tection achieving high performance analytics and the impor- queries that belong to the detected context. In other words, </span></p><p class="c3"><span class="c2">tance of temporal windows and their processing techniques. context processing queries are dependent on the results of </span></p><p class="c3"><span class="c2">The Proposed </span><span class="c14">CAESAR </span><span class="c2">Approach. In [25], we for- context deriving queries and a mechanism ensuring their cor- </span></p><p class="c3"><span class="c2">mally defined the first context-aware event query processing rect execution must be employed. </span></p><p class="c3"><span class="c2">model for which we now design the Context-Aware Event The system responsiveness can be substantially improved </span></p><p class="c3"><span class="c2">Stream Analytics in Real time system, </span><span class="c14">CAESAR </span><span class="c2">for short. by exploiting the optimization opportunities enabled by the </span></p><p class="c3"><span class="c2">Our </span><span class="c14">CAESAR </span><span class="c2">model supports context windows as first- application contests. (1) Only those event queries that are </span></p><p class="c3"><span class="c2">class citizens and associates appropriate event queries with relevant in the current contexts should be executed. All ir- </span></p><p class="c3"><span class="c2">each context window. Event queries that process events relevant computations should be suspended. (2) Workloads </span></p><p class="c3"><span class="c2">within a context are called context processing queries. Event of overlapping contexts should be shared. Furthermore, ap- </span></p><p class="c3"><span class="c2">queries that derive a context are called context deriving queries. plication contexts break the application semantics into mod- </span></p><p class="c3"><span class="c2">Both types of queries operate within context windows, a new ules that facilitate the modular development and runtime </span></p><p class="c3"><span class="c2">class of event query window we define. maintenance of an event stream processing application. </span></p><p class="c3"><span class="c2">To achieve near real-time system responsiveness, the </span><span class="c14">CAE- </span><span class="c2">Challenges. To enable such event stream processing ap- </span></p><p class="c3"><span class="c14">SAR </span><span class="c2">model is transformed into a stream query plan com- plications, the following challenges must be tackled: </span></p><p class="c3"><span class="c2">posed of context-aware operators of the </span><span class="c14">CAESAR </span><span class="c2">algebra. Context-aware specification model. As motivated above, </span></p><p class="c3"><span class="c2">This algebra serves as foundation for the </span><span class="c14">CAESAR </span><span class="c2">optimizer. event stream processing applications need to express rich se- </span></p><p class="c3"><span class="c2">The optimizer exploits the notion of context windows to mantics. In particular, they have to specify application con- </span></p><p class="c3"><span class="c2">avoid unnecessary computations by suspending those oper- texts as first class citizens and enable linkage of appropriate </span></p><p class="c3"><span class="c2">ators which are irrelevant to the current context. Further- event queries to their respective context. Furthermore, this </span></p><p class="c3"><span class="c2">more, the optimizer saves computations by sharing work- model must be in a convenient human-readable format to </span></p><p class="c3"><span class="c2">loads of overlapping context windows. Finally, we built the facilitate on-the-fly reconfiguration, easy maintenance and </span></p><p class="c3"><span class="c14">CAESAR </span><span class="c2">runtime infrastructure for correct yet efficient ex- avoid fatal specification mistakes. </span></p><p class="c3"><span class="c2">ecution of inter-dependent context-aware event queries. Context-exploiting optimization techniques. To meet the </span></p><p class="c3"><span class="c2">Contributions can be summarized as follows: demanding latency constraints of time-critical applications, </span></p><p class="c3"><span class="c2">1) We introduce a new notion of windows, called con- this powerful context-aware application model must be trans- </span></p><p class="c3"><span class="c2">text windows, to enable context-aware event query process- lated into an efficient physical query plan. This query plan </span></p><p class="c3"><span class="c2">ing critical to modeling event-based systems. The proposed must be optimized by exploiting the optimization opportu- </span></p><p class="c3"><span class="c2">human-readable context-aware </span><span class="c14">CAESAR </span><span class="c2">model significantly nities enabled by context-aware event stream analytics. This </span></p><p class="c3"><span class="c2">simplifies the specification of rich event-driven application is complicated by the fact that the duration of a context is </span></p><p class="c3"><span class="c2">semantics by explicit support of context windows</span><span class="c7">2</span><span class="c2">. It also unknown at compile time and potentially unbounded. </span></p><p class="c3"><span class="c2">opens new multi-query optimization opportunities by asso- Context-driven execution infrastructure. An efficient run- </span></p><p class="c3"><span class="c2">ciating appropriate event queries with each context. time execution infrastructure is required to support multiple </span></p><p class="c3"><span class="c2">2) We define the </span><span class="c14">CAESAR </span><span class="c2">algebra for our context-aware concurrent contexts. To ensure correct query execution, the </span></p><p class="c3"><span class="c2">event query processing. The </span><span class="c14">CAESAR </span><span class="c2">optimizer pushes the inter-dependencies between complex context deriving and </span></p><p class="c3"><span class="c2">context windows down to suspend the execution of irrelevant context processing queries must be taken into account. </span></p><p class="c3"><span class="c2">operators. Furthermore, we propose the context window State-of-the-Art. The challenges described above have </span></p><p class="c3"><span class="c2">grouping algorithm that exploits the sharing opportunities so far not been addressed in a comprehensive fashion. </span></p><p class="c3"><span class="c2">from workloads of overlapping context windows. Since the duration of a context varies, state-of-the-art win- </span></p><p class="c3"><span class="c2">3) We built the </span><span class="c14">CAESAR </span><span class="c2">runtime execution infrastruc- dow semantics such as fixed-length tumbling and sliding win- </span></p><p class="c3"><span class="c2">ture that guarantees correct and efficient execution of inter- dows [22, 8] are inadequate to model the proposed notion of </span></p><p class="c3"><span class="c2">dependent context deriving and context processing queries. a context. Classical predicate windows [15] have variable du- </span></p><p class="c3"><span class="c2">4) We evaluate the performance of the </span><span class="c14">CAESAR </span><span class="c2">system ration. However, conditions leading to an application con- </span></p><p class="c3"><span class="c2">and its optimization strategies using the Linear Road stream text can be rather complex and thus resource-consuming, </span></p><p class="c3"><span class="c2">benchmark [9] as well as the real world data set [26]. Our worse yet they can be dependent on the previous contexts </span></p><p class="c3"><span class="c14">CAESAR </span><span class="c2">system performs on average 8-fold faster than the (Figure 1). Since predicate windows are independent from </span></p><p class="c3"><span class="c2">context-independent solution for a wide range of cases. each other, they fail to express context windows. </span></p><p class="c3"><span class="c2">Outline. We start with preliminaries in Section 2 and While some event query languages (e.g., CQL [10], SASE [5, </span></p><p class="c3"><span class="c2">introduce the </span><span class="c14">CAESAR </span><span class="c2">model in Section 3. We present our 34]) could be used to hard-code the equivalent of a con- </span></p><p class="c3"><span class="c2">algebraic execution paradigm in Section 4 and its optimiza- text construct by queries that detect the context bounds. </span></p><p class="c3"><span class="c2">tion techniques in Section 5. Section 6 is devoted to the run- However, this approach is cumbersome and error-prone &ndash; </span></p><p class="c3"><span class="c2">time execution infrastructure. We conduct the performance requiring the careful specification of multiple complex inter- </span></p><p class="c3"><span class="c2">study in Section 7. Related work is discussed in Section 8, dependent event queries [25]. Furthermore, no optimiza- </span></p><p class="c3"><span class="c2">and Section 9 concludes the article. tion techniques have been developed to exploit the benefits of context-awareness such as suspension of irrelevant event queries nor the sharing workloads of overlapping contexts. </span></p><p class="c3"><span class="c2">Business models [16, 28] focus on powerful modeling con- </span></p><p class="c3"><span class="c10">2</span><span class="c13">Visual editor for the </span><span class="c53">CAESAR </span><span class="c13">model and its evaluation, out of the scope of this article, are subjects for future research. In [25] we compare our model to a set of CQL event queries. </span></p><p class="c3"><span class="c16">414 </span></p><p class="c55"><span class="c2">Figure 2: Key concepts of the </span><span class="c14">CAESAR </span><span class="c2">model </span></p><p class="c110"><span class="c24">2. PRELIMINARIES </span></p><p class="c33"><span class="c2">Time. Time is represented by a linearly ordered set of time points (T,&le;), where T &sube; Q</span><span class="c7">+ </span><span class="c2">and Q</span><span class="c7">+ </span><span class="c2">denotes the set of non-negative rational numbers. The set of time intervals is TI = {[start, end] | start &isin; T, end &isin; T, start &le; end}. For a time point t &isin; T and an interval w &isin; TI we say that t is within w, denoted t &#8849; w, if w.start &le; t &le; w.end. </span></p><p class="c19"><span class="c2">Event. An event is a message indicating that something of interest happens in the real world. Each event e belongs to a particular event type E, denoted e.type = E. An event type E is defined by a schema which specifies the set of event attributes and the domains of their values. An event e has an occurrence time e.time &isin; T assigned by the event source. For example, a vehicle position report in [9] has the following attributes: expressway, direction, segment, car identifier etc. The values of these attributes are integers. </span></p><p class="c19"><span class="c2">Event Stream. Events can be simple or complex. Sim- ple events are sent by event producers (e.g., sensors) to event consumers (e.g., a traffic control center) on an input event stream I to be processed to derive higher-level complex events. The occurrence time of a complex event comprises the occurrence time of all events it was derived from [23]. </span></p><p class="c85"><span class="c24">3. CAESAR MODEL </span></p><p class="c50"><span class="c24">3.1 Key Concepts of the CAESAR Model </span></p><p class="c83"><span class="c2">Application contexts are real-world higher-order situ- ations the duration of which is not known at their detection time and potentially unbounded. This differentiates con- texts from events. The duration of a context is called a context window. For example, congestion is a higher-order situation in the traffic control application (Figure 2). Its bounds are detected based on position reports sent from cars in the same road segment in the same time period. As long as a road segment remains congested, the context window congestion is said to hold. Hence, the duration of a context window cannot be predetermined. </span></p><p class="c45"><span class="c2">Context deriving queries associated with a particular context determine when this context should be terminated and when a particular other context is to be initiated based on events. For example, two context transitions are possible from the congestion context. If the number of cars reduces and they start moving at higher speed the system transitions into the clear context. If two cars stop in the same location and at the same time the accident context is activated. </span></p><p class="c19"><span class="c2">Context processing queries correspond to the work- load associated with a particular context, i.e., the analytics to be computed based on events received while the system remains in this context. For example, cars entering a con- </span></p><p class="c3 c113"><span class="c2">gested road segment are charged toll to discourage drivers from driving during rush hours. </span><span class="c24">3.2 Benefits of Context-Awareness Property </span></p><p class="c29"><span class="c2">Event Query Relevance. At each point of time, a con- text window re-targets all efforts of the system to the current situation by activating only those event queries (both con- text deriving and context processing queries) which belong to one of the currently active application contexts. All other event queries are suspended as they are irrelevant within the current contexts. This saves both CPU and memory re- sources. For example, toll is charged only during congestion on a road. This query is neither relevant in the clear nor in the accident contexts. Thus, it is evaluated only during congestion and suspended in all other contexts. </span></p><p class="c0"><span class="c2">Event Query Simplification. The concept of an appli- cation context provides event queries with situational knowl- edge that allows us to specify simpler event queries. For ex- ample, if the event query computing toll is evaluated only during the congestion context, the complex conditions that determine that there is a traffic jam on the road are already implied by the context. Thus, there is no need to repeatedly double-check them in each of the active workload queries. </span></p><p class="c12"><span class="c2">Context Derivation. The task of context derivation is the dedicated responsibility of the context deriving queries. For example, once too many slow cars in a road segment are detected, the context congestion is activated. There- after, the query detecting congestion is no longer evaluated. Also, all event queries that are evaluated during congestion leverage the insight detected by the context deriving query rather than re-evaluating the congestion condition over and over at each individual event query level. </span><span class="c24">3.3 Context Window </span></p><p class="c48"><span class="c2">Definition 1. (Context type and Context window.) A context type is defined by a name c and a workload of context deriving queries Q</span><span class="c7">c</span><span class="c10">d </span><span class="c25">and context processing queries </span><span class="c2">Q</span><span class="c7">c</span><span class="c10">p </span><span class="c25">which are appropriate in this context. </span></p><p class="c1 c51"><span class="c2">Let C be the set of context types. Then, a context window w</span><span class="c10">c </span><span class="c2">is defined by a type c &isin; C and a duration (t</span><span class="c10">i</span><span class="c2">,t</span><span class="c10">t</span><span class="c2">] &isin; TI where t</span><span class="c4">i </span><span class="c13">is the time point when a query q</span><span class="c4">i </span><span class="c13">&isin; Q</span><span class="c10">c </span><span class="c4">d </span><span class="c2">matched the event stream and thus w</span><span class="c10">c </span><span class="c2">got initiated and t</span><span class="c10">t </span><span class="c2">is the time point when a query q</span><span class="c4">t </span><span class="c13">&isin; Q</span><span class="c10">c</span><span class="c4">d </span><span class="c2">matched the event stream and thus w</span><span class="c10">c </span><span class="c2">got terminated where c &isin; C. </span></p><p class="c51 c105"><span class="c2">Context windows of different types may overlap. Indeed, there can be a congestion and an accident in the same road segment at the same time such that two sets of event queries handling both situations must be executed concurrently. </span></p><p class="c66 c93"><span class="c2">Definition 2. (Context window relationships.) Con- text windows of type c</span><span class="c10">1 </span><span class="c2">and c</span><span class="c10">2 </span><span class="c2">are guaranteed to overlap if based on the predicates of the respective context deriving queries it can be determined that for each window of type c</span><span class="c4">1 </span><span class="c13">there is a window of type c</span><span class="c4">2 </span><span class="c13">with w</span><span class="c4">c</span><span class="c39">1</span><span class="c2">.start &#8849; w</span><span class="c4">c</span><span class="c39">2</span><span class="c2">. If in addition w</span><span class="c4">c</span><span class="c39">1</span><span class="c2">.end &#8849; w</span><span class="c4">c</span><span class="c39">2 </span><span class="c2">can be determined, a window of type c</span><span class="c10">1 </span><span class="c2">is contained in a window of type c</span><span class="c10">2</span><span class="c2">. </span></p><p class="c105 c51"><span class="c2">In general, the predicates of the context deriving queries can be analyzed to determine if they imply such conditions. For example, Figure 7 shows the predicates that determine the bounds of the context windows w</span><span class="c10">c</span><span class="c39">1 </span><span class="c2">and w</span><span class="c10">c</span><span class="c39">2</span><span class="c2">. It is easy </span></p><p class="c106"><span class="c16">415 </span></p><p class="c3"><span class="c2">to conclude that the windows overlap. </span><span class="c14">CAESAR </span><span class="c2">employs established approaches for predicate subsumption [14]. </span></p><p class="c1"><span class="c2">The same event query can be appropriate in several dif- ferent application contexts. For example, accident detection happens in both the clear and the congestion contexts. In contrast to that, the event query detecting accident clear- ance is executed only in the accident context. </span></p><p class="c3"><span class="c2">For simplicity, we have made two assumptions: (1) Event queries associated with different contexts are independent, meaning that they do not produce events that are consumed by event queries in other contexts. (2) Only one context win- dow of the same type can hold at a time per road segment. If there are multiple accidents in a road segment the context window accident holds until all of them are cleared. </span><span class="c24">3.4 Context-aware Event Queries </span></p><p class="c3"><span class="c31">1 </span></p><p class="c3"><span class="c22">DERIVE </span><span class="c31">TollNotification(p.vid, p.sec, </span><span class="c22">PATTERN </span><span class="c31">NewTravelingCar p </span></p><p class="c3"><span class="c68">congestion </span><span class="c118">accident </span></p><p class="c3"><span class="c22">INITIATE CONTEXT </span><span class="c31">accident </span><span class="c22">PATTERN </span><span class="c31">Accident </span><span class="c94">[</span><span class="c43">CONTEXT </span><span class="c94">congestion] </span></p><p class="c3"><span class="c31">5) </span></p><p class="c3"><span class="c94">[</span><span class="c43">CONTEXT </span><span class="c94">congestion] </span><span class="c31">2 </span><span class="c60">DERIVE </span><span class="c47">NewTravelingCar(p2.vid, </span><span class="c31">p2.lane, p2.pos, p2.sec) </span></p><p class="c3"><span class="c47">p2.xway, p2.dir, p2.seg, </span></p><p class="c3"><span class="c22">PATTERN SEQ</span><span class="c31">(</span><span class="c22">NOT </span><span class="c31">PositionReport p1, PositionReport p2) </span><span class="c22">WHERE </span><span class="c31">p1.sec+30=p2.sec </span><span class="c22">AND </span><span class="c31">p1.vid=p2.vid </span><span class="c22">AND </span><span class="c31">p2.lane&ne;&ldquo;exit&rdquo; </span><span class="c94">[</span><span class="c43">CONTEXT </span><span class="c94">congestion] </span></p><p class="c3"><span class="c31">3 </span></p><p class="c3"><span class="c2">Figure 3: Context-aware event queries </span></p><p class="c1"><span class="c2">The two application contexts, congestion and accident, are shown in Figure 3. Different event queries are appropriate within them. For compactness, only three of them within the congestion context are shown. Clauses in square brackets are optional since they are implied by the model. The </span><span class="c14">CAESAR </span><span class="c2">event query language grammar is defined in Figure 4. </span></p><p class="c3"><span class="c2">Definition 3. (Context-aware event queries.) A context-aware event query consists of several clauses. Each clause performs one of the following tasks: </span></p><p class="c3"><span class="c2">&ndash; Context initiation (INITIATE CONTEXT clause). &ndash; Context switch (SWITCH CONTEXT clause). &ndash; Context termination (TERMINATE CONTEXT clause). &ndash; Complex event derivation (DERIVE clause). &ndash; Event pattern matching (PATTERN clause). &ndash; Event filtering (WHERE clause). &ndash; Context window specification (CONTEXT clause). </span></p><p class="c1"><span class="c2">Context deriving queries perform three actions: (1) ini- tiate a new context window w</span><span class="c4">c</span><span class="c13">, (2) terminate an existing </span><span class="c2">context window w</span><span class="c4">c</span><span class="c13">, or (3) switch from the current context </span><span class="c2">window Context w</span><span class="c10">c</span><span class="c39">1 </span><span class="c2">initiation into a new and context termination window can w</span><span class="c10">c</span><span class="c2">be </span><span class="c39">2</span><span class="c2">. </span></p><p class="c1"><span class="c2">used to express overlapping context windows. For example, accident and congestion may overlap. That is, query 3 initiates the con- text window accident when an accident is detected (Fig- ure 3). However, query 3 does not terminate the context window congestion. The event queries that detect accidents are not shown for compactness. </span></p><p class="c23"><span class="c2">In contrast, context switch expresses a sequence of two non-overlapping context windows. It corresponds to the ter- mination of the previous context window tion context of the overlaps new context neither window accident w</span><span class="c4">c</span><span class="c2">nor </span><span class="c39">2</span><span class="c2">. For example, w</span><span class="c4">c</span><span class="c39">1 </span><span class="c2">and the initia- the clear congestion contexts. </span></p><p class="c1"><span class="c2">Context processing queries analyze the stream of simple or complex events to derive higher-level knowledge in form of complex events. For example, query 2 detects the cars en- tering a congested road segment. These are vehicles which are not on an exit lane and for which there is no previous po- sition report from the same road segment within 30 seconds. Query 1 derives toll notifications for such vehicles. </span></p><p class="c1"><span class="c2">Both context deriving and context processing queries con- sume events that arrive during the context windows that these queries are associated with. Hence, both types of queries utilize event pattern matching and event filtering clauses which are commonly used in event queries [34, 23]. Section 4.1 defines when these clauses match. </span></p><p class="c3"><span class="c2">Query := &#12296;W indow&#12297;|&#12296;Retrieval&#12297; W indow := (</span><span class="c14">INITIATE </span><span class="c2">| </span><span class="c14">SWITCH </span><span class="c2">| </span><span class="c14">TERMINATE</span><span class="c2">) </span></p><p class="c3"><span class="c14">CONTEXT </span><span class="c2">Context Retrieval := &#12296;Derive&#12297; &#12296;P attern&#12297; &#12296;Where&#12297;? &#12296;Context&#12297; Derive := </span><span class="c14">DERIVE </span><span class="c2">EventType ((V ar.)? Attr,?)+ P attern := </span><span class="c14">PATTERN </span><span class="c2">&#12296;P att&#12297; W here := </span><span class="c14">WHERE </span><span class="c2">&#12296;Expr&#12297; Context := </span><span class="c14">CONTEXT </span><span class="c2">(Context ,?)+ P att := </span><span class="c14">NOT</span><span class="c2">? EventT ype V ar? | </span><span class="c14">SEQ</span><span class="c2">( (&#12296;P att&#12297; ,?)+ ) Expr := Constant | Attr | &#12296;Expr&#12297; &#12296;Op&#12297; &#12296;Expr&#12297; Op := +|&minus;|/|&lowast;|%| = | = | &gt; |&ge;| &lt; |&le;|AND|OR </span></p><p class="c3"><span class="c2">Figure 4: </span><span class="c14">CAESAR </span><span class="c2">event query language grammar </span></p><p class="c1"><span class="c2">Putting the application contexts, transitions between them (Definitions 1 and 2) and context-aware event queries (Def- inition 3) together, we now define the </span><span class="c14">CAESAR </span><span class="c2">model. </span></p><p class="c1"><span class="c2">Definition 4. (</span><span class="c14">CAESAR </span><span class="c2">model.) A </span><span class="c14">CAESAR </span><span class="c2">model is a tuple (I,O,C,c</span><span class="c4">d</span><span class="c13">) where I and O are unbounded input and </span><span class="c2">output event streams and C is a finite set of context types with the default context type c</span><span class="c4">d </span><span class="c13">&isin; C. </span></p><p class="c1"><span class="c2">While the goal of classical automata is to define a lan- guage, the </span><span class="c14">CAESAR </span><span class="c2">model is designed for context-aware event query execution. Thus, final contexts are omitted. The </span><span class="c14">CAESAR </span><span class="c2">model has a default context that holds when no other context does, e.g., at the system startup (the clear context in our example). The runtime processing of the model is defined in Section 4.1. </span></p><p class="c3"><span class="c24">4. CAESAR ALGEBRA </span></p><p class="c3"><span class="c2">The </span><span class="c14">CAESAR </span><span class="c2">model explicitly supports application con- texts and the transition network to facilitate context-aware event query specification (Figure 3). However, at execution level an algebraic query plan tends to be easier to optimize than an automaton-based model [30].</span><span class="c7">3 </span><span class="c2">We thus define the </span><span class="c14">CAESAR </span><span class="c2">algebra and the translation rules of the </span><span class="c14">CAESAR </span><span class="c2">model into an algebraic query plan. </span><span class="c24">4.1 CAESAR Operators </span></p><p class="c1"><span class="c2">The </span><span class="c14">CAESAR </span><span class="c2">algebra consists of six operators. While event pattern, filter and projection are quite common for other stream algebras [30], [34], context initiation, termina- tion and context window are unique operators of the </span><span class="c14">CAE- SAR </span><span class="c2">algebra. Context initiation and termination consume a stream I of events produced by other operators of the con- text deriving queries and the set of current context windows </span><span class="c10">3</span><span class="c13">There are approaches to optimization and distribution of simpler automata than the </span><span class="c53">CAESAR </span><span class="c13">model however. We describe them in detail in Section 8. </span></p><p class="c3"><span class="c16">416 </span></p><p class="c3"><span class="c2">W. They update the set of the current context windows and return the updated set. </span></p><p class="c1"><span class="c2">Context initiation CI</span><span class="c10">c </span><span class="c2">starts a new context window w</span><span class="c10">c</span><span class="c2">, adds it to the set of current context windows and removes the de- fault context window CI</span><span class="c4">c</span><span class="c13">(I,W) := {W | If w</span><span class="c2">w</span><span class="c10">c</span><span class="c4">c </span><span class="c35">d </span><span class="c13">&isin; </span><span class="c2">from </span><span class="c13">W </span><span class="c2">the set, if there. </span></p><p class="c3"><span class="c13">then W = W. Otherwise W = </span><span class="c2">W&cup;we.time Context </span><span class="c4">c </span><span class="c2">= </span><span class="c13">and </span><span class="c2">w</span><span class="c10">c</span><span class="c2">termination </span><span class="c13">if </span><span class="c35">d</span><span class="c2">.end </span><span class="c13">w</span><span class="c4">c</span><span class="c35">d </span><span class="c2">= &isin; W w</span><span class="c10">c</span><span class="c2">.start}. </span></p><p class="c23"><span class="c2">CTthen </span><span class="c4">c </span><span class="c2">W </span><span class="c13">ends </span><span class="c2">= W /w</span><span class="c4">c</span><span class="c35">d </span><span class="c2">where e &isin; I and </span><span class="c13">the context window w</span><span class="c4">c</span><span class="c13">, re- </span><span class="c2">moves it from the set of current context windows, if the set becomes empty adds the default context CT</span><span class="c10">c</span><span class="c2">(I,W) := {W | If |W| &gt; 1 then W = window W/w</span><span class="c10">c </span><span class="c2">welse </span><span class="c4">c</span><span class="c35">d </span><span class="c2">to it. </span></p><p class="c3"><span class="c2">W = {wContext </span><span class="c4">c</span><span class="c35">d</span><span class="c2">} where window e &isin; I CWand </span><span class="c4">c </span><span class="c13">consumes </span><span class="c2">e.time = w</span><span class="c13">an </span><span class="c4">c</span><span class="c13">.end event = stream w</span><span class="c4">c</span><span class="c35">d</span><span class="c2">.start}. </span></p><p class="c1"><span class="c13">I and the </span><span class="c2">set of all current context windows W and returns the stream of events that occur during the current context window w</span><span class="c4">c</span><span class="c13">. </span><span class="c2">CW</span><span class="c10">c</span><span class="c2">(I,W) := {e | w</span><span class="c10">c </span><span class="c2">&isin; W, e &isin; I, e.time &#8849; w</span><span class="c10">c</span><span class="c2">}. </span></p><p class="c1"><span class="c2">Filter FI</span><span class="c4">&theta; </span><span class="c13">with a predicate &theta; consumes an event stream I </span><span class="c2">and returns a stream composed of all events that satisfy &theta;. FI</span><span class="c10">&theta;</span><span class="c2">(I) := {e | e &isin; I, e satisfies &theta;}. </span></p><p class="c3"><span class="c2">Projection PR</span><span class="c4">A,E </span><span class="c13">with a set of attributes A and an event </span><span class="c2">type E consumes an event stream I, restricts each input event to the set of attributes A and returns the stream of these restricted events of type E. PR</span><span class="c4">A,E</span><span class="c13">(I) := {e | e.type = E, e &isin; I, e.a = e .a for a &isin; A}. </span></p><p class="c1"><span class="c2">Pattern P consumes an event stream I and constructs event sequences matched by the pattern P. For each event sequence, the operator outputs an event consisting of the at- tribute values of all events in the sequence. Let A be the set of all attributes of events of types E</span><span class="c4">1</span><span class="c13">, ..., E</span><span class="c4">n </span><span class="c13">and 1 &le; i &le; n. </span><span class="c2">The pattern P is one of the following: </span></p><p class="c3"><span class="c2">1) Event matching E returns input events of type E. E(I) := {e | e &isin; I, e.type = E}. quences 2) Sequence of n events without such negation that an SEQi</span><span class="c7">th </span><span class="c4">(E</span><span class="c2">event </span><span class="c39">1</span><span class="c10">,...E</span><span class="c2">is </span><span class="c35">n</span><span class="c4">) </span><span class="c2">of </span><span class="c13">constructs </span><span class="c2">type E</span><span class="c4">i</span><span class="c13">. </span></p><p class="c3"><span class="c13">se- </span></p><p class="c23"><span class="c2">SEQe</span><span class="c4">n</span><span class="c13">.type </span><span class="c4">(E</span><span class="c39">1</span><span class="c10">,...E</span><span class="c13">= E</span><span class="c35">n</span><span class="c4">)n</span><span class="c13">, </span><span class="c2">(I) </span><span class="c13">e</span><span class="c4">1</span><span class="c13">.time </span><span class="c2">:= {e | </span><span class="c13">&lt; </span><span class="c2">e</span><span class="c10">1</span><span class="c2">, ..., e</span><span class="c10">n </span><span class="c2">&isin; I, e</span><span class="c10">1</span><span class="c2">.type = E</span><span class="c10">1</span><span class="c2">, ..., </span><span class="c13">... &lt; e</span><span class="c4">n</span><span class="c13">.time, e.a = e</span><span class="c4">i</span><span class="c13">.a for a &isin; </span><span class="c2">A, e.time = [e</span><span class="c10">1</span><span class="c2">.time, e</span><span class="c10">n</span><span class="c2">.time]}. 3) Sequence with negation event is no event sequences of type SEQE </span><span class="c4">S</span><span class="c2">between </span><span class="c39">1</span><span class="c10">,S</span><span class="c39">2 </span><span class="c2">without SEQnegation </span><span class="c4">(S</span><span class="c39">1</span><span class="c10">,NOT E,S</span><span class="c2">such </span><span class="c39">2</span><span class="c10">) </span><span class="c25">constructs </span><span class="c2">that there the sub-sequences constructed by SEQ&isin; e</span><span class="c4">m+1</span><span class="c13">.time, </span><span class="c2">SEQSEQ</span><span class="c4">(S</span><span class="c39">1</span><span class="c4">S</span><span class="c10">,NOT </span><span class="c4">S</span><span class="c39">21 </span><span class="c2">, and SEQ&exist;e </span><span class="c10">E,S</span><span class="c39">2</span><span class="c2">&isin; </span><span class="c10">)</span><span class="c25">(I) </span><span class="c2">I </span><span class="c4">S</span><span class="c39">2</span><span class="c25">:= </span><span class="c2">. </span></p><p class="c23"><span class="c2">with </span><span class="c25">{e </span><span class="c2">e </span><span class="c25">| </span><span class="c2">.type </span><span class="c25">e</span><span class="c7">1</span><span class="c25">, ..., </span><span class="c2">= </span><span class="c25">e</span><span class="c7">m </span><span class="c2">E, </span><span class="c25">&isin; </span><span class="c2">e</span><span class="c4">m</span><span class="c13">.time </span><span class="c25">SEQ</span><span class="c10">S</span><span class="c39">1</span><span class="c2">, </span><span class="c13">&lt; </span><span class="c2">e</span><span class="c10">m+1</span><span class="c2">, </span><span class="c13">e .time </span><span class="c2">..., e</span><span class="c10">n </span><span class="c13">&lt; e.a = e</span><span class="c4">i</span><span class="c13">.a for a &isin; A, e.time = [e</span><span class="c4">1</span><span class="c13">.time, e</span><span class="c4">n</span><span class="c13">.time]}. </span><span class="c2">A negated event can start or end an event sequence. In this case, temporal constraints must define the time interval within which the negated event may not occur [34]. </span></p><p class="c3"><span class="c24">4.2 Context-preserving Plan Generation </span></p><p class="c1"><span class="c2">At compile time, the </span><span class="c14">CAESAR </span><span class="c2">model (Figure 3) is trans- lated into an executable query plan that is than input to the </span><span class="c14">CAESAR </span><span class="c2">optimizer (Section 5). The </span><span class="c14">CAESAR </span><span class="c2">model translation happens in two phases (Figure 5). They are: </span></p><p class="c1"><span class="c2">Phase 1: </span><span class="c14">CAESAR </span><span class="c2">model to a query set. First, the model is translated into a machine-readable query set. During this phase, contexts that are implied by the </span><span class="c14">CAESAR </span><span class="c2">model (the optional clauses in square brackets in Figure 3) become mandatory clauses of the </span><span class="c14">CAESAR </span><span class="c2">event queries. As a result, an event query that belongs to a context c has a mandatory clause CONTEXT c. For example, all queries in Figure 5 explicitly specify the context they belong to. </span></p><p class="c3"><span class="c2">Phase 2: Query set to a combined query plan. Dur- </span></p><p class="c3"><span class="c37">Q1 </span><span class="c62">TERMINATE </span><span class="c59">Q3 </span></p><p class="c3"><span class="c62">CONTEXT Cf </span></p><p class="c103"><span class="c37">C0 </span><span class="c59">Cf </span><span class="c27">INITIATE CONTEXT C0 </span></p><p class="c3"><span class="c37">SWITCH CONTEXT Cf </span><span class="c27">Q0 </span><span class="c37">Q2 </span></p><p class="c3"><span class="c44">Phase 1 </span></p><p class="c3"><span class="c28">Context deriving queries: </span></p><p class="c3"><span class="c27">INITIATE CONTEXT C0 Q0 </span></p><p class="c3"><span class="c37">SWITCH CONTEXT Cf </span></p><p class="c3"><span class="c20">: levell acigo</span><span class="c28">L</span><span class="c37">Q2 CONTEXT C0 </span></p><p class="c3"><span class="c59">TERMINATE CONTEXT Cf Q3 CONTEXT Cf </span><span class="c28">Context processing queries: </span></p><p class="c3"><span class="c37">Q1 CONTEXT C0 </span></p><p class="c3"><span class="c44">Phase 2 </span></p><p class="c23"><span class="c20">l edomR ASEA</span><span class="c28">C</span><span class="c20">: levell acisyh</span><span class="c28">P</span><span class="c20">a rbeglaR ASEA</span><span class="c28">C</span><span class="c13">Figure 5: CAESAR model translation </span></p><p class="c3"><span class="c2">Event query clause Operator INITIATE CONTEXT c CI</span><span class="c4">c </span><span class="c2">SWITCH CONTEXT c CI</span><span class="c10">c</span><span class="c2">,CT</span><span class="c10">curr </span><span class="c2">TERMINATE CONTEXT c CT</span><span class="c4">c </span><span class="c2">DERIVE E(A) PR</span><span class="c4">A,E </span><span class="c13">PATTERN P P </span><span class="c2">WHERE &theta; FI</span><span class="c4">&theta; </span><span class="c2">CONTEXT c CW</span><span class="c10">c </span></p><p class="c3"><span class="c2">Table 1: Individual query plan construction </span></p><p class="c3"><span class="c2">ing this phase, the machine-readable query set is translated into an executable query plan. This happens in two steps: 1) Individual query plan construction. Each event query is translated into a sequence of algebra operators such that each clause of the query corresponds to a set of operators as defined in Table 1. curr denotes the current context the context deriving query is associated with. 2) Combined query plan construction. Individual query plans are composed into a combined query plan such that if one query plan produces events which are consumed by another query plan then the output of the first plan is the input of the second plan. Since event queries in different contexts are independent (Section 3.3), all event queries in a combined query plan belong to the same context. </span></p><p class="c1"><span class="c2">For example, queries 1 and 2 in Figure 3 are translated into the combined query plan in Figure 6(a). Query 1 is translated into the individual query plan consisting of oper- ators 1&ndash;4. Query 2 corresponds to the individual query plan composed of operators 5&ndash;7. Since the first query plan pro- duces complex events consumed by the second query plan, they are composed into a single combined query plan. </span></p><p class="c3"><span class="c24">5. CAESER OPTIMIZATION </span></p><p class="c3"><span class="c24">5.1 CAESAR Optimization Problem Statement </span></p><p class="c3"><span class="c2">Definition 5. Given a workload of context-aware event queries where each query is associated with an application </span></p><p class="c3"><span class="c16">417 </span></p><p class="c129"><span class="c2">context. Our </span><span class="c14">CAESAR </span><span class="c2">optimization problem is to find an optimized query plan for all queries such that the CPU costs are minimized by suspending event queries that are irrel- evant to the current application contexts and sharing the workload of overlapping context windows. </span></p><p class="c95"><span class="c2">To avoid reinventing the wheel, we borrow the CPU cost estimation of event pattern construction from [24], and thus do not repeat here. Instead, we now discuss the cost of the context-specific operators. We maintain the information about current context windows in the context bit vector W with one bit for each context type. Since the number of possible context types for an application is predefined and constant, the size of the vector is also constant. The context initiation and termination operators update one bit and the time stamp of the context bit vector. Context windows look up one value in the vector to determine whether a context window of a certain type currently holds. In other words, the CPU cost of these operators is constant. Section 6 provides further implementation details. </span><span class="c24">5.2 Context Window Push Down </span></p><p class="c33"><span class="c2">Since some operators of the CAESAR algebra are similar to other stream algebras, existing approaches, from operator reordering [24] to operator merging [30, 6], can be exploited by the </span><span class="c14">CAESAR </span><span class="c2">optimizer as well. For example, projections and filters can be executed in any order assuming that a projection that is pushed down below a filter discards no attributes accessed by the filter. Adjacent filters can be merged into a single filter by combining their predicates. However, these existing techniques are oblivious to the no- tion of contexts, and consequently do not avoid superfluous computations in the current contexts. </span></p><p class="c19"><span class="c2">To avoid unnecessary computations when event queries are executed &ldquo;out&rdquo; of their respective context windows, we introduce the context window push-down strategy. Context window push-down can prevent the continuous execution of operators in the query plan. In other words, no event will be passed up by the context window operator if the current event stream does not qualify for the context window. </span></p><p class="c127"><span class="c2">For instance, the two bottom most operators in Figure 6(a) are always executed regardless of the application contexts. Once the context window is pushed down to the bottom in Figure 6(b), it avoids the execution of all operators higher in the plan when they are irrelevant to the current contexts. All event queries in a combined query plan belong to the same context (Section 4.2). By definition, a context window specifies the scope of its queries. Thus, pushing a context window down does not change the semantics of its queries. That is, context window push down strategy is correct. </span></p><p class="c32"><span class="c2">Pushing a context window down seems similar to pushing a predicate or a traditional window down only at first sight. Differences are twofold: 1) Context windows suspend the entire query plan &ldquo;above them&rdquo; as long as the application is in different contexts. In contrast to that, a predicate or a traditional window is a filter on a stream that selects certain events to be passed through. It does not control the suspension of the above operators and keeps them in busy waiting state that wastes valuable resources and degrades system performance. 2) Our context-driven stream router directs event stream portions during application contexts to the appropriate event queries (Section 6.2). In contrast to that, predicates and traditional time constraints (e.g., event sequence within 4 </span></p><p class="c3 c121"><span class="c2">(b) Optimized query plan </span></p><p class="c56"><span class="c2">Figure 6: Query plans </span></p><p class="c66 c71"><span class="c2">hours [34]) typically filter events one by one at the individual event level. This is a resource consuming slow process. </span></p><p class="c136"><span class="c2">Theorem 1 proves that pushing a context window down in a combined query plan leads to lower execution costs than placing it at any other position in the query plan. </span></p><p class="c66 c102"><span class="c2">Theorem 1. Given a query q, let P be the set of all pos- sible query plans for q, p &isin; P be a query plan for q and cost(p) be the cost of executing the plan p. With the context window pushed down, the new plan, denoted by p&rsquo;, has cost cost(p&rsquo;). Then &forall;p &isin; P, p = p . cost(p ) &le; cost(p). </span></p><p class="c66 c99"><span class="c2">Proof. As described in Section 5.1, the cost of the con- text window operator is constant. That is, it adds constant cost to the overall execution costs of a query plan no matter its position in the query plan. The context window operator completely suspends the execution of all upstream operators while the context is not active. In that case, the cost of a query plan is reduced, i.e., cost(p) &lt; cost(p ). In an unlikely case when the context window happens to be always active, the costs of the query plans p and p are equal. </span><span class="c24">5.3 Context Workload Sharing </span></p><p class="c29"><span class="c2">Inspired by traditional multi-query optimization, group- ing the event queries enables computation sharing among these queries. We observe the opportunity that substantial computational savings can be achieved by executing only one instance of each context deriving query for each context. Without context window grouping, each context processing query has to run its respective context deriving queries sep- arately so to determine its current context to ensure correct execution. If this context analytics is performed on an in- dividual query level, significant computational resources are wasted and system responsiveness suffers. </span></p><p class="c0"><span class="c2">Sharing query workloads of overlapping context windows is challenging for the following reasons: (1) The duration of context windows may vary and their bounds are unknown at compile time. So, we have to infer whether context win- dows overlap. (2) Different contexts may contain identical or similar event queries in their query workloads. How to share query workloads driven by contexts is crucial to achieve high performance execution. We now propose an efficient solu- tion that addresses this problem by splitting and grouping overlapping context windows. </span></p><p class="c8"><span class="c2">For overlapping context windows, a naive solution would be to merge these context windows to form a larger encom- passing context window. Inside this large context window, </span></p><p class="c126"><span class="c16">418 </span></p><p class="c3 c57"><span class="c2">(a) Initial query plan </span></p><p class="c3 c57"><span class="c2">(a) Initial query plan </span></p><p class="c103"><span class="c2">Figure 7: Context Window Grouping We now propose an effective strategy for splitting the original user-defined overlapping context windows into finer granularity context windows and grouping them into non- overlapping context windows. For example, the context windows w</span><span class="c4">c</span><span class="c39">1 </span><span class="c2">and w</span><span class="c4">c</span><span class="c39">2 </span><span class="c2">in Figure 7 overlap. The window w</span><span class="c4">c</span><span class="c39">1 </span><span class="c13">is split into w</span><span class="c4">11 </span><span class="c13">and w</span><span class="c4">12</span><span class="c13">, while the window w</span><span class="c4">c</span><span class="c39">2 </span><span class="c2">is split into w</span><span class="c4">21 </span><span class="c13">and w</span><span class="c4">22</span><span class="c13">. Since the windows w</span><span class="c4">12 </span><span class="c13">and w</span><span class="c4">21 </span><span class="c13">cover the same </span><span class="c2">time interval, the event queries associated with them are merged to form the workload of the grouped context window w. The context deriving queries are adjusted accordingly. </span></p><p class="c3"><span class="c5">1 </span><span class="c14">Input : Set W of user-defined context windows. A window is </span></p><p class="c23"><span class="c14">described by start, end and queries. </span><span class="c5">3 </span><span class="c14">Output : Set G of grouped context windows G = W.extractNonOverlappingWindows() </span><span class="c5">5 </span><span class="c14">W = W.sortByStart() </span></p><p class="c3"><span class="c14">W = W.mergeIdenticalWindows() </span><span class="c5">7 </span><span class="c14">Q = &empty; </span></p><p class="c103"><span class="c14">while W.hasNextWindowBound() </span><span class="c5">9 </span><span class="c14">next = W.getNextWindowBound() </span></p><p class="c3"><span class="c14">S = W.getStartingW indows(next) </span><span class="c5">11 </span><span class="c14">E = W.getEndingW indows(next) </span></p><p class="c3"><span class="c14">i f Q.isEmpty() </span><span class="c5">13 </span><span class="c14">then Q = S.queries </span></p><p class="c3"><span class="c14">else new window w = (previous, next, Q) </span><span class="c5">15 </span><span class="c14">G = G &cup; w </span></p><p class="c3"><span class="c14">Q = Q &minus; E.queries &cup; S.queries </span><span class="c5">17 </span><span class="c14">end i f </span></p><p class="c3"><span class="c14">previous = next </span><span class="c5">19 </span><span class="c14">end while </span></p><p class="c3"><span class="c14">for each w &isin; G </span><span class="c5">21 </span><span class="c14">w.queries = w.queries.dropDuplicates() </span></p><p class="c3"><span class="c14">end for each </span><span class="c5">23 </span><span class="c14">return G </span></p><p class="c3"><span class="c2">Listing 1: Context window grouping algorithm </span></p><p class="c1"><span class="c2">Consider our context window grouping algorithm in List- ing 1. It takes a set of user-defined context windows as input. Context windows which do not overlap any other window remain unchanged (line 4). The algorithm sorts the overlapping context windows in increasing order by start time (line 5). Even though the exact start time of context </span></p><p class="c1"><span class="c2">we can now analyze the associated event query workloads to optimize their executions. However, such solution could do more harm than good in some cases. For example, if all context windows were to be overlapping, then only one huge all encompassing context window would be formed as a re- sult. This would forfeit the purpose of being context-aware. Consequently, redundant computations would be incurred. </span></p><p class="c23"><span class="c2">windows is not known at compile time, the order of their beginning can be determined for overlapping context win- dows. For example, it is known at compile time that the window than the wwindow </span><span class="c4">c</span><span class="c39">2 </span><span class="c2">text windows, in Figure 7 will start at the same time or later the algorithm w</span><span class="c10">c</span><span class="c39">1</span><span class="c2">. If there are several identical con- only keeps one by merging the workloads of identical windows (line 6). The core of the algo- rithm (lines 8-19) forms a new grouped context window for each time interval between two subsequent bounds of origi- nal context windows and associates the query workload with it that is appropriate during this time interval. For each grouped context window, the algorithm deletes duplicate event queries (lines 20-22) and returns the set of grouped context windows (line 23). Since several subsequent grouped context windows correspond to one original context window, an event query within a grouped context window may need access to its partial matches in the previous grouped context windows to ensure completeness of its results. In Section 6, we introduce a customized design (called context history) to ensure correct grouped context window execution. </span></p><p class="c23"><span class="c2">The time complexity of the algorithm is O(n log(n) &lowast; m) where n is the number of original user-defined context types that are sorted (line 5) and m is the number of predicates that have to be analyzed while comparing two context types. Based on the newly produced non-overlapping context windows, we now further exploit traditional multi-query op- timization (MQO) techniques [31, 27, 21] to produce an optimized shared query execution plan for each group of event queries. This opens opportunities to share the simi- lar workload within a context which further saves computa- tional costs and reduces query latency. MQO is an NP-Hard problem due to the exponential search space. Thus, the so- lutions [31, 27, 21] of MQO tend to be expensive. </span></p><p class="c1"><span class="c2">Our context window grouping solution divides the event query workloads into smaller groups based on their time overlap. Hence, the search space for an optimal query plan within each group is substantially reduced compared to the global space. Any state-of-the-art MQO solution can lever- age this idea to return an optimized query plan efficiently. </span></p><p class="c1"><span class="c2">The search space for multi-query optimization is doubly exponential in the size of the queries (n). The spectrum of possible multi-query groupings ranges from a separate group per each individual query (i.e., non-sharing) in the given query workload to a single group for all queries. The upper- bound for all possible multi-query groups corresponds to the number of distinct ways of assigning n event queries to one or more groups. The number that describes this value is the Bell number B</span><span class="c4">n</span><span class="c13">, which represents the number of different </span><span class="c2">groupings of a set of n elements. The Bell number is the sum of Stirling numbers. A Stirling number S(n, k) is the number of ways to partition n elements into k partitions [20]: </span></p><p class="c3"><span class="c2">B</span><span class="c4">n </span><span class="c13">= </span></p><p class="c3"><span class="c13">&sum;</span><span class="c10">n</span><span class="c4">k=1</span><span class="c2">( </span></p><p class="c3"><span class="c2">1</span><span class="c13">k! </span></p><p class="c3"><span class="c13">&sum;</span><span class="c10">k</span><span class="c4">j=1</span><span class="c2">(&minus;1)</span><span class="c7">k&minus;j</span><span class="c25">(</span><span class="c2">n</span><span class="c13">k</span><span class="c2">)</span><span class="c13">j</span><span class="c10">n</span><span class="c25">) </span><span class="c2">By dividing our n queries first into m groups, we subse- quently only need to optimize the small shared set one by one and thus reduce the search space to: </span></p><p class="c3"><span class="c2">B </span><span class="c4">n </span><span class="c2">= </span></p><p class="c3"><span class="c13">&sum;</span><span class="c10">n</span><span class="c4">k=1</span><span class="c2">S(n, k) = </span></p><p class="c3"><span class="c10">n/m</span><span class="c13">&sum;</span><span class="c4">k =1</span><span class="c2">&#63723;</span><span class="c13">&#63725; k </span><span class="c2">1</span><span class="c13">! </span></p><p class="c3"><span class="c2">)</span><span class="c13">j</span><span class="c10">n/m</span><span class="c25">&#63734;</span><span class="c2">&#63736; </span></p><p class="c3"><span class="c2">As confirmed by our experimental study in Section 7.2, </span></p><p class="c3"><span class="c16">419 </span></p><p class="c3"><span class="c10">n/m</span><span class="c13">&sum;&sum;</span><span class="c10">k </span><span class="c4">k =1 </span></p><p class="c3"><span class="c4">j=1</span><span class="c25">(</span><span class="c2">S(n/m, k ) = </span></p><p class="c3"><span class="c2">(&minus;1)</span><span class="c7">k &minus;j</span><span class="c2">n/m</span><span class="c13">k </span></p><p class="c111"><span class="c2">the </span><span class="c14">CAESAR </span><span class="c2">optimizer produces a context-aware query plan 2712 times faster than the state-of-the-art context-independent multi-query optimization approaches. </span></p><p class="c133"><span class="c24">6. CAESAR EXECUTION INFRASTRUCTURE </span></p><p class="c100"><span class="c24">6.1 Overview of the CAESAR Infrastructure </span></p><p class="c11"><span class="c2">Figure 8 shows the </span><span class="c14">CAESAR </span><span class="c2">execution infrastructure. The boxes represent the system components. </span></p><p class="c130"><span class="c2">Figure 8: CAESAR infrastructure Specification Layer. A </span><span class="c14">CAESAR </span><span class="c2">model is specified by the application designer using the visual </span><span class="c14">CAESAR </span><span class="c2">editor [25]. As explained in Section 4, we then translate it into an alge- braic query plan. </span></p><p class="c19"><span class="c2">Optimization Layer. As described in Section 5, the query plan is optimized using several context-aware opti- mization strategies to produce an execution plan. </span></p><p class="c19"><span class="c2">Execution Layer. The optimized query plan is for- warded to the transaction manager that forms transactions. These transaction are submitted for execution by the sched- uler that guarantees correctness. These components build the core of the </span><span class="c14">CAESAR </span><span class="c2">execution infrastructure. They are described in details in Section 6.2. </span></p><p class="c109"><span class="c2">Storage Layer. The event distributor buffers the in- coming events in the event queues. The current context windows and the context history are compactly maintained in-memory. The garbage collector ensures that only the val- ues which are relevant to the current contexts are kept. </span><span class="c24">6.2 Core of the CAESAR Infrastructure </span></p><p class="c41"><span class="c2">The core of the </span><span class="c14">CAESAR </span><span class="c2">execution infrastructure consists of the context derivation, context processing, context-aware stream routing and scheduling of these processes (Figure 9). Context Derivation. For each stream partition (unidi- rectional road segment in the traffic management use case), we save which context windows currently hold in the context bit vector W. This vector W has a time stamp W.time and a one-bit entry for each context type, i.e., W.size = |C|. The entries are sorted alphabetically by context names to allow for constant time access. The entry 1 (0) for a context c means that the context window w</span><span class="c4">c </span><span class="c13">holds (does not hold) </span><span class="c2">at the time W.time. Since context windows may overlap, multiple entries in the vector may be set to 1. </span></p><p class="c32"><span class="c2">The context window vector is updated by the context de- riving queries. Since each event in the stream can potentially </span></p><p class="c3 c134"><span class="c2">Figure 9: Core of the </span><span class="c14">CAESAR </span><span class="c2">infrastructure </span></p><p class="c51 c72"><span class="c2">Context-aware stream routing is a light-weight process for the following reasons. First, the lookup of all vector entries set to 1 takes constant time. Second, this routing happens for stream batches (multiple subsequent events in the input event stream) rather than for single events. </span></p><p class="c8"><span class="c2">Context Processing. The </span><span class="c14">CAESAR </span><span class="c2">model allows the application designer to specify the scope of event queries in terms of their context windows. When a user-defined context window ends, all event queries associated with it are suspended and thus will not produce new matches until they become activated again. Therefore, their partial matches, called context history, can be safely discarded. </span></p><p class="c8"><span class="c2">When a user-defined context window w</span><span class="c4">c </span><span class="c13">with its associ- </span><span class="c2">ated query workload Q</span><span class="c7">c </span><span class="c2">is split into smaller non-overlapping context windows w</span><span class="c10">c</span><span class="c39">1 </span><span class="c2">and w</span><span class="c10">c</span><span class="c39">2 </span><span class="c2">partial matches of the queries Q</span><span class="c7">c </span><span class="c2">are maintained across these newly grouped windows w</span><span class="c4">c</span><span class="c39">1 </span><span class="c13">and w</span><span class="c4">c</span><span class="c39">2 </span><span class="c2">to ensure correctness of these queries Q</span><span class="c7">c</span><span class="c2">. Therefore, for each event query we save the grouped context windows across which the results of the query are kept. For exam- ple, the event query q</span><span class="c4">1 </span><span class="c13">in Figure 7 is executed during all 3 </span><span class="c2">grouped context windows. However, when the third window begins, the partial results within the first window expire. </span></p><p class="c8"><span class="c2">Correct Context Management. Context processing queries are dependent on the results of context deriving queries. Due to bursty input streams, network and pro- cessing delays context derivation might not happen on time. To avoid race conditions, these inter-dependencies must be taken into account to guarantee correct execution. </span></p><p class="c0"><span class="c2">We define a stream transaction as a sequence of operations that are triggered by all input events with the same time stamp. The application time stamp of a transaction (and all its operations) coincides with the application time stamp of the triggering events. An algorithm for scheduling read and write operations on the shared context data is correct if conflicting operations</span><span class="c7">4 </span><span class="c2">are processed sorted by time stamps. </span></p><p class="c128"><span class="c10">4</span><span class="c13">Two operations on the same value such that at least one of </span></p><p class="c1 c107"><span class="c2">update a context window, the context deriving queries pro- cesses all input events. W.time is the application time when the vector W was last updated. Since events arrive in-order by time stamps, only one most recent version of the context bit vector is kept. </span></p><p class="c58"><span class="c2">Context-aware Stream Routing. Based on the con- text window vector, the system is aware of the currently active event query workloads. For each current context win- dow w</span><span class="c4">c</span><span class="c13">, it routes all its events to the query plan associated </span><span class="c2">with the context c. Query plans of all currently inactive con- text windows do not receive any input. They are suspended to avoid busy waiting, i.e., waste of resources. </span></p><p class="c15"><span class="c16">420 </span></p><p class="c90"><span class="c2">While existing transaction schedulers can be deployed in the </span><span class="c14">CAESAR </span><span class="c2">system, we now describe a time-driven sched- uler. For each time stamp t, our scheduler waits till the event distributor progress is larger than t and the context deriva- tion for all transactions with time stamps smaller than t is completed. Then, the scheduler extracts all events with the time stamp t from the event queues, wraps their processing into transactions (one transaction per road segment for the traffic control use case) and submits them for execution. </span></p><p class="c96"><span class="c24">7. PERFORMANCE EVALUATION </span></p><p class="c112"><span class="c24">7.1 Experimental Setup and Methodology </span></p><p class="c33"><span class="c2">Experimental Infrastructure. We have implemented our </span><span class="c14">CAESAR </span><span class="c2">system in Java with JRE 1.7.0 25 running on Linux CentOS release 6.3 with 16-core 3.4GHz QEMU Vir- tual CPU and 48GB of RAM. We execute each experiment three times and report their average results here. </span></p><p class="c19"><span class="c2">Linear Road Benchmark. We have chosen this bench- mark [9] to evaluate the effectiveness of the </span><span class="c14">CAESAR </span><span class="c2">system for the following reasons: (1) it expresses a variety of appli- cation contexts such that the system reactions to an event depends on the current context, and (2) it is time critical since it poses tight latency constraint of 5 seconds. </span></p><p class="c19"><span class="c2">Event Queries. We focused on a subset of event queries of the benchmark that based on input events derive toll notifications and accident warnings. The queries depicted in Figure 3 are simplified versions of the actual benchmark queries to illustrate the key concepts of our model in the paper only. We simulate low, average and high query work- loads by replicating the event queries of the benchmark. </span></p><p class="c3 c66"><span class="c2">data set (1.6GB) [26]. It contains physical activity reports from 14 people during 1 hour 15 minutes. </span></p><p class="c0"><span class="c2">Metrics. We measure two metrics common for stream systems, namely maximal latency and scalability. Addition- ally, we measure the win ratio of context-aware over context independent event stream analytics in terms of CPU pro- cessing time. Maximal latency is the maximal time interval elapsed from the event arrival time (i.e., system time when a position report is generated) till the complex event deriva- tion time (i.e., system time when a toll notification or an accident warning is derived based on this position report). As mentioned above, the benchmark restricts the query la- tency to be within 5 seconds. The system scalability is de- termined by the L-factor, the maximal number of roads that are processed without violating this constraint. The win ra- tio of context-aware over context-independent event stream analytics is computed as the maximal latency of context- independent processing divided by the maximal latency of context-aware processing of the same event query workload against the same input event stream. </span></p><p class="c12"><span class="c2">Methodology. To show the efficiency of our context- aware query optimization, we compare it to the exhaustive search approach by varying the number of operators in a query plan. To measure the effectiveness of our optimized context-aware query plan, we compare the performance of our context-aware query execution plan to the state-of-the- art solution [34, 5]. To demonstrate the effectiveness of our context-aware workload sharing technique, we compare it to our default non-sharing solution. We conduct these compar- isons by varying the following parameters: number of oper- ators in a query plan, input event stream rate, number of event queries, data distribution in a context window, length of a context window, number of context windows, number of overlapping context windows, overlapping ratio of con- text windows, and the shared workload size. Since context windows are derived from the input events, context window related parameters can be varied only through input data manipulation. Thus, for the experiments on real data set we vary the number of event queries. </span><span class="c24">7.2 Efficiency of CAESAR Optimizer </span></p><p class="c3 c88"><span class="c2">(a) Events per road segment </span></p><p class="c3 c116"><span class="c2">(b) Events per minute </span></p><p class="c3 c51"><span class="c2">Methodology. To show the efficiency of our context- aware query optimization, we compare it to the exhaustive search approach by varying the number of operators in a query plan. To measure the effectiveness of our optimized context-aware query plan, we compare the performance of our context-aware query execution plan to the state-of-the- art solution [34, 5]. To demonstrate the effectiveness of our context-aware workload sharing technique, we compare it to our default non-sharing solution. We conduct these compar- isons by varying the following parameters: number of oper- ators in a query plan, input event stream rate, number of event queries, data distribution in a context window, length of a context window, number of context windows, number of overlapping context windows, overlapping ratio of con- text windows, and the shared workload size. Since context windows are derived from the input events, context window related parameters can be varied only through input data manipulation. Thus, for the experiments on real data set we vary the number of event queries. </span><span class="c24">7.2 Efficiency of CAESAR Optimizer </span></p><p class="c3 c51"><span class="c2">Methodology. To show the efficiency of our context- aware query optimization, we compare it to the exhaustive search approach by varying the number of operators in a query plan. To measure the effectiveness of our optimized context-aware query plan, we compare the performance of our context-aware query execution plan to the state-of-the- art solution [34, 5]. To demonstrate the effectiveness of our context-aware workload sharing technique, we compare it to our default non-sharing solution. We conduct these compar- isons by varying the following parameters: number of oper- ators in a query plan, input event stream rate, number of event queries, data distribution in a context window, length of a context window, number of context windows, number of overlapping context windows, overlapping ratio of con- text windows, and the shared workload size. Since context windows are derived from the input events, context window related parameters can be varied only through input data manipulation. Thus, for the experiments on real data set we vary the number of event queries. </span><span class="c24">7.2 Efficiency of CAESAR Optimizer </span></p><p class="c101"><span class="c2">Figure 10: Event streams Event Streams. Event distribution across road seg- ments varies. Figure 10(a) shows the number of processed and derived events per segment of a randomly chosen unidi- rectional road</span><span class="c7">7</span><span class="c2">. There are more cars in some road segments than in others. In some road segments accidents and traffic jams happen more often than in others. Hence, more toll notifications and accident warnings are triggered for them. </span></p><p class="c19"><span class="c2">Event distribution across time also varies. Event rate gradually increases during 3 hours of an experiment. Fig- ure 10(b) shows the number of processed and derived events per minute by for a randomly chosen unidirectional road segment</span><span class="c7">7 </span><span class="c2">and visualizes the application contexts. Accident warnings are derived only during accidents (minutes 30-50). The benchmark requires zero toll derivation during accidents and clear road conditions (minutes 0-70). During traffic jams, real toll is computed (minutes 70-180). </span></p><p class="c32"><span class="c2">Real Data Set. In addition to the benchmark, we eval- uate our system using the physical activity monitoring real them is a write are called conflicting operations. </span><span class="c10">7</span><span class="c13">We have observed a similar event distribution for other roads and roads segments. </span></p><p class="c3 c38"><span class="c2">(b) L-factor </span></p><p class="c30"><span class="c2">Figure 11: </span><span class="c14">CAESAR </span><span class="c2">Optimization Techniques </span></p><p class="c51 c123"><span class="c2">In Figure 11(a), we vary the number of operators in a query plan and measure the CPU time required for the query plan search (logarithmic scale on Y-axis). We compare the context-independent (CI) exhaustive to the context-aware (CA) greedy query plan search. As confirmed by the search space analysis (Section 5.3), the processing time of the ex- haustive search grows exponentially with the number of op- erators in a query plan. In contrast, the CPU time required for our context-aware search stays fairly constant while vary- ing the query plan size. At size 24, </span><span class="c14">CAESAR</span><span class="c2">&rsquo;s optimizer is 2712-fold faster than the exhaustive search. This is due to </span></p><p class="c126"><span class="c16">421 </span></p><p class="c3 c64"><span class="c2">(a) </span><span class="c14">CAESAR </span><span class="c2">optimizer </span></p><p class="c3 c64"><span class="c2">(a) </span><span class="c14">CAESAR </span><span class="c2">optimizer </span></p><p class="c3 c64"><span class="c2">(a) </span><span class="c14">CAESAR </span><span class="c2">optimizer </span></p><p class="c3"><span class="c2">processing the fact that our context window push down and context </span></p><p class="c3"><span class="c2">is 8-fold faster than the context-independent solu- grouping techniques substantially reduce the search space. </span></p><p class="c3"><span class="c2">tion using the Linear Road benchmark data (LR). </span><span class="c14">CAESAR </span></p><p class="c3"><span class="c24">7.3 Efficiency of CAESAR Runtime </span></p><p class="c3"><span class="c2">achieves the same win using the Physical Activity Monitor- ing data set (PAM) and 20 event queries. Our system has a Next, we conduct a comprehensive evaluation to demon- strate the efficiency of our </span><span class="c14">CAESAR </span><span class="c2">solution. The baseline approach is the context-independent processing commonly used in most state-of-the-art solutions [34, 5, 32]. We first demonstrate the superiority of our </span><span class="c14">CAESAR</span><span class="c2">&rsquo;s context-aware event processing compared to the state-of-the-art context- independent approach by strictly following the constraints of the benchmark. Then we evaluate the </span><span class="c14">CAESAR</span><span class="c2">&rsquo;s context window sharing technique. </span></p><p class="c1"><span class="c2">clear win in this case because the context-aware event stream analytics suspends those event queries which are irrelevant to the current context. </span></p><p class="c23"><span class="c2">Varying event stream rates. In Figure 12(b), we vary the number of considered roads. We measure the maximal latency of context-aware versus context-independent pro- cessing. The maximal latency grows linearly with an in- creasing input stream rate (number of roads). For 7 roads, context-aware processing is 9-fold faster than the context independent solution. The results show that the </span><span class="c14">CAESAR </span><span class="c114">7.3.1 Efficiency of Context-Aware Stream Analytics </span></p><p class="c1"><span class="c2">L-factor. In Figure 11(b), we vary the input stream rate by increasing the number of roads and measure the maximal latency. We compare the latency of the optimized versus non-optimized query plan. As the figure shows, the opti- mized query plan processes at most 7 roads without vio- lating the latency constraint of 5 seconds. In contrast, the non-optimized query plan can process at most 5 roads under this constraint. Intuitively, our context-aware optimization approach successfully avoids the unnecessary computations by executing different queries only at appropriate time pe- riods (contexts). On the other hand, the state-of-the-art solution suffers from executing all queries all the time. </span></p><p class="c1"><span class="c2">Next, we compare continuous context-independent stream processing to context-aware solution when some of the in- volved event query workloads are appropriate only in certain critical contexts and can be suspended in other contexts. Unless stated otherwise, in Figures 12 and 13, we consider 3 roads (1.7GB) and assume that 2 critical non-overlapping context windows of length 3 minutes process 10 event queries each. These queries can be suspended in other contexts. </span></p><p class="c3"><span class="c2">Evaluating diverse context window distributions. </span></p><p class="c3"><span class="c2">system is more robust to the event stream rate increase com- pared to the context-independent solution. </span></p><p class="c1"><span class="c2">Varying context window lengths. In Figure 12(c), we vary the length of the context windows and measure the win ratio of the context-aware over context-independent process- ing. The numbers above the bars indicate the percentage of the input event stream covered by the context windows that allow suspension of complex event query workload. Given that </span><span class="c14">CAESAR </span><span class="c2">only keeps one context active at a time, the win ratio exceeds 3 if such context windows cover more than 80% of the stream. It becomes negligible (almost 1) when they cover less than 50% of the input event stream. </span></p><p class="c1"><span class="c2">Varying the number of context windows. A similar trend can be observed while varying the number of context windows that allow suspension of irrelevant event queries (Figure 12(d)). Again, we measure the win ratio of the context-aware over context-independent stream processing. The numbers above the bars indicate the percentage of the input event stream covered by the context windows. Simi- larly, the win ratio exceeds 2 if the context windows cover more than 80% of the input event stream. It becomes neg- ligible (almost 1) when they cover less than 50%. </span></p><p class="c3"><span class="c2">In Figure 13, we vary the number of event queries per con- text window and measure the maximal latency. We compare </span></p><p class="c3"><span class="c114">7.3.2 Efficiency of Context-Aware Workload Sharing </span><span class="c2">three setups, namely, uniform context window distribution </span></p><p class="c3"><span class="c2">Next, we measure the effect of the shared workload pro- versus their Poisson distribution with positive skew (&lambda; is the </span></p><p class="c3"><span class="c2">cessing of overlapping context windows (Figure 14). Unless first second) and with negative skew (&lambda; is the last second). </span></p><p class="c3"><span class="c2">stated otherwise, 30 windows of length 15 minutes each over- Context window bounds vary across different window distri- </span></p><p class="c3"><span class="c2">lap by 10 minutes. Each of them processes 4 event queries. butions. The rest of the stream is identical for these setups. </span></p><p class="c3"><span class="c2">Varying the number of overlapping context win- As expected, when the context windows are at the end </span></p><p class="c3"><span class="c2">dows. In Figure 14(a), we vary the maximal number of over- of the experiment where the stream rate is high, the maxi- </span></p><p class="c3"><span class="c2">lapping context windows and measure the maximal latency mal latency remains almost constant with the growing event </span></p><p class="c3"><span class="c2">of shared versus non-shared query processing. As expected, query workload. This is explained by the fact that most </span></p><p class="c3"><span class="c2">the larger the number of overlapping context windows are queries are irrelevant for these contexts and thus are sus- </span></p><p class="c3"><span class="c2">the more significant is the gain of the event query sharing. If pended. In contrast, if these windows are uniformly dis- </span></p><p class="c3"><span class="c2">45 context windows overlap, the workload sharing strategy tributed or are at the beginning of the experiment when the </span></p><p class="c3"><span class="c2">outperforms the default non-shared solution by factor of 10. stream rate is low, the maximal latency grows linearly with </span></p><p class="c3"><span class="c2">The reason is that the </span><span class="c14">CAESAR </span><span class="c2">context window grouping the number of queries. The maximal latency of 20 event </span></p><p class="c3"><span class="c2">technique exploits the sharing opportunities within overlap- queries with uniform context window distribution is 1.8-fold </span></p><p class="c3"><span class="c2">ping context windows at a fine granularity level by splitting faster than with Poisson distribution with positive skew and </span></p><p class="c3"><span class="c2">the overlapping context windows into non-overlapping parts 11-fold slower than with Poisson distribution with negative </span></p><p class="c3"><span class="c2">and sharing event query processing within them. skew. Thus, to achieve fair results we consider uniform con- </span></p><p class="c3"><span class="c2">Varying the length of context window overlap. In text window distribution in all following experiments. </span></p><p class="c3"><span class="c2">Figure 14(b), we vary the minimal length of context window Scaling event query workload. In Figure 12(a), we </span></p><p class="c3"><span class="c2">overlap and measure the maximal latency of shared versus vary the number of event queries per context window and </span></p><p class="c3"><span class="c2">non-shared workload execution. The gain of sharing grows measure the maximal latency of context-aware versus context- </span></p><p class="c3"><span class="c2">linearly with the length of overlap. If 30 context windows independent event stream processing. The maximal latency </span></p><p class="c3"><span class="c2">overlap by 15 minutes, our workload sharing strategy per- grows linearly in the number of event queries. For an average </span></p><p class="c3"><span class="c2">forms 6-fold faster than the non-shared solution. This is due workload of 10 event queries, we find that the context-aware </span></p><p class="c3"><span class="c2">to the fact that similar workloads can be shared for a longer </span></p><p class="c3"><span class="c16">422 </span></p><p class="c3"><span class="c2">(c) </span><span class="c14">Shared workload size </span><span class="c2">Figure 13: Context window distribution </span></p><p class="c3"><span class="c2">Figure 14: Shared workload of overlapping context windows </span></p><p class="c23"><span class="c2">time period, and hence more computational savings can be harvested from the overlapping part of the context windows. Shared workload size. In Figure 14(c), we vary the number of event queries per context window and measure the maximal latency of shared versus non-shared event query processing. As expected, the more event queries can be shared the more significant is the gain of the event query sharing. If each context window contains 10 queries that can be shared with other context windows, the workload sharing strategy outperforms the default non-shared solu- tion by factor of 9 using the Linear Road benchmark data (LR). A similar trend can be observed with the Physical Activity Monitoring data set (PAM). </span></p><p class="c3"><span class="c24">8. RELATED WORK </span></p><p class="c1"><span class="c2">Context-aware Event Stream Models [11, 33] pro- pose a fuzzy ontology to support uncertainty in event queries. Context-aware event queries are rewritten into context-inde- pendent and processed in parallel on different stream parti- tions. Isoyama et al. [18] propose to allocate event queries to event processors so that the state of event processing (i.e., in- termediate event query results) is efficiently managed. These ideas are orthogonal to our optimization techniques. </span></p><p class="c1"><span class="c2">Hermosillo et al. [17] and Proton software prototype [1] feature a model similar to the </span><span class="c14">CAESAR </span><span class="c2">model. However, these approaches lack a formal definition of the event lan- guage, optimization techniques and experimental evaluation. The recent emergence of these approaches confirms the im- portance of the notion of context-aware event queries that has been formally defined by us in [25]. Our current work on </span><span class="c14">CAESAR </span><span class="c2">now is completed by the context-aware optimiza- tion strategies and their experimental evaluation. </span></p><p class="c1"><span class="c2">Event Stream Processing Automata [34, 5, 13] con- tinuously evaluate the same set of single event queries us- ing traditional windows of fixed length. These automata capture single query processing states. Their runs corre- </span></p><p class="c1"><span class="c2">spond to independent event query instances. In contrast to that, the </span><span class="c14">CAESAR </span><span class="c2">model expresses the semantics of the whole stream-based application rather than single isolated event queries. Our model captures application contexts of variable, statically unknown duration. Its runs correspond to interdependent processes traveling through application contexts, triggering appropriate context-aware event queries and incrementally maintaining their results. </span></p><p class="c1"><span class="c2">Event Query Languages, like CQL [10] and SASE [5, 34], lack explicit support of application contexts. These contexts could possibly be hard-coded by queries deriving events that mark context bounds. However, this approach is cumbersome and error prone. It is neither modular nor human-readable. It requires tedious specification of multi- ple complex event queries &ndash; placing an unnecessary burden on the designer [25]. Inter-dependencies between context deriving and context processing queries would have to be taken into account to avoid re-computations, waste of valu- able resources, delayed responsiveness and even incorrect results. Special optimization techniques would have to be developed to enable the benefits of context-awareness &ndash; as accomplished by our approach (Section 3.2). Furthermore, workload sharing among overlapping context windows has not been addressed in prior research. </span></p><p class="c1"><span class="c2">Event Query Optimization Techniques [29] are often based on stream algebras [30, 34, 12]. Operators of these stream algebras work on events. In these approaches, appli- cation contexts are not supported as first-class citizens and thus they are not available for the operators. Hence, these approaches miss the event query optimization opportunities enabled by context-aware stream processing. Application contexts are first class citizens in our CAESAR model. They enable context-aware optimization techniques (Section 5). </span></p><p class="c1"><span class="c2">Business Process Models [16, 28] explicitly support ap- plication contexts in a readable manner and allow to specify context-aware system reactions. However, these models tar- get business process specification. They were not designed </span></p><p class="c3"><span class="c16">423 </span></p><p class="c3"><span class="c2">(a) </span><span class="c14">Event query workload </span></p><p class="c3"><span class="c2">(b) </span><span class="c14">Event stream rate </span></p><p class="c3"><span class="c2">(d) </span><span class="c14">Context window number </span></p><p class="c3"><span class="c2">Figure 12: Context-aware event stream analytics </span></p><p class="c3"><span class="c2">(a) </span><span class="c14">Degree of overlap </span></p><p class="c3"><span class="c2">(c) </span><span class="c14">Context window length </span></p><p class="c3"><span class="c2">(b) </span><span class="c14">Length of overlap </span></p><p class="c135"><span class="c2">for CEP and neglect its peculiarities. In particular, the event-driven nature of streaming applications and the im- portance of temporal aspect are not given enough attention. Indeed, context transitions in these models are triggered by conditions and process flow rather than by events [19]. Tem- poral constraints are specified on clocks rather than on event time stamps [7]. These models do not derive higher level knowledge in form of complex events. </span></p><p class="c80"><span class="c24">9. CONCLUSIONS </span></p><p class="c33"><span class="c2">The responsiveness of time-critical decision-making event stream processing applications can be substantially speed- up by evaluating only those event queries which are relevant to the current situation. Inspired by this observation, we propose the first context-aware event stream processing so- lution, called </span><span class="c14">CAESAR</span><span class="c2">, which is composed of the following key components: (1) To allow for human-readable context- aware event query specification, we propose the </span><span class="c14">CAESAR </span><span class="c2">model that visually captures the application contexts and allows the designer to associate appropriate event queries with each context. (2) To achieve prompt system responsive- ness, the model is translated into a query plan composed of the context-aware operators of the </span><span class="c14">CAESAR </span><span class="c2">algebra we pro- pose. This algebra serves as a foundation for the </span><span class="c14">CAESAR </span><span class="c2">optimizer that suspends those event queries which are irrele- vant to the current application context and detects workload sharing opportunities of overlapping contexts. (3) We built the </span><span class="c14">CAESAR </span><span class="c2">runtime execution infrastructure that guaran- tees correct and efficient execution of inter-dependent con- text deriving and context processing queries. The context- aware processing is shown to perform 8-fold faster on aver- age than the context-independent solution when using the Linear Road stream benchmark and real world data sets. </span></p><p class="c115"><span class="c24">ACKNOWLEDGEMENTS </span><span class="c2">This work was supported by NSF grants IIS 1018443 and IIS 1343620. </span></p><p class="c78"><span class="c24">10. REFERENCES </span></p><p class="c91"><span class="c18">[1] Proton. </span><span class="c70">https://github.com/ishkin/Proton</span><span class="c18">, 2015. [Online; </span></p><p class="c26"><span class="c18">accessed 10-December-2015]. [2] The Wall Street Journal. </span><span class="c70">http://www.wsj.com/articles/ </span></p><p class="c87"><span class="c70">SB10001424052970203733504577024000381790904</span><span class="c18">, 2015. [Online; accessed 24-July-2015]. [3] USA Today. </span><span class="c70">http://usatoday30.usatoday.com/news/nation/ </span></p><p class="c86"><span class="c70">2011-05-25-traffic-pollution-premature-deaths-emissions_n. htm</span><span class="c18">, 2015. [Online; accessed 24-July-2015]. [4] Wikipedia. </span><span class="c70">https://en.wikipedia.org/wiki/List_of_countries_ </span></p><p class="c46"><span class="c70">by_traffic-related_death_rate</span><span class="c18">, 2015. [Online; accessed 24-July-2015]. [5] J. Agrawal et al. Efficient pattern matching over event streams. </span></p><p class="c98"><span class="c18">In Proc. of Int. Conf. on Management of data, SIGMOD &rsquo;08, pages 147&ndash;160. ACM, 2008. [6] M. Akdere et al. Plan-based complex event detection across distributed sources. Proc. VLDB Endow., 1(1):66&ndash;77, Aug. 2008. [7] R. Alur et al. Automata for modeling real-time systems. In </span></p><p class="c17"><span class="c18">Proc. of Int. Colloquium on Automata, Languages and Programming, ICALP&rsquo;90, pages 322&ndash;335. Springer, 1990. [8] A. Arasu and J. Widom. Resource sharing in continuous </span></p><p class="c9"><span class="c18">sliding-window aggregates. In Proc. of Int. Conf. on Very Large Data Bases - Volume 30, VLDB &rsquo;04, pages 336&ndash;347. VLDB Endowment, 2004. [9] A. Arasu et al. Linear road: A stream data management </span></p><p class="c132"><span class="c18">benchmark. In Proc. of Int. Conf. on Very Large Data Bases, volume 30 of VLDB&rsquo;04, pages 480&ndash;491. VLDB Endowment, 2004. </span></p><p class="c3 c65"><span class="c18">[10] A. Arasu et al. The CQL continuous query language: semantic </span></p><p class="c40"><span class="c18">foundations and query execution. The VLDB Journal, 15(2):121&ndash;142, 2006. [11] K. Cao et al. Context-aware distributed complex event </span></p><p class="c104"><span class="c18">processing method for event cloud in internet of things. Journal of Advances in Information Science and Service Sciences, 5(8):1212&ndash;1222, April 2013. [12] S. Chakravarthy et al. Integrating stream and complex event </span></p><p class="c122"><span class="c18">processing. In Stream Data Processing: A Quality of Service Perspective, volume 36 of Advances in Database Systems, pages 187&ndash;214. Springer US, 2009. [13] A. Demers et al. Cayuga: A general purpose event monitoring </span></p><p class="c124"><span class="c18">system. In Proc. of Int. Conf. on Innovative Data Systems Research, pages 411&ndash;422, 2007. [14] K. P. Eswaran et al. The notions of consistency and predicate </span></p><p class="c75"><span class="c18">locks in a database system. Commun. ACM, 19(11):624&ndash;633, 1976. [15] T. M. Ghanem et al. Exploiting predicate-window semantics </span></p><p class="c82"><span class="c18">over data streams. SIGMOD Rec., 35(1):3&ndash;8, Mar. 2006. [16] A. Grosskopf et al. The Process: Business Process Modeling </span></p><p class="c61"><span class="c18">using BPMN. Meghan Kiffer Press, 2009. [17] G. Hermosillo et al. Complex Event Processing for </span></p><p class="c54"><span class="c18">Context-Adaptive Business Processes. In Belgium-Netherlands Software Evolution Seminar, pages 19&ndash;24, Dec. 2009. [18] K. Isoyama et al. A scalable complex event processing system and evaluations of its performance. In Proc. of Int. Conf. on Distributed Event-Based Systems, pages 123&ndash;126, New York, NY, USA, 2012. ACM. [19] F. Joyce. Programming Logic and Design, Comprehensive. </span></p><p class="c6"><span class="c18">Thomson, 2008. [20] M. Klazar. Bell numbers, their relatives, and algebraic </span></p><p class="c34"><span class="c18">differential equations. J. Comb. Theory, Ser. A, 102(1):63&ndash;87, 2003. [21] W. Le et al. Scalable multi-query optimization for sparql. In </span></p><p class="c81"><span class="c18">ICDE, pages 666&ndash;677, 2012. [22] J. Li et al. No pane, no gain: Efficient evaluation of </span></p><p class="c63"><span class="c18">sliding-window aggregates over data streams. SIGMOD Rec., 34(1):39&ndash;44, Mar. 2005. [23] M. Liu et al. E-Cube: Multi-dimensional event sequence </span></p><p class="c74"><span class="c18">analysis using hierarchical pattern query sharing. In Proc. of Int. Conf. on Management of data, SIGMOD&rsquo;11, pages 889&ndash;900. ACM, 2011. [24] Y. Mei and S. Madden. ZStream: A Cost-based Query </span></p><p class="c52"><span class="c18">Processor for Adaptively Detecting Composite Events. In Proc. of the SIGMOD Int. Conf. on Management of Data, SIGMOD&rsquo;09, pages 193&ndash;206. ACM, 2009. [25] O. Poppe et al. The HIT model: Workflow-aware event stream </span></p><p class="c49"><span class="c18">monitoring. In A. Hameurlain et al., editor, Advanced data stream management and continuous query processing, volume 8 of Transactions on large-scale data and knowledge-centered systems, pages 26&ndash;50. Springer, 2013. [26] A. Reiss et al. Creating and benchmarking a new dataset for </span></p><p class="c89"><span class="c18">physical activity monitoring. In Proc. of Int. Conf. on PErvasive Technologies Related to Assistive Environments, PETRA&rsquo;12, pages 40:1&ndash;40:8. ACM, 2012. [27] P. Roy et al. Efficient and extensible algorithms for multi query </span></p><p class="c120"><span class="c18">optimization. In ACM SIGMOD, pages 249&ndash;260, 2000. [28] N. Russell et al. On the suitability of UML 2.0 activity </span></p><p class="c76"><span class="c18">diagrams for business process modelling. In Proc. Asia-Pacific Conf. on Conceptual Modelling, volume 53 of APCCM, pages 95&ndash;104. Australian Computer Society, Inc., 2006. [29] S. Schneider et al. Tutorial: Stream Processing Optimizations. </span></p><p class="c73"><span class="c18">In Proc. of Int. Conf. on Distributed Event-Based Systems, DEBS&rsquo;13, pages 249&ndash;258. ACM, 2013. [30] N. P. Schultz-M&oslash;ller et at. Distributed Complex Event </span></p><p class="c125"><span class="c18">Processing with query rewriting. In Proc. of Int. Conf. on Distributed Event-Based Systems, DEBS &rsquo;09, pages 1&ndash;12. ACM, 2009. [31] T. K. Sellis. Multiple-query optimization. ACM Trans. </span></p><p class="c67"><span class="c18">Database Syst., 13(1):23&ndash;52, Mar. 1988. [32] D. Wang, E. A. Rundensteiner, and R. T. Ellison, III. Active </span></p><p class="c36"><span class="c18">complex event processing over event streams. Proc. VLDB Endow., 4(10):634&ndash;645, July 2011. [33] Y. Wang et al. Context-aware complex event processing for event cloud in internet of things. In Proc. of Int. Conf. on Wireless Communications and Signal Processing, pages 1&ndash;6. IEEE, 2012. [34] E. Wu et al. High-performance Complex Event Processing over </span></p><p class="c79"><span class="c18">streams. In Proc. of Int. Conf. on Management of data, SIGMOD &rsquo;06, pages 407&ndash;418. ACM, 2006. </span></p><p class="c69"><span class="c16">424 </span></p></body></html>