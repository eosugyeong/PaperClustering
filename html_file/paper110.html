<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c55{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11pt;font-family:"Arial";font-style:normal}.c98{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.7pt;font-family:"Arial";font-style:italic}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c44{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.8pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:5.9pt;font-family:"Arial";font-style:normal}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.5pt;font-family:"Arial";font-style:normal}.c54{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.2pt;font-family:"Arial";font-style:italic}.c72{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:9.9pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c74{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.3pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.4pt;font-family:"Arial";font-style:normal}.c30{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.5pt;font-family:"Arial";font-style:italic}.c100{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.6pt;font-family:"Times New Roman";font-style:normal}.c49{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.1pt;font-family:"Courier New";font-style:normal}.c91{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:17.9pt;font-family:"Arial";font-style:normal}.c88{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:4.3pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.6pt;font-family:"Arial";font-style:italic}.c67{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c64{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.2pt;font-family:"Courier New";font-style:normal}.c85{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c94{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:9.9pt;font-family:"Arial";font-style:normal}.c50{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13pt;font-family:"Arial";font-style:italic}.c52{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.1pt;font-family:"Arial";font-style:italic}.c33{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.8pt;font-family:"Arial";font-style:normal}.c93{color:#ffffff;font-weight:400;text-decoration:none;vertical-align:sub;font-size:17.1pt;font-family:"Courier New";font-style:normal}.c42{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:19.9pt;font-family:"Arial";font-style:normal}.c35{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.8pt;font-family:"Arial";font-style:italic}.c46{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.2pt;font-family:"Arial";font-style:italic}.c92{color:#ffffff;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.9pt;font-family:"Courier New";font-style:normal}.c66{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12pt;font-family:"Arial";font-style:normal}.c48{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Courier New";font-style:normal}.c95{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.8pt;font-family:"Arial";font-style:italic}.c45{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.7pt;font-family:"Arial";font-style:italic}.c82{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.6pt;font-family:"Arial";font-style:italic}.c41{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.6pt;font-family:"Arial";font-style:italic}.c61{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.3pt;font-family:"Arial";font-style:normal}.c90{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.6pt;font-family:"Arial";font-style:italic}.c79{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c89{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.5pt;font-family:"Arial";font-style:italic}.c78{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.8pt;font-family:"Arial";font-style:italic}.c73{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c70{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.6pt;font-family:"Times New Roman";font-style:normal}.c87{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Courier New";font-style:normal}.c51{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:17.1pt;font-family:"Courier New";font-style:normal}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c75{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.3pt;font-family:"Arial";font-style:italic}.c57{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:9.6pt;font-family:"Arial";font-style:italic}.c37{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c43{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.5pt;font-family:"Arial";font-style:italic}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.1pt;font-family:"Arial";font-style:italic}.c21{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10.1pt;font-family:"Arial";font-style:italic}.c76{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11pt;font-family:"Arial";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c97{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:12.4pt;font-family:"Arial";font-style:italic}.c81{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.9pt;font-family:"Arial";font-style:italic}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.5pt;font-family:"Arial";font-style:italic}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:normal}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.1pt;font-family:"Arial";font-style:normal}.c34{color:#ffffff;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.1pt;font-family:"Courier New";font-style:normal}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12.4pt;font-family:"Arial";font-style:italic}.c56{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.3pt;font-family:"Arial";font-style:italic}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.3pt;font-family:"Arial";font-style:normal}.c86{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.2pt;font-family:"Arial";font-style:italic}.c36{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:italic}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.2pt;font-family:"Arial";font-style:normal}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.6pt;font-family:"Arial";font-style:italic}.c83{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.4pt;font-family:"Arial";font-style:italic}.c99{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:16.9pt;font-family:"Courier New";font-style:normal}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.8pt;font-family:"Arial";font-style:italic}.c77{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.3pt;font-family:"Times New Roman";font-style:normal}.c63{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.4pt;font-family:"Arial";font-style:italic}.c39{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.1pt;font-family:"Arial";font-style:normal}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.2pt;font-family:"Arial";font-style:normal}.c38{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c58{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:5.5pt;font-family:"Arial";font-style:normal}.c53{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10.1pt;font-family:"Arial";font-style:italic}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.4pt;font-family:"Arial";font-style:italic}.c96{color:#ffffff;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.8pt;font-family:"Courier New";font-style:normal}.c29{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.5pt;font-family:"Arial";font-style:italic}.c69{color:#ffffff;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.2pt;font-family:"Courier New";font-style:normal}.c47{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:normal}.c32{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:italic}.c27{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c65{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.8pt;font-family:"Arial";font-style:italic}.c60{color:#7f7f7f;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.6pt;font-family:"Arial";font-style:normal}.c71{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.8pt;font-family:"Courier New";font-style:normal}.c59{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Courier New";font-style:normal}.c62{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.3pt;font-family:"Arial";font-style:italic}.c68{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.9pt;font-family:"Courier New";font-style:normal}.c80{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.2pt;font-family:"Arial";font-style:italic}.c84{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.8pt;font-family:"Arial";font-style:normal}.c25{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c18{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c12{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c13{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c13"><p class="c25"><span class="c67">PARAGON: Parallel Architecture-Aware Graph Partition Refinement Algorithm </span></p><p class="c0"><span class="c31">Angen Zheng </span><span class="c62">University of Pittsburgh Pittsburgh, PA, USA </span><span class="c42">anz28@cs.pitt.edu </span></p><p class="c0"><span class="c31">Alexandros Labrinidis </span><span class="c62">University of Pittsburgh Pittsburgh, PA, USA </span><span class="c42">labrinid@cs.pitt.edu </span></p><p class="c0"><span class="c31">Patrick Pisciuneri </span><span class="c62">University of Pittsburgh Pittsburgh, PA, USA </span><span class="c42">php8@pitt.edu </span><span class="c31">Panos K. Chrysanthis </span><span class="c62">University of Pittsburgh Pittsburgh, PA, USA </span><span class="c42">panos@cs.pitt.edu </span></p><p class="c0"><span class="c31">Peyman Givi </span><span class="c62">University of Pittsburgh Pittsburgh, PA, USA </span><span class="c42">pgivi@pitt.edu </span></p><p class="c0"><span class="c31">ABSTRACT </span><span class="c14">With the explosion of large, dynamic graph datasets from various fields, graph partitioning and repartitioning are becoming more and more critical to the performance of many graph-based Big Data ap- plications, such as social analysis, web search, and recommender systems. However, well-studied graph (re)partitioners usually as- sume a homogeneous and contention-free computing environment, which contradicts the increasing communication heterogeneity and shared resource contention in modern, multicore high performance computing clusters. To bridge this gap, we introduce P</span><span class="c11">ARAGON</span><span class="c14">, a parallel architecture-aware graph partition refinement algorithm, which mitigates the mismatch by modifying a given decomposition according to the nonuniform network communication costs and the contentiousness of the underlying hardware topology. To further reduce the overhead of the refinement, we also make P</span><span class="c11">ARAGON </span><span class="c14">itself architecture-aware. </span></p><p class="c0"><span class="c14">Our experiments with a diverse collection of datasets showed that on average P</span><span class="c11">ARAGON </span><span class="c14">improved the quality of graph decom- positions computed by the de-facto standard (hashing partitioning) and two state-of-the-art streaming graph partitioning heuristics (de- terministic greedy and linear deterministic greedy) by 43%, 17%, and 36%, respectively. Furthermore, our experiments with an MPI implementation of Breadth First Search and Single Source Short- est Path showed that, in comparison to the state-of-the-art stream- ing and multi-level graph (re)partitioners, P</span><span class="c11">ARAGON </span><span class="c14">achieved up to 5.9x speedups. Finally, we demonstrated the scalability of P</span><span class="c11">ARAGON </span><span class="c14">by scaling it up to a graph with 3.6 billion edges using only 3 ma- chines (60 physical cores). </span></p><p class="c0"><span class="c31">1. INTRODUCTION </span></p><p class="c18"><span class="c14">It is well-known that graph (re)partitioning has been extensively studied in the area of scientific simulations [14, 34]. Yet, its impor- tance is continuously increasing due to the explosion of large graph datasets from various fields, such as the World Wide Web, Pro- </span></p><p class="c18"><span class="c14">c </span><span class="c1">2016, Copyright is with the authors. Published in Proc. 19th Inter- national Conference on Extending Database Technology (EDBT), March 15-18, 2016 - Bordeaux, France: ISBN 978-3-89318-070-7, on OpenPro- ceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0 </span></p><p class="c18"><span class="c14">tein Interaction Networks, Social Networks, Financial Networks, and Transportation Networks. This has led to the development of graph-specialized parallel computing frameworks, e.g., Pregel [21], GraphLab [19], and PowerGraph [13]. </span></p><p class="c0"><span class="c14">Pregel, as a representative of these computing frameworks, em- braces a vertex-centric approach where the graph is partitioned across multiple servers for parallel computation. Computations are often divided into a sequence of supersteps separated by a global syn- chronization barrier. During each superstep, a user-defined func- tion is computed against each vertex based on the messages it re- ceived from its neighbors in the previous step. The function can change the state and outgoing edges of the vertex, send messages to the neighbors of the vertex, or even add or remove vertices/edges to the graph. Traditional Graph Partitioners Clearly, the distribution of the graph data across servers may impact the performance of target ap- plications significantly. Graph partitioning has been studied for decades [14, 34], attempting to provide a good partitioning of the graph data, whereby both the skewness and the communication (edge-cut) among partitions are minimized as much as possible, in order to minimize the total response time for the entire compu- tation. However, classic graph partitioners such as M</span><span class="c11">ETIS </span><span class="c14">[23] and Chaco [7] do not scale well with large graphs. Streaming Graph Partitioners Streaming graph partitioners (e.g., DG/LDG [39], arXiv&rsquo;13 [11], and Fennel [42]) have been proposed in order to overcome the scalability challenges of classic graph partitioners, by examining the graph incrementally. One of the main shortcomings of these approaches is that they also assume uniform network communication costs among partitions as classic graph partitioners do. That is, they all assume that the communica- tion cost is proportional only to the amount of data communicated among partitions. This assumption is no longer valid in modern parallel architectures due to the increasing communication hetero- geneity [47, 8]. For example, on a 4 &lowast; 4 &lowast; 4 3D-torus interconnect, the distance to different nodes starting from a single node varies from 0 to 6 hops. Architecture-Aware Graph Partitioners Architecture-aware graph partitioners [24, 8, 46] have been proposed to improve the map- ping of the application&rsquo;s communication patterns to the underlying hardware topology. Chen et al. [8] (SoCC&rsquo;12) took architecture- awareness a step further, by making the partitioning algorithm itself partially aware of the communication heterogeneity. However, both [8] and [24] (ICA3PP&rsquo;08) are built on top of existing heavyweight graph partitioners, namely, M</span><span class="c11">ETIS </span><span class="c14">[23] and P</span><span class="c11">AR</span><span class="c14">M</span><span class="c11">ETIS </span><span class="c14">[30], which </span></p><p class="c0"><span class="c27">Series ISSN: 2367-2005 365 </span><span class="c85">10.5441/002/edbt.2016.34 </span></p><p class="c0"><span class="c84">F</span><span class="c76">most </span></p><p class="c0"><span class="c79">PARAGON </span><span class="c91">s erutae</span><span class="c79">ARAGON </span></p><p class="c0"><span class="c38">ica3pp&#39;08tkde&#39;15 </span></p><p class="c0"><span class="c38">Zoltan ParMetis </span><span class="c76">socc&#39;12 </span><span class="c38">catchW Metis </span></p><p class="c0"><span class="c55">arXiv&#39;13 </span></p><p class="c0"><span class="c38">scotch LogGP </span></p><p class="c0"><span class="c55">Mizan </span></p><p class="c0"><span class="c38">Fennel </span></p><p class="c0"><span class="c55">chaco sheep </span><span class="c38">xdgp Hermes least DG/LDG worst best </span></p><p class="c0"><span class="c14">Figure 1: Classification of all graph partitioners/re-partitioners ac- cording to their features vs performance profile. </span></p><p class="c0"><span class="c14">are known to be the best graph partitioners and repartitioners in terms of partitioning quality but have poor scalability. Finally, al- though Xu et al. [46] (TKDE&rsquo;15) proposed a lightweight architecture- aware streaming graph partitioner, the partitioner may lead to sub- optimal performance for dynamic graphs [43]. Traditional Graph Repartitioners Most real-world graphs are often non-static, and continue to evolve over time. Because of this graph dynamism, both the quality of the initial partitioning and the mapping of the application communication pattern to the underly- ing hardware topology will continuously degrade over time, lead- ing to (a potentially significant) load imbalance and additional com- munication overhead. Considering the sheer scale of real-world graphs, repartitioning the entire graph from scratch using [46, 8, 24], even in parallel, is often impractical, either because of the long partitioning time or the huge volume of data migration the repartitioning may introduce. To address this, several graph repar- titioning algorithms have been proposed, such as Zoltan [1, 6] and P</span><span class="c11">AR</span><span class="c14">M</span><span class="c11">ETIS </span><span class="c14">[30, 33]. Although they are able to greatly reduce the data migration cost, they are all architecture-agnostic and do not scale well with massive graphs. Parallel/Lightweight Graph Repartitioners Parallel lightweight graph repartitioners (e.g., CatchW [37], xdgp [43], Hermes [26], Mizan [17], arXiv&rsquo;13 [11], and LogGP [45]) have been proposed to improve the performance and scalability of graph repartitioning. Instead of seeking an optimal partitioning at once, these algorithms adapt the graph decomposition to changes efficiently by incremen- tally migrating vertices from one partition to another based on some local heuristics. However, they are all oblivious of the nonuniform network communication costs among partitions. Limitations of the State-of-the-Art Despite the plethora of graph partitioners and repartitioners (Figure 1), the current state-of-the- art is suffering from two main problems: </span></p><p class="c0"><span class="c14">&bull; Graph (re)partitioners either consider architecture-awareness (for CPU/network heterogeneity) or consider performance (i.e., par- allel/lightweight implementation), but never both. This is illus- trated in Figure 1, where the top-right corner is empty (except for P</span><span class="c11">ARAGON</span><span class="c14">, which is presented in this paper). </span></p><p class="c0"><span class="c14">&bull; No existing graph (re)partitioner considers the issue of shared resource contention in modern multicore high performance com- puting (HPC) clusters. Shared resource contention is a well- known issue in multicore systems and has received a lot of at- tention in system-level research [15, 41]. </span></p><p class="c0"><span class="c60">Partitioners Streaming </span></p><p class="c0"><span class="c60">Partitioners </span></p><p class="c0"><span class="c84">Performance </span></p><p class="c25"><span class="c60">Re- Partitioners </span></p><p class="c25"><span class="c60">Parallel Re-Partitioners </span></p><p class="c0"><span class="c14">Our prior work We have previously presented an architecture- aware graph repartitioner, A</span><span class="c11">RAGON</span><span class="c14">LB [48]. Although A</span><span class="c11">RAGON</span><span class="c14">LB considers the communication heterogeneity for target applications, it disregards the issue of shared resource contention, and the repar- titioning itself is not architecture-aware. Moreover, the refinement algorithm that A</span><span class="c11">RAGON</span><span class="c14">LB uses to improve the mapping of the ap- plication communication pattern to the underlying hardware topol- ogy requires the entire graph to be stored in memory by a single server, which is infeasible for large graphs. Furthermore, the re- finement algorithm is performed sequentially, which may become a performance bottleneck. Finally, A</span><span class="c11">RAGON</span><span class="c14">LB assumes that com- pute nodes used for parallel computation have the same number of cores and memory hierarchies, which may not always be true. Contributions In this paper, we present P</span><span class="c11">ARAGON</span><span class="c14">, which over- comes both limitations of the current state-of-the-art graph reparti- tioners by extending A</span><span class="c11">RAGON</span><span class="c14">LB in the following aspects. </span></p><p class="c0"><span class="c14">1. We separate the refinement algorithm, A</span><span class="c11">RAGON</span><span class="c14">, from A</span><span class="c11">RAGON</span><span class="c14">LB as an independent component, and develop a parallelized ver- sion of A</span><span class="c11">RAGON</span><span class="c14">, P</span><span class="c11">ARAGON</span><span class="c14">, for large graphs (Section 3, 4, 5). We further reduce the overhead of P</span><span class="c11">ARAGON </span><span class="c14">by making it aware of the nonuniform network communication costs (explained in Section 2.1). 2. We identify and consider the issue of shared resource contention in modern HPC clusters for graph partitioning (Section 2.2 &amp; 6). 3. We perform an extensive experimental study of P</span><span class="c11">ARAGON </span><span class="c14">with a diverse set of 13 datasets and two real-world applications, demonstrating the effectiveness and scalability of P</span><span class="c11">ARAGON </span><span class="c14">(Section 7). </span></p><p class="c0"><span class="c31">2. MOTIVATION </span></p><p class="c0"><span class="c14">In this section we explain the importance of architecture-awareness (i.e., communication heterogeneity and shared resource contention) for efficient graph (re)partitioners. </span><span class="c31">2.1 Communication Heterogeneity </span></p><p class="c0"><span class="c14">For distributed graph computations on multicore systems, com- munication can be either inter-node (i.e., among cores of different compute nodes) or intra-node (i.e., among cores of the same com- pute node). In general, intra-node communication is an order of magnitude faster than inter-node communication. This is because in many modern parallel programming models like MPI [27, 25], a predominant messaging standard for HPC applications, intra-node communication is implemented via shared memory/cache [16, 5], while inter-node communication needs to go through the network interface. Additionally, both inter-node and intra-node communi- cation are themselves nonuniform. Nonuniform Inter-Node Network Communication Modern par- allel architectures, like supercomputers, usually consist of a large number of compute nodes linked via a network. Consequently, the communication costs among compute nodes vary a lot because of their varying locations. For example, in the Gordon supercom- puter [28], the network topology is a 4x4x4 3D torus of switches with 16 compute nodes attached to each switch. As a result, the dis- tance to different compute nodes starting from a single node varies from 0 to 6 hops. Also, supercomputers often allow multiple jobs to concurrently run on different compute nodes and contend for the shared network links, limiting the effective network bandwidth available for each job and thus amplifying the heterogeneity. Nonuniform Intra-Node Network Communication Communi- cation among cores of the same compute node is also nonuniform because of the complex memory hierarchy. Communication among </span></p><p class="c0"><span class="c27">366 </span></p><p class="c0"><span class="c10">Socket 0 Socket 1 </span></p><p class="c0"><span class="c10">L2 L2 </span></p><p class="c0"><span class="c10">FSB Interface </span></p><p class="c0"><span class="c10">Socket 1 </span></p><p class="c0"><span class="c10">core core core core </span></p><p class="c0"><span class="c10">L1 L1 L1 L1 </span></p><p class="c0"><span class="c10">L2 L2 L2 L2 </span></p><p class="c0"><span class="c10">L3 </span></p><p class="c25"><span class="c10">Memory Controller </span></p><p class="c0"><span class="c10">Socket 0 </span></p><p class="c0"><span class="c10">core core core core </span></p><p class="c0"><span class="c10">core core core core </span></p><p class="c0"><span class="c10">L1 L1 L1 L1 </span></p><p class="c0"><span class="c10">L1 L1 L1 L1 </span></p><p class="c0"><span class="c10">FSB Interface </span></p><p class="c0"><span class="c10">FSB Interface </span></p><p class="c0"><span class="c58">FSB FSB </span></p><p class="c0"><span class="c88">QPI/HT </span></p><p class="c0"><span class="c1">(a) Uniform Memory Access (UMA) Architecture </span></p><p class="c0"><span class="c10">core core core core </span></p><p class="c0"><span class="c10">L1 L1 L1 L1 </span></p><p class="c0"><span class="c10">L2 L2 </span></p><p class="c0"><span class="c10">L2 L2 L2 L2 FSB Interface </span></p><p class="c0"><span class="c10">L3 </span></p><p class="c25"><span class="c10">Memory Controller </span></p><p class="c0"><span class="c1">(b) Nonuniform Memory Access (NUMA) Architecture </span><span class="c3">Figure 2: Example Architectures of Modern Compute Nodes </span></p><p class="c0"><span class="c14">cores sharing more cache-levels can achieve lower latency and higher effective bandwidth than cores sharing fewer cache-levels. For ex- ample, in the architecture described by Figure 2a, communication among cores sharing L2 caches (e.g., between the first and second core of Socket 0) offers the highest performance, while commu- nication among cores of the same socket but not sharing any L2 cache (e.g., between the first and third core of Socket 0) delivers the next highest performance. Communication among cores of dif- ferent sockets performs the worst. Similarly, in Figure 2b, cores of the same socket (intra-socket communication) usually commu- nicate faster than cores residing on different sockets (inter-socket communication). This is because intra-socket communication can be achieved via the shared caches, while inter-socket communica- tion has to go through the front-side bus and the off-chip memory controller (Figure 2a) or the inter-socket link controller (Figure 2b). Take-away To improve the performance of graph-based big-data applications, we should not only minimize the number of edges across different partitions (edge-cut), but also the number of edge- cuts among partitions having higher network communication costs (hop-cut). This is the major difference between architecture-agnostic solutions (that only minimize edge-cut) and architecture-aware ones (that try to minimize both edge-cut and hop-cut). </span><span class="c31">2.2 Intra-Node Shared Resource Contention </span></p><p class="c18"><span class="c14">As mentioned above, MPI intra-node communication is imple- mented via shared memory, which can either be user-space or kernel- based [16, 5]. Current MPI implementations often use the former for small messages and the latter for large messages. The user- space approach requires two memory copies. The sender first needs to load the send buffer into its cache and then write the data to the shared buffer (which may require loading the shared buffer block into the sender&rsquo;s cache first). Then, the receiver reads the data from the shared memory (which may demand loading the shared mem- ory block and receiving buffer into the receiver&rsquo;s cache first). For kernel-based approaches, the receiver first loads the send buffer di- rectly to its cache with the help of the OS kernel. Then, the receiver writes the data to the receiving buffer (which may require loading the receiving buffer into its cache first). Clearly, kernel-based ap- proaches reduce the number of memory copies to be one, mitigating the traffic on the memory subsystem. However, it demands a trap to the kernel on both the sender and receiver, making it inefficient for small messages. As can be seen, intra-node communication generates lots of memory traffic and cache pollution, which may saturate the memory subsystem if we put too much communica- tion within each compute node. This issue is further amplified by the increasing contentiousness of the shared resources in modern multicore systems. Table 1 summarizes the resources that different cores may have to compete for when they are communicating with each other for the architectures presented in Figures 2a and 2b. The </span></p><p class="c0"><span class="c10">Inter-socket </span></p><p class="c0"><span class="c10">Inter-socket Link Controller </span></p><p class="c0"><span class="c10">Link Controller Memory Controller (Northbridge) </span></p><p class="c0"><span class="c10">Memory </span></p><p class="c0"><span class="c10">Memory Memory </span></p><p class="c0"><span class="c14">Table 1: Intra-Node Shared Resource Contention </span><span class="c38">Cores/Resources Sharing Contention </span></p><p class="c0"><span class="c38">Core Groups Socket LLC LLC FSB/QPI(HT) Memory Controller </span></p><p class="c12"><span class="c38">G1 UMA G2 Fig. 2a G3 NUMA G1 Fig. 2b G2 </span></p><p class="c0"><span class="c14">summary is based on whether the cores are on the same socket and whether they share the last level cache (LLC). Take-away Focusing solely on placing neighboring vertices as close as possible is not sufficient to achieve superior performance. In fact, putting too much communication within each compute node may even hurt the performance due to the traffic congestion on memory subsystems. Counter-intuitively, offloading a certain amount of intra-node communication across compute nodes may some- times achieve better performance. This is because inter-node com- munication is often implemented using Remote Direct Memory Access (RDMA) and rendezvous protocols [40], which allow a compute node to read data from the memory of another compute node without involving the processor, cache, or operating system of either node, thus alleviating the traffic on memory subsystems and cache pollution. Additionally, it is reported in [3] that modern RDMA-enabled networks can deliver comparable network band- width as that of memory channels. This requires us to examine the impact of multi-core architecture on graph partitionings more care- fully, especially for small HPC clusters, since the network may no longer be the bottleneck. </span></p><p class="c0"><span class="c31">3. PROBLEM STATEMENT </span></p><p class="c0"><span class="c14">Let G = (V,E) be a graph, where V is the vertex set and E is the edges set, and P be a partitioning of G with n partitions, where </span></p><p class="c0"><span class="c14">P = {P</span><span class="c2">i </span><span class="c14">: &cup;</span><span class="c37">n</span><span class="c2">i=1</span><span class="c6">P</span><span class="c37">i </span><span class="c6">= V and P</span><span class="c37">i </span><span class="c6">&cap; P</span><span class="c37">j </span><span class="c6">= &phi; for any i = j} (1) </span><span class="c14">and let M be the current assignment of partitions to servers, where P</span><span class="c15">i </span><span class="c3">is assigned to server M[i]. The server can be either a hardware </span><span class="c14">thread, a core, a socket, or a machine. Architecture-aware graph partition refinement aims to improve the mapping of the application communication pattern to the un- derlying hardware topology by modifying the current partitioning of the graph, such that the communication cost of the target appli- cation, given the specific hardware topology, is minimized. The modification usually involves migrating vertices from one partition to another partition. Hence, in addition to the communication cost, the refinement should also minimize the data migration cost among partitions. Also, to ensure balanced load distribution in terms of the computation requirement, the refinement should keep the skewness of the partitioning as small as possible. </span></p><p class="c0"><span class="c27">367 </span></p><p class="c0"><span class="c49">f</span><span class="c2">P2(N2) </span></p><p class="c0"><span class="c49">fe </span></p><p class="c0"><span class="c72">P2(N2) </span></p><p class="c0"><span class="c49">e </span><span class="c2">P2(N2) </span><span class="c64">fe </span><span class="c71">g </span></p><p class="c0"><span class="c49">d</span><span class="c68">g </span></p><p class="c0"><span class="c49">dc </span></p><p class="c0"><span class="c51">g </span><span class="c64">a d</span><span class="c72">P3(N3) </span></p><p class="c0"><span class="c71">b </span></p><p class="c0"><span class="c2">P1(N1) </span><span class="c2">P3(N3) </span><span class="c68">b </span></p><p class="c0"><span class="c94">P1(N1) </span></p><p class="c0"><span class="c2">P3(N3) </span><span class="c51">b </span><span class="c2">P1(N1) </span><span class="c64">c </span></p><p class="c0"><span class="c14">Figure 3: Old Decomposition </span></p><p class="c0"><span class="c14">Figure 4: Better Decomposition </span></p><p class="c0"><span class="c14">Figure 5: Best Decomposition </span></p><p class="c0"><span class="c14">N</span><span class="c2">1 </span><span class="c14">N</span><span class="c2">2 </span><span class="c14">N</span><span class="c2">3 </span></p><p class="c0"><span class="c34">hj a </span></p><p class="c0"><span class="c49">c </span></p><p class="c12"><span class="c34">hj </span><span class="c99">a </span><span class="c69">h</span><span class="c96">i </span><span class="c92">i </span><span class="c93">i </span></p><p class="c0"><span class="c14">N</span><span class="c15">1 </span><span class="c3">1 6 </span><span class="c14">N</span><span class="c2">2 </span><span class="c14">1 1 N</span><span class="c15">3 </span><span class="c3">6 1 </span><span class="c14">Figure 6: Relative Network Communication Costs </span></p><p class="c0"><span class="c14">We define the communication cost of a partitioning P as: comm(G, P) = &alpha; &lowast; </span><span class="c6">&sum; </span></p><p class="c25"><span class="c2">e=(u,v)&isin;E and u&isin;P</span><span class="c47">i </span><span class="c2">and v&isin;P</span><span class="c47">j </span><span class="c2">and i =j </span></p><p class="c0"><span class="c69">j </span></p><p class="c12"><span class="c14">A</span><span class="c11">RAGON </span><span class="c14">can only refine one partition pair at a time, it is repeatedly applied to all partition pairs sequentially. w(e) &lowast; c(P</span><span class="c15">i</span><span class="c3">,P</span><span class="c15">j</span><span class="c3">) (2) </span></p><p class="c0"><span class="c14">The gain of moving vertex v from its current partition, P</span><span class="c15">i</span><span class="c3">, to its </span><span class="c14">refinement partner, P</span><span class="c2">j</span><span class="c14">, is defined as: </span></p><p class="c0"><span class="c14">where &alpha; specifies the relative importance between communication </span></p><p class="c0"><span class="c14">g</span><span class="c37">i,j</span><span class="c14">(v) = g</span><span class="c2">std</span><span class="c37">i,j </span></p><p class="c0"><span class="c6">(v) + g</span><span class="c2">topo</span><span class="c37">i,j </span></p><p class="c0"><span class="c6">(v) + g</span><span class="c2">mig</span><span class="c37">i,j</span><span class="c6">(v) (5) </span></p><p class="c18"><span class="c14">and migration cost, which is usually set to be the number of su- persteps carried out between two consecutive refinement/reparti- tioning steps, w(e) is the edge weight, indicating the amount of data communicated along the edge per superstep, and c(P</span><span class="c15">i</span><span class="c3">,P</span><span class="c15">j</span><span class="c3">) </span></p><p class="c0"><span class="c14">Here, munication g</span><span class="c2">std</span><span class="c37">i,j </span></p><p class="c0"><span class="c6">(v) </span><span class="c14">between </span><span class="c6">considers </span><span class="c14">P</span><span class="c15">i </span><span class="c3">and </span><span class="c6">the </span><span class="c3">P</span><span class="c6">impact </span><span class="c15">j</span><span class="c3">, defined </span><span class="c6">of the movement </span><span class="c3">as: </span></p><p class="c0"><span class="c6">on the com- </span></p><p class="c0"><span class="c14">g</span><span class="c2">std</span><span class="c37">i,j </span></p><p class="c0"><span class="c6">(v) = &alpha; &lowast; (d</span><span class="c2">ext</span><span class="c14">(v, P</span><span class="c15">j</span><span class="c3">) &minus; d</span><span class="c15">ext</span><span class="c3">(v, P</span><span class="c15">i</span><span class="c3">)) &lowast; c(P</span><span class="c15">i</span><span class="c3">,P</span><span class="c15">j</span><span class="c3">) (6) </span><span class="c14">can be either the relative network communication cost, the degree of shared resource contentiousness between P</span><span class="c2">i </span><span class="c14">and P</span><span class="c2">j </span><span class="c14">or a hybrid of both. Existing architecture-agnostic graph (re)partitioners usu- ally assume c(P</span><span class="c2">i</span><span class="c14">,P</span><span class="c2">j</span><span class="c14">)=1. </span></p><p class="c0"><span class="c14">where d</span><span class="c15">ext</span><span class="c3">(v, P</span><span class="c15">i</span><span class="c3">) denotes the amount of data v communicates with </span><span class="c14">vertices of partition P</span><span class="c15">i</span><span class="c3">, formally </span><span class="c14">d</span><span class="c2">ext</span><span class="c14">(v, P</span><span class="c2">i</span><span class="c14">) = </span><span class="c3">defined as </span></p><p class="c0"><span class="c6">&sum; </span></p><p class="c0"><span class="c14">The migration cost of the refinement is defined as: </span></p><p class="c0"><span class="c2">e=(v,u)&isin;E </span><span class="c14">mig(G, P, P ) = </span><span class="c6">&sum;</span><span class="c2">and v&isin;P</span><span class="c47">i </span><span class="c2">and u&isin;P</span><span class="c47">j </span><span class="c2">and i =j </span></p><p class="c25"><span class="c2">v&isin;V and v&isin;P</span><span class="c47">i </span><span class="c2">and v&isin;P </span><span class="c47">j </span><span class="c2">and i =j </span></p><p class="c0"><span class="c14">w(e) (7) </span></p><p class="c0"><span class="c14">vs(v) &lowast; c(P</span><span class="c15">i</span><span class="c3">,P </span><span class="c15">j</span><span class="c14">) (3) </span></p><p class="c0"><span class="c14">The second term the movement on of the Equation communication 5, g</span><span class="c2">topo</span><span class="c37">i,j </span></p><p class="c12"><span class="c6">(v), considers the impact of </span><span class="c14">between v and its neighbors in other partitions in addition to P</span><span class="c2">i </span><span class="c14">and P</span><span class="c2">j</span><span class="c14">. We define it as: where vs(v) is the vertex size, reflecting the amount of application data represented by v, and P denotes the partitioning after being refined/repartitioned. </span></p><p class="c0"><span class="c14">g</span><span class="c2">topo</span><span class="c37">i,j </span></p><p class="c0"><span class="c6">(v) = &alpha;&lowast; </span></p><p class="c0"><span class="c14">The skewness of a partitioning, P, is defined as: </span></p><p class="c0"><span class="c14">skewness(G, P) = </span><span class="c6">max{w(P</span><span class="c37">1</span><span class="c6">),w(P</span><span class="c37">2</span><span class="c6">),&middot;&middot;&middot; </span><span class="c59">&sum;</span><span class="c47">ni=1 </span><span class="c2">w(P</span><span class="c47">i</span><span class="c2">) </span></p><p class="c0"><span class="c6">,w(P</span><span class="c37">n</span><span class="c6">)} </span><span class="c2">n </span></p><p class="c0"><span class="c3">&sum;</span><span class="c2">n</span><span class="c15">and k =i k=1 and k =j </span></p><p class="c0"><span class="c14">d</span><span class="c15">ext</span><span class="c3">(v, P</span><span class="c15">k</span><span class="c3">)&lowast;(c(P</span><span class="c15">i</span><span class="c3">,P</span><span class="c15">k</span><span class="c3">)&minus;c(P</span><span class="c15">j</span><span class="c3">,P</span><span class="c15">k</span><span class="c3">)) </span></p><p class="c0"><span class="c14">(8) </span></p><p class="c0"><span class="c14">(4) </span></p><p class="c0"><span class="c14">The movement third term on migration of Equation cost, 5, which g</span><span class="c2">mig</span><span class="c37">i,j</span><span class="c6">(v), considers the impact of the </span></p><p class="c0"><span class="c14">is defined as: </span></p><p class="c0"><span class="c14">wherew(P(i.e., the computation </span><span class="c15">i</span><span class="c3">) = </span><span class="c14">&sum;</span><span class="c15">v&isin;P</span><span class="c14">requirement </span><span class="c47">i </span><span class="c14">w(v) with of w(v) the denoting vertex). </span></p><p class="c0"><span class="c14">the vertex weight </span></p><p class="c12"><span class="c14">g</span><span class="c2">mig</span><span class="c37">i,j</span><span class="c6">(v) = vs(v) &lowast; (c(P</span><span class="c2">i</span><span class="c14">,P</span><span class="c15">k</span><span class="c3">) &minus; c(P</span><span class="c15">j</span><span class="c3">,P</span><span class="c15">k</span><span class="c3">)) </span><span class="c14">where P</span><span class="c15">k </span><span class="c3">is the owner of v in the original decomposition. (9) The </span><span class="c14">Self Architecture-Awareness In fact, the refinement algorithm it- self should be architecture-aware (during its execution), since the </span></p><p class="c12"><span class="c14">current owner of v, P</span><span class="c15">i</span><span class="c3">, may be different from its original owner, </span><span class="c14">P</span><span class="c2">k</span><span class="c14">, due to the refinement. refinement may also result in a lot of communication. </span></p><p class="c12"><span class="c14">Example In the decomposition shown in Figure 3, we have a graph with unit weights and sizes and is initially distributed across 3 ma- </span><span class="c31">4. OUR PRIOR WORK: ARAGON </span></p><p class="c18"><span class="c14">A</span><span class="c11">RAGON </span><span class="c14">is a serial, architecture-aware graph partition refine- ment algorithm proposed by us in [48]. It is a variant of the Fiduccia- Mattheyses (FM) algorithm [12]. It tries to reduce the application communication cost by modifying the current decomposition ac- cording to the nonuniform network communication costs of the un- derlying hardware topology. Each time it takes as input two par- titions of the n-way decomposition and the relative network com- munication costs among partitions. For each input partition pair, it attempts to improve the mapping of the application communi- cation pattern to the underlying hardware topology by iteratively moving vertices between them. During each iteration, it tries to find a single vertex such that moving it from its current partition to the alternative partition would lead to a maximal gain, where the </span></p><p class="c18"><span class="c14">chines: N</span><span class="c15">1</span><span class="c3">, N</span><span class="c15">2</span><span class="c3">, and N</span><span class="c15">3</span><span class="c3">. The relative network communication </span><span class="c14">costs among partitions are shown in Figure 6. Clearly, the number of edges among partitions goes from 4 in Figure 3, to 3 in Fig- ure 4. In fact, if we assume uniform network communication costs among partitions, Figure 4 would be the optimal decomposition of the graph. However, if we consider the case where all network costs are not equal (as in Figure 6), then the decomposition in Fig- ure 4 can be further improved by moving vertex a to P</span><span class="c15">2 </span><span class="c3">(Figure 5). </span><span class="c14">Even though moving vertex a from P</span><span class="c15">1 </span><span class="c3">to P</span><span class="c15">2 </span><span class="c3">increases the com- </span><span class="c14">munication cost between P</span><span class="c2">1 </span><span class="c14">and P</span><span class="c2">2 </span><span class="c14">by 1, it actually reduces the communication cost between a and j by 5, since the relative net- work communication cost between P</span><span class="c2">1 </span><span class="c14">and P</span><span class="c2">3 </span><span class="c14">is 6, while that of P</span><span class="c2">2 </span><span class="c14">and P</span><span class="c15">3 </span><span class="c3">is 1. For the same reason, moving a to P</span><span class="c15">2 </span><span class="c3">also decreases </span><span class="c14">the migration cost of a by 5, since vertex a was originally in P</span><span class="c15">3</span><span class="c3">. </span></p><p class="c0"><span class="c14">gain is defined as the reduction in the communication and migration cost. Upon each movement of a vertex, v, it also updates the gain </span></p><p class="c0"><span class="c31">5. PARAGON </span></p><p class="c0"><span class="c14">of v&rsquo;s neighbors of the partition pair. This process is repeated until </span></p><p class="c0"><span class="c14">Motivation Clearly, one naive implementation of A</span><span class="c11">RAGON </span><span class="c14">could all vertices are moved once or the decomposition cannot be fur- </span></p><p class="c0"><span class="c14">be as follows: server M[i] is responsible for the refinement of P</span><span class="c15">i </span><span class="c14">ther improved after a certain number of vertex movements. Since </span></p><p class="c0"><span class="c14">with all its partners P</span><span class="c2">i+1</span><span class="c14">,P</span><span class="c2">i+2</span><span class="c14">,&middot;&middot;&middot;,P</span><span class="c2">n</span><span class="c14">, and server M[i + 1] can </span></p><p class="c0"><span class="c27">368 </span></p><p class="c18"><span class="c14">not start its refinement for P</span><span class="c2">i+1 </span><span class="c14">until server M[i] finishes its re- finement. One major issue of this approach is that it requires the entire graph to be sent across network </span><span class="c37">n&minus;1 </span></p><p class="c18"><span class="c2">2 </span><span class="c6">times. An advantage of </span><span class="c14">this approach is that each server only needs to hold two partitions in memory at a time (one for its local partition and the other one for the refinement partner). In our prior work [48], A</span><span class="c11">RAGON </span><span class="c14">goes for another extreme, where all servers send their local partitions to a single server that is responsible for the refinement of all partition pairs. By doing this, A</span><span class="c11">RAGON </span><span class="c14">only needs to send the entire graph over network once, significantly reducing the communication traf- fic. One drawback of this approach is that it requires the server to store the entire graph in memory. Another issue is that the server can easily become a performance and scalability bottleneck. </span></p><p class="c18"><span class="c14">Overview Based on the observation above, P</span><span class="c11">ARAGON </span><span class="c14">takes a mid- dle point of the two extremes, where it allows multiple servers to do the refinement in parallel, each of which is responsible for the refinement of a group of partitions. In this way, we can enjoy the benefits of both extremes without worrying about their drawbacks. Algorithm 1 describes the main idea of P</span><span class="c11">ARAGON</span><span class="c14">. During refine- ment, each server runs an instance of the algorithm with its local partition P</span><span class="c15">l </span><span class="c3">and the relative network communication cost matrix c </span><span class="c14">as its input. The algorithm first selects a server as master node (Line 1), and then computes everything needed by the master node to make the parallization decision (Line 2). The master node decides how to split partitions into groups such that each group can be re- fined independently on different servers and the selection of group servers (Line 4&ndash;6). The group servers take responsibility of the re- finement of each group. Once the decision has been made, each server will send their vertices to the corresponding group servers (Line 7). Upon receiving all the vertices from their group members, group servers will start to do the refinement of each group indepen- dently (Line 8&ndash;13). After finishing the refinement of its group, group servers will notify their group members about the new loca- tions of their vertices (Line 15). Then, each server will physically migrate vertices to their new owners accordingly (Line 16). </span></p><p class="c18"><span class="c14">Partition Grouping To assign a partition to a group, we con- sider three factors: (1) to minimize the refinement time, each group should have roughly equal number of partitions; (2) members of each group should be carefully selected, since the gain of refining each partition pair may vary a lot. Thus, to maximize the effective- ness of refinement, we should group together partitions leading to high refinement gain; and (3) we should minimize the cross-group refinement interference, because the gain of refining one partition pair heavily relies on the amount of data they communicate with other partitions. This is different from the standard FM algorithms, which solely compute the gain of migrating each vertex based on the data it communicates with vertices of the partition pair. For example, in the decomposition of Figure 4, the communication be- tween vertex a and j contributes most to the gain of moving a from P</span><span class="c15">1 </span><span class="c3">to P</span><span class="c15">2 </span><span class="c3">for P</span><span class="c66">ARAGON</span><span class="c3">. However, for standard FM algorithms, the </span><span class="c14">gain of migrating a to P</span><span class="c2">2 </span><span class="c14">will be -1, since a has two neighbors in P</span><span class="c2">1 </span><span class="c14">and 1 in P</span><span class="c15">2</span><span class="c3">. Unfortunately, there is no clear way to do the group- </span><span class="c14">ing, since we could not use the state-of-the-art graph partitioners (i.e., M</span><span class="c11">ETIS</span><span class="c14">) to compute a high-quality initial decomposition, due to their poor scalability. As a result, the input decomposition to P</span><span class="c11">ARAGON </span><span class="c14">will probably have edge-cuts across all partitions. For- tunately, we find that random grouping along with the shuffle re- finement (the remedy technique presented below) works quite well. </span></p><p class="c18"><span class="c14">Shuffle Refinement To mitigate the impact of cross-group refine- ment interference and increase the gain of the refinement, we per- form an additional round of refinement once all the group servers finish the refinement of their own groups. We call this shuffle refine- </span></p><p class="c0"><span class="c14">Algorithm 1: P</span><span class="c11">ARAGON </span></p><p class="c0"><span class="c1">Data: P</span><span class="c15">l</span><span class="c1">,c Result: new locations of vertices of P</span><span class="c15">l </span><span class="c26">1 </span><span class="c1">masterNodeSelection(c) </span><span class="c26">2 </span><span class="c1">partitionStat(P</span><span class="c15">l</span><span class="c1">, ps) </span><span class="c26">3 </span><span class="c1">if server M[l] is master node then </span><span class="c26">4 </span><span class="c1">pg = partitionGrouping() </span><span class="c26">5 </span><span class="c1">gs = optGroupServerSelection(pg, ps, c) </span><span class="c26">6 </span><span class="c1">partitionGroupServerBcast(gs); </span><span class="c26">7 </span><span class="c1">sendPartitionToGroupServers(P</span><span class="c15">l</span><span class="c1">, gs) </span><span class="c26">8 </span><span class="c1">if server M[l] is a group server then </span><span class="c26">9 </span><span class="c1">pg = recvPartitionsFromMyGroupMembers(gs) </span><span class="c26">10 </span><span class="c1">foreach P</span><span class="c15">i </span><span class="c22">&isin; pg do </span><span class="c26">11 </span><span class="c1">foreach P</span><span class="c15">j </span><span class="c22">&isin; pg do </span><span class="c26">12 </span><span class="c1">if i = j then </span><span class="c26">13 </span><span class="c1">AragonRefinement(P</span><span class="c15">i</span><span class="c22">,P</span><span class="c15">j</span><span class="c22">,c) </span></p><p class="c0"><span class="c26">14 </span><span class="c1">shuffleRefinement(pg) </span><span class="c26">15 </span><span class="c1">vertexLocationUpdate(pg) </span></p><p class="c0"><span class="c26">16 </span><span class="c1">physicalDataMigraton(P</span><span class="c15">l</span><span class="c1">) </span></p><p class="c18"><span class="c14">ment. In this round, each group server first exchanges the changes it made to the decompositions such that each group server has the up-to-date load information of each partition and the up-to-date lo- cations of the neighbors of each vertex. Then, each group server swaps some of its partitions randomly with other group servers. Subsequently, each group server starts another round of refinement with the new grouping. </span></p><p class="c18"><span class="c14">The reason why shuffle refinement is a remedy to the above is- sue is because it increases the number of partition pairs refined by P</span><span class="c11">ARAGON </span><span class="c14">and thus the solution space that P</span><span class="c11">ARAGON </span><span class="c14">explores. For example, for a graph with 4 partitions and 2 groups, P</span><span class="c11">ARAGON </span><span class="c14">originally only refines 2 out of the 6 partition pairs. However, if the group servers swap one of their partitions, P</span><span class="c11">ARAGON </span><span class="c14">will refine 4 partition pairs instead of 2. In fact, we can repeat this shuffle refine- ment multiple times to further expand the solution space P</span><span class="c11">ARAGON </span><span class="c14">explores, thus further alleviating the impact of cross-group refine- ment interference and increasing the gain we can obtain. </span></p><p class="c18"><span class="c14">The idea of shuffle refinement is very straightforward, but it is not easy to efficiently implement, especially the propagation of the changes that each group server made. One easy way to achieve this is to use a distributed data directory, like the one provided by Zoltan [1]. In this scheme, each group server only needs to make an update to the data directory first, and then all the group servers can pull the up-to-date locations for the neighbors of their vertices. We found that this approach is very inefficient for really big graphs in terms of both memory footprint and execution time. It requires around O(|V |+|E|) data communication. </span></p><p class="c18"><span class="c14">Another way to achieve this is to maintain an array at each group server, forming a mapping from vertex global identifiers</span><span class="c37">1 </span><span class="c14">to their locations. In this way, the exchange can easily be achieved via a single (MPI) reduce operation, requiring only O(|V |) data commu- nication. This approach is much more efficient than the distributed data directory approach in terms of execution time, but it is not memory scalable for large graphs. </span></p><p class="c18"><span class="c14">In our implementation, we adopt a variant of the second ap- proach. That is, we first chunk the entire global vertex identifier space into multiple smaller equal sized regions. Each region con- tains vertices within a contiguous range. By default, the region size </span></p><p class="c0"><span class="c2">1</span><span class="c3">In distributed graph computation, each vertex has a unique global identifier across all partitions and a unique local identifier within each partition. </span></p><p class="c0"><span class="c27">369 </span></p><p class="c12"><span class="c14">equals k = min{2</span><span class="c37">26</span><span class="c14">,|V |}, where V is the vertex set of the entire graph. Correspondingly, the exchange is split into multiple rounds. Each round only exchanges the locations of vertices of one region. With this scheme, we only need to maintain a smaller array at each group sever and thus the amount of data communication remains unchanged. Although this scheme requires scanning the edge lists of each partition multiple times, it is much more efficient than the distributed data directory approach. Degree of Refinement Parallelism Theoretically, the number of groups we can n is the number have can of partitions be any integer between 1 of the graph. Clearly, and if the </span><span class="c37">n</span><span class="c2">2 </span><span class="c6">, </span><span class="c14">number </span><span class="c6">where </span></p><p class="c18"><span class="c14">of groups equals 1, P</span><span class="c11">ARAGON </span><span class="c14">degrades to A</span><span class="c11">RAGON</span><span class="c14">, in which all servers will send their local partitions to a single group server for sequential refinement. The reason why there is an upper bound is because each group needs to have at least 2 partitions for the re- finement to proceed. Typically, the higher the number is, the faster the refinement will finish. However, there is a tradeoff between the degree of parallelism and the quality of the resulting decomposi- tion we can have. This is because the higher the number is, the fewer partitions each group will have and thus the fewer partition pairs will be refined. Given a graph with n partitions and m groups, fines tually P</span><span class="c11">ARAGON </span><span class="c14">all select </span><span class="c37">n(n&minus;1) </span></p><p class="c0"><span class="c14">only </span><span class="c2">2 </span><span class="c14">an optimal refines </span><span class="c6">partition </span><span class="c37">n(n&minus;m) </span></p><p class="c12"><span class="c14">migration </span><span class="c6">pairs. </span><span class="c2">2m </span><span class="c6">partition pairs, while A</span><span class="c73">RAGON </span><span class="c6">re- In other words, A</span><span class="c73">RAGON </span><span class="c6">will even- </span><span class="c14">destination among all partitions for each vertex, whereas P</span><span class="c11">ARAGON </span><span class="c14">only considers a subset of the partitions for each vertex. This also explains the reason why the re- sulting decompositions computed by P</span><span class="c11">ARAGON </span><span class="c14">are usually poorer than those of A</span><span class="c11">RAGON</span><span class="c14">. Fortunately, the shuffle refinement tech- nique we proposed helps to address the issue. Group Server Selection Once the master node finishes the group- ing process, it will select an optimal server for each group, such that the cost of sending partitions of the group to the group server is minimized. For example, in case of Figure 4, where we assume that P</span><span class="c15">1</span><span class="c3">,P</span><span class="c15">2</span><span class="c3">, and P</span><span class="c15">3 </span><span class="c3">are of one group, we should select server M[2] </span><span class="c14">as the group server intuitively since c(P</span><span class="c15">1</span><span class="c3">,P</span><span class="c15">2</span><span class="c3">) = c(P</span><span class="c15">2</span><span class="c3">,P</span><span class="c15">3</span><span class="c3">) = 1 </span><span class="c14">while c(P1,P</span><span class="c2">3</span><span class="c14">)=6. To achieve this, we define the cost of select- ing server M[s] as the group server for group g as: </span></p><p class="c0"><span class="c14">&sum;</span><span class="c15">P</span><span class="c47">i</span><span class="c2">&isin;g </span></p><p class="c0"><span class="c14">ps[i] &lowast; c(P</span><span class="c2">i</span><span class="c14">,P</span><span class="c2">s</span><span class="c14">) &lowast; (1 + </span><span class="c6">&sigma;(s) </span></p><p class="c0"><span class="c14">drp </span><span class="c6">) (10) </span></p><p class="c18"><span class="c14">Here, ps[i] denotes the number of edges associated with vertices of P</span><span class="c15">i</span><span class="c3">, which is a good approximation for the amount of data each </span><span class="c14">server needs to send to their group servers. &sigma;(s) is the number of group servers that have been designated on the compute node that server M[s] belongs to. It should be noticed that server M[s] can be a hardware thread, a core, a socket, or a machine. drp is the degree of refinement parallelism (number of group servers). The last centration term (1 of + multiple </span><span class="c37">&sigma;(s) </span></p><p class="c0"><span class="c2">drp </span><span class="c6">) is </span><span class="c14">group </span><span class="c6">the penalty </span><span class="c14">servers </span><span class="c6">that </span><span class="c14">into </span><span class="c6">is </span><span class="c14">a </span><span class="c6">added </span><span class="c14">single </span><span class="c6">to </span><span class="c14">compute </span><span class="c6">avoid the </span><span class="c14">node, </span><span class="c6">con- </span></p><p class="c0"><span class="c14">reducing the chance of memory exhaustion. Once all group servers are selected, the master node will broadcast the group servers of all groups to all slave nodes. Then, each server will send its vertices (as well as their edge lists) to their corresponding group servers, after which the group servers will start to refine partitions of their own groups independently. Reducing Communication Volume Clearly, P</span><span class="c11">ARAGON </span><span class="c14">with the shuffle refinement disabled requires the entire graph to be sent over the network once, and P</span><span class="c11">ARAGON </span><span class="c14">with the shuffle refinement en- abled demands more data communication. For really big graphs, the communication volume may get very high. Thus, we follow the same approach proposed in [35] to reduce the communication </span></p><p class="c18"><span class="c14">volume. Specifically, instead of sending the entire partition to their group servers, each server only needs to send vertices that can be reached by a breadth-first search from boundary vertices of each partition within k-hop traversal. Boundary vertices are vertices that have neighbors in other partitions. The rationale behind this is that if a vertex is very far from the boundary vertex, the chance that it get moved by P</span><span class="c11">ARAGON </span><span class="c14">to another partition to improve the decom- position is very small. Surprisingly, we find that P</span><span class="c11">ARAGON </span><span class="c14">is not sensitive to k in terms of the partitioning quality, and that a larger k does not always lead to partitionings of higher quality. However, it may increase the refinement time greatly. Thus, in our imple- mentation, we set k = 0 by default. In other words, we only send boundary vertices of each partition to the group servers. </span></p><p class="c0"><span class="c14">In fact, [35] has presented a solution to parallellize the standard FM algorithms [12]. However, it may require a graph with n parti- tions to be sent over the network n &minus; 1 times in case the initial de- composition has edge-cuts across all partition pairs. Furthermore, the presence of communication heterogeneity complicates things greatly. First, A</span><span class="c11">RAGON </span><span class="c14">has to be applied to all partition pairs, whereas standard FM algorithms, which assume uniform network communication costs, only need to refine partition pairs that have edge-cuts between them. Second, during each refinement iteration of a single partition pair, standard FM algorithms only need to con- sider migrating vertices of both partitions that have neighbors in the alternative partition. On the other hand, P</span><span class="c11">ARAGON </span><span class="c14">has to consider migrating all boundary vertices. Master Node Selection As presented so far, each server (slave node) needs to send some auxiliary data (i.e., the number of ver- tices/neighbors) of their local partitions to the master node for the parallelization decision, and the master needs to broadcast the deci- sion it made to all slave nodes. To reduce the communication cost between the master node and the slave nodes, we also select the master node in an intelligent way using the following heuristic: </span></p><p class="c0"><span class="c14">min </span><span class="c15">m&isin;[1,n] </span></p><p class="c25"><span class="c3">&sum; </span><span class="c2">ni=1 and i =m </span></p><p class="c0"><span class="c14">c(P</span><span class="c15">i</span><span class="c3">,P</span><span class="c15">m</span><span class="c3">) (11) </span></p><p class="c0"><span class="c14">The heuristic tries to find a server M[m] that will result in minimal network communication cost as the master node. For example, in case of Figure 4, we should select server M[2] as the master node. Clearly, the selection of master node can be made locally by each server without synchronizing with each other. Physical Data Migration To support efficient distributed com- putation, we also provide a basic migration service for graph work- loads. Considering that physical data migration is highly application- dependent, the migration service only takes responsibility for the redistribution of the graph data itself. It is the users who are re- sponsible for the migration of any application data associated with each vertex. That is, the users should save the application context before using our migration service and restore the context after- wards. For example, in breadth first search, each vertex is usually associated with a value indicating its current distance to the source vertex. Users need to keep track of the distance value of each vertex while migrating. For complicated workloads, users can exploit the migration service provided by Zoltan [1] to simplify the migration. </span></p><p class="c0"><span class="c31">6. CONTENTION AWARENESS </span></p><p class="c18"><span class="c14">So far, we have presented how we parallelize A</span><span class="c11">RAGON</span><span class="c14">. In this section, we will cover how we make P</span><span class="c11">ARAGON </span><span class="c14">aware of the issue of shared resource contention in multicore systems. We know that, guided by a given network communication cost matrix, P</span><span class="c11">ARAGON </span><span class="c14">is able to gather neighboring vertices as close as possible, and that </span></p><p class="c0"><span class="c27">370 </span></p><p class="c18"><span class="c14">the contention is caused by the fact that we put too much commu- nication within the compute nodes. Hence, to avoid serious intra- node shared resource contention, we can simply penalize intra-node network communication costs by a score. The score is computed based on the degree of contentiousness between the communica- tion peers. By doing this, the amount of intra-node communication will decrease accordingly. In our implementation, we refine the intra-node communication costs as follows: </span></p><p class="c0"><span class="c14">c(P</span><span class="c15">i</span><span class="c3">,P</span><span class="c15">j</span><span class="c3">) = c(P</span><span class="c15">i</span><span class="c3">,P</span><span class="c15">j</span><span class="c3">) + &lambda; &lowast; (s</span><span class="c15">1 </span><span class="c3">+ s</span><span class="c15">2</span><span class="c3">) (12) </span></p><p class="c18"><span class="c14">where P</span><span class="c2">i </span><span class="c14">and P</span><span class="c2">j </span><span class="c14">are two partitions collocated in a single compute node; &lambda; is a value between 0 and 1, denoting the degree of con- tention; and s</span><span class="c15">1 </span><span class="c3">denotes the maximal inter-node network commu- </span><span class="c14">nication cost, while s</span><span class="c2">2 </span><span class="c14">equals 0 if P</span><span class="c2">i </span><span class="c14">and P</span><span class="c2">j </span><span class="c14">reside on different sockets and equals the maximal inter-socket network communica- tion cost otherwise. Clearly, if &lambda; = 0, P</span><span class="c11">ARAGON </span><span class="c14">will only consider the communication heterogeneity, and &lambda; = 1 means that intra-node shared resource contention is the biggest performance bottleneck, which should be prioritized over the communication heterogeneity. It should be noticed that P</span><span class="c11">ARAGON </span><span class="c14">with any &lambda; &isin; (0,1] considers both the contention and the communication heterogeneity. Consid- ering the impact of both resource contention and communication heterogeneity is highly application- and hardware-dependent, users will need to do simple profiling of the target applications on the ac- tual computing environment to determine the ideal &lambda; for them. </span></p><p class="c0"><span class="c31">7. EVALUATION </span></p><p class="c0"><span class="c14">In this section, we first evaluate the sensitivity of P</span><span class="c11">ARAGON </span><span class="c14">to varying input decompositions computed by different initial parti- tioners and the impact of its two important parameters: the degree of parallelism and the number of shuffle refinement times (Sec- tion 7.1). We then validate the effectiveness of P</span><span class="c11">ARAGON </span><span class="c14">using two real-world graph workloads: Breadth-First Search (BFS) [4] and Single-Source Shortest Path (SSSP) [20], which we implemented using MPI (Section 7.2). Finally, we demonstrate the scalability of P</span><span class="c11">ARAGON </span><span class="c14">via a billion-edge graph (Section 7.3). Datasets Table 2 describes the datasets used. By default, the graphs were (re)partitioned with vertex weights (i.e., computational requirement) set to be their vertex degree, with vertex sizes (i.e., amount of the data of the vertex) set to be their vertex degree, and with edge weights (i.e., amount of data communicated) set to 1. The degree of each vertex is often a good approximation of the computational requirement and the migration cost of each vertex, while a uniform edge weight of 1 is a close estimation of the com- munication pattern of many graph algorithms, like BFS and SSSP. Given the fact that communication cost is usually more impor- tant than migration cost, all the experiments were performed with &alpha; = 10 (Eq. 2). Unless explicitly specified, all the graphs were initially partitioned by DG (deterministic greedy heuristic), a state- of-the-art streaming graph partitioner [39], across cores of the com- pute node used (one partition per core). The partitionings were then improved by P</span><span class="c11">ARAGON</span><span class="c14">. During the (re)partitioning, we allowed up to 2% load imbalance among partitions. For fairness, DG/LDG were extended to support vertex- and edge-weighted graphs. Platforms We evaluated P</span><span class="c11">ARAGON </span><span class="c14">on two clusters: PittMPIClus- ter [32] and Gordon supercomputer [28]. PittMPICluster had a flat network topology, with all 32 compute nodes connected to a single switch via 56GB/s FDR Infiniband. On the other hand, the Gordon network topology was a 4x4x4 3D torus of switches connected via 8GB/s QDR Infiniband with 16 compute nodes attached to each switch. Table 3 depicts the compute node configuration of both clusters. The results presented were the means of 5 runs, except the </span></p><p class="c25"><span class="c14">Table 2: Datasets used in our experiments </span><span class="c44">Dataset |V | |E| Description wave [38] 156,317 2,118,662 2D/3D FEM auto [38] 448,695 6,629,222 3D FEM 333SP [10] 3,712,815 22,217,266 2D FE Triangular Meshes CA-CondMat [2] 108,300 373,756 Collaboration Network DBLP [18] 317,080 1,049,866 Collaboration Network Email-Eron [2] 36,692 183,831 Communication Network </span></p><p class="c12"><span class="c44">as-skitter [2] 1,696,415 22,190,596 Internet Topology Amazon [2] 334,863 925,872 Product Network USA-roadNet [9] 23,947,347 58,333,344 Road Network PA-roadNet [2] 1,090,919 6,167,592 Road Network </span></p><p class="c12"><span class="c44">YouTube [18] 3,223,589 24,447,548 Social Network com-LiveJournal [2] 4,036,537 69,362,378 Social Network Friendster [2] 124,836,180 3,612,134,270 Social Network </span></p><p class="c0"><span class="c14">Table 3: Cluster Compute Node Configuration </span><span class="c44">Node Configuration (Intel </span><span class="c61">PittMPICluster </span></p><p class="c0"><span class="c44">Haswell Processor) </span></p><p class="c12"><span class="c44">Gordon (Intel Sandy Bridge Processor) Sockets 2 2 </span></p><p class="c25"><span class="c44">Cores 20 16 Clock Speed 2.6 GHz 2.6 GHz L3 Cache 25 MB 20 MB Memory Capacity 128 GB 64 GB Memory Bandwidth 65 GB/s 85 GB/s </span></p><p class="c0"><span class="c14">execution of SSSP on Gordon (Section 7.2) and the scalability test (Section 7.3). Network Communication Cost Modelling The relative network communication costs among partitions (cores) were approximated using a variant of the osu_latency benchmark [29]. To ensure the correctness of the cost matrix, each MPI rank (process) was bound to a core using the mechanism provided by MVAPICH2 1.9 [25] on Gordon and OpenMPI 1.8.6 [27] on PittMPICluster. MVAPICH2 and OpenMPI were two different MPI implementations available on the clusters. </span><span class="c31">7.1 MicroBenchmarks </span></p><p class="c0"><span class="c21">7.1.1 Varying Degree of Parallelism </span></p><p class="c0"><span class="c14">Configuration In this experiment, we examined the impact of the degree of parallelism in terms of both the refinement time (i.e., the time that the refinement took) and the refinement quality (i.e., the communication cost of the resulting decomposition). Towards this, we first partitioned the com-lj dataset into 40 partitions using DG across 2 compute nodes of PittMPICluster, and then applied P</span><span class="c11">ARAGON </span><span class="c14">to the decompositions with varying degree of refinement parallelism but with shuffle refinement disabled. Results (Figures 7a &amp; 7b) Figure 7a plots the runtime of P</span><span class="c11">ARAGON </span><span class="c14">on the com-lj dataset for various degrees of parallelism. As ex- pected, the higher the degree of parallelism, the faster the refine- ment would finish, and P</span><span class="c11">ARAGON </span><span class="c14">significantly reduced the refine- ment time of A</span><span class="c11">RAGON </span><span class="c14">(P</span><span class="c11">ARAGON </span><span class="c14">with degree of parallelism of 1). However, the speedup was achieved at the cost of higher communi- cation cost of the resulting decompositions (Figure 7b). The com- munication costs presented were normalized to that of the initial decomposition computed by DG. However, in the end, P</span><span class="c11">ARAGON </span><span class="c14">still resulted in lower communication cost in all cases when com- pared to the initial decompositions. </span></p><p class="c0"><span class="c21">7.1.2 Impact of Shuffle Refinement </span></p><p class="c18"><span class="c14">Configuration In our second experiment, we were interested to see whether the shuffle refinement technique could address the issue we identified in the previous experiment. Towards this, we repeated the same experiment but with a fixed degree of refinement parallelism (8) and varying number of shuffle refinement times (from 8 to 15). </span></p><p class="c0"><span class="c27">371 </span></p><p class="c0"><span class="c36">0.0 </span><span class="c89">) s(emiTt nemenif</span><span class="c52">e</span><span class="c29">R</span><span class="c52">35 30 25 20 15 10 5 0 </span></p><p class="c0"><span class="c52">1 2 4 6 8 </span><span class="c52">Degree of Refinement </span><span class="c52">1</span><span class="c29">0 </span><span class="c52">1</span><span class="c29">2 </span><span class="c52">Parallelism </span></p><p class="c25"><span class="c52">1</span><span class="c29">4 </span><span class="c52">1</span><span class="c29">6 </span><span class="c52">1</span><span class="c29">8 </span><span class="c52">2</span><span class="c29">0 </span><span class="c83">t soCm moCd ezilamr</span><span class="c36">o</span><span class="c63">N</span><span class="c83">1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 </span><span class="c36">1 2 4 6 8 </span><span class="c36">Degree of Refinement </span><span class="c36">1</span><span class="c63">0 </span><span class="c36">1</span><span class="c63">2 </span><span class="c36">Parallelism </span><span class="c36">1</span><span class="c63">4 </span><span class="c36">1</span><span class="c63">6 </span><span class="c36">1</span><span class="c63">8 </span><span class="c36">2</span><span class="c63">0 </span><span class="c1">(a) Refinement time (b) Normalized communication cost of the resulting decompositions </span><span class="c3">Figure 7: Refinement time and communication costs of the com-lj decompositions after being refined with varying degree of refinement </span><span class="c14">parallelism on two 20-core compute nodes. The communication costs presented were normalized to that of the initial decomposition. </span></p><p class="c25"><span class="c70">1.4 1.2 1 0.8 0.6 0.4 0.2 </span><span class="c77">0 0 2 4 6 8 10 12 Refinement Time (s) </span></p><p class="c0"><span class="c90">) 7^01(</span><span class="c46">140 </span><span class="c90">t soC</span><span class="c46">C</span><span class="c46">w</span><span class="c41">ave</span><span class="c46">a</span><span class="c41">uto</span><span class="c46">3</span><span class="c41">33S</span><span class="c46">r</span><span class="c41">PoadNet&minus;PA </span></p><p class="c0"><span class="c17">HP</span><span class="c50">DGLDG METIS </span></p><p class="c18"><span class="c90">m mo</span><span class="c14">Figure 9: Communication cost of the initial decompositions com- puted by HP, DG, LDG, and M</span><span class="c11">ETIS </span><span class="c14">across cores of two 20-core compute nodes for a variety of graphs. </span></p><p class="c0"><span class="c14">scratch). It was reported in prior work [42] that M</span><span class="c11">ETIS </span><span class="c14">took up to 8.5 hours to partition a graph with 1.46 billion edges. Unex- pectedly, DG outperformed LDG, the best streaming partitioning heuristic among the ones presented in [39]. This was probably be- cause the order in which the vertices were presented to the parti- tioner favored DG over LDG (the results of DG and LDG rely on the order in which vertices are presented). This was also the reason why we picked DG as the default initial partitioner for P</span><span class="c11">ARAGON</span><span class="c14">. Quality of the Resulting Decompositions (Figures 10a &amp; 10b) Figures 10a and 10b show the corresponding communication cost of the resulting decompositions and the improvement achieved by P</span><span class="c11">ARAGON </span><span class="c14">in terms of the communication cost when compared to the initial decompositions. As shown, the better the initial decom- position was, the better the resulting decomposition would be. In comparison with the initial decompositions computed by HP, DG, and LDG, P</span><span class="c11">ARAGON </span><span class="c14">reduced the communication cost of the de- compositions by up to 58% (43% on average), 29% (17% on aver- age), and 53% (36% on average), respectively. Although P</span><span class="c11">ARAGON </span><span class="c14">did not improve significantly the decompositions computed by M</span><span class="c11">ETIS </span><span class="c14">for easily partitioned FEM and road networks (left 7 datasets), it achieved an improvement of up to 4.5% for complex networks (right 5 datasets). Given the size of the dataset, the improvement was still non-negligible. Fortunately, we found that P</span><span class="c11">ARAGON </span><span class="c14">with DG as its initial partitioner can achieve even better performance than M</span><span class="c11">ETIS </span><span class="c14">on real-world workloads (Section 7.2). Refinement Overhead (Figures 11a &amp; 11b) We also noticed that the quality of the initial decomposition impacted the refine- ment overhead greatly. Figures 11a and 11b plot the migration cost (Eq. 3) and the refinement time. Clearly, the poorer the initial decomposition was, the higher the migration cost and the longer the refinement time would be. Finally, for decompositions, which P</span><span class="c11">ARAGON </span><span class="c14">failed to make much improvement, P</span><span class="c11">ARAGON </span><span class="c14">only led to a very small amount of overhead. </span></p><p class="c0"><span class="c27">372 </span></p><p class="c12"><span class="c46">120 100 </span><span class="c77">0 </span><span class="c100">1 2 3 4 5 6 7 8 9 10111213 1415 </span></p><p class="c0"><span class="c46">80 </span></p><p class="c18"><span class="c14">Figure 8: Y-axis corresponds to the communication costs of the com-lj decompositions after being refined with varying number of shuffle refinement times on two 20-core compute nodes when they were normalized to that of the decompositions refined by A</span><span class="c11">RAGON</span><span class="c14">; X-axis denotes the corresponding refinement time; the labels on each data point were the number of refinement times. </span></p><p class="c18"><span class="c14">Results (Figure 8) Figure 8 shows the corresponding refinement time and the normalized communication costs of resulting decom- positions with the decompositions computed by A</span><span class="c11">RAGON </span><span class="c14">as the baseline. As shown, P</span><span class="c11">ARAGON </span><span class="c14">(with shuffle refinement enabled) not only produced decompositions of lower communication costs than A</span><span class="c11">RAGON </span><span class="c14">(when the number of shuffle refinement times was greater than 11), but also completed the refinement faster (A</span><span class="c11">RAGON </span><span class="c14">took around 33s to finish the refinement vs 8.12s by P</span><span class="c11">ARAGON </span><span class="c14">with 11 shuffle refinement times). </span></p><p class="c0"><span class="c21">7.1.3 Impact of Initial Partitioners </span></p><p class="c0"><span class="c14">Configuration This experiment examined the refinement overhead and the quality of the resulting decompositions, when P</span><span class="c11">ARAGON </span><span class="c14">was provided with decompositions computed by four different par- titioners: (a) HP, the default graph partitioner of many parallel graph computing engines; (b) DG and LDG, two state-of-the-art streaming graph partitioning heuristics [39]; and (c) M</span><span class="c11">ETIS</span><span class="c14">, a state- of-the-art multi-level graph partitioner [23]. The graphs were ini- tially partitioned across the same two machines used in our prior experiments but with both the degree of refinement parallelism and the number of shuffle refinement times set to 8. Quality of the Initial Decompositions (Figure 9) Figure 9 de- notes the communication cost of the initial decompositions com- puted by HP, DG, LDG, and M</span><span class="c11">ETIS </span><span class="c14">for a variety of graphs. As anticipated, M</span><span class="c11">ETIS </span><span class="c14">performed the best and HP the worst. How- ever, M</span><span class="c11">ETIS </span><span class="c14">is a heavyweight serial graph partitioner, making it in- feasible for large-scale distributed graph computation either as an initial partitioner or as an online repartitioner (repartitioning from </span></p><p class="c12"><span class="c46">60 40 20 0 </span></p><p class="c0"><span class="c46">U</span><span class="c41">SA&minus;</span><span class="c46">C</span><span class="c41">rAo&minus;aCd</span><span class="c46">c</span><span class="c41">o&minus;ond </span></p><p class="c0"><span class="c41">md&minus;M</span><span class="c46">c</span><span class="c41">daobt </span></p><p class="c0"><span class="c41">ml&minus;p </span></p><p class="c0"><span class="c46">E</span><span class="c41">ammaaiz</span><span class="c46">Y</span><span class="c41">loo&minus;n </span></p><p class="c12"><span class="c41">uETn</span><span class="c46">a</span><span class="c41">ursbo&minus;e n s</span><span class="c46">c</span><span class="c41">koimt&minus;tlej r </span></p><p class="c12"><span class="c40">0 </span><span class="c56">) 7^01(t soCm mo</span><span class="c40">C</span><span class="c56">100 90 80 70 60 50 40 30 20 10 </span><span class="c46">PARAGON+HP </span><span class="c41">PARAGON+DG PARAGON+LDG PARAGON+METIS </span></p><p class="c0"><span class="c40">w</span><span class="c5">ave</span><span class="c40">a</span><span class="c5">uto</span><span class="c40">3</span><span class="c5">33S</span><span class="c40">r</span><span class="c5">PoadNet&minus;PA </span><span class="c1">(a) Communication cost of the decompositions after being refined. </span></p><p class="c0"><span class="c43">100% </span><span class="c80">t nemevorpm</span><span class="c43">I</span><span class="c89">PARAGON+HP </span><span class="c52">PARAGON+DG </span><span class="c29">PARAGON+LDG PARAGON+METIS </span></p><p class="c0"><span class="c43">w</span><span class="c54">ave</span><span class="c43">a</span><span class="c54">uto</span><span class="c43">3</span><span class="c54">33S</span><span class="c43">r</span><span class="c54">PoadNet&minus;PA </span><span class="c3">Figure 10: P</span><span class="c66">ARAGON</span><span class="c3">&rsquo;s sensitivity to varying initial decompositions in terms </span><span class="c1">(b) Improvement </span><span class="c3">of the communication </span><span class="c1">achieved by </span><span class="c3">cost </span><span class="c1">P</span><span class="c8">ARAGON </span><span class="c3">for a variety </span><span class="c1">against the </span><span class="c3">of </span><span class="c1">initial </span><span class="c3">graphs, </span><span class="c1">decomposition. </span><span class="c3">which were </span><span class="c14">initially partitioned by HP, DG, LDG, and M</span><span class="c11">ETIS </span><span class="c14">across cores of two 20-core compute nodes. </span></p><p class="c0"><span class="c45">0 </span></p><p class="c0"><span class="c43">80% </span></p><p class="c0"><span class="c43">60% </span></p><p class="c0"><span class="c43">40% </span></p><p class="c0"><span class="c43">20% </span></p><p class="c0"><span class="c40">U</span><span class="c5">SA&minus;</span><span class="c40">C</span><span class="c5">rAo&minus;aCd</span><span class="c40">c</span><span class="c5">o&minus;ond </span></p><p class="c0"><span class="c5">md&minus;M</span><span class="c40">c</span><span class="c5">daobt </span></p><p class="c0"><span class="c5">ml&minus;p </span></p><p class="c0"><span class="c40">E</span><span class="c5">ammaaiz</span><span class="c40">Y</span><span class="c5">loo&minus;n </span></p><p class="c12"><span class="c5">uETn</span><span class="c40">a</span><span class="c5">ursbo&minus;e n s</span><span class="c40">c</span><span class="c5">koimt&minus;tlej r </span></p><p class="c0"><span class="c43">0% </span></p><p class="c0"><span class="c43">U</span><span class="c54">SA&minus;</span><span class="c43">C</span><span class="c54">rAo&minus;aCd</span><span class="c43">c</span><span class="c54">o&minus;ond </span></p><p class="c0"><span class="c54">md&minus;M</span><span class="c43">c</span><span class="c54">daobt </span></p><p class="c0"><span class="c54">ml&minus;p </span></p><p class="c0"><span class="c43">E</span><span class="c54">ammaaiz</span><span class="c43">Y</span><span class="c54">loo&minus;n </span></p><p class="c12"><span class="c54">uETn</span><span class="c43">a</span><span class="c54">ursbo&minus;e n s</span><span class="c43">c</span><span class="c54">koimt&minus;tlej r </span></p><p class="c0"><span class="c82">) 7^01(t soC</span><span class="c75">PARAGON+HP </span><span class="c81">PARAGON+DG PARAGON+LDG PARAGON+METIS </span><span class="c82">n oitarg</span><span class="c45">i</span><span class="c7">M</span><span class="c45">7 </span></p><p class="c0"><span class="c45">w</span><span class="c7">ave</span><span class="c45">a</span><span class="c7">uto</span><span class="c45">3</span><span class="c7">33S</span><span class="c45">r</span><span class="c7">PoadNet&minus;PA </span></p><p class="c0"><span class="c45">c</span><span class="c7">om&minus;lj </span><span class="c1">(a) Migration Cost </span></p><p class="c12"><span class="c30">) s(emiTt nemenife</span><span class="c45">R</span><span class="c75">PARAGON+HP </span><span class="c95">PARAGON+DG PARAGON+LDG PARAGON+METIS </span><span class="c45">c</span><span class="c23">om&minus;lj </span><span class="c1">(b) Refinement Time </span><span class="c3">Figure 11: Overhead of the refinement on varying decompositions that were initially partitioned by HP, DG, LDG, and M</span><span class="c66">ETIS </span><span class="c3">across cores </span><span class="c14">of two 20-core compute nodes. </span></p><p class="c0"><span class="c31">7.2 Real-World Applications (BFS &amp; SSSP) </span></p><p class="c18"><span class="c14">Configuration This experiment evaluated P</span><span class="c11">ARAGON </span><span class="c14">using BFS and SSSP on the YouTube, as-skitter, and com-lj datasets. Initially, the graphs were partitioned across cores of three compute nodes of two clusters using DG. Then, the decomposition was improved by P</span><span class="c11">ARAGON </span><span class="c14">with the degree of refinement parallelism and the num- ber of shuffle refinement times both set to 8. During the execution of BFS/SSSP, we grouped multiple (8 for YouTube and as-skitter dataset and 16 for com-lj dataset) messages sent by each MPI rank to the same destination into a single one. </span></p><p class="c18"><span class="c14">Resource Contention Modeling To capture the impact of resource contention, we carried out a profiling experiment for BFS and SSSP with the 3 datasets on both clusters by increasing &lambda; gradually from 0 to 1. Interestingly, we found that intra-node shared resource con- tention was more critical to the performance on PittMPICluster, while inter-node communication was the bottleneck on Gordon. This was probably caused by the differences in network topolo- gies (flat vs hierarchical), core count per node (20 vs 16), mem- ory bandwidth (65GB vs 85GB), and network bandwidth (56GB vs 8GB) between the two clusters, and that BFS/SSSP had to compete with other jobs running on Gordon for the network resource, while there was no contention on the network communication links on PittMPICluster. Hence, we fixed &lambda; to be 1 on PittMPICluster and 0 on Gordon for the experiment. </span></p><p class="c12"><span class="c14">Job Execution Time (Tables 4 &amp; 5) Tables 4 and 5 show the overall execution time of BFS and SSSP with 15 randomly selected source vertices on the three datasets and the overhead of P</span><span class="c11">ARAGON</span><span class="c14">. The JET job has, execution = while </span><span class="c6">&sum;</span><span class="c2">n</span><span class="c15">i=1 </span><span class="c14">time SET(i) SET(i), of a denotes distributed where the n graph computation is defined as: is the number of supersteps the execution time of the ith super- </span></p><p class="c0"><span class="c45">35 </span><span class="c45">6 </span></p><p class="c0"><span class="c45">30 </span><span class="c45">5 </span></p><p class="c0"><span class="c45">25 </span><span class="c45">4 </span></p><p class="c0"><span class="c45">20 </span><span class="c45">3 </span></p><p class="c0"><span class="c45">15 </span><span class="c45">2 </span></p><p class="c0"><span class="c45">10 </span><span class="c45">1 </span></p><p class="c0"><span class="c45">5 </span><span class="c45">U</span><span class="c7">SA&minus;</span><span class="c45">C</span><span class="c7">rAo&minus;aCd</span><span class="c45">c</span><span class="c7">o&minus;ond </span></p><p class="c0"><span class="c7">md&minus;M</span><span class="c45">c</span><span class="c7">daobt </span></p><p class="c0"><span class="c7">ml&minus;p </span></p><p class="c0"><span class="c45">E</span><span class="c7">ammaaiz</span><span class="c45">Y</span><span class="c7">loo&minus;n </span></p><p class="c12"><span class="c7">uETn</span><span class="c45">a</span><span class="c7">ursbo&minus;e n skitter </span></p><p class="c0"><span class="c45">0 </span></p><p class="c0"><span class="c45">w</span><span class="c23">ave</span><span class="c45">a</span><span class="c23">uto</span><span class="c45">3</span><span class="c23">33S</span><span class="c45">r</span><span class="c23">Poad</span><span class="c45">U</span><span class="c23">NSeAt&minus;&minus;</span><span class="c45">C</span><span class="c23">rPAoA </span></p><p class="c0"><span class="c23">&minus;aCd</span><span class="c45">c</span><span class="c23">o&minus;ond </span></p><p class="c0"><span class="c23">md&minus;M</span><span class="c45">c</span><span class="c23">daobt </span></p><p class="c0"><span class="c23">ml&minus;p </span></p><p class="c0"><span class="c45">E</span><span class="c23">ammaaiz</span><span class="c45">Y</span><span class="c23">loo&minus;n </span></p><p class="c18"><span class="c23">uETn</span><span class="c45">a</span><span class="c23">ursbo&minus;e n skitter </span></p><p class="c18"><span class="c14">step and is defined as the ith superstep execution time of the slowest MPI rank. In the table, DG and M</span><span class="c11">ETIS </span><span class="c14">mean that BFS/SSSP was performed on the datasets without any repartitioning/refinement, P</span><span class="c11">AR</span><span class="c14">M</span><span class="c11">ETIS </span><span class="c14">is a state-of-the-art multi-level graph repartitioner [30], </span><span class="c11">UNI</span><span class="c14">P</span><span class="c11">ARAGON </span><span class="c14">was a variant of P</span><span class="c11">ARAGON </span><span class="c14">that assumes homoge- neous and contention-free computing environment, and the num- bers within the parentheses were the overhead of repartitioning/re- fining the decomposition computed by DG. </span></p><p class="c18"><span class="c14">As expected, P</span><span class="c11">ARAGON </span><span class="c14">beat DG, P</span><span class="c11">AR</span><span class="c14">M</span><span class="c11">ETIS</span><span class="c14">, and </span><span class="c11">UNI</span><span class="c14">P</span><span class="c11">ARAGON </span><span class="c14">in all cases. Compared to DG, P</span><span class="c11">ARAGON </span><span class="c14">reduced the execution time of BFS and SSSP on Gordon by up to 60% and 62%, respec- tively, and up to 83% and 78% on PittMPICluster, respectively. If we time the improvements by the number of MPI ranks (48 for Gordon and 64 for PittMPICluster), the improvements were more remarkable. Yet, the overhead P</span><span class="c11">ARAGON </span><span class="c14">exerted (the sum refine- ment time and physical data migration time) was very small in com- parison to the improvement it achieved and the job execution time. By comparing the results of </span><span class="c11">UNI</span><span class="c14">P</span><span class="c11">ARAGON </span><span class="c14">with DG, we can con- clude that P</span><span class="c11">ARAGON </span><span class="c14">not only improved the mapping of the appli- cation communication pattern to the underlying hardware, but also the quality of the initial decomposition (edge-cut). Also, if we com- pare the execution time of BFS/SSSP on both clusters, we would find that the speedup P</span><span class="c11">ARAGON </span><span class="c14">achieved by increasing the number of cores from 48 to 60 was much higher than that of DG. What we did not expect was that P</span><span class="c11">ARAGON </span><span class="c14">with DG as its initial partitioner outperformed the gold standard, M</span><span class="c11">ETIS</span><span class="c14">, in 4 out the 6 cases and was comparable to M</span><span class="c11">ETIS </span><span class="c14">in other cases. </span></p><p class="c18"><span class="c14">Communication Volume Breakdown (Figures 12 &amp; 13) To fur- ther confirm our observations, we also collected the total amount of data remotely exchanged per superstep by BFS and SSSP among cores of the same socket (intra-socket communication volume), </span></p><p class="c0"><span class="c27">373 </span></p><p class="c25"><span class="c14">Table 5: SSSP Job Execution Time (s) </span><span class="c1">Algorithm/Dataset YouTube as-skitter com-lj </span></p><p class="c25"><span class="c1">PittMPICluster DG 2136 1823 5196 M</span><span class="c8">ETIS </span><span class="c1">545 822 955 P</span><span class="c8">AR</span><span class="c1">M</span><span class="c8">ETIS </span><span class="c1">1842 (19.00) 582 (9.28) 3268 (4.50) </span><span class="c8">UNI</span><span class="c1">P</span><span class="c8">ARAGON </span><span class="c1">1805 (2.45) 1031 (2.07) 3136 (6.98) P</span><span class="c8">ARAGON </span><span class="c1">468 (3.88) 472 (3.14) 1549 (9.71) </span></p><p class="c25"><span class="c1">Gordon DG 3436 7092 10732 </span><span class="c8">UNI</span><span class="c1">P</span><span class="c8">ARAGON </span><span class="c1">3402 (2.76) 3355 (2.13) 7831 (9.75) P</span><span class="c8">ARAGON </span><span class="c1">2838 (3.89) 2731 (2.97) 6841 (29.00) </span></p><p class="c0"><span class="c24">0 </span></p><p class="c25"><span class="c14">Table 4: BFS Job Execution Time (s) </span><span class="c1">Algorithm/Dataset YouTube as-skitter com-lj </span></p><p class="c25"><span class="c1">PittMPICluster DG 30 59 218 M</span><span class="c8">ETIS </span><span class="c1">8.50 67 27 P</span><span class="c8">AR</span><span class="c1">M</span><span class="c8">ETIS </span><span class="c1">29 (21.00) 59 (9.65) 185 (4.71) </span><span class="c8">UNI</span><span class="c1">P</span><span class="c8">ARAGON </span><span class="c1">25 (2.70) 27 (2.26) 159 (7.54) P</span><span class="c8">ARAGON </span><span class="c1">8 (4.00) 10 (3.31) 40 (10.00) </span></p><p class="c25"><span class="c1">Gordon DG 322 577 4319 </span><span class="c8">UNI</span><span class="c1">P</span><span class="c8">ARAGON </span><span class="c1">264 (2.70) 350 (2.07) 3310 (6.98) P</span><span class="c8">ARAGON </span><span class="c1">220 (3.83) 228 (2.96) 2586 (9.08) </span></p><p class="c0"><span class="c65">) BM(emuloV</span><span class="c24">2,500 2,000 1,500 </span></p><p class="c0"><span class="c98">Inter&minus;Node </span><span class="c86">Inter&minus;Socket Intra&minus;Socket </span><span class="c24">1,000 </span></p><p class="c0"><span class="c65">m mo</span><span class="c24">CYouTube as&minus;skitter com&minus;lj </span></p><p class="c0"><span class="c24">Inter&minus;Node </span><span class="c35">Inter&minus;Socket Intra&minus;Socket </span></p><p class="c0"><span class="c24">500 </span></p><p class="c0"><span class="c24">D</span><span class="c35">G </span><span class="c24">M</span><span class="c35">ET</span><span class="c24">P</span><span class="c35">ISAR</span><span class="c24">u</span><span class="c35">MEniT</span><span class="c24">P</span><span class="c35">PIS </span></p><p class="c0"><span class="c35">AARRAAGGOON </span></p><p class="c0"><span class="c35">N</span><span class="c24">D</span><span class="c35">G </span><span class="c24">M</span><span class="c35">ET</span><span class="c24">P</span><span class="c35">ISAR</span><span class="c24">u</span><span class="c35">MEniT</span><span class="c24">P</span><span class="c35">PIAS </span></p><p class="c0"><span class="c35">ARRAAGGOON </span></p><p class="c0"><span class="c35">N</span><span class="c24">D</span><span class="c35">G </span><span class="c24">M</span><span class="c35">ET</span><span class="c24">P</span><span class="c35">ISAR</span><span class="c24">u</span><span class="c35">MEniTI</span><span class="c24">P</span><span class="c35">PAS </span></p><p class="c0"><span class="c35">ARRAAGGOON N </span></p><p class="c0"><span class="c14">Figure 12: The breakdown of the accumulated communication volume </span></p><p class="c0"><span class="c14">Figure 13: The breakdown of the accumulated communication volume across all supersteps for BFS on PittMPICluster. </span></p><p class="c0"><span class="c14">across all supersteps for BFS on Gordon. </span></p><p class="c18"><span class="c14">among cores of the same compute node but belonging to differ- ent sockets (inter-socket communication volume), and among cores of different compute nodes (inter-node communication volume). Since we observed similar patterns for BFS and SSSP in all the cases, we only present the breakdown of the accumulated commu- nication volume across all supersteps for BFS here. </span></p><p class="c0"><span class="c14">As shown in Figures 12 (for PittMPICluster) and 13 (for Gor- don), P</span><span class="c11">ARAGON </span><span class="c14">and </span><span class="c11">UNI</span><span class="c14">P</span><span class="c11">ARAGON </span><span class="c14">have much lower remote com- munication volume than DG in all cases, and P</span><span class="c11">ARAGON </span><span class="c14">has the lowest inter-node communication volume and highest intra-node (inter-socket &amp; intra-socket) communication volume on Gordon (vice versa on PittMPICluster), which was expected given our choice for &lambda;. It is worth mentioning that on PittMPICluster, intra-node data communication was the bottleneck. Another interesting thing was that in spite of its higher total communication volume when compared to M</span><span class="c11">ETIS</span><span class="c14">, P</span><span class="c11">AR</span><span class="c14">M</span><span class="c11">ETIS</span><span class="c14">, and </span><span class="c11">UNI</span><span class="c14">P</span><span class="c11">ARAGON</span><span class="c14">, P</span><span class="c11">ARAGON </span><span class="c14">still outperformed them in most cases due to the reduced commu- nication on critical components. </span></p><p class="c18"><span class="c14">Graph Dynamism (Figure 14) To further validate the effective- ness of P</span><span class="c11">ARAGON </span><span class="c14">in the presence of graph dynamism, we split the YouTube dataset (a collection of YouTube users and their friend- ship connections over a period of 225 days) into 5 snapshots with an interval of 45 days. Thus, snapshot S</span><span class="c15">i </span><span class="c3">denotes the collection </span><span class="c14">of YouTube users and their friendship connections appearing dur- ing the first 45 &lowast; i days. We then ran BFS on snapshot S</span><span class="c2">1 </span><span class="c14">across three 20-core machines and injected vertices newly appeared in each snapshot to the system using DG whenever BFS finished its computation for every 15 randomly selected vertices. The injec- tion also triggered the execution of P</span><span class="c11">ARAGON</span><span class="c14">, </span><span class="c11">UNI</span><span class="c14">P</span><span class="c11">ARAGON</span><span class="c14">, and P</span><span class="c11">AR</span><span class="c14">M</span><span class="c11">ETIS </span><span class="c14">on the decomposition. </span></p><p class="c18"><span class="c14">Figure 14 plots the BFS execution time for 15 randomly selected source vertices on each snapshot. As shown, both architecture- awareness and the capability to cope with graph dynamism were critical to achieve superior performance. This is especially true as the graph changes a lot from its original version: at snapshot S</span><span class="c15">5</span><span class="c3">, </span><span class="c14">P</span><span class="c11">ARAGON </span><span class="c14">performed 90% better than DG, 85% better than M</span><span class="c11">ETIS</span><span class="c14">, 73% better than P</span><span class="c11">AR</span><span class="c14">M</span><span class="c11">ETIS</span><span class="c14">, and 89% better than </span><span class="c11">UNI</span><span class="c14">P</span><span class="c11">ARAGON</span><span class="c14">. </span></p><p class="c25"><span class="c97">) BM(emuloVm mo</span><span class="c4">C2,500 2,000 1,500 1,000 500 0 </span></p><p class="c0"><span class="c4">D</span><span class="c20">G </span><span class="c4">YouTube </span><span class="c4">u</span><span class="c20">niP</span><span class="c4">P</span><span class="c20">AARRAAGGOON N </span></p><p class="c0"><span class="c4">as&minus;skitter com&minus;lj </span></p><p class="c0"><span class="c4">D</span><span class="c20">G </span><span class="c4">u</span><span class="c20">niP</span><span class="c4">P</span><span class="c20">AARRAAGGOON </span></p><p class="c0"><span class="c20">N </span></p><p class="c0"><span class="c4">PD</span><span class="c20">G </span><span class="c4">u</span><span class="c20">niPAARRAAGGOON </span></p><p class="c0"><span class="c20">N </span></p><p class="c0"><span class="c31">7.3 Billion-Edge Graph Scaling </span></p><p class="c0"><span class="c14">Configuration In this experiment, we investigated the scalabil- ity of P</span><span class="c11">ARAGON </span><span class="c14">as the graph scale increased. Towards this, we generated three additional datasets by sampling the edge list of the friendster dataset (3.6 billion edges). We denote the datasets gen- erated as friendster-p, where p was the probability that each edge was kept while sampling. Hence, friendster-p would have around 3.6&lowast;p billion edges. Interestingly, the number of vertices remained almost unchanged in spite of the sampling. We ran the experiment on three compute nodes of PittMPICluster with the degree of re- finement parallelism, the number of shuffle refinement times, and the message grouping size set to 10, 10, and 256, respectively. Results (Figures 15 &amp; 16) Figures 15 and 16 present the execu- tion time of BFS with 15 randomly selected source vertices and the overhead of P</span><span class="c11">ARAGON </span><span class="c14">at different graph scales. As shown, P</span><span class="c11">ARAGON </span><span class="c14">not only led to lower job execution times, but also to lower speed in which the job execution time increased as the graph size increased. It should be noticed that P</span><span class="c11">ARAGON </span><span class="c14">reduced the execution time of all machines (3*20 cores) not just one. Also, the refinement time increased at a much slower rate (from 140s, to 236s, to 312s, and to 410s) than that of the graph size. The rea- son why we did not present the results of M</span><span class="c11">ETIS </span><span class="c14">or P</span><span class="c11">AR</span><span class="c14">M</span><span class="c11">ETIS </span><span class="c14">here was because they failed to (re)partition the graphs (even for the first dataset, of 0.9 billion edges). </span></p><p class="c0"><span class="c31">8. RELATED WORK </span></p><p class="c0"><span class="c14">Graph partitioning and repartitioning are receiving more and more attention in recent years due to the proliferation of large graph datasets. In this section, we categorize existing approaches of graph (re)partitioners into three types: (a) heavyweight, (b) lightweight, and (c) streaming, which are presented next. Heavyweight Graph (Re)Partitioning Graph partitioning and repar- titioning has been studied for decades (e.g., M</span><span class="c11">ETIS </span><span class="c14">[23], P</span><span class="c11">AR</span><span class="c14">M</span><span class="c11">ETIS </span><span class="c14">[30], Scotch [36], Chaco [7], and Zoltan [1]). These graph (re)partitioners are well-known for their capability of producing high-quality graph decompositions. However, they usually require full knowledge of the entire graph for (re)partitioning, making them scale poorly against </span></p><p class="c0"><span class="c27">374 </span></p><p class="c0"><span class="c9">500 </span><span class="c53">200 180 160 140 120 100 80 60 40 20 </span><span class="c32">0 </span><span class="c32">S</span><span class="c16">1 </span><span class="c32">S</span><span class="c16">2 </span><span class="c32">S</span><span class="c16">3 </span><span class="c32">S</span><span class="c16">4 </span><span class="c32">S</span><span class="c16">5 </span></p><p class="c0"><span class="c9">15000 </span><span class="c74">) s(TEJS F</span><span class="c9">B12500 </span></p><p class="c0"><span class="c9">DG PARAGON </span></p><p class="c0"><span class="c9">0 0.9 1.8 2.7 3.6 </span></p><p class="c25"><span class="c74">) s(emiTt nemen</span><span class="c9">PARAGON </span><span class="c53">) s(TEJ</span><span class="c74">eR</span><span class="c9">0 0.9 1.8 2.7 3.6 Approximate # of edges (billions) Approximate # of edges (billions) </span><span class="c14">Figure 15: BFS JET vs Graph Size </span></p><p class="c0"><span class="c14">Figure 16: Refinement Time vs Graph Size </span></p><p class="c18"><span class="c14">large graphs even if performed in parallel. Furthermore, they are all architecture-agnostic. Although [24], a M</span><span class="c11">ETIS </span><span class="c14">variant, consid- ers the communication heterogeneity, it is a sequential static graph partitioner, which is inapplicable for massive graphs or dynamic graphs. Several recent works [48, 8] have been proposed to cope with the heterogeneity and dynamism. However, they are also too heavyweight for massive graphs because of the high communica- tion volume they generate. As a consequence, they are not ap- propriate for online graph repartitioning in large-scale distributed graph computation. Furthermore, they disregard the issue of re- source contention in multicore systems. </span></p><p class="c18"><span class="c14">Lightweight Graph Repartitioning As a result of the shortcom- ings of heavyweight graph (re)partitioners, many lightweight graph repartitioners [37, 43, 26, 17, 45] have been proposed. They ef- ficiently adapt the partitioning to changes by incrementally mi- grating vertices among partitions based on some heuristics (rather than repartitioning the entire graph). Nevertheless, they are not architecture-aware. Also, many of them assume uniform vertex weights and sizes, and some [43, 26] even assume uniform edge weights, which may not always be true. </span></p><p class="c18"><span class="c14">In fact, work [17] is a Pregel-like graph computing engine, which migrates vertices based on runtime characteristics of the workload (i.e., # of message sent/received by each vertex and response time) instead of the graph structure (i.e., the distribution of vertex neigh- bors, edge weights, and vertex sizes). Paper [45] also presents a repartitioning system that migrates vertices on-the-fly based on some runtime statistics (i.e., the average compute and communica- tion time of each superstep and the probability of a vertex becoming active in the next superstep). </span></p><p class="c18"><span class="c14">Recently, a novel distributed graph partitioner, Sheep [22], has been proposed for large graphs. It is similar in spirit to M</span><span class="c11">ETIS</span><span class="c14">. That is, they both first reduce the original graph to a smaller tree or a sequence of smaller graphs, then do a partition of the tree or the smallest graph, and finally map the partitioning back to the origi- nal graph. In terms of partitioning time, Sheep outperforms both M</span><span class="c11">ETIS </span><span class="c14">and streaming partitioners. For partitioning quality, Sheep is competitive with M</span><span class="c11">ETIS </span><span class="c14">for a small number of partitions and is competitive with streaming graph partitioners for larger numbers of partitions. However, Sheep is unable to deal with both weighted and dynamic graphs, and it is architecture-agnostic. </span></p><p class="c18"><span class="c14">Streaming Graph Partitioning Recently, a new family of graph partitioning heuristics, streaming graph partitioning [39, 11, 42], has been proposed for online graph partitioning. They are able to produce partitionings comparable to the heavyweight graph par- titioner, M</span><span class="c11">ETIS</span><span class="c14">, within a relative short time. However, they are architecture-agnostic. Although [46] has presented a streaming graph partitioner with awareness of both compute and communi- cation heterogeneity, it may lead to suboptimal performance in the presence of graph dynamism. </span></p><p class="c18"><span class="c14">Vertex-Cut Graph Partitioning Several vertex-cut graph parti- tioners [44, 31, 13] were also proposed to improve the performance of distributed graph computation. Vertex-cut solutions partition </span></p><p class="c0"><span class="c9">400 </span><span class="c53">S F</span><span class="c32">B</span><span class="c78">DG</span><span class="c57">METIS PARMETIS uniPARAGON PARAGON </span></p><p class="c0"><span class="c32">Snapshots </span></p><p class="c0"><span class="c9">10000 </span></p><p class="c0"><span class="c14">Figure 14: BFS JET with Graph Dynamism </span></p><p class="c0"><span class="c9">7500 5000 </span></p><p class="c0"><span class="c9">300 200 </span></p><p class="c0"><span class="c9">2500 </span></p><p class="c0"><span class="c9">100 200 </span></p><p class="c0"><span class="c9">0 </span></p><p class="c0"><span class="c14">the graph by assigning edges of the graph across partitions in- stead of vertices. It has been shown that vertex-cut solutions re- duce the communications with respect to edge-cut ones, especially on power-law graphs. However, it also has to deal with the issue of communication heterogeneity and the issue of shared-resource contention, since vertices appearing in multiple partitions need to communicate with each other during the computation. Neverthe- less, its discussion is beyond the scope of this paper. Overview of Related Work Table 6 visually classifies the state- of-the-art graph (re)partitioners according to algorithm and graph properties. In terms of algorithm properties, we characterize each approach as to whether it (a) runs in parallel and (b) is architecture- aware (i.e., CPU heterogeneity, network cost non-uniformity, and resource contention). In terms of graph properties, we charac- terize each approach as to whether it can handle graphs with (a) dynamism, (b) weighted vertices (i.e., nonuniform computation), (c) weighted edges (i.e., nonuniform data communication), and (d) vertex sizes (i.e., nonuniform data sizes on each vertex). </span></p><p class="c0"><span class="c31">9. CONCLUSIONS </span></p><p class="c18"><span class="c14">In this paper, we presented P</span><span class="c11">ARAGON</span><span class="c14">, a parallel architecture- aware graph partition refinement algorithm that bridges the mis- match between the application communication pattern and the un- derlying hardware topology. P</span><span class="c11">ARAGON </span><span class="c14">achieves this by modify- ing a given decomposition according to the nonuniform network communication costs and consideration of the contentiousness of the underlying hardware. To further reduce its overhead, we made P</span><span class="c11">ARAGON </span><span class="c14">itself architecture-aware. Compared to the state-of-the- art, P</span><span class="c11">ARAGON </span><span class="c14">improved the quality of graph decompositions by up to 53%, achieved up to 5.9x speedups on real workloads, and successfully scaled up to a 3.6 billion-edge graph. </span></p><p class="c0"><span class="c31">10. ACKNOWLEDGMENTS </span></p><p class="c18"><span class="c14">We would like to thank Jack Lange, Albert DeFusco, Kim Wong, Mark Silvis, and the anonymous reviewers for their valuable help on the paper. This work was funded in part by NSF awards CBET- 1250171 and OIA-1028162. </span></p><p class="c0"><span class="c31">11. REFERENCES </span><span class="c22">[1] </span><span class="c87">http://www.cs.sandia.gov/zoltan/</span><span class="c22">. </span></p><p class="c0"><span class="c1">[2] </span><span class="c48">http://snap.stanford.edu/data</span><span class="c1">. [3] C. Binnig, U. &Ccedil;etintemel, A. Crotty, A. Galakatos, T. Kraska, </span></p><p class="c0"><span class="c1">E. Zamanian, and S. B. Zdonik. The End of Slow Networks: It&rsquo;s Time for a Redesign. CoRR, 2015. [4] A. Bulu&ccedil; and K. Madduri. Parallel Breadth-First Search on Distributed Memory Systems. CoRR, abs/1104.4518, 2011. [5] D. Buntinas, B. Goglin, D. Goodell, G. Mercier, and S. Moreaud. </span></p><p class="c0"><span class="c1">Cache-efficient, intranode, large-message MPI communication with MPICH2-Nemesis. In ICPP, 2009. [6] U. V. Catalyurek, E. G. Boman, K. D. Devine, D. Bozda &#774;g, R. T. </span></p><p class="c0"><span class="c1">Heaphy, and L. A. Riesen. A repartitioning hypergraph model for dynamic load balancing. J Parallel Distr Com, 2009. [7] </span><span class="c48">http://www.sandia.gov/~bahendr/chaco.html</span><span class="c1">. </span></p><p class="c0"><span class="c27">375 </span></p><p class="c0"><span class="c14">Table 6: State-of-the-art Graph (Re)Partitioners </span></p><p class="c0"><span class="c33">Name/Reference </span></p><p class="c0"><span class="c33">Algorithm Properties Graph Properties Parallel </span><span class="c28">CPU </span><span class="c39">Architecture-Aware </span><span class="c28">Network Contention </span><span class="c33">Dynamism </span><span class="c28">Vertex </span><span class="c39">Weighted </span><span class="c28">Edge </span></p><p class="c0"><span class="c33">Vertex Size Graph Partitioners M</span><span class="c19">ETIS </span><span class="c33">[23] ICA3PP&rsquo;08 [24] Chaco [7] DG/LDG [39]/Fennel [42] Yes/No </span></p><p class="c0"><span class="c33">arXiv&rsquo;13 [11] TKDE&rsquo;15 [46] Yes/No </span></p><p class="c25"><span class="c33">SoCC&rsquo;12 [8] Sheep [22] </span></p><p class="c0"><span class="c33">Graph Repartitioners P</span><span class="c19">AR</span><span class="c33">M</span><span class="c19">ETIS </span><span class="c33">[30] Zoltan [1] Scotch [36] CatchW [37] xdgp [43] Hermes [26] Mizan [17] LogGP [45] A</span><span class="c19">RAGON </span><span class="c33">[48] P</span><span class="c19">ARAGON </span></p><p class="c0"><span class="c1">[8] R. Chen, M. Yang, X. Weng, B. Choi, B. He, and X. Li. Improving large graph processing on partitioned graphs in the cloud. In SoCC, 2012. [9] </span><span class="c48">http://www.dis.uniroma1.it/challenge9</span><span class="c1">. [10] </span><span class="c48">http://www.cc.gatech.edu/dimacs10/</span><span class="c1">. [11] L. M. Erwan, L. Yizhong, and T. Gilles. (Re) partitioning for </span></p><p class="c25"><span class="c1">stream-enabled computation. arXiv:1310.8211, 2013. [12] C. M. Fiduccia and R. M. Mattheyses. A linear-time heuristic for </span></p><p class="c25"><span class="c1">improving network partitions. In DAC, 1982. [13] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin. </span></p><p class="c0"><span class="c1">PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs. In OSDI, 2012. [14] B. Hendrickson and T. G. Kolda. Graph partitioning models for </span></p><p class="c0"><span class="c1">parallel computing. Parallel computing, 2000. [15] R. Hood, H. Jin, P. Mehrotra, J. Chang, J. Djomehri, S. Gavali, D. Jespersen, K. Taylor, and R. Biswas. Performance impact of resource contention in multicore systems. In IPDPS, 2010. [16] H.-W. Jin, S. Sur, L. Chai, and D. K. Panda. Limic: Support for </span></p><p class="c0"><span class="c1">high-performance mpi intra-node communication on linux cluster. In ICPP, 2005. [17] Z. Khayyat, K. Awara, A. Alonazi, H. Jamjoom, D. Williams, and </span></p><p class="c0"><span class="c1">P. Kalnis. Mizan: a system for dynamic load balancing in large-scale graph processing. In EuroSys, 2013. [18] </span><span class="c48">http://konect.uni-koblenz.de/networks/</span><span class="c1">. [19] Y. Low, J. E. Gonzalez, A. Kyrola, D. Bickson, C. E. Guestrin, and </span></p><p class="c0"><span class="c1">J. Hellerstein. Graphlab: A new framework for parallel machine learning. arXiv:1408.2041, 2014. [20] Y. Lu, J. Cheng, D. Yan, and H. Wu. Large-scale distributed graph </span></p><p class="c0"><span class="c1">computing systems: An experimental evaluation. VLDB, 2014. [21] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, </span></p><p class="c0"><span class="c1">N. Leiser, and G. Czajkowski. Pregel: a system for large-scale graph processing. In SIGMOD, 2010. [22] D. Margo and M. Seltzer. A Scalable Distributed Graph Partitioner. </span></p><p class="c0"><span class="c1">VLDB, 2015. [23] </span><span class="c48">http://glaros.dtc.umn.edu/gkhome/metis/metis/ </span></p><p class="c0"><span class="c48">overview</span><span class="c1">. [24] I. Moulitsas and G. Karypis. Architecture aware partitioning </span></p><p class="c0"><span class="c1">algorithms. In ICA3PP, 2008. [25] </span><span class="c48">http://mvapich.cse.ohio-state.edu/</span><span class="c1">. [26] D. Nicoara, S. Kamali, K. Daudjee, and L. Chen. Hermes: Dynamic </span></p><p class="c0"><span class="c1">partitioning for distributed social network graph databases. In EDBT, 2015. [27] </span><span class="c48">http://www.open-mpi.org/</span><span class="c1">. [28] </span><span class="c48">https://portal.xsede.org/sdsc-gordon</span><span class="c1">. </span></p><p class="c0"><span class="c1">[29] </span><span class="c48">http://mvapich.cse.ohio-state.edu/benchmarks/</span><span class="c1">. [30] </span><span class="c48">http://glaros.dtc.umn.edu/gkhome/metis/ </span></p><p class="c0"><span class="c48">parmetis/overview</span><span class="c1">. [31] F. Petroni, L. Querzoni, K. Daudjee, S. Kamali, and G. Iacoboni. HDRF: Stream-Based Partitioning for Power-Law Graphs. 2015. [32] </span><span class="c48">http://core.sam.pitt.edu/MPIcluster</span><span class="c1">. [33] K. Schloegel, G. Karypis, and V. Kumar. A unified algorithm for load-balancing adaptive scientific simulations. In SC, 2000. [34] K. Schloegel, G. Karypis, and V. Kumar. Graph partitioning for high </span></p><p class="c25"><span class="c1">performance scientific simulations. AHPCRC, 2000. [35] C. Schulz. Scalable parallel refinement of graph partitions. PhD </span></p><p class="c25"><span class="c1">thesis, Karlsruhe Institute of Technology, May 2009. [36] </span><span class="c48">http://www.labri.u-bordeaux.fr/perso/pelegrin/ </span></p><p class="c0"><span class="c48">scotch/</span><span class="c1">. [37] Z. Shang and J. X. Yu. Catch the wind: Graph workload balancing on </span></p><p class="c0"><span class="c1">cloud. In ICDE, 2013. [38] </span><span class="c48">http: </span></p><p class="c0"><span class="c48">//staffweb.cms.gre.ac.uk/~wc06/partition/</span><span class="c1">. [39] I. Stanton and G. Kliot. Streaming graph partitioning for large </span></p><p class="c0"><span class="c1">distributed graphs. In SIGKDD, 2012. [40] S. Sur, H.-W. Jin, L. Chai, and D. K. Panda. RDMA read based </span></p><p class="c0"><span class="c1">rendezvous protocol for MPI over InfiniBand: design alternatives and benefits. In PPoPP, 2006. [41] L. Tang, J. Mars, N. Vachharajani, R. Hundt, and M. L. Soffa. The impact of memory subsystem resource sharing on datacenter applications. In ISCA, 2011. [42] C. Tsourakakis, C. Gkantsidis, B. Radunovic, and M. Vojnovic. </span></p><p class="c0"><span class="c1">Fennel: Streaming graph partitioning for massive scale graphs. In WSDM, 2014. [43] L. Vaquero, F. Cuadrado, D. Logothetis, and C. Martella. xdgp: A </span></p><p class="c0"><span class="c1">dynamic graph processing system with adaptive partitioning. CoRR, 2013. [44] C. Xie, L. Yan, W.-J. Li, and Z. Zhang. Distributed Power-law Graph Computing: Theoretical and Empirical Analysis. In NIPS. 2014. [45] N. Xu, L. Chen, and B. Cui. LogGP: a log-based dynamic graph </span></p><p class="c0"><span class="c1">partitioning method. VLDB, 2014. [46] N. Xu, B. Cui, L.-n. Chen, Z. Huang, and Y. Shao. Heterogeneous </span></p><p class="c0"><span class="c1">Environment Aware Streaming Graph Partitioning. TKDE, 2015. [47] C. Zhang, X. Yuan, and A. Srinivasan. Processor affinity and MPI </span></p><p class="c25"><span class="c1">performance on SMP-CMP clusters. In IPDPSW, 2010. [48] A. Zheng, A. Labrinidis, and P. K. Chrysanthis. Architecture-Aware </span></p><p class="c0"><span class="c1">Graph Repartitioning for Data-Intensive Scientific Computing. In BigGraphs, 2014. </span></p><p class="c0"><span class="c27">376 </span></p></body></html>