<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:10pt;font-family:"Arial";font-style:normal}.c39{margin-left:-16.6pt;padding-top:3.8pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c117{margin-left:-20.9pt;padding-top:1.4pt;text-indent:35pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-5.6pt}.c52{margin-left:-20.9pt;padding-top:1.7pt;text-indent:35pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9pt}.c125{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.7pt;font-family:"Arial";font-style:normal}.c147{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:3.6pt;font-family:"Arial";font-style:normal}.c21{margin-left:-25.7pt;padding-top:3.8pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.8pt}.c120{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Courier New";font-style:normal}.c62{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.5pt;font-family:"Arial";font-style:normal}.c58{color:#404040;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.5pt;font-family:"Arial";font-style:normal}.c66{color:#404040;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c47{margin-left:-25.7pt;padding-top:0.5pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c145{color:#404040;font-weight:400;text-decoration:none;vertical-align:super;font-size:13.5pt;font-family:"Arial";font-style:normal}.c44{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:16.6pt;font-family:"Arial";font-style:normal}.c114{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.8pt;font-family:"Courier New";font-style:normal}.c67{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Courier New";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c73{margin-left:-16.6pt;padding-top:1.7pt;text-indent:35.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-2.4pt}.c104{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.6pt;font-family:"Arial";font-style:normal}.c127{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:18.3pt;font-family:"Arial";font-style:normal}.c107{margin-left:-16.6pt;padding-top:1.7pt;text-indent:35.6pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:24.2pt}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.7pt;font-family:"Arial";font-style:normal}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5pt;font-family:"Arial";font-style:normal}.c54{color:#595959;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c126{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17.9pt;font-family:"Arial";font-style:normal}.c91{margin-left:-25.7pt;padding-top:3.8pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c48{margin-left:-25.7pt;padding-top:3.8pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-18.8pt}.c115{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.3pt;font-family:"Courier New";font-style:normal}.c151{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.7pt;font-family:"Arial";font-style:normal}.c51{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c43{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c30{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Arial";font-style:normal}.c149{margin-left:-25.7pt;padding-top:1.7pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-20pt}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:5.8pt;font-family:"Arial";font-style:normal}.c123{color:#404040;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.4pt;font-family:"Arial";font-style:normal}.c69{margin-left:-25.7pt;padding-top:1.4pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c128{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Courier New";font-style:normal}.c86{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:8.3pt;font-family:"Arial";font-style:normal}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6pt;font-family:"Arial";font-style:normal}.c79{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.1pt;font-family:"Arial";font-style:normal}.c56{margin-left:-16.6pt;padding-top:3.8pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-30.7pt}.c25{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:5.8pt;font-family:"Courier New";font-style:normal}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:italic}.c36{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:6.8pt;font-family:"Arial";font-style:normal}.c88{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:3.5pt;font-family:"Courier New";font-style:normal}.c118{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:6pt;font-family:"Courier New";font-style:normal}.c53{margin-left:-25.7pt;padding-top:1.7pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c35{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.7pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:11.6pt;font-family:"Arial";font-style:normal}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c95{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:9.5pt;font-family:"Arial";font-style:normal}.c63{margin-left:-25.7pt;padding-top:3.8pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c46{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:7.7pt;font-family:"Arial";font-style:normal}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:9.5pt;font-family:"Arial";font-style:normal}.c75{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.6pt;font-family:"Arial";font-style:normal}.c49{color:#404040;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8.1pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.2pt;font-family:"Arial";font-style:normal}.c42{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:14.9pt;font-family:"Arial";font-style:normal}.c92{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:5.8pt;font-family:"Courier New";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:6pt;font-family:"Arial";font-style:normal}.c61{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c60{margin-left:-16.6pt;padding-top:1.7pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c20{margin-left:-25.7pt;padding-top:10.3pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c38{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:4.8pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.7pt;font-family:"Arial";font-style:normal}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:13.3pt;font-family:"Arial";font-style:normal}.c136{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:8.3pt;font-family:"Courier New";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:14.9pt;font-family:"Arial";font-style:normal}.c116{margin-left:-25.7pt;padding-top:4.1pt;text-indent:34.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:11.6pt;font-family:"Arial";font-style:normal}.c113{color:#000000;font-weight:400;text-decoration:none;vertical-align:super;font-size:6pt;font-family:"Courier New";font-style:normal}.c33{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:5.2pt;font-family:"Courier New";font-style:normal}.c77{margin-left:-25.7pt;padding-top:58.1pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c141{margin-left:-25.7pt;padding-top:22.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c23{margin-left:-16.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3.1pt}.c119{margin-left:-20.2pt;padding-top:6.2pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-15.4pt}.c65{margin-left:-25.7pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:27.5pt}.c110{margin-left:-25.7pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:106.5pt}.c134{margin-left:-12.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-20.7pt}.c132{margin-left:-20.9pt;padding-top:0.5pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-16.4pt}.c100{margin-left:-20.9pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c108{margin-left:115.9pt;padding-top:258.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:116.2pt}.c84{margin-left:-20.9pt;padding-top:4.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9.7pt}.c159{margin-left:-12.5pt;padding-top:15.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-3.2pt}.c59{margin-left:-16.6pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:43.2pt}.c29{margin-left:-16.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.2pt}.c96{margin-left:-23.8pt;padding-top:77.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24pt}.c98{margin-left:-25.7pt;padding-top:12pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:121.4pt}.c139{margin-left:-25.7pt;padding-top:10.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c93{margin-left:-16.6pt;padding-top:10.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-27.6pt}.c50{margin-left:-16.6pt;padding-top:1.4pt;text-indent:35.6pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c83{margin-left:220.6pt;padding-top:75.8pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-33.4pt}.c45{margin-left:-16.6pt;padding-top:13pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:69.8pt}.c57{margin-left:-25.7pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c32{margin-left:-25.7pt;padding-top:19pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:38.6pt}.c158{margin-left:-16.6pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-26.4pt}.c105{margin-left:-16.6pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-3.8pt}.c155{margin-left:-25.7pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c101{margin-left:220.6pt;padding-top:66.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-33.4pt}.c80{margin-left:-16.6pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-20.6pt}.c131{margin-left:-25.7pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-3.9pt}.c130{margin-left:-25.7pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:103.1pt}.c163{margin-left:-16.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-24.7pt}.c55{margin-left:-20.9pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9pt}.c152{margin-left:8.6pt;padding-top:19.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:8.6pt}.c15{margin-left:220.6pt;padding-top:67.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-33.4pt}.c87{margin-left:-16.6pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-19.2pt}.c37{margin-left:-16.6pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-25.4pt}.c150{margin-left:220.6pt;padding-top:65.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-33.4pt}.c138{margin-left:-25.7pt;padding-top:7.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:53.4pt}.c68{margin-left:-25.7pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:114.2pt}.c27{margin-left:-16.6pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-31pt}.c142{margin-left:-25.7pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-12.8pt}.c122{margin-left:2.3pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:0.7pt}.c97{margin-left:-20.9pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-7pt}.c24{margin-left:-25.7pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-16.4pt}.c102{margin-left:-25.7pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c111{margin-left:-25.7pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c121{margin-left:220.6pt;padding-top:71.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-33.4pt}.c90{margin-left:-16.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-16.1pt}.c72{margin-left:-25.7pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:127.1pt}.c133{margin-left:13.9pt;padding-top:29.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:52.7pt}.c31{margin-left:-16.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-10.6pt}.c161{margin-left:-25.7pt;padding-top:3.8pt;padding-bottom:0pt;line-height:1.15;text-align:right;margin-right:-17.4pt}.c74{margin-left:-25.7pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-9.7pt}.c160{margin-left:-25.7pt;padding-top:46.1pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c17{margin-left:-12.2pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:justify;margin-right:-16.4pt}.c41{margin-left:-25.7pt;padding-top:11.5pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:137.9pt}.c70{margin-left:-6.7pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:11pt}.c137{margin-left:-25.7pt;padding-top:5.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-16.4pt}.c146{margin-left:-25.7pt;padding-top:12.2pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:148.7pt}.c76{margin-left:-25.7pt;padding-top:8.9pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-24.1pt}.c124{margin-left:220.6pt;padding-top:76.3pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:-33.4pt}.c71{margin-left:-25.7pt;padding-top:18.7pt;padding-bottom:0pt;line-height:1.15;text-align:left;margin-right:27pt}.c129{margin-left:-16.6pt;padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:center;margin-right:-8.4pt}.c154{padding-top:1.4pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c148{padding-top:3.8pt;text-indent:25.8pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c28{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c81{padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c157{padding-top:1.4pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c135{padding-top:10.3pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c144{padding-top:1.7pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c106{padding-top:6.5pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c112{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c82{margin-left:15.3pt;margin-right:6.5pt}.c85{margin-left:-16.6pt;margin-right:4.3pt}.c164{margin-left:-16.6pt;margin-right:-25.2pt}.c64{margin-left:-16.6pt;margin-right:-25.4pt}.c153{margin-left:-12.2pt;margin-right:-16.4pt}.c89{margin-left:-16.6pt;margin-right:27.4pt}.c156{margin-left:-12.2pt;margin-right:-19.8pt}.c78{margin-left:50.1pt;margin-right:1.4pt}.c143{margin-left:-95.6pt;margin-right:158.4pt}.c103{margin-right:-1.2pt}.c94{margin-right:-15.1pt}.c99{margin-right:-15.8pt}.c140{margin-right:-16.3pt}.c109{margin-right:-12.7pt}.c162{margin-right:-18.5pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c112"><p class="c152"><span class="c126">GraphCache: A Caching System for Graph Queries </span></p><p class="c133"><span class="c18">Jing Wang </span><span class="c44">School of Computing Science University of Glasgow, UK j.wang.3@research.gla.ac.uk </span></p><p class="c1 c143"><span class="c18">Nikos Ntarmos </span><span class="c44">School of Computing Science University of Glasgow, UK nikos.ntarmos@glasgow.ac.uk </span></p><p class="c1 c78"><span class="c18">Peter Triantafillou </span><span class="c44">School of Computing Science University of Glasgow, UK peter.triantafillou@glasgow.ac.uk </span></p><p class="c1 c78"><span class="c18">Peter Triantafillou </span><span class="c44">School of Computing Science University of Glasgow, UK peter.triantafillou@glasgow.ac.uk </span></p><p class="c160"><span class="c18">ABSTRACT </span><span class="c4">Graph query processing is essential for graph analytics, but can be very time-consuming as it entails the NP-Complete problem of subgraph isomorphism. Traditionally, caching plays a key role in expediting query processing. We thus put forth GraphCache (GC), the first full-fledged caching system for general subgraph/supergraph queries. We con- tribute the overall system architecture and implementation of GC. We study a number of novel graph cache replace- ment policies and show that different policies win over dif- ferent graph datasets and/or queries; we therefore contribute a novel hybrid graph replacement policy that is always the best or near-best performer. Moreover, we discover the re- lated problem of cache pollution and propose a novel cache admission control mechanism to avoid cache pollution. Fur- thermore, we show that GC can be used as a front end, com- plementing any graph query processing method as a plug- gable component. Currently, GC comes bundled with 3 top- performing filter-then-verify (FTV) subgraph query meth- ods and 3 well-established direct subgraph-isomorphism (SI) algorithms &ndash; representing different categories of graph query processing research. Finally, we contribute a comprehensive performance evaluation of GC. We employ more than 6 mil- lion queries, generated using different workload generators, and executed against both real-world and synthetic graph datasets of different characteristics, quantifying the benefits and overheads, emphasizing the non-trivial lessons learned. </span></p><p class="c146"><span class="c18">CCS Concepts </span></p><p class="c137"><span class="c4">&bull;Information systems &rarr; Database query processing; </span></p><p class="c65"><span class="c18">Keywords </span><span class="c4">Graph analytics; Graph queries, Caching system </span></p><p class="c110"><span class="c18">1. INTRODUCTION </span></p><p class="c91"><span class="c4">Graph datasets are proliferating nowadays, due to their ability to capture and allow for the analysis of complex re- </span></p><p class="c77"><span class="c4">&copy;</span><span class="c61">2017, Copyright is with the authors. Published in Proc. 20th Inter- national Conference on Extending Database Technology (EDBT), March 21-24, 2017 - Venice, Italy: ISBN 978-3-89318-073-8, on OpenProceed- ings.org. Distribution of this paper is permitted under the terms of the Cre- ative Commons license CC-by-nc-nd 4.0 </span></p><p class="c2 c64"><span class="c4">lations among objects, by modelling entities with nodes and their relations/interactions with edges. Graphs have thus been used, with great success, in a wide variety of appli- cation areas, from chemical and bioinformatics datasets to social networks. Central to graph analytics is the ability to locate patterns in dataset graphs. Informally, given a query (pattern) graph g, the system is called to return the set of dataset graphs that contain g (subgraph query) or are con- tained in g (supergraph query), aptly named the answer set of g. Unfortunately, these operations can be very costly, as they entail the NP-Complete problem of subgraph iso- morphism[5] and even the popular algorithms [4, 18, 31] are known to be computationally expensive. To this end, the research community has contributed a number of innovative solutions over the last few years. A large number of these fol- low the &ldquo;filter-then-verify&rdquo; (FTV) paradigm: dataset graphs are indexed so as to allow for the exclusion (filtering) of a number of those that are definitely not in the query&rsquo;s answer set; the remaining graphs, called the candidate set of g, need then to undergo testing (verification) for subgraph isomor- phism (abbreviated as sub-iso or SI in the rest of this work). However, recently extensive evaluations of FTV methods [9, 12] show significant performance limitations. </span></p><p class="c64 c81"><span class="c4">Although FTV solutions can produce candidate sets that are much smaller than the original dataset, they still end up executing unnecessary sub-iso tests: in the simplest of cases, if the same query is submitted twice to the system, it will also be sub-iso tested twice against its candidate set. Furthermore, a key observation we can make is that in many real-world applications, graph queries submitted in the past bear subgraph or supergraph relations with future queries. These relationships arise naturally. Queries against a bio- chemical dataset range from queries for simple molecules and aminoacids, all the way to queries for proteins of multi- cell organisms. In exploratory smart-city analytics, queries referring to road networks may pertain to neighbourhoods, towns, metro areas, etc. In social networking queries, ex- ploratory queries may start off broad (e.g., all people in a geographic location) and become increasingly narrower (e.g., by homing in on specific demographics). In time-series graph analytics, queries are typically associated with time inter- vals, which contain (or are contained within) other intervals. Based on these observations, we proposed in [34] a new graph query processing method, in which queries (and their answers) are indexed and used to expedite future query pro- cessing with FTV methods. Underpinned by our method in [34], this work presents a novel full-fledged caching system, where any general subgraph/supergraph query method in </span></p><p class="c96"><span class="c43">Series ISSN: 2367-2005 13 </span><span class="c11">10.5441/002/edbt.2017.03 </span></p><p class="c142"><span class="c4">the literature could be plugged in, and overall contributes: </span></p><p class="c144 c156"><span class="c4">&bull; GraphCache (GC), a full-fledged caching system for sub-/supergraph queries, with detailed discussions of design issues, its architecture and implementation, deal- ing with resource management (memory and threads) and dynamic management of the cache index; </span></p><p class="c17"><span class="c4">&bull; A fresh perspective to expedite state-of-the-art solu- tions for the general subgraph isomorphism problems (SI methods) by GC (in addition to FTV methods); </span></p><p class="c153 c157"><span class="c4">&bull; A semantic graph cache which harness sub/supergraph cache hits, extending the traditional exact-match-only hit and leading to significant speedup for GC; </span></p><p class="c134"><span class="c4">&bull; A number of graph cache replacement strategies with different trade-offs, including a novel hybrid graph cache replacement policy with performance always better or on par with the best alternative; </span></p><p class="c144 c153"><span class="c4">&bull; A novel cache admission control mechanism enhancing the performance gains of GC; </span></p><p class="c24"><span class="c4">&bull; Comprehensive evaluations (with millions of queries) utilising well-established FTV and SI methods, against real-world and synthetic datasets with different charac- teristics and different workload generators, quantifying benefits/overheads and uncovering key insights. To the best of our knowledge, GC is the first caching system in the literature for general subgraph/supergraph queries. </span></p><p class="c130"><span class="c18">2. RELATED WORK </span></p><p class="c116"><span class="c4">Subgraph/supergraph queries entail the subgraph isomor- phism problem, which has two versions. The decision prob- lem answers Y/N as to whether the query is contained in each graph in the dataset. The matching problem locates all occurrences of the query graph within a large graph (or a dataset of graphs). For both the decision and match- ing problems, the brute-force approach is to execute sub-iso tests of the query against all dataset graphs. However, sub- iso tests are costly, being NP-Complete[5]. Several heuristic algorithms have been proposed over the years. [9] provides an insightful presentation and comparison of several such (SI) algorithms (which could be integrated within GC). </span></p><p class="c149"><span class="c4">SI algorithms deteriorate when the dataset is comprised of a large number of graphs, as each graph has to be tested. Thus appeared the&ldquo;filter-then-verify&rdquo;(FTV) paradigm. FTV methods try to reduce the set of graphs against which to run the sub-iso test, by filtering out graphs which definitely do not belong to the query answer set. At the heart of these methods lies an index on the dataset graphs. Briefly, dataset graphs are decomposed into features (i.e., paths, trees, cy- cles or arbitrary subgraphs), which are then recorded in an indexing structure (e.g., trie [2, 6], hash-based bitmap [14], etc.). Query processing then proceeds in two stages. First, in the filtering stage, the query graph g is decomposed to its features, which are then used to retrieve from the dataset index the IDs of those graphs containing all of them; the re- sult is a subset of the dataset graphs, named the candidate set of g. Then, in the verification stage, g undergoes sub-iso testing against each graph in the candidate set. </span></p><p class="c53"><span class="c4">Similarly, [29] presents a solution for subgraph queries against historical (i.e., snapshotted) graphs &ndash; a variation of typical graph queries where snapshots can be viewed as dif- ferent graphs; the main focus of this work is on reconstruct- ing minimal snapshots around candidate matching nodes, by using a set of indices allowing for the retrieval of nodes with specific labels/neighbourhoods at given time points. </span></p><p class="c2 c64"><span class="c4">[9] presents an insightful performance evaluation and [12] provides a systematic performance and scalability study of subgraph FTV methods. Though we are not aware of similar in-depth studies on supergraph FTV solutions, [36] provides a concise overview for studies published prior to 2013 and recently [20] proposes an efficient solution for supergraph queries. GC is capable of expediting query processing for both subgraph and supergraph FTV methods. </span></p><p class="c60"><span class="c4">The community has also looked into subgraph queries against a single, very large graph (consisting of possibly bil- lions of nodes). [16] and [30] employ scale-out architectures and large memory clusters with massive parallelism respec- tively. [8] and [28] provide a centralised solution to the same problem, via advanced pruning approaches addressing the matching order issues faced by most other SI algorithms. GC does not target such use cases for the time being and extending our system to queries against a single massive graph or distributed operation is left for future work. </span></p><p class="c60"><span class="c4">Caching of query results has long been a mainstay in data management systems, from filesystem block caching to web proxy caching and the cache of query result sets in rela- tional databases. In the realm of graph-structured queries, however, little work has been done. For XML datasets, views have been used to accelerate path/tree queries [1, 19, 21]; Besides, [17] firstly proposed the MCR (maximally contained rewriting) approach for tree pattern queries and [33] revisited it by providing alternatives; both exhibit false negatives for the query answer. Our GC does not produce any false negative or false positive (formal proof of correct- ness in [34]). Also, GC is capable of dealing with much more complex graph-structured queries, which entail the NP-Complete problem of subgraph isomorphism. </span></p><p class="c60"><span class="c4">More recently, caching has also been utilized to optimize SPARQL query processing for RDF graphs. [22] introduced the first SPARQL cache, where a relational DB was em- ployed to store the metadata. [27] contributed a cache for SPARQL queries based on a novel canonical labelling scheme (to identify cache hits) and on a popular dynamic program- ming planner [23]. Similar to GC, query optimization in [27] does not require any a priori knowledge on datasets/- workloads and is workload adaptive. However, like XML queries, SPARQL queries are less expressive than general graph queries and thus less challenging [13, 30]; SPARQL query processing consists of solving the subgraph homomor- phism problem, which is different from the subgraph isomor- phism problem, as the former drops the injective property of the latter. Moreover, GC discovers subgraph, supergraph, and exact-match relationships between a new query and the queries in the cache, something that the canonical labelling scheme in [27] fails to achieve. SPARQL query processing also aims at optimizing join execution plans [7] (based on join selectivity estimator statistics and related cost func- tions), and the cache in [27] is focusing on this goal, whereas GC aims to avoid/reduce costs associated with executing SI heuristics whose execution time can be highly unpredictable and much higher. As such, the overall rationale of GC and the way cache contents are exploited differs from that in [27] and in related SPARQL result caching solutions. </span></p><p class="c64 c154"><span class="c4">Finally, [15] presents a cache for historical queries against a large social graph, in which each query is centered around a node in the social graph, and where the aim is to avoid maintaining/reconstructing complete snapshots of the social graph but to instead use a set of static &ldquo;views&rdquo; (snapshots </span></p><p class="c121"><span class="c43">14 </span></p><p class="c76"><span class="c4">of neighborhoods of nodes) to rewrite incoming queries. [15] does not deal with subgraph/supergraph queries per se; rather, the nature of the queries means that containment can be decided by just measuring the distance of the central query node to the centre of each view. Moreover, [15] does not deal with central issues of a cache system (cache replace- ment, admission control, overall architecture/design, etc.). </span></p><p class="c32"><span class="c18">3. DESIGN ISSUES AND GOALS </span></p><p class="c21"><span class="c4">GraphCache is implemented for undirected labelled graphs, as is typical in the literature (e.g., [2, 10, 14]). For simplic- ity, we assume that only vertices have labels; all our re- sults straightforwardly generalize to directed graphs and/or graphs with edge labels. </span></p><p class="c69"><span class="c4">Formally, a labelled graph G = (V,E,l) consists of a set of vertices V and edges E = {(u, v), u, v &isin; V }, and a function l : V &rarr; U, where U is the domain of labels. A graph G</span><span class="c13">i </span><span class="c16">= (V</span><span class="c13">i</span><span class="c16">,E</span><span class="c13">i</span><span class="c16">,l</span><span class="c13">i</span><span class="c16">) is subgraph-isomorphic to a graph G</span><span class="c13">j </span><span class="c16">= </span><span class="c4">(V</span><span class="c3">j</span><span class="c4">,E</span><span class="c3">j</span><span class="c4">,l</span><span class="c3">j</span><span class="c4">), by abuse of notation denoted by G</span><span class="c3">i </span><span class="c4">&sube; G</span><span class="c3">j</span><span class="c4">, when there exists an injection &phi; : V</span><span class="c13">i </span><span class="c16">&rarr; V</span><span class="c13">j</span><span class="c16">, such that &forall;(u, v) &isin; </span><span class="c4">E</span><span class="c3">i</span><span class="c4">, u, v &isin; V</span><span class="c3">i</span><span class="c4">,&rArr; (&phi;(u),&phi;(v)) &isin; E</span><span class="c3">j </span><span class="c4">and &forall;u &isin; V</span><span class="c3">i</span><span class="c4">,l</span><span class="c3">i</span><span class="c4">(u) = l</span><span class="c13">j</span><span class="c16">(&phi;(u)). Informally, there is a subgraph isomorphism G</span><span class="c13">i </span><span class="c16">&sube; </span><span class="c4">G</span><span class="c13">j </span><span class="c16">if G</span><span class="c13">j </span><span class="c16">contains a subgraph that is isomorphic to G</span><span class="c13">i</span><span class="c16">, and </span><span class="c4">we say that G</span><span class="c3">i </span><span class="c4">is a subgraph of (contained in) G</span><span class="c3">j</span><span class="c4">, or that G</span><span class="c3">j </span><span class="c4">is a supergraph of (contains) G</span><span class="c13">i </span><span class="c16">denoted by G</span><span class="c13">j </span><span class="c16">&supe; G</span><span class="c13">i</span><span class="c16">. As is </span><span class="c4">common in the relevant literature, we focus on non-induced subgraph isomorphism. Last, the subgraph (supergraph) querying problem entails a set D = {G</span><span class="c3">1</span><span class="c4">,...,G</span><span class="c3">n</span><span class="c4">} containing n graphs, and a query graph g, and determines all graphs G</span><span class="c13">i </span><span class="c16">&isin; D such that g &sube; G</span><span class="c13">i </span><span class="c16">(g &supe; G</span><span class="c13">i</span><span class="c16">, respectively). </span></p><p class="c47"><span class="c4">In designing GraphCache, we identified a set of design is- sues and goals, pertaining to the characteristics of (i) the query workloads, (ii) the underlying graph datasets, and (iii) the algorithmic and system context within which GC will operate (e.g., categories of research methods GC will complement). Overall, GC is intended to expedite graph queries whatever the algorithm of choice may be and across a wide variety of query workloads and graph datasets. </span></p><p class="c102"><span class="c51">Query Workloads. </span><span class="c4">As with any caching system, the as- sumption is that previous queries can help expedite future queries. This is reasonable, given the example applications mentioned in &sect;1. Most works [2, 6, 9, 14, 35] test algorithms for queries directly generated from dataset graphs. Though this is of particular interest, workloads should also include queries that are not guaranteed to have any answer. Further- more, in general, of particular interest to any caching system is the probability distribution of possible queries. For GC this in effect refers to the popularity of query graphs or of re- gions of the dataset graphs. GC should thus be able to deal effectively with various skewness levels of this distribution (e.g., from uniform to highly skewed Zipf distributions). Fi- nally, a practical problem emerges: workloads must contain a large number of queries so as to obtain reliable results on the performance of any method but subgraph isomorphism is NP-Complete. This leads to queries with possibly very long execution times, regardless of the heuristic used, making the experiments very time consuming. Nevertheless, we utilized well over 6 million queries for our performance evaluation. </span></p><p class="c155"><span class="c51">Graph Datasets. </span><span class="c4">Fortunately there exist a number of real- world graph datasets commonly used in related research. </span></p><p class="c1 c82"><span class="c4">Figure 1: GraphCache System Architecture </span></p><p class="c64 c106"><span class="c4">These help concretize the effects of any solution on real- world data and allow direct comparison of methods and re- sult repeatability. For this reason we will report evaluations conducted over three popular graph datasets: AIDS[24], PDBS[11], and PCM[32]. However, it is worth creating ad- ditional synthetic datasets so to perform evaluations under characteristics unseen in the real-world datasets. Specif- ically, we created a synthetic dataset presenting interest- ing characteristics regarding the number, size and node de- grees of graphs in the dataset. Interestingly, with respect to dataset with graphs having a high average node degree, we found that GC needs special mechanisms without which its performance benefits degrade. </span></p><p class="c37"><span class="c51">Algorithmic Context. </span><span class="c4">GraphCache is intended to be a ge- neral-purpose front-end for graph query processing. GC en- tails a query indexing strategy that, as explained in [34], can accommodate both subgraph and supergraph queries. In addition, the design of GraphCache must be able to accom- modate both FTV methods and SI algorithms; its current implementation comes bundled with well-established FTV methods and SI algorithms. In fact, any such algorithm is viewed as a pluggable component into the architecture, al- lowing any future algorithm to be incorporated. </span></p><p class="c59"><span class="c18">4. SYSTEM ARCHITECTURE </span></p><p class="c39"><span class="c4">GraphCache is designed from the ground up as a scal- able semantic cache for subgraph/supergraph queries, ca- pable of expediting any SI or FTV method (henceforth de- noted Method M). Figure 1 shows its main architectural com- ponents, comprising three major subsystems: Method M, Query Processing Runtime, and Cache Manager. The last two are internal subsystems of GC; the first is the method that GC is called to expedite and hence external to GC. </span></p><p class="c60"><span class="c4">The Method M subsystem includes, at a minimum, the base graph dataset and a sub-iso test implementation, de- noted M</span><span class="c13">verifier</span><span class="c16">. Additionally, if M is a FTV method, then </span><span class="c4">it also features its index, denoted M</span><span class="c13">index</span><span class="c16">, and a filtering </span><span class="c4">component, M</span><span class="c13">filter</span><span class="c16">. The index is built in a pre-processing </span><span class="c4">step, by using Method M&rsquo;s indexing component (not shown in Figure 1 for simplicity). When GC is not used, sub- /supergraph query processing proceeds by first using M</span><span class="c13">index </span><span class="c16">through M</span><span class="c13">filter </span><span class="c16">to prune away dataset graphs definitely not </span><span class="c4">containing (or contained in) the query, thus forming its can- didate set, M</span><span class="c3">CS</span><span class="c4">. Then M</span><span class="c3">verifier </span><span class="c4">executes a sub-iso test against all graphs in M</span><span class="c13">CS</span><span class="c16">, reading their structure directly </span><span class="c4">from the graph dataset store. For SI methods, M</span><span class="c3">CS </span><span class="c4">contains all graphs in dataset. </span></p><p class="c101"><span class="c43">15 </span></p><p class="c108"><span class="c4">Figure 2: GraphCache System Data and Control Flow </span></p><p class="c20"><span class="c4">Within GC, the Query Processing Runtime is responsi- ble for the execution of queries and the monitoring of key operational metrics. It comprises: a resource/thread man- ager dispatching queries to the various filtering/verification modules, the internal subgraph/supergraph query proces- sors, the logic for GC&rsquo;s candidate set pruning, and a statis- tics monitor. These components communicate with Method M and the Cache Manager via well-defined APIs. </span></p><p class="c69"><span class="c4">In turn, the Cache Manager deals with the management of data and metadata stored in the cache. It comprises the cache replacement mechanisms, a Window Manager respon- sible for cache admission control and maintenance of the cache contents, a Statistics Manager responsible for meta- data pertaining to past or current queries, as well as the stores for all GC-related data including cached queries and their answer sets, currently executing (not cached) queries, and metadata/statistics for both past and current queries. </span></p><p class="c69"><span class="c4">Figure 2 depicts the flow of control and data in GC dur- ing processing of a query. The query first arrives at the Resource Manager (1) and is then dispatched to M</span><span class="c13">filter </span><span class="c16">and </span><span class="c4">GC&rsquo;s filtering processors in parallel (2). At the same time, a copy of the query is added to the set of currently processed queries, called the Window (discussed shortly). The filtering components use their respective indexes to produce inter- mediate candidate sets (3). More specifically, M</span><span class="c3">filter </span><span class="c4">uses M</span><span class="c13">index</span><span class="c16">, while the two GraphCache processors use GC</span><span class="c13">index</span><span class="c16">, </span><span class="c4">the set of cached graph queries and their answer sets. The re- sults of this stage are then fed to the Candidate Set Pruner which produces the final candidate set GC</span><span class="c13">CS </span><span class="c16">(4); at the </span><span class="c4">same time, statistics regarding GC</span><span class="c3">CS </span><span class="c4">and the contribution of cached graphs are gathered by the Statistics Monitor and forwarded to the Statistics Manager. The final candidate set then undergoes sub-iso testing using M</span><span class="c13">verifier</span><span class="c16">(5); meta- </span><span class="c4">data pertaining to the verification time are also gathered by the Statistics Monitor and sent to the Statistics Manager. When the Window is full, the Window Manager selects the set of current queries to be considered for admission in the </span></p><p class="c1 c64"><span class="c4">cache (6) and invokes the cache replacement algorithm (7); i.e., updates to the Cache are batched through the Window. </span></p><p class="c45"><span class="c18">5. QUERY PROCESSING </span></p><p class="c64 c148"><span class="c4">This section discusses the design and implementation of GraphCache&rsquo;s Query Processing subsystem, responsible for the execution of queries and the monitoring of key opera- tional metrics. For the sake of clarity we first describe GC&rsquo;s operation when caching subgraph queries; we shall then dis- cuss how GC can be used for supergraph queries as well. </span><span class="c18">5.1 Candidate Set Pruning </span></p><p class="c39"><span class="c4">This subsection overviews the essence of [34]. For more details and formal proofs of correctness, please refer to [34]. Initially, if Method M is a FTV method, its indexing subsys- tem is used to build its graph dataset index as per usual. The GraphCache&rsquo;s data stores are initially all empty and are then populated as queries arrive and are processed. When a query g arrives at the system, M</span><span class="c13">filter </span><span class="c16">is used to produce a first can- </span><span class="c4">didate set. Concurrently, GraphCache checks whether the query graph is a subgraph or supergraph of previous query graphs, through its GC</span><span class="c13">sub</span><span class="c16">/GC</span><span class="c13">super </span><span class="c16">Processors. </span></p><p class="c64 c135"><span class="c51">GraphCache</span><span class="c13">sub </span><span class="c127">Processor. </span><span class="c16">The GraphCache</span><span class="c13">sub </span><span class="c16">Proces- </span><span class="c4">sor is responsible for identifying when a new query g is a subgraph of a previous query g . When g was executed , GC indexed g &rsquo;s features in GC</span><span class="c13">index </span><span class="c16">and stored its result set </span><span class="c4">and relevant statistics in the cache data stores. </span></p><p class="c60"><span class="c4">Figure 3(a) depicts an example flowchart for this case. The new query g is processed through M</span><span class="c13">filter</span><span class="c16">, producing </span><span class="c4">candidate set CS</span><span class="c3">M</span><span class="c4">(g) (with four graphs {G</span><span class="c3">1</span><span class="c4">,G</span><span class="c3">2</span><span class="c4">,G</span><span class="c3">3</span><span class="c4">,G</span><span class="c3">4</span><span class="c4">}). Similarly, g is processed by the GC</span><span class="c13">sub </span><span class="c16">Processor, determin- </span><span class="c4">ing that there exists a previous query g , such that g &sube; g . GC then retrieves g &rsquo;s cached answer set, {G</span><span class="c3">1</span><span class="c4">, G</span><span class="c3">2</span><span class="c4">}. Now, consider graph G</span><span class="c13">1 </span><span class="c16">&isin; CS</span><span class="c13">M</span><span class="c16">(g). Since g &sube; g and from the </span><span class="c4">answer set of g we know that g &sube; G</span><span class="c3">1</span><span class="c4">, it necessarily fol- lows that g &sube; G</span><span class="c13">1 </span><span class="c16">(and, similarly, g &sube; G</span><span class="c13">2</span><span class="c16">). Therefore, we </span></p><p class="c150"><span class="c43">16 </span></p><p class="c1"><span class="c5">CS</span><span class="c33">M</span><span class="c6">index </span></p><p class="c1"><span class="c118">M</span><span class="c75">(g) = 1G</span><span class="c118">1</span><span class="c75">,G</span><span class="c118">2</span><span class="c75">,G</span><span class="c118">3</span><span class="c75">,G</span><span class="c118">4</span><span class="c75">l </span></p><p class="c1"><span class="c5">CS</span><span class="c147">M</span><span class="c5">(g) </span><span class="c33">n </span><span class="c5">Answer</span><span class="c147">super</span><span class="c5">(g) = </span></p><p class="c1"><span class="c5">P rocessor </span></p><p class="c1"><span class="c5">Answer</span><span class="c6">super</span><span class="c75">(g) = 1G</span><span class="c6">1</span><span class="c75">,G</span><span class="c6">5</span><span class="c75">l </span></p><p class="c1"><span class="c67">M</span><span class="c12">index </span></p><p class="c1"><span class="c34">CS</span><span class="c25">M</span><span class="c34">(g) = 1G</span><span class="c25">1</span><span class="c34">,G</span><span class="c25">2</span><span class="c34">,G</span><span class="c25">3</span><span class="c34">,G</span><span class="c25">4</span><span class="c34">l </span></p><p class="c1"><span class="c34">CS</span><span class="c12">M</span><span class="c30">(g) </span><span class="c136">- </span><span class="c30">Answer</span><span class="c12">sub</span><span class="c30">(g) = </span></p><p class="c1"><span class="c34">P rocessor </span></p><p class="c1"><span class="c67">g &Ccedil; g</span><span class="c92">0 </span></p><p class="c28"><span class="c34">,g</span><span class="c92">0 </span><span class="c67">&Ccedil; </span><span class="c34">1G</span><span class="c25">1</span><span class="c34">,G</span><span class="c25">2</span><span class="c34">l Answer</span><span class="c12">sub</span><span class="c34">(g) = 1G</span><span class="c25">1</span><span class="c34">,G</span><span class="c25">2</span><span class="c34">l </span><span class="c86">Answer(g) = Answer </span><span class="c115">U </span><span class="c86">1G</span><span class="c88">1</span><span class="c34">,G</span><span class="c25">2</span><span class="c30">l </span><span class="c4">(a) Subgraph Case </span></p><p class="c1"><span class="c125">Subgraph Query </span><span class="c120">g </span></p><p class="c1"><span class="c33">M</span><span class="c6">verifier </span></p><p class="c1"><span class="c5">Answer</span><span class="c33">(</span><span class="c5">g</span><span class="c33">) </span></p><p class="c1"><span class="c5">GraphCache</span><span class="c6">super </span><span class="c5">, g</span><span class="c113">00 </span><span class="c33">&Ccedil; </span><span class="c5">1G</span><span class="c6">1</span><span class="c75">,G</span><span class="c6">5</span><span class="c75">l </span></p><p class="c1"><span class="c104">Subgraph Query </span><span class="c114">g </span></p><p class="c28"><span class="c86">1G</span><span class="c88">3</span><span class="c86">,G</span><span class="c88">4</span><span class="c86">l </span><span class="c67">M</span><span class="c12">verifier </span><span class="c46">Answer </span><span class="c34">GraphCache</span><span class="c12">sub </span></p><p class="c1"><span class="c5">1</span><span class="c33">G</span><span class="c118">1</span><span class="c75">l </span></p><p class="c1"><span class="c33">g &#9670; g</span><span class="c113">00 </span></p><p class="c1"><span class="c4">(b) Supergraph Case </span></p><p class="c1"><span class="c4">Figure 3: GraphCache Processing of a Subgraph Query g </span></p><p class="c2"><span class="c4">can safely remove G</span><span class="c3">1 </span><span class="c4">and G</span><span class="c3">2 </span><span class="c4">from CS</span><span class="c3">M</span><span class="c4">(g) and add them directly to the final answer set. In the general case, g may be a subgraph of multiple previous the set of graphs that need be query graphs sub-iso tested is given g </span><span class="c13">i</span><span class="c4">. Then, </span></p><p class="c1"><span class="c4">by: </span></p><p class="c1"><span class="c4">CS</span><span class="c13">GC</span><span class="c30">sub</span><span class="c4">(g) = CS</span><span class="c13">M</span><span class="c16">(g) \ </span><span class="c128">&#8899; </span></p><p class="c1"><span class="c3">g </span><span class="c30">i</span><span class="c3">&isin;Result</span><span class="c30">sub</span><span class="c3">(g) </span></p><p class="c1"><span class="c4">dicting the fact that Answer(g ) = &empty;; thus, no such graph g can exist and the final result set is necessarily empty. </span></p><p class="c1"><span class="c51">Supergraph Query Processing. </span><span class="c4">As mentioned earlier, GC can expedite both subgraph and supergraph query process- </span></p><p class="c1"><span class="c4">Answer(g </span><span class="c13">i</span><span class="c4">) (1) </span></p><p class="c14"><span class="c4">ing. In the latter case, the filtering components of GC re- main unchanged, but the handling of the return answer sets is the exact inverse of what happens for subgraph queries. where Result</span><span class="c13">sub</span><span class="c16">(g) contains all query graphs currently in </span><span class="c4">GC</span><span class="c13">index </span><span class="c16">of which g is a subgraph. </span></p><p class="c1"><span class="c4">Briefly, given a supergraph query processing Method M and a supergraph query g, the union of the answer sets of graphs in Result</span><span class="c3">super</span><span class="c4">(g) are removed from CS</span><span class="c3">M</span><span class="c4">(g) and added to </span><span class="c51">GraphCache</span><span class="c3">super </span><span class="c51">Processor. </span><span class="c4">In turn, the GC</span><span class="c13">super </span><span class="c16">Pro- </span><span class="c4">cessor is responsible for identifying when a new query g is a supergraph of a previous query g . Figure 3(b) de- picts an example flowchart for this case. Again, Method M produces its candidate set, CS</span><span class="c3">M</span><span class="c4">(g) (e.g., {G</span><span class="c3">1</span><span class="c4">,G</span><span class="c3">2</span><span class="c4">,G</span><span class="c3">3</span><span class="c4">,G</span><span class="c3">4</span><span class="c4">}). </span></p><p class="c1"><span class="c4">Answertersection </span><span class="c13">GC</span><span class="c4">of </span><span class="c30">super</span><span class="c4">the (g), answer and the sets graphs of graphs not appearing in Result</span><span class="c13">sub</span><span class="c16">(g) </span><span class="c4">in the </span><span class="c16">are </span><span class="c4">in- </span></p><p class="c1"><span class="c4">completely subtracted from CS</span><span class="c13">M</span><span class="c16">(g). Also, the first special </span><span class="c4">case still holds, but for the second special case processing ter- minates when &exist;g &isin; Result</span><span class="c13">sub</span><span class="c16">(g) such that Answer(g ) = &empty;. </span><span class="c4">GC</span><span class="c13">super </span><span class="c16">then determines that there exists a previous query </span><span class="c4">graph g such that g &sube; g and whose cached answer set is {G</span><span class="c13">1</span><span class="c16">,G</span><span class="c13">5</span><span class="c16">}. The reasoning then proceeds as follows. Consider </span><span class="c4">graph G</span><span class="c13">2 </span><span class="c16">&isin; CS</span><span class="c13">M</span><span class="c16">(g). We know from the cached answer set </span><span class="c4">above that G</span><span class="c3">2 </span><span class="c4">is not in the answer set of g . Since g &sube; g, if g &sube; G</span><span class="c13">2 </span><span class="c16">were to be true then it should also hold that g &sube; G</span><span class="c13">2</span><span class="c16">; </span><span class="c4">i.e., the answer set of g would contain G</span><span class="c3">2</span><span class="c4">, which is a con- tradiction. Therefore, it is safe to conclude that g </span><span class="c128">&#8840; </span><span class="c4">G</span><span class="c13">2 </span><span class="c16">and </span><span class="c4">thus G</span><span class="c3">2 </span><span class="c4">can be removed from CS</span><span class="c3">M</span><span class="c4">(g). In the general case, g may be a supergraph of multiple previous query graphs g </span><span class="c13">j </span><span class="c4">. Then, the set of graphs tested for sub-iso by GC is: </span></p><p class="c1"><span class="c4">CS</span><span class="c13">GC</span><span class="c30">super</span><span class="c4">(g) = CS</span><span class="c13">M</span><span class="c16">(g) &cap; </span><span class="c128">&#8898; </span></p><p class="c1"><span class="c3">g </span><span class="c30">j </span><span class="c3">&isin;Result</span><span class="c30">super</span><span class="c3">(g)</span><span class="c18">5.2 Statistics Monitoring </span></p><p class="c2"><span class="c4">The final component of this subsystem is the Statistics Monitor. This is a lightweight layer, implemented as a wrap- per library allowing components of this subsystem to record various statistics (see &sect;6.1) and to communicate them to the Statistics Manager component of the Cache Manager sub- system. Currently the following quantities are monitored: </span></p><p class="c1"><span class="c4">&bull; Static query metrics such as the number of nodes, edges and distinct labels in the query. </span></p><p class="c1"><span class="c4">&bull; Total filtering and verification time of the query when first executed. </span></p><p class="c1"><span class="c4">&bull; Break-down of total filtering times of the query to the three filtering components. </span></p><p class="c1"><span class="c4">&bull; Number of times the query was matched by either GC Processors and number of special-case matches. </span></p><p class="c1"><span class="c4">&bull; Most recent time a cached query was hit, expressed as the serial no. of last benefited query. </span></p><p class="c2"><span class="c4">&bull; Total reduction in the candidate set size of new queries. This statistic is easily monitored, as the Candidate Set Pruner knows exactly which graphs from the answer set of each matched cached query where removed from the candidate set of any given new query (through ap- plication of equations (1) and (2)). </span></p><p class="c2"><span class="c4">&bull; Total time saving due to the cached query. This statis- tic is computed as the sum of the estimated costs of all sub-iso tests alleviated, as mentioned above. The estimation of the individual sub-iso test time c(g, G) for a query graph g against a dataset graph G, is per- formed where L using is the the number formula[34]: of distinct c(g, labels, G) = n </span><span class="c3">L</span><span class="c86">n+1</span><span class="c4">the </span><span class="c7">N&times;N! </span></p><p class="c1"><span class="c3">&times;(N&minus;n)!</span><span class="c4">number </span><span class="c42">, </span></p><p class="c1"><span class="c4">of nodes in g, and N &ge; n the number of nodes in G. </span></p><p class="c1"><span class="c18">6. CACHE MANAGEMENT </span></p><p class="c2"><span class="c4">GC&rsquo;s Cache Manager subsystem, running in parallel with the Query Processing Runtime subsystem, deals with the management of the data and metadata stored in the cache. </span></p><p class="c14"><span class="c43">17 </span><span class="c42">Answer(g </span><span class="c3">j </span><span class="c42">)</span><span class="c4">(2) where Result</span><span class="c3">super</span><span class="c4">(g) contains all query graphs currently con- tained in GC</span><span class="c13">index </span><span class="c16">of which g is a supergraph. </span></p><p class="c2"><span class="c51">Putting It All Together. </span><span class="c4">The Candidate Set Pruner collects CS</span><span class="c13">M </span><span class="c16">and the results of the above two Processors; it then </span><span class="c4">first applies equation (1) on CS</span><span class="c3">M</span><span class="c4">, then applies (2) on the result of the previous operation. The end result is a reduced candidate set, which is then sub-iso tested by M</span><span class="c13">verifier</span><span class="c16">. </span></p><p class="c14"><span class="c51">Two Special Cases. </span><span class="c4">Additionally, there are two cases that warrant attention since they yield the greatest possible gains. First, note that GC can easily recognize the case where a new query, g, is isomorphic to a previous cached query. For connected query graphs, this holds when &exist;g &isin; GC</span><span class="c3">index </span><span class="c4">such that g &sube; g or g &supe; g , and g and g have the same number of nodes and edges. Thus, GC can return the cached result of g directly and completely avoid any further processing. </span></p><p class="c2"><span class="c4">Second, consider that &exist;g &isin; Result</span><span class="c13">super</span><span class="c16">(g) (i.e., g &sube; g) </span><span class="c4">and Answer(g ) = &empty;; then GC can directly return with an empty result set. The reason is that if there were a dataset graph g such that g &sube; g , since g &sube; g we would conclude that g &sube; g , which implies that g &isin; Answer(g ), contra- </span></p><p class="c111"><span class="c4">We first discuss the various data stores handled by this sub- system, then dive into the design of its various components. </span></p><p class="c41"><span class="c18">6.1 Data Layer </span></p><p class="c63"><span class="c4">GraphCache&rsquo;s Cache Manger maintains a number of com- plementary data stores, conceptually bundled together into two groups: the Cache stores and the Window stores. </span></p><p class="c53"><span class="c4">The Cache stores include three components: First, a com- ponent storing copies of cached queries (i.e., the actual graph submitted as a query to GC) alongside their result sets (i.e., the sets of dataset graph IDs containing (for subgraph queries) or being contained in (for supergraph-queries) the query graph). This component is implemented as an in- memory hash table, loaded from disk on startup and writ- ten back to disk on shutdown of the Cache Manager sub- system. In said hash table, the serial number of the query is used as the key and the query graph and result set as the value. At startup, an upper limit is set on the size of this hash table (expressed in number of records); the Cache is deemed full when this upper limit is reached. Second, a combined subgraph/supergraph index, indexing the afore- mentioned query graphs to expedite subgraph/supergraph matching of future queries against past queries. We have loosely based our query index design on the GraphGrepSX subgraph query index[2], augmented with additional meta- data to allow for the processing of supergraph queries. This index is loaded on startup and written back on shutdown of the Cache Manager subsystem. Our index design allows us to have a single index for both subgraph and supergraph queries, thus providing for lower disk space and I/O over- head, and a memory footprint low enough to allow for the index to be easily resident in main memory throughout the lifetime of the Cache Manager process. Third, a component storing statistics for each cached query, implemented as an in-memory key-value store, loaded from disk on startup and written back on shutdown of GC. The query serial number is again used as the key, pointing to a variable size array of columns, sorted by column name. Columns in this store include, but are not limited to: static query such as the number of nodes, edges and distinct labels in the query; to- tal filtering and verification time of the query when first exe- cuted; count of times the query was matched by either of the GC</span><span class="c3">sub</span><span class="c4">/GC</span><span class="c3">super </span><span class="c4">Processors plus number of optimal matches (see &sect;5.1); last (most recent) time a query contributes, ex- pressed as the serial number of the benefited query; total contribution of the cached query in reducing the candidate sets and processing times of future queries, expressed as the number of dataset graphs removed from the candidate set of queries due to their being in the cached query&rsquo;s answer set and the cumulative sub-iso test time alleviated; etc. </span></p><p class="c69"><span class="c4">On the other hand, the Window stores include two com- ponents: First, a component storing new graph queries and their result sets, implemented in the same manner as the first component of the Cache stores above. An upper limit on the size of this store is also configured at startup; the Window is deemed full when said limit is reached. Second, a component storing statistics for each query in the previous component, also implemented as an in-memory key-value store like the statistics component of the Cache stores. In this case, the statistics include only static information regarding the new queries, including the number of nodes, edges and distinct labels in the query, as well as the total filtering and verifica- tion time of the query.New queries are sent to the Window </span></p><p class="c2 c64"><span class="c4">Manager directly from the Query Dispatcher to be added to the appropriate store, while their answer sets are added at the end of their processing. </span></p><p class="c60"><span class="c4">All updates to the query statistics stores are performed through the Statistics Manager using values supplied by the Statistics Monitor. The Statistics Manager is currently im- plemented as a lightweight wrapper library, encapsulating accesses to the statistics stores. The design of this sub- system has explicitly been abstract enough to allow for an easy replacement of the data stores with other in-memory, on-disk or even remote/distributed stores without requiring changes to the rest of our code. The Statistics Manager ex- poses an interface akin to that of contemporary key-value stores; i.e., it stores triplets of the form {key, column name, column value}, accessible either by key (returns a &ldquo;row&rdquo; with all triplets with the given key), or by column name alone (returns a &ldquo;column&rdquo; with all triplets with the given column name), or by key and column name (returns a single triplet). </span></p><p class="c93"><span class="c18">6.2 Window Manager with Admission Control </span></p><p class="c39"><span class="c4">The Window Manager, implemented as a separate thread, is the brain of the Cache Manager subsystem. It keeps track of the queries in the current Window and invokes the Cache Admission Control algorithm to decide whether each new query should be considered as a candidate for addition to the cache. It also executes the Cache Replacement algorithms when the Window is full, and rebuilds GC</span><span class="c13">index </span><span class="c16">to reflect </span><span class="c4">any changes in the cached queries store. In the latter case, the Window Manager first computes the new contents of the cache (by replacing evicted queries with admitted Win- dow queries) and invokes the indexing mechanism; queries arriving at the system while this procedure is taking place, continue being served by the old index and update the old statistics. Once the re-indexing is over, the new cache con- tents and index are swapped in place of the old ones, and any statistics entries corresponding to evicted queries are re- moved lazily from the statistics store. The driving force be- hind this design was the fact that, much like all index-based graph-matching methods, our current version of GC</span><span class="c13">index </span><span class="c4">does not support dynamic concurrent updates. Neverthe- less, our design allows for low-latency/high-throughput pro- cessing of new queries, even while the index is rebuilt, and incurs minimal locking overhead (i.e., only for the swapping of old and new cache contents/index structures, actually im- plemented as simple in-memory reference (pointer) swaps), trading off some possible cache hits against window queries. </span></p><p class="c37"><span class="c51">Cache Admission Control. </span><span class="c4">While experimenting with dif- ferent workloads and datasets we observed that often the performance of GraphCache would be lower than expected; that is, although GraphCache benefited the majority of que- ries, the overall speedup achieved was very low (close to 1). The reason behind this proved to be that the cache was polluted, storing and improving the performance pri- marily of inexpensive graph queries. To alleviate this situ- ation, we make the natural conjecture that past expensive (time-wise) queries are more likely to benefit later coming expensive queries as they will help in alleviating more ex- pensive sub-iso tests (and vice-versa for inexpensive queries). We therefore propose a novel admission control mechanism, part of the Window Manager component, which optimises the graph cache by preventing inexpensive queries from be- ing added to the cache. To quantify the expensiveness of a </span></p><p class="c101"><span class="c43">18 </span></p><p class="c159"><span class="c4">Table 1: Running Example: Cached Query Statistics </span></p><p class="c119"><span class="c4">SerialNo / Last Hit Number CS</span><span class="c13">M </span><span class="c16">SI Cost </span><span class="c4">Query ID of Hits Reduction Reduction 11 91 23 170 2600 13 51 32 80 1200 37 69 26 376 780 53 78 13 210 360 82 90 5 120 150 91 95 4 10 270 </span></p><p class="c139"><span class="c4">query graph, we use the ratio of its verification time over its filtering time. Each executed query is thus assigned an &ldquo;ex- pensiveness&rdquo; score and only queries with such a score above a threshold are considered as candidates for entering the graph cache (a threshold value of 0 disables this compo- nent). To compute said threshold, our mechanism examines the queries in the first few windows and computes an expen- siveness value which would result in a predefined percentage of queries being classified as expensive. We have also experi- mented with more dynamic approaches (e.g., greedily adapt- ing the threshold using an exponential back-off approach until the achieved time speedup reaches a local maximum); without loss of generality and due to lack of space, we omit further discussion of these techniques. The reasoning be- hind the above lies in the fact that, given a graph query processing framework, the filtering time is relatively con- stant across queries, in contrast to the dramatic variance of verification times. Moreover, the verification stage is known to dominate the query time[9, 12], and the larger the verifi- cation time the more overwhelming this dominance. Thus, the above mechanism is a simple yet effective technique to guarantee that more complex queries are prioritised. </span></p><p class="c138"><span class="c18">6.3 Cache Replacement Policies </span></p><p class="c161"><span class="c4">[34] used a specific graph replacement policy (PINC). We have developed and tested a number of new different cache replacement strategies (POP, PIN and HD), each offering different trade-offs and performance characteristics for dif- ferent datasets and query workloads. We describe the vari- ous strategies here and report on their relative performance in &sect;7. In all cases, the replacement strategies access query statistics through the Statistics Manager&rsquo;s key-value store interface, and return the IDs of queries to be cached out. In order to compute this set, queries are assigned a &ldquo;utility&rdquo; value and those with the lowest such values are cached out. Below we present all cache replacement algorithms con- sidered in this work. We use Table 1, presenting a snapshot of GC</span><span class="c3">stats </span><span class="c4">for a number of hypothetical cached queries, as our running example. In all cases, assume that the replace- ment algorithm is invoked at time point 99 (i.e., right after the query with serial number 99 was executed) and needs to remove two entries from the cache, thus has to find the two entries with the lowest utility value. </span></p><p class="c102"><span class="c51">Least Recently Used (LRU). </span><span class="c4">LRU discards the least re- cently used items from the cache. The utility of each cached graph is its last hit time, i.e., the serial no. of the last query that is expedited by said cached graph. In our running ex- ample, cached queries with serial number 13 and 37 would be cached out. LRU is a simple and very popular policy in several traditional caches. However, it builds on the as- sumption of temporal locality of reference and thus fails to </span></p><p class="c2 c64"><span class="c4">identify cases of queries which have contributed huge sav- ings to query processing although not having been used in a while. In our example, we can see that query 13 has con- tributed the most times, but still is evicted. </span></p><p class="c37"><span class="c51">Popularity-based Ranking (POP). </span><span class="c4">Ideally we would pre- fer a replacement policy that would take into account the popularity of queries. This leads to the second policy con- sidered here: POP (short for Popularity-based Ranking). This policy assigns each cached graph a utility value equal to H/A, where H is the number of times a query has con- tributed and A is its age in the cache, computed as the num- ber of processed queries since the said graph enters cache; this function manages to combine query popularity and age. In our example, this policy would evict queries 11 and 53. </span></p><p class="c37"><span class="c51">POP + Number of Sub-Iso Tests (PIN). </span><span class="c4">As mentioned, unlike traditional exact-match caches in which a cache hit saves a disk/network IO, cache hits in GraphCache may result into vastly different reductions in query processing times. One of the reasons why this is so, is that cache hits reduce the candidate set of the coming query by possibly vastly different amounts. However, neither LRU nor POP (actually, none of the known replacement policies) take this into account. This gives rise to the next, exclusive to Graph- Cache, replacement policy: PIN (short for Popularity and sub-Iso test Number) Instead of looking just at the number of hits H of a cached query, PIN assigns each cached graph a utility value equal to R/A, where R is the total number of subgraph isomorphism tests alleviated by said cached query, and A is the same aging factor as above. The utility formula of PIN can also be rewritten as: </span><span class="c7">R</span><span class="c3">A </span><span class="c42">= </span><span class="c7">H</span><span class="c3">A </span><span class="c42">&middot; </span><span class="c7">R</span><span class="c3">H </span><span class="c42">, which can be </span><span class="c4">interpreted as the probability of the query being a hit (i.e., its popularity), times the average savings in number of sub- graph isomorphism test per hit. In our running example, this policy would evict queries 13 and 91. </span></p><p class="c158"><span class="c51">PIN + Sub-Iso Tests Costs (PINC). </span><span class="c4">PIN takes into ac- count the number of sub-iso tests alleviated. Another Graph- Cache-exclusive replacement policy PINC further considers the possibly vast differences in query execution times. PINC assigns each cached query a utility value equal to C/A, where A is the same aging factor as above, and C is the total de- crease in query processing time due to the cached query. Alas, this figure cannot be computed unless the relevant sub-iso tests are actually performed, which is a moot point in our case; instead, as mentioned (&sect;5.2), we use a heuristic to estimate this cost. PINC may improve upon PIN&rsquo;s util- ity value computation by considering the actual (estimated) time cost of alleviated sub-iso tests instead of deeming them all equivalent. PINC&rsquo;s utility formula can be rewritten as: </span><span class="c3">C</span><span class="c13">A </span><span class="c4">= </span><span class="c7">H</span><span class="c3">A </span><span class="c42">&middot; </span><span class="c7">R</span><span class="c3">H </span><span class="c42">&middot; </span><span class="c7">C</span><span class="c3">R</span><span class="c42">, interpreted as the probability of a cached </span><span class="c4">graph being hit, times the average savings in number of sub- iso tests per hit, times the average estimated time cost per saved sub-iso test. In our running example, PINC would evict queries 53 and 82. </span></p><p class="c37"><span class="c51">The Hybrid Dynamic Policy (HD). </span><span class="c4">As the cost compo- nent in PINC is only an estimation, using it does not always lead to improvements in GC&rsquo;s net query processing time. As a matter of fact, we have observed through a large number of experiments, that when the values of the R utility com- </span></p><p class="c83"><span class="c43">19 </span></p><p class="c57"><span class="c4">ponent exhibit a high variability, they are discriminative enough on their own. In such cases, taking the estimated cost into account can actually lead to lower time gains (i.e., PIN performing better than PINC). However, when the val- ues of R exhibit a low variability, adding in the C component leads to considerable query processing time improvements. </span></p><p class="c53"><span class="c4">Thus, the last replacement policy considered in this work (also exclusive to GraphCache), coined the hybrid policy (HD), coalesces both PIN and PINC. More specifically, when the HD policy is invoked, it first retrieves the R compo- nent from GC</span><span class="c13">stats </span><span class="c16">and computes its variability[25] by using </span><span class="c4">the (squared) coefficient of variation (CoV ). CoV is de- fined as the ratio of the (square of the) standard deviation over the (square of the) mean of the distribution. When CoV &gt; 1, the associated distribution is deemed of high vari- ability, as exponential distributions have CoV = 1 and typ- ically hyper-exponential distributions (which capture many high-variance, heavy tailed distributions) have CoV &gt; 1. In this case, HD performs cache eviction using PIN&rsquo;s scoring scheme; otherwise, it uses PINC&rsquo;s scoring scheme. </span></p><p class="c53"><span class="c4">In our running example, the mean R value is &mu; = 161 and its standard deviation &sigma; &asymp; 126; then CoV = &sigma;/&mu; &asymp; 0.78 &lt; 1 and thus HD will use PINC and evict queries 53 and 82. </span></p><p class="c71"><span class="c18">7. PERFORMANCE EVALUATION </span></p><p class="c72"><span class="c18">7.1 System Setup </span></p><p class="c63"><span class="c4">We have implemented all aforementioned components and subsystems of GraphCache in Java over &asymp;6,000 lines of code. Experiments were performed on a Dell R920 host (4 Intel Xeon E7-4870 CPUs (15 cores each), with 320GB of RAM and 4&times;1TB disks, running Ubuntu Linux 14.04.4LTS. </span></p><p class="c69"><span class="c4">We used GraphCache on top of three subgraph FTV and three SI methods (due to space limitations, we only present results for subgraph queries). The default value for the up- per limit on the sizes C of the Cache and W of the Window stores were C = 100 and W = 20 respectively; we also ex- perimented with other values for both C (200, 300) and W (50, 100, 200) to test their impact on GC&rsquo;s performance. Last, the sizes of the various thread pools are all set to 1 so as to show just the benefits of using a graph query cache. For the FTV methods we chose GraphGrepSX [2] (GGSX), Grapes [6], and CT-Index [14], specifically because they are proven to be top performers in their class[12]. Grapes and GGSX were configured to index paths up to length 4, and CT-Index to index trees up to size 6 and cycles up to size 8 using 4,096-bit-wide bitmaps. For Grapes, we examine two alternatives, Grapes1 and Grapes6, with 1 and 6 threads re- spectively. To be fair, we altered the code of Grapes so to stop query processing after the first match in each dataset graph. Please note that all mentioned values match their default configurations in [2, 6, 14]. For the SI methods we used GraphQL[10] as provided by [18] and a modified ver- sion of VF2[4] (denoted VF2+) provided by [14], again for being well-established and good performers[9, 12]; we also used vanilla VF2[4] since it has been used by several FTV implementations [2, 6, 9]. GC uses the Java Native Inter- face to directly execute the native C++ implementations of Grapes, GGSX, GraphQL and VF2, while CT-Index and VF2+ are implemented in Java and thus invoked directly from GC. This diversity in the implementation languages of the incorporated methods attests to GC&rsquo;s flexibility. </span></p><p class="c1 c89"><span class="c18">7.2 Datasets and Query Workloads </span></p><p class="c56"><span class="c4">We employ three real-world (AIDS, PDBS, PCM) and one synthetic graph datasets with different characteristics. More specifically, AIDS[24] &ndash; the Antiviral Screen Dataset of the National Cancer Institute &ndash; contains topological structures of 40,000 molecules. Graphs in AIDS contain on average &asymp;45 vertices (std.dev.: 22, max: 245) and &asymp;47 edges (std.dev.: 23, max: 250) each, whereby the few largest graphs have an order of magnitude more vertices and edges. PDBS[11] is a dataset of graphs representing DNA, RNA and pro- teins, consisting of fewer (600) but larger graphs compared to AIDS, with on average &asymp;2,939 vertices (std.dev.: 3,215, max: 16,341) and &asymp;3,064 edges (std.dev.: 3,261, max: 16,781) per graph. PCM[32] consists of 200 graphs representing pro- tein interaction maps, with on average &asymp;377 nodes (std.dev.: 187, max: 883) and &asymp;4,340 edges (std.dev.: 1,912, max: 9,416) per graph. Last, the Synthetic dataset was created using [3] and contained 1,000 graphs with on average &asymp;892 nodes (std.dev.: 417, max: 7,135) and &asymp;7,991 edges (std.dev.: 5.09, max: 8,007) per graph. We created this dataset as a larger counterpart to the PCM dataset, consisting of 5&times; more graphs, each being 2-3&times; larger on average than the average PCM graph. Graphs in AIDS and PDBS have low average node degree (AIDS &asymp;2.09, PDBS &asymp;2.13), whereas graphs of PCM and Synthetic have much higher average node degrees (PCM &asymp;22.39, Synthetic &asymp;19.52). </span></p><p class="c60"><span class="c4">We follow the established principle for the generation of our workloads, using two different algorithms to synthesize queries from the dataset graphs, outlined below. </span></p><p class="c37"><span class="c51">Type A Workloads. </span><span class="c4">Queries in these workloads are gener- ated in the following manner: first, a source graph is selected randomly from the dataset graphs; then, a node is selected randomly in said graph; finally, a query size is selected uni- formly at randomly from several pre-defined sizes and a BFS is performed starting from the selected node. For each new node, all its edges connecting it to already visited nodes are added to the generated query, until the desired query size is reached. For the first two random selections above, we have used two different distributions; namely, Uniform (U) and Zipf (Z), with the probability density function of the latter given by p(x) = x</span><span class="c7">&minus;&alpha;</span><span class="c4">/&zeta;(&alpha;), where &zeta; is the Riemann Zeta function[26]. Ultimately, we had three categories of Type A workloads: &ldquo;UU&rdquo;, &ldquo;ZU&rdquo; and &ldquo;ZZ&rdquo;, where the first letter in each pair denotes the distribution used for selecting the starting graph, and the second for the starting node. </span></p><p class="c27"><span class="c51">Type B Workloads (with no-answer queries). </span><span class="c4">These work- loads are generated as follows. For each of the query sizes, we first create two query pools: a 10,000-query pool with queries with non-empty answer sets against the dataset, and a second 3,000-query pool with no match in any dataset graph (i.e., empty result set). Queries for the first pool are extracted from dataset graphs by uniformly selecting a start node across all nodes in all dataset graphs, and then per- forming a random walk till the required query graph size is reached. Generation of no-answer queries has one extra step: we continuously relabel the nodes in the query with randomly selected labels from the dataset, until the resulting query has a non-empty candidate set but an empty answer set against the dataset graphs. Once the query pools are filled up, we generate workloads by first flipping a biased </span></p><p class="c124"><span class="c43">20 </span></p><p class="c2"><span class="c4">coin to choose between the two pools (with the &ldquo;no-answer&rdquo; pool selected with probability 0%, 20% or 50%), then ran- domly (Zipf) selecting a query from the chosen pool. We thus have three categories of Type B workloads: &ldquo;0%&rdquo;, &ldquo;20%&rdquo; and &ldquo;50%&rdquo;, denoting the above probability used. </span></p><p class="c2"><span class="c4">We use Zipf &alpha; = 1.4 by default; we also use &alpha; = 1.1 representing a smaller skewness and &alpha; = 1.7 for a higher skewness. As a reference point, web page popularities fol- low a Zipf distribution with &alpha; = 2.4 [26]. Query graphs are generated in different sizes: 4, 8, 12, 16 and 20-edge graphs for the smaller AIDS and PDBS datasets; 20, 25, 30, 35 and 40-edge queries for the larger PCM and Synthetic datasets (as almost half of the dataset graphs in AIDS contain no more than 40 edges, larger queries are not usable). Such sizes are typical in the literature [6, 14, 35]. Workloads for AIDS and PDBS consist of 10,000 queries, while workloads for PCM and Synthetic contain 5,000 queries for practical reasons, as PCM/Synthetic queries take much longer to ex- ecute. We only allow one for one Window (i.e., 20 queries) before starting measuring GC&rsquo;s performance. </span></p><p class="c2"><span class="c4">We report on both the benefits and the overheads of GC. Reported metrics include query time and number of sub-iso tests per query, along with the speedups introduced by GC. Speedup is defined as the ratio of the average performance (query time or number of sub-iso tests) of the base Method M over the average performance of GC when deployed over Method M (i.e., speedups &gt;1 indicate improvements). The results were produced over more than 6 million queries! As a yardstick, [21] (also a cache but for XML databases) report a query time speedup of 2.6&times; with 10,000-query workloads generated using Zipf &alpha; = 1.5, and a 1,500-query warm-up. </span></p><p class="c1"><span class="c18">7.3 Results and Takeaways </span></p><p class="c2"><span class="c4">Figure 4 depicts the speedups attained by GraphCache when CT-Index and GGSX where used as Method M (re- sults for other FTV and SI methods showed similar trends and are thus omitted for space reasons). We can see that GraphCache attains significant speedups (up to 10&times; lower query processing times in this case), and that it is always one of the GC-exclusive policies (PIN, PINC) that produces the best results. A more subtle observation, though, is that there are cases where PIN wins over PINC and vice-versa; for example, PIN dominates the scene for queries against the AIDS dataset but it is PINC that takes the lead when querying the PDBS dataset. Ultimately, different cache re- placement policies exhibit different performance depending on the workload and dataset characteristics. The question then is how to choose a replacement policy when said charac- teristics are unknown a-priori. Our answer to this question then, and the first takeaway message, is: When in doubt, use the HD replacement policy, as it always manages to do better or on par with the best of the alternatives. For the remainder of this section we will be using HD as the replacement policy; results for other caching policies show similar trends and are thus omitted for space reasons. </span></p><p class="c2"><span class="c4">Figure 5 depicts speedups in query processing time against all FTV methods for queries on the PDBS dataset (results for other datasets are similar). Query processing time speed- ups range from 1.60&times; (i.e., 37.5% lower processing time) to more than 42&times;. A similar picture is drawn in Figure 6 for speedups in the number of sub-iso tests performed. Jux- taposing Figure 5 and 6 leads to the following interesting insight: Reductions in the number of sub-iso tests do </span></p><p class="c1"><span class="c62">LRU POP PIN PINC HD </span><span class="c36">10.00 </span></p><p class="c1"><span class="c36">0.00 </span></p><p class="c1"><span class="c36">ZZ ZU UU 0% 20% 50% </span></p><p class="c1"><span class="c36">AIDS </span></p><p class="c1"><span class="c36">4.00 </span></p><p class="c1"><span class="c36">ZZ ZU UU 0% 20% 50% </span></p><p class="c1"><span class="c36">PDBS </span><span class="c4">Figure 4: Query Processing Time Speedups over CT-Index Across Replacement Policies </span></p><p class="c2"><span class="c4">not translate directly into reductions in query time; this validates our claim that cache hits in GraphCache ren- der different benefits. In all cases, though, GraphCache achieves significant improvements in both query pro- cessing time and number of sub-iso tests performed. </span></p><p class="c2"><span class="c4">Figure 7 shows the speedups achieved by GraphCache for Type B workloads against the AIDS dataset, for various val- ues of the Zipf &alpha; skewness parameter (results for number of sub-iso tests and other workloads show similar trends and are omitted for space reasons). We can see that the more skewed the query distribution, the higher the gains from caching. This is, of course, expected and has been shown times and again in related work on traditional caches, as caches are built on the premise of (temporal) locality of reference and thus more skewed query distributions have the potential to translate to higher hit ratios. A subtler, but equally important observation here, reached by examining Figure 5 in the light of the above result, is that Graph- Cache leads to significant performance gains even for query workloads with uniform query popularity distri- butions. These distributions represent worst-case scenarios for caching schemes, but we can see speedups from 1.29&times; (&asymp;20% lower times) up to &asymp;11&times; for the UU workloads, em- phasizing a significant characteristic of GraphCache where the realm of &ldquo;locality&rdquo; is extended by subgraph/supergraph matches among queries, in addition to the traditional exact- match of isomorphic queries. </span></p><p class="c2"><span class="c4">Figure 8 shows the performance of GC against GGSX for queries on AIDS and PDBS, for varying cache sizes (results for other methods and datasets show similar trends). We can see that increasing the cache size improves the perfor- mance of the cache. However, this does not mean that one can increase the size of the cache indefinitely; the size of the cache is first limited by the amount of main mem- ory available for GC, then by the overhead associated with updating the cache contents (more on this shortly). </span></p><p class="c2"><span class="c4">Figure 9 shows the speedups in query time (9(a)) and number of sub-iso tests (9(b)) against Grapes6 for the PCM and Synthetic datasets, attained when the cache admission control is disabled (C) and enabled (C + AC). For clarity, performance without specific notes refer to turning off the cache admission control (C) by default. We can see that cache admission control leads to even higher speed- ups, thus validating our observation regarding cache pollu- tion and the appropriateness of our &ldquo;expensiveness&rdquo;-based mechanism. A subtler observation is that the correspond- ing speedup in the number of sub-iso tests is reduced when cache admission control is enabled, as shown in Figure 9(b). For better understanding of this trend, let us concentrate on </span></p><p class="c1"><span class="c43">21 </span></p><p class="c1"><span class="c36">8.00 </span></p><p class="c1"><span class="c36">3.00 </span><span class="c36">6.00 </span></p><p class="c1"><span class="c36">2.00 </span><span class="c36">4.00 </span></p><p class="c1"><span class="c36">2.00 </span></p><p class="c1"><span class="c36">1.00 </span></p><p class="c1"><span class="c36">0.00 </span></p><p class="c1"><span class="c9">C + AC </span></p><p class="c1"><span class="c4">(b) Speedups in Number of Sub-Iso Tests </span></p><p class="c1"><span class="c4">Figure 9: GraphCache Performance vs Grapes6 for Type B Workloads on PCM/Synthetic Datasets </span></p><p class="c1"><span class="c0">Average Query Time (milliseconds) Overhead (milliseconds) </span></p><p class="c1"><span class="c62">1,285 </span></p><p class="c1"><span class="c0">7 20 </span><span class="c26">31 </span></p><p class="c1"><span class="c49">9.49 </span></p><p class="c1"><span class="c49">4.35 </span><span class="c58">3.31 </span></p><p class="c1"><span class="c0">132 </span><span class="c26">68 60 </span></p><p class="c1"><span class="c8">M d ohte</span><span class="c0">M</span><span class="c8">0 2b-001</span><span class="c0">c</span><span class="c4">Figure 10: Average Execution Time and Overhead (millisec- onds) per Query for the 20% workload on AIDS </span></p><p class="c1"><span class="c8">0 2b-003</span><span class="c0">c</span><span class="c8">0 </span></p><p class="c1"><span class="c8">2b-005</span><span class="c0">c</span><span class="c8">M </span></p><p class="c1"><span class="c8">d ohte</span><span class="c0">M</span><span class="c8">0 </span></p><p class="c1"><span class="c8">2b-001</span><span class="c0">c</span><span class="c8">0 </span></p><p class="c1"><span class="c8">2b-003</span><span class="c0">c</span><span class="c8">0 </span></p><p class="c1"><span class="c8">2b-005</span><span class="c0">c</span><span class="c8">M </span></p><p class="c1"><span class="c8">d ohte</span><span class="c0">M</span><span class="c8">0 </span></p><p class="c1"><span class="c8">2b-001</span><span class="c0">c</span><span class="c8">0 </span></p><p class="c1"><span class="c8">2b-003</span><span class="c0">c</span><span class="c8">0 2b-005</span><span class="c0">cCT-Index GGSX Grapes6 </span></p><p class="c1"><span class="c79">ZZ ZU UU ZZ ZU UU ZZ ZU UU ZZ ZU UU </span></p><p class="c1"><span class="c79">VF2+ GQL VF2+ GQL </span></p><p class="c1"><span class="c79">AIDS PDBS </span></p><p class="c1"><span class="c4">Figure 11: GC Query Time Speedups vs SI Methods </span></p><p class="c1"><span class="c43">22 </span><span class="c0">697 </span></p><p class="c1"><span class="c49">8.85 </span></p><p class="c1"><span class="c49">6.49 </span><span class="c145">7.18 </span><span class="c49">6.11 </span><span class="c58">4.80 4.15 3.56 </span></p><p class="c1"><span class="c0">130 </span><span class="c26">93 89 </span></p><p class="c1"><span class="c49">2.02 1.99 </span><span class="c0">664 </span></p><p class="c1"><span class="c0">6 </span></p><p class="c1"><span class="c0">21 34 </span></p><p class="c1"><span class="c0">7 </span><span class="c26">18 31 </span></p><p class="c1"><span class="c0">338 335 </span><span class="c26">320 </span></p><p class="c1"><span class="c10">8.88 </span><span class="c95">9.33 </span></p><p class="c1"><span class="c10">7.31 </span></p><p class="c28"><span class="c10">ZZ ZU UU 0% 20% 50% </span><span class="c4">(d) Grapes6 </span></p><p class="c1"><span class="c4">Figure 6: GraphCache Speedup in Number of Sub-iso Tests for PDBS across all Methods M </span></p><p class="c1"><span class="c38">zipf 1.1 zipf 1.4 zipf 1.7 </span></p><p class="c1"><span class="c5">4.42 4.22 4.09 </span></p><p class="c1"><span class="c4">(a) CT-Index </span></p><p class="c1"><span class="c38">zipf 1.1 zipf 1.4 zipf 1.7 </span></p><p class="c1"><span class="c4">(b) GGSX </span></p><p class="c1"><span class="c38">zipf 1.1 zipf 1.4 zipf 1.7 </span></p><p class="c1"><span class="c4">(c) Grapes1 </span></p><p class="c1"><span class="c38">zipf 1.1 zipf 1.4 zipf 1.7 </span></p><p class="c1"><span class="c5">22.99 23.31 </span></p><p class="c1"><span class="c5">16.55 </span></p><p class="c1"><span class="c4">(d) Grapes6 </span></p><p class="c1"><span class="c4">Figure 7: GraphCache Speedup in Query Time for Type B workloads on the AIDS dataset, for various value of Zipf &alpha; </span></p><p class="c1"><span class="c5">3.39 </span><span class="c19">3.00 2.81 </span></p><p class="c1"><span class="c5">10.22 </span><span class="c19">9.52 8.27 </span></p><p class="c1"><span class="c5">3.70 5.02 </span><span class="c19">4.82 </span><span class="c151">4.10 </span><span class="c5">3.45 </span></p><p class="c1"><span class="c19">4.25 </span></p><p class="c1"><span class="c5">1.66 1.96 2.17 </span><span class="c19">1.57 </span><span class="c5">1.96 2.18 </span><span class="c19">1.56 </span></p><p class="c1"><span class="c19">1.73 </span></p><p class="c1"><span class="c19">1.99 </span></p><p class="c1"><span class="c5">9.68 9.76 </span><span class="c19">8.43 </span></p><p class="c1"><span class="c5">5.47 5.38 </span><span class="c19">4.98 </span></p><p class="c1"><span class="c5">2.66 </span><span class="c19">2.52 2.42 </span><span class="c5">2.82 2.70 2.65 </span></p><p class="c1"><span class="c38">0% 20% 50% </span></p><p class="c1"><span class="c38">0% 20% 50% </span></p><p class="c1"><span class="c38">0% 20% 50% </span></p><p class="c1"><span class="c38">0% 20% 50% </span></p><p class="c1"><span class="c38">c100-b20 c300-b20 c500-b20 </span></p><p class="c1"><span class="c4">(a) AIDS/Type A Workloads </span></p><p class="c1"><span class="c38">c100-b20 c300-b20 c500-b20 </span></p><p class="c1"><span class="c4">(b) AIDS/Type B Workloads </span></p><p class="c1"><span class="c38">c100-b20 c300-b20 c500-b20 </span></p><p class="c1"><span class="c4">(c) PDBS/Type A Workloads </span></p><p class="c1"><span class="c38">c100-b20 c300-b20 c500-b20 </span><span class="c5">4.07 4.31 </span><span class="c19">3.82 4.00 3.87 4.05 </span></p><p class="c1"><span class="c5">7.94 8.48 </span><span class="c19">7.51 7.86 </span></p><p class="c1"><span class="c19">6.34 </span></p><p class="c1"><span class="c5">6.53 </span></p><p class="c1"><span class="c5">8.92 </span></p><p class="c1"><span class="c5">10.00 </span></p><p class="c1"><span class="c5">5.23 6.83 </span></p><p class="c1"><span class="c19">4.28 </span><span class="c5">5.47 </span><span class="c19">4.11 </span></p><p class="c1"><span class="c151">5.80 </span><span class="c5">5.47 5.38 </span><span class="c19">4.98 </span></p><p class="c1"><span class="c5">5.72 </span></p><p class="c1"><span class="c5">3.88 </span></p><p class="c1"><span class="c5">1.86 2.68 3.08 </span><span class="c19">1.53 </span></p><p class="c1"><span class="c19">2.04 </span></p><p class="c1"><span class="c19">2.30 </span></p><p class="c1"><span class="c5">2.83 </span><span class="c19">2.17 </span></p><p class="c1"><span class="c38">ZZ ZU UU </span></p><p class="c1"><span class="c38">0% 20% 50% </span></p><p class="c1"><span class="c38">ZZ ZU UU </span></p><p class="c28"><span class="c38">0% 20% 50% </span><span class="c4">(d) PDBS/Type B Workloads </span></p><p class="c1"><span class="c4">Figure 8: GraphCache Speedup in Query Time against GGSX with various cache sizes </span></p><p class="c1"><span class="c10">3.43 </span></p><p class="c1"><span class="c10">1.60 </span><span class="c22">1.29 </span></p><p class="c1"><span class="c10">9.60 </span></p><p class="c1"><span class="c10">4.46 </span><span class="c22">3.52 </span></p><p class="c1"><span class="c10">2.54 </span><span class="c22">2.20 </span></p><p class="c1"><span class="c10">1.43 </span></p><p class="c28"><span class="c10">ZZ ZU UU 0% 20% 50% </span><span class="c4">(a) CT-Index </span></p><p class="c1"><span class="c10">8.77 </span><span class="c95">9.17 </span><span class="c10">7.80 </span></p><p class="c28"><span class="c10">ZZ ZU UU 0% 20% 50% </span><span class="c4">(a) CT-Index </span></p><p class="c1"><span class="c9">C </span><span class="c61">5.71 4.35 </span></p><p class="c1"><span class="c61">3.04 4.05 </span></p><p class="c14"><span class="c9">C </span><span class="c61">5.44 2.94 </span><span class="c9">+ AC </span></p><p class="c1"><span class="c40">1.67 </span><span class="c61">2.50 </span><span class="c40">1.73 2.24 1.47 </span></p><p class="c1"><span class="c40">1.92 </span></p><p class="c1"><span class="c61">0% 20% 50% 0% 20% 50% </span></p><p class="c1"><span class="c61">PCM Synthetic </span></p><p class="c1"><span class="c4">(a) Query Time Speedups </span></p><p class="c1"><span class="c10">22.09 </span></p><p class="c1"><span class="c10">11.24 </span><span class="c22">8.29 </span><span class="c10">11.10 </span><span class="c22">10.39 7.93 </span></p><p class="c28"><span class="c10">ZZ ZU UU 0% 20% 50% </span><span class="c4">(d) Grapes6 </span></p><p class="c1"><span class="c4">Figure 5: GraphCache Speedup in Query Time for PDBS Across All Methods M </span></p><p class="c1"><span class="c10">5.72 </span></p><p class="c1"><span class="c10">1.86 </span><span class="c22">1.53 </span></p><p class="c1"><span class="c10">9.11 </span></p><p class="c1"><span class="c10">4.05 </span><span class="c22">3.25 </span></p><p class="c1"><span class="c10">3.88 </span><span class="c22">2.83 2.17 </span></p><p class="c28"><span class="c10">ZZ ZU UU 0% 20% 50% </span><span class="c4">(b) GGSX </span></p><p class="c1"><span class="c10">7.88 </span><span class="c22">6.09 4.19 </span></p><p class="c28"><span class="c10">ZZ ZU UU 0% 20% 50% </span><span class="c4">(b) GGSX </span></p><p class="c1"><span class="c10">42.37 </span></p><p class="c1"><span class="c10">14.72 </span><span class="c22">10.92 </span><span class="c10">14.92 </span><span class="c95">16.44 </span><span class="c10">11.69 </span></p><p class="c28"><span class="c10">ZZ ZU UU 0% 20% 50% </span><span class="c4">(c) Grapes1 </span></p><p class="c1"><span class="c10">10.56 </span></p><p class="c1"><span class="c10">4.86 </span><span class="c22">3.75 </span></p><p class="c1"><span class="c10">8.88 </span><span class="c95">9.33 </span></p><p class="c1"><span class="c10">7.31 </span></p><p class="c28"><span class="c10">ZZ ZU UU 0% 20% 50% </span><span class="c4">(c) Grapes1 </span></p><p class="c1"><span class="c61">4.36 3.20 2.57 </span><span class="c40">2.97 2.31 2.50 </span></p><p class="c1"><span class="c40">2.28 1.93 4.05 1.95 3.97 </span></p><p class="c1"><span class="c61">2.59 </span></p><p class="c1"><span class="c61">0% 20% 50% 0% 20% 50% </span></p><p class="c1"><span class="c61">PCM Synthetic </span></p><p class="c1"><span class="c9">C </span></p><p class="c1"><span class="c10">10.56 </span></p><p class="c1"><span class="c10">4.86 </span><span class="c22">3.75 </span></p><p class="c1"><span class="c54">c100-b20 </span></p><p class="c1"><span class="c54">c500-b20 </span></p><p class="c1"><span class="c66">3.58 </span></p><p class="c1"><span class="c66">1.82 1.80 1.85 </span></p><p class="c1"><span class="c0">1.82 </span></p><p class="c1"><span class="c0">1.02 </span><span class="c66">1.69 </span><span class="c35">0.86 </span><span class="c0">0.74 </span><span class="c35">0.55 </span><span class="c0">1.02 </span></p><p class="c1"><span class="c123">1.35 </span></p><p class="c1"><span class="c0">ZZ ZU UU ZZ ZU UU </span></p><p class="c1"><span class="c0">AIDS PDBS </span></p><p class="c1"><span class="c4">Figure 12: Query Time Speedups of GC/VF2+ vs CTIndex </span></p><p class="c2"><span class="c4">the Synthetic-50% workload: GC without admission control yields a speedup as high as &asymp;4&times; in the number of sub-iso tests, but the resulting query time speedup is only &asymp;1.5&times;. The reason is that top expensive queries do not benefit as much when the cache is polluted: more specifically, the av- erage time for the top-1% most time-consuming queries is &asymp;16.5 seconds with Grapes6, going down to &asymp;15 seconds for GraphCache without admission control &ndash; a 1.1&times; speedup; the remaining 99% &ldquo;inexpensive&rdquo; queries enjoy speedups of 2&times;, going from &asymp;0.200 seconds down to &asymp;0.100 seconds, but they account for a much smaller percentage of the overall query processing time compared to the top-1% ones. When we enable the admission control mechanism, these top-1% expensive queries are prioritized, with their average query processing time going down considerably to &asymp;10 seconds &ndash; a much improved 1.65&times; speedup. Hence, despite the lower speedup in number of sub-iso tests, the overall query processing time benefits greatly. </span></p><p class="c2"><span class="c4">We have shown so far that GraphCache leads to significant decreases in the query processing time and number of sub- iso tests of FTV (and SI) methods. We know that sub-iso tests take up the majority of the query processing time for FTV methods. A logical consideration, then, would be to try and increase the filtering power of these methods so as to further decrease the size of the resulting candidate set. This can be accomplished by increasing the size of the features recorded by FTV methods; larger features bear higher dis- criminative power as, obviously, the larger a feature the less its occurrences in dataset graphs. To this end, we reconfig- ured all FTV methods increasing their feature sizes by just one (i.e., max path length of 5 for Grapes and GGSX; trees of size 7, cycles of size 9, and 8192 bits per bitmap for CT- Index). This minimal increase in feature size indeed led to better performance, with the average query processing time going down by approximately 10%; however, it also led to an almost doubling of the space required for the FTV indexes across all methods. At the same time, GC accomplishes its speedup for a negligible space overhead; for example, for the AIDS dataset the memory and disk space required by GraphCache was just over 1% of the space required for the indexes of the various FTV methods, but leading to time speedups of up to 40&times; (figure omitted for space reasons). </span></p><p class="c2"><span class="c4">Figure 10 depicts a break-down of query processing time for FTV methods and GraphCache, showing how much of GC time is spent (on average) to update the Window and Cache data stores (including executing the cache replace- ment algorithms and re-indexing the cached query graphs), for various cache sizes. As we can see, the time overhead for cache maintenance chores is trivial. Another inter- esting observation is that, although increasing the size of the cache improves query processing time (as also shown in Fig- </span></p><p class="c2"><span class="c4">ure 8), it also leads to an increase in the overhead associated with the maintenance of the cache contents. For the cache sizes considered in this work we can see that what we lose in maintenance overhead, we gain in query time. That means that, if we had designed our architecture to update the cache contents in-line (i.e., not in parallel) with query processing, we would see diminishing returns with larger cache sizes. Our current design does not suffer from this problem; how- ever, we expect that, for considerably larger cache sizes, this overhead may outgrow the time required for the Window to fill (and thus for a new replacement/re-indexing round to begin). The upside is, though, that even with the meagre cache sizes used in this work, the performance gains are enough to not warrant a much larger cache. </span></p><p class="c2"><span class="c4">Figure 11 depicts the query processing speedups of GC over the two well-established SI methods considered in this work &ndash; GQL and VF2+ (vanilla VF2 results where simi- lar and are omitted for space and readability reasons). We can see that GC improves the performance of well- established SI methods, with the same meagre 100-query cache configuration as above. This is significant in that GC provides a new way to expedite sub-iso tests (as opposed to developing yet another SI heuristic) which is usable with any mainstream SI method. Note the interesting finding that VF2+ speedup for AIDS UU work- load is close to that of AIDS ZU (7.18 vs 6.49), whereas one might have expected a different outcome. Intuitively, the ZU workload bears more exact-match hits than UU, due to the skewness of selecting source graphs during query generation (see &sect;7.2). And it does: we measured circa 2.5X the number of exact-match cache hits in ZU vs UU. However, recall that GC exploits also sub/supergraph hits. When exact-matches are not frequent, GC loads graphs in the cache that can help with their sub/supergraph relationships. Indeed, we mea- sured circa 2X such matches for the UU workload vs ZU. Of course, the overall performance result is a very complex picture and depends on how big benefit is each saved exact- match vs each saved sub/supergraph match. But the key in- sight here is that by utilizing exact-matches and sub/su- pergraph matches, GC can introduce significant ben- efits in both skewed and non-skewed workloads. </span></p><p class="c2"><span class="c4">Let us now take a step back and look at how FTV meth- ods and GC operate: they both expedite queries by filter- ing out dataset graphs, thus producing a reduced candi- date set. The logical question then is: what happens if we pitch a full-blown FTV method against GC operating on top of a simple SI method? Figure 12 shows the re- sults when comparing GC on top of VF2+ against CT-Index (also using VF2+ for its verification chores), across several datasets and Type-A workloads (results for Type-B work- loads omitted for space reasons). For the small 100-query cache, GC performs on par or better than CT-Index in six out of nine cases, slightly worse in two other cases, and takes up to double the time of CT-Index in the remaining worst case. Note, though, that GC&rsquo;s space requirements are un- der &asymp;15% of the space requirements of CT-Index&rsquo;s index for PDBS and under 0.2% for AIDS, and that CT-Index has the fastest verification algorithm and by far the smallest index among all FTV methods considered in this work. The situ- ation is more impressive when using the larger (500-query) cache, where GC matches or outperforms CT-Index across the board (by a factor of 1.8&times; on average). Note that even for this &ldquo;larger&rdquo; cache, GC&rsquo;s space requirements are less than </span></p><p class="c1"><span class="c43">23 </span></p><p class="c141"><span class="c4">&asymp;70% of CT-Index&rsquo;s index size for PDBS and less than 1% for AIDS (and comparable to the latter against GGSX and Grapes). The conclusion is then that GC can replace the best-performing FTV methods, achieving comparable or better performance for a fraction of the space and no pre-processing cost as no indexing is needed. </span></p><p class="c68"><span class="c18">8. CONCLUSIONS </span></p><p class="c48"><span class="c4">We presented GraphCache, to the best of our knowledge the first full-fledged caching system for general subgraph/su- pergraph query processing, including its architecture meet- ing demanding design goals, a number of GC-exclusive graph- query-aware cache replacement policies, and an accompa- nying cache admission control mechanism. The proposed system can be used to expedite all current FTV and SI methods (bridging these two, alas, separate threads of re- search so far), and is applicable for both subgraph and su- pergraph queries. Our extensive performance evaluation has proven the applicability and appropriateness of our ap- proach. GC achieves considerable improvements in query processing time for meagre space overheads. Our work also revealed a number of key lessons, pertaining to graph caching and query processing. Future work currently focuses on two big ticket items: first, to develop a distributed/decentral- ized version of GraphCache; second, to extend GraphCache to benefit subgraph queries when finding all occurrences of a query graph against a single massive stored graph. </span></p><p class="c98"><span class="c18">9. REFERENCES </span></p><p class="c84"><span class="c4">[1] A. Balmin et al. A framework for using materialized XPath views in XML query processing. In Proc. VLDB, 2004. [2] V. Bonnici et al. Enhancing graph database indexing </span></p><p class="c117"><span class="c4">by suffix tree structure. In Proc. IAPR PRIB, 2010. [3] J. Cheng, Y. Ke, and W. Ng. GraphGen. </span></p><p class="c52"><span class="c4">http://www.cse.ust.hk/graphgen/. [4] L. P. Cordella, P. Foggia, C. Sansone, and M. Vento. </span></p><p class="c55"><span class="c4">A (sub) graph isomorphism algorithm for matching large graphs. IEEE TPAMI, 26(10):1367&ndash;1372, 2004. [5] M. Garey and D. Johnson. Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. Freeman, 1979. [6] R. Giugno et al. Grapes: A software for parallel </span></p><p class="c97"><span class="c4">searching on biological graphs targeting multi-core architectures. PloS One, 8(10):e76911, 2013. [7] A. Gubichev and T. Neumann. Exploiting the query </span></p><p class="c100"><span class="c4">structure for efficient join ordering in SPARQL queries. In Proc. EDBT, 2014. [8] W.-S. Han, J. Lee, and J.-H. Lee. Turbo</span><span class="c13">ISO </span><span class="c16">: Towards </span></p><p class="c132"><span class="c4">ultrafast and robust subgraph isomorphism search in large graph databases. In Proc. SIGMOD, 2013. [9] W.-S. Han, J. Lee, M.-D. Pham, and J. X. Yu. iGraph: </span></p><p class="c131"><span class="c4">A framework for comparisons of disk-based graph indexing techniques. PVLDB, 3(1-2):449&ndash;459, 2010. [10] H. He and A. K. Singh. Graphs-at-a-time: Query </span></p><p class="c74"><span class="c4">language and access methods for graph databases. In Proc. SIGMOD, 2008. [11] Y. He et al. Structure of decay-accelerating factor </span></p><p class="c70"><span class="c4">bound to echovirus 7: a virus-receptor complex. PNAS, 99:10325&ndash;10329, 2002. </span></p><p class="c1 c164"><span class="c4">[12] F. Katsarou, N. Ntarmos, and P. Triantafillou. </span><span class="c16">Performance and scalability of indexed subgraph query </span></p><p class="c50 c162"><span class="c4">processing methods. PVLDB, 8(12):1566&ndash;1577, 2015. [13] J. Kim, H. Shin, and W.-S. Han. Taming subgraph </span></p><p class="c23"><span class="c4">isomorphism for RDF query processing. PVLDB, 8(11):1238&ndash;1249, 2015. [14] K. Klein, N. Kriege, and P. Mutzel. CT-index: </span></p><p class="c29"><span class="c4">Fingerprint-based graph indexing combining cycles and trees. In Proc. ICDE, 2011. [15] G. Koloniari and E. Pitoura. Partial view selection for </span></p><p class="c50"><span class="c4">evolving social graphs. In Proc. GRADES, 2013. [16] L. Lai et al. Scalable subgraph enumeration in </span></p><p class="c31"><span class="c4">MapReduce. PVLDB, 8(10):974&ndash;985, 2015. [17] L. V. S. Lakshmanan et al. Answering tree pattern </span></p><p class="c105"><span class="c4">queries using views. In Proc. VLDB, 2006. [18] J. Lee et al. An in-depth comparison of subgraph </span></p><p class="c80"><span class="c4">isomorphism algorithms in graph databases. PVLDB, 6(2):133&ndash;144, 2012. [19] K. Lillis and E. Pitoura. Cooperative XPath caching. </span></p><p class="c50 c103"><span class="c4">In Proc. SIGMOD, 2008. [20] B. Lyu et al. Scalable supergraph search in large </span></p><p class="c90"><span class="c4">graph databases. In Proc. ICDE, 2016. [21] B. Mandhani and D. Suciu. Query caching and view </span></p><p class="c163"><span class="c4">selection for XML databases. In Proc. VLDB, 2005. [22] M. Martin, J. Unbehauen, and S. Auer. Improving the </span></p><p class="c81 c85"><span class="c4">performance of semantic web applications with SPARQL query caching. In Proc. ESWC, 2010. [23] G. Moerkotte and T. Neumann. Dynamic </span></p><p class="c50 c94"><span class="c4">programming strikes back. In Proc. SIGMOD, 2008. [24] NCI - DTP AIDS antiviral screen dataset. </span></p><p class="c73"><span class="c4">http://dtp.nci.nih.gov/docs/aids/aids data.html. [25] R. Nelson. Probability, Stochastic Processes, and </span></p><p class="c129"><span class="c4">Queueing Theory. Springer Verlag, 1995. [26] M. Newman. Power laws, Pareto distributions and </span></p><p class="c50 c140"><span class="c4">Zipf&rsquo;s law. Contemporary Physics, 46:323&ndash;351, 2005. [27] N. Papailiou, D. Tsoumakos, P. Karras, and </span></p><p class="c64 c144"><span class="c4">N. Koziris. Graph-aware , workload-adaptive SPARQL query caching. In Proc. SIGMOD, 2015. [28] X. Ren and J. Wang. Exploiting vertex relationships in speeding up subgraph isomorphism over large graphs. PVLDB, 8(5):617&ndash;628, 2015. [29] K. Semertzidis and E. Pitoura. Durable graph pattern queries on historical graphs. In Proc. ICDE, 2016. [30] Z. Sun et al. Efficient subgraph matching on billion </span></p><p class="c107"><span class="c4">node graphs. PVLDB, 5(9):788&ndash;799, 2012. [31] J. R. Ullmann. An algorithm for subgraph </span></p><p class="c31"><span class="c4">isomorphism. J. ACM, 23(1):31&ndash;42, 1976. [32] C. Vehlow et al. CMView: Interactive contact map </span></p><p class="c87"><span class="c4">visualization and analysis. Bioinformatics, 27:1573&ndash;1577, 2011. [33] J. Wang, J. Li, and J. X. Yu. Answering tree pattern </span></p><p class="c50 c99"><span class="c4">queries using views: a revisit. In Proc. EDBT, 2011. [34] J. Wang, N. Ntarmos, and P. Triantafillou. Indexing query graphs to speedup graph query processing. In Proc. EDBT, 2016. [35] X. Yan et al. Graph indexing: a frequent </span></p><p class="c50 c109"><span class="c4">structure-based approach. In Proc. SIGMOD, 2004. [36] D. Yuan, P. Mitra, and C. L. Giles. Mining and </span></p><p class="c122"><span class="c4">indexing graphs for supergraph search. PVLDB, 6(10):829&ndash;840, 2013. </span></p><p class="c15"><span class="c43">24 </span></p></body></html>